#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ·±åº¦å­¦ä¹ æ¨¡å—ç®¡ç†
ç®¡ç†LSTMå’ŒTransformeræ¨¡å‹çš„è®­ç»ƒã€é¢„æµ‹å’Œä¼˜åŒ–
"""

import os
import math
import traceback
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler

from .deep_learning_models import LSTMPredictor, TransformerPredictor

class DeepLearningModule:
    """æ·±åº¦å­¦ä¹ æ¨¡å— - ç®¡ç†LSTMå’ŒTransformeræ¨¡å‹"""
    
    def __init__(self, input_size=60, device='cpu'):
        # ç¡®ä¿deviceæ˜¯æ­£ç¡®çš„torch.deviceå¯¹è±¡
        if isinstance(device, str):
            self.device = torch.device(device)
        else:
            self.device = device
    
        self.input_size = input_size
        self.models = {}
        self.optimizers = {}
        self.schedulers = {}
        self.scalers = {}
        self.training_history = {}
        self.batch_size = 32
        self.sequence_length = 10
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        print(f"ğŸ¤– æ·±åº¦å­¦ä¹ æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    def _init_models(self):
        """åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        try:
            print(f"ğŸ”§ å¼€å§‹åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
            # ç¡®ä¿è®¾å¤‡å¯ç”¨
            if not torch.cuda.is_available() and str(self.device) != 'cpu':
                print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
                self.device = torch.device('cpu')
        
            # LSTMæ¨¡å‹
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LSTMæ¨¡å‹...")
            lstm_model = LSTMPredictor(
                input_size=self.input_size,
                hidden_size=128,
                num_layers=2,
                output_size=10,
                dropout=0.2
            )
            self.models['lstm'] = lstm_model.to(self.device)
            print("âœ… LSTMæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            self.optimizers['lstm'] = optim.AdamW(
                self.models['lstm'].parameters(),
                lr=0.001,
                weight_decay=0.01
            )
        
            self.schedulers['lstm'] = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizers['lstm'],
                mode='min',
                factor=0.5,
                patience=10
            )
        
            # Transformeræ¨¡å‹
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–Transformeræ¨¡å‹...")
            transformer_model = TransformerPredictor(
                input_size=self.input_size,
                d_model=128,
                nhead=8,
                num_encoder_layers=4,
                dim_feedforward=512,
                dropout=0.1,
                output_size=10
            )
            self.models['transformer'] = transformer_model.to(self.device)
            print("âœ… Transformeræ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            self.optimizers['transformer'] = optim.AdamW(
                self.models['transformer'].parameters(),
                lr=0.0005,
                weight_decay=0.01
            )
        
            self.schedulers['transformer'] = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizers['transformer'],
                mode='min',
                factor=0.5,
                patience=10
            )
        
            # æ•°æ®æ ‡å‡†åŒ–å™¨
            print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ•°æ®æ ‡å‡†åŒ–å™¨...")
            for model_name in self.models.keys():
                self.scalers[model_name] = MinMaxScaler()
                self.training_history[model_name] = {
                    'loss': [],
                    'accuracy': [],
                    'best_loss': float('inf'),
                    'epochs_trained': 0
                }
        
            print(f"âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {list(self.models.keys())}")
            print(f"ğŸ“Š åˆå§‹åŒ–äº† {len(self.models)} ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹")
        
        except Exception as e:
            print(f"âŒ æ·±åº¦å­¦ä¹ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {str(e)}")
            traceback.print_exc()
            self.models = {}
    
    def prepare_sequence_data(self, features_list, labels_list):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        if len(features_list) < self.sequence_length:
            return None, None
        
        sequences = []
        targets = []
        
        for i in range(len(features_list) - self.sequence_length + 1):
            # åˆ›å»ºåºåˆ—
            sequence = features_list[i:i + self.sequence_length]
            target = labels_list[i + self.sequence_length - 1]
            
            sequences.append(sequence)
            targets.append(target)
        
        return np.array(sequences), np.array(targets)
    
    def batch_train(self, data_list, epochs=50, validation_split=0.2):
        """æ‰¹é‡è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        if not self.models:
            return {'success': False, 'message': f'æ·±åº¦å­¦ä¹ æ¨¡å‹æœªåˆå§‹åŒ–ã€‚å½“å‰æ¨¡å‹æ•°é‡: {len(self.models)}ã€‚è¯·æ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–è¿‡ç¨‹æ˜¯å¦æœ‰é”™è¯¯ã€‚'}
        
        if len(data_list) < self.sequence_length + 10:
            return {'success': False, 'message': f'æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{self.sequence_length + 10}æœŸæ•°æ®'}
        
        try:
            print(f"ğŸš€ å¼€å§‹æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒï¼Œæ•°æ®é‡: {len(data_list)} æœŸ")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            features_list = []
            labels_list = []
            
            for i in range(len(data_list) - 1):
                # ä½¿ç”¨å‰i+1æœŸæ•°æ®ä½œä¸ºç‰¹å¾
                feature_data = data_list[i+1:]
                features = self._extract_deep_features(feature_data)
                features_list.append(features)
                
                # ä½¿ç”¨ä¸‹ä¸€æœŸçš„å°¾æ•°ä½œä¸ºæ ‡ç­¾
                actual_tails = data_list[i].get('tails', [])
                # åˆ›å»ºå¤šæ ‡ç­¾ï¼ˆ10ä¸ªå°¾æ•°çš„äºŒåˆ†ç±»ï¼‰
                label = np.zeros(10)
                for tail in actual_tails:
                    if 0 <= tail <= 9:
                        label[tail] = 1
                labels_list.append(label)
            
            # å‡†å¤‡åºåˆ—æ•°æ®
            sequences, targets = self.prepare_sequence_data(features_list, labels_list)
            if sequences is None:
                return {'success': False, 'message': 'åºåˆ—æ•°æ®å‡†å¤‡å¤±è´¥'}
            
            # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
            split_idx = int(len(sequences) * (1 - validation_split))
            train_sequences = sequences[:split_idx]
            train_targets = targets[:split_idx]
            val_sequences = sequences[split_idx:]
            val_targets = targets[split_idx:]
            
            results = {}
            
            # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
            for model_name, model in self.models.items():
                print(f"ğŸ“š è®­ç»ƒ {model_name.upper()} æ¨¡å‹...")
                
                # æ•°æ®æ ‡å‡†åŒ–
                scaler = self.scalers[model_name]
                train_seq_scaled = scaler.fit_transform(
                    train_sequences.reshape(-1, self.input_size)
                ).reshape(train_sequences.shape)
                val_seq_scaled = scaler.transform(
                    val_sequences.reshape(-1, self.input_size)
                ).reshape(val_sequences.shape)
                
                # è½¬æ¢ä¸ºå¼ é‡
                train_dataset = TensorDataset(
                    torch.FloatTensor(train_seq_scaled).to(self.device),
                    torch.FloatTensor(train_targets).to(self.device)
                )
                val_dataset = TensorDataset(
                    torch.FloatTensor(val_seq_scaled).to(self.device),
                    torch.FloatTensor(val_targets).to(self.device)
                )
                
                train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
                val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
                
                # è®­ç»ƒæ¨¡å‹
                model_result = self._train_single_model(
                    model_name, model, train_loader, val_loader, epochs
                )
                results[model_name] = model_result
            
            return {
                'success': True,
                'message': f'æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒå®Œæˆ',
                'results': results,
                'train_samples': len(train_sequences),
                'val_samples': len(val_sequences)
            }
            
        except Exception as e:
            print(f"âŒ æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒå¤±è´¥: {e}")
            traceback.print_exc()
            return {'success': False, 'message': f'è®­ç»ƒå¤±è´¥: {str(e)}'}
    
    def _train_single_model(self, model_name, model, train_loader, val_loader, epochs):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        criterion = nn.BCELoss()
        optimizer = self.optimizers[model_name]
        scheduler = self.schedulers[model_name]
        
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = 15
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_features, batch_targets in train_loader:
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                outputs = model(batch_features)
                loss = criterion(outputs, batch_targets)
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                predicted = (outputs > 0.5).float()
                train_correct += (predicted == batch_targets).sum().item()
                train_total += batch_targets.numel()
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch_features, batch_targets in val_loader:
                    outputs = model(batch_features)
                    loss = criterion(outputs, batch_targets)
                    
                    val_loss += loss.item()
                    
                    predicted = (outputs > 0.5).float()
                    val_correct += (predicted == batch_targets).sum().item()
                    val_total += batch_targets.numel()
            
            # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            train_acc = train_correct / train_total
            val_acc = val_correct / val_total
            
            # æ›´æ–°å­¦ä¹ ç‡
            try:
                scheduler.step(avg_val_loss)
            except Exception as scheduler_e:
                print(f"   å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°å¤±è´¥: {scheduler_e}")
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history[model_name]['loss'].append(avg_val_loss)
            self.training_history[model_name]['accuracy'].append(val_acc)
            self.training_history[model_name]['epochs_trained'] += 1
            
            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                self.training_history[model_name]['best_loss'] = best_val_loss
                patience_counter = 0

                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                models_dir = getattr(self, 'models_dir', None)
                if models_dir:
                    try:
                        os.makedirs(models_dir, exist_ok=True)
                        model_path = os.path.join(models_dir, f'{model_name}_best.pth')
                        torch.save(model.state_dict(), model_path)
                        print(f"âœ… æ¨¡å‹ {model_name} æœ€ä½³çŠ¶æ€å·²ä¿å­˜åˆ°: {model_path}")
                    except Exception as save_e:
                        print(f"âŒ ä¿å­˜æ¨¡å‹ {model_name} æœ€ä½³çŠ¶æ€å¤±è´¥: {save_e}")
                else:
                    print(f"âš ï¸ æ¨¡å‹ç›®å½•æœªè®¾ç½®ï¼Œè·³è¿‡æ¨¡å‹ä¿å­˜")

                patience_counter += 1
            
            if patience_counter >= max_patience:
                print(f"   æ—©åœè§¦å‘äºç¬¬ {epoch+1} è½®")
                break
            
            # æ¯10è½®æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"   Epoch {epoch+1}/{epochs}: "
                      f"Train Loss: {avg_train_loss:.4f}, "
                      f"Val Loss: {avg_val_loss:.4f}, "
                      f"Val Acc: {val_acc:.4f}")
        
        return {
            'final_train_loss': avg_train_loss,
            'final_val_loss': avg_val_loss,
            'final_val_accuracy': val_acc,
            'best_val_loss': best_val_loss,
            'epochs_completed': epoch + 1
        }
    
    def _extract_deep_features(self, data_list, window_size=15):
        """ä¸ºæ·±åº¦å­¦ä¹ æå–ç‰¹å¾"""
        window_size = min(window_size, len(data_list))
        recent_data = data_list[:window_size]
        
        features = []
        
        # 1. åŸºç¡€é¢‘ç‡ç‰¹å¾ï¼ˆ10ç»´ï¼‰
        for tail in range(10):
            count = sum(1 for period in recent_data if tail in period.get('tails', []))
            features.append(count / len(recent_data))
        
        # 2. çŸ­æœŸé¢‘ç‡ç‰¹å¾ï¼ˆ10ç»´ï¼‰- æœ€è¿‘5æœŸ
        recent_5 = recent_data[:5] if len(recent_data) >= 5 else recent_data
        for tail in range(10):
            count = sum(1 for period in recent_5 if tail in period.get('tails', []))
            features.append(count / len(recent_5) if recent_5 else 0)
        
        # 3. è¿ç»­æ€§ç‰¹å¾ï¼ˆ10ç»´ï¼‰
        for tail in range(10):
            consecutive = 0
            for period in recent_data:
                if tail in period.get('tails', []):
                    consecutive += 1
                else:
                    break
            features.append(consecutive / len(recent_data))
        
        # 4. é—´éš”ç‰¹å¾ï¼ˆ10ç»´ï¼‰
        for tail in range(10):
            last_seen = -1
            for i, period in enumerate(recent_data):
                if tail in period.get('tails', []):
                    last_seen = i
                    break
            features.append((last_seen + 1) / len(recent_data) if last_seen >= 0 else 1.0)
        
        # 5. è¶‹åŠ¿ç‰¹å¾ï¼ˆ10ç»´ï¼‰
        mid = len(recent_data) // 2
        for tail in range(10):
            early_count = sum(1 for period in recent_data[mid:] if tail in period.get('tails', []))
            late_count = sum(1 for period in recent_data[:mid] if tail in period.get('tails', []))
            trend = (late_count - early_count) / max(mid, 1) if mid > 0 else 0
            features.append(trend)
        
        # 6. ç»Ÿè®¡ç‰¹å¾ï¼ˆ10ç»´ï¼‰
        tail_counts_per_period = [len(period.get('tails', [])) for period in recent_data]
        if tail_counts_per_period:
            features.extend([
                np.mean(tail_counts_per_period),
                np.std(tail_counts_per_period),
                np.min(tail_counts_per_period),
                np.max(tail_counts_per_period),
                np.median(tail_counts_per_period)
            ])
        else:
            features.extend([0] * 5)
        
        # è¡¥å……ç‰¹å¾åˆ°å›ºå®šç»´åº¦ï¼ˆ60ç»´ï¼‰
        for tail in range(5):  # é¢å¤–5ç»´ç‰¹å¾
            features.append(0.0)
        
        # ç¡®ä¿ç‰¹å¾ç»´åº¦ä¸º60
        while len(features) < 60:
            features.append(0.0)
        
        return np.array(features[:60])
    
    def predict_single(self, features):
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå•æ¬¡é¢„æµ‹"""
        if not self.models:
            return {}
        
        predictions = {}
        
        try:
            with torch.no_grad():
                for model_name, model in self.models.items():
                    model.eval()
                    
                    # æ ‡å‡†åŒ–ç‰¹å¾
                    scaler = self.scalers[model_name]
                    if hasattr(scaler, 'scale_') and scaler.scale_ is not None:
                        features_scaled = scaler.transform(features.reshape(1, -1))
                    else:
                        features_scaled = features.reshape(1, -1)
                    
                    # è½¬æ¢ä¸ºå¼ é‡
                    input_tensor = torch.FloatTensor(features_scaled).to(self.device)
                    
                    # é¢„æµ‹
                    output = model(input_tensor)
                    probabilities = output.cpu().numpy().flatten()
                    
                    predictions[model_name] = probabilities
            
            return predictions
            
        except Exception as e:
            print(f"æ·±åº¦å­¦ä¹ é¢„æµ‹å¤±è´¥: {e}")
            return {}
    
    def get_training_stats(self):
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for model_name, history in self.training_history.items():
            stats[model_name] = {
                'epochs_trained': history['epochs_trained'],
                'best_loss': history['best_loss'],
                'current_accuracy': history['accuracy'][-1] if history['accuracy'] else 0.0,
                'loss_history': history['loss'][-10:],  # æœ€è¿‘10è½®çš„æŸå¤±
                'accuracy_history': history['accuracy'][-10:]  # æœ€è¿‘10è½®çš„å‡†ç¡®ç‡
            }
        return stats
    
    def learn_online_single(self, features, actual_tails):
        """åœ¨çº¿å­¦ä¹ å•ä¸ªæ ·æœ¬"""
        if not self.models:
            return {}
        
        learning_results = {}
        
        try:
            # åˆ›å»ºå¤šæ ‡ç­¾ç›®æ ‡ï¼ˆ10ä¸ªå°¾æ•°çš„äºŒåˆ†ç±»ï¼‰
            target = np.zeros(10)
            for tail in actual_tails:
                if 0 <= tail <= 9:
                    target[tail] = 1
            
            for model_name, model in self.models.items():
                    try:
                        model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
                        
                        # æ ‡å‡†åŒ–ç‰¹å¾
                        scaler = self.scalers[model_name]
                        if hasattr(scaler, 'scale_') and scaler.scale_ is not None:
                            # åœ¨çº¿æ›´æ–°æ ‡å‡†åŒ–å™¨
                            features_scaled = scaler.transform(features.reshape(1, -1))
                            # ç®€å•çš„åœ¨çº¿æ›´æ–°ï¼ˆå¯é€‰ï¼‰
                            scaler.partial_fit(features.reshape(1, -1))
                        else:
                            # åˆæ¬¡å­¦ä¹ ï¼Œå…ˆæ‹Ÿåˆæ ‡å‡†åŒ–å™¨
                            scaler.fit(features.reshape(1, -1))
                            features_scaled = scaler.transform(features.reshape(1, -1))
                        
                        # è½¬æ¢ä¸ºå¼ é‡
                        input_tensor = torch.FloatTensor(features_scaled).to(self.device)
                        input_tensor.requires_grad_(True)
                        target_tensor = torch.FloatTensor(target).unsqueeze(0).to(self.device)
                        
                        # å‰å‘ä¼ æ’­
                        model.train()
                        optimizer = self.optimizers[model_name]
                        optimizer.zero_grad()
                        
                        output = model(input_tensor)
                        
                        # è®¡ç®—æŸå¤±
                        criterion = nn.BCELoss()
                        loss = criterion(output, target_tensor)
                        
                        # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        optimizer.step()
                        
                        # è®¡ç®—å‡†ç¡®ç‡
                        predicted = (output > 0.5).float()
                        accuracy = (predicted == target_tensor).float().mean().item()
                        
                        # æ›´æ–°è®­ç»ƒå†å²
                        self.training_history[model_name]['loss'].append(loss.item())
                        self.training_history[model_name]['accuracy'].append(accuracy)
                        
                        # ä¿æŒå†å²è®°å½•é•¿åº¦
                        if len(self.training_history[model_name]['loss']) > 1000:
                            self.training_history[model_name]['loss'].pop(0)
                        if len(self.training_history[model_name]['accuracy']) > 1000:
                            self.training_history[model_name]['accuracy'].pop(0)
                        
                        learning_results[model_name] = {
                            'loss': loss.item(),
                            'accuracy': accuracy,
                            'status': 'success'
                        }
                        
                    except Exception as e:
                        print(f"æ·±åº¦å­¦ä¹ æ¨¡å‹ {model_name} åœ¨çº¿å­¦ä¹ å¤±è´¥: {e}")
                        learning_results[model_name] = {
                            'status': 'failed',
                            'error': str(e)
                        }
            
            return learning_results
            
        except Exception as e:
            print(f"æ·±åº¦å­¦ä¹ åœ¨çº¿å­¦ä¹ å¤±è´¥: {e}")
            return {}
    
    def update_learning_rate(self, model_name, factor=0.95):
        """åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡"""
        if model_name in self.optimizers:
            scheduler = self.schedulers[model_name]
            current_lr = self.optimizers[model_name].param_groups[0]['lr']
            
            # åŸºäºæœ€è¿‘çš„æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
            if model_name in self.training_history:
                recent_losses = self.training_history[model_name]['loss'][-10:]
                if len(recent_losses) >= 5:
                    # å¦‚æœæœ€è¿‘æŸå¤±æ²¡æœ‰ä¸‹é™ï¼Œé™ä½å­¦ä¹ ç‡
                    if recent_losses[-1] >= recent_losses[-5]:
                        for param_group in self.optimizers[model_name].param_groups:
                            param_group['lr'] *= factor
                        print(f"è°ƒæ•´ {model_name} å­¦ä¹ ç‡: {current_lr:.6f} -> {param_group['lr']:.6f}")