# feature_engineering.py - ç‰¹å¾å·¥ç¨‹æ¨¡å—

import numpy as np
import math
from typing import List, Dict, Any

class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹å¤„ç†å™¨"""
    
    def __init__(self, ai_config, feature_selector=None, feature_combiner=None, 
                 timeseries_enhancer=None, feature_weighter=None, feature_assessor=None):
        self.ai_config = ai_config
        
        # æ™ºèƒ½ç‰¹å¾å¤„ç†ç»„ä»¶
        self.feature_selector = feature_selector
        self.feature_combiner = feature_combiner
        self.timeseries_enhancer = timeseries_enhancer
        self.feature_weighter = feature_weighter
        self.feature_assessor = feature_assessor
        
        # ç‰¹å¾å¤„ç†ç»Ÿè®¡
        self.feature_processing_stats = {
            'enhancement_count': 0,
            'selection_updates': 0,
            'quality_assessments': 0,
            'last_quality_report': {}
        }
    
    def extract_enhanced_features(self, data_list: List[Dict], current_index: int = 0, main_app_ref=None) -> np.ndarray:
        """æå–å¢å¼ºç‰¹å¾ï¼ˆä½¿ç”¨æ™ºèƒ½ç‰¹å¾å¤„ç†ç»„ä»¶ï¼‰"""
    
        # æ·»åŠ æ•°æ®é‡è¯Šæ–­
        data_count = len(data_list) if data_list else 0
        if data_count < 30:
            print(f"ğŸ” AIç‰¹å¾æå–ï¼šæ¥æ”¶åˆ°{data_count}æœŸæ•°æ®ï¼ˆæ•°æ®é‡è¾ƒå°‘ï¼‰")
    
        # å°è¯•ä»ä¸»åº”ç”¨è·å–é¢„å¤„ç†ç‰¹å¾
        if main_app_ref:
            try:
                preprocessed_features = main_app_ref.extract_preprocessed_features(current_index)
                if preprocessed_features is not None and len(preprocessed_features) == 60:
                    return self._apply_intelligent_feature_processing(preprocessed_features)
            except Exception as e:
                print(f"ä½¿ç”¨é¢„å¤„ç†ç‰¹å¾å¤±è´¥ï¼Œå›é€€åˆ°åŸæ–¹æ³•: {e}")
    
        # å›é€€åˆ°åŸå§‹ç‰¹å¾æå–æ–¹æ³•
        try:
            basic_features = self._extract_features_fallback(data_list, current_index)
            if basic_features is None or len(basic_features) == 0:
                print(f"âš ï¸ å›é€€ç‰¹å¾æå–è¿”å›ç©ºå€¼ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
                basic_features = np.zeros(60, dtype=float)
            return self._apply_intelligent_feature_processing(basic_features)
        except Exception as fallback_e:
            print(f"âŒ å›é€€ç‰¹å¾æå–ä¹Ÿå¤±è´¥: {fallback_e}")
            # æœ€åçš„å®‰å…¨ç½‘ï¼šè¿”å›å›ºå®šçš„é›¶ç‰¹å¾
            return np.zeros(60, dtype=float)
    
    def _extract_features_fallback(self, data_list: List[Dict], current_index: int = 0) -> np.ndarray:
        """å›é€€çš„ç‰¹å¾æå–æ–¹æ³•ï¼Œç¡®ä¿æ•°æ®å®‰å…¨æ€§"""
        try:
            if not data_list or len(data_list) == 0:
                print(f"âš ï¸ å›é€€ç‰¹å¾æå–ï¼šæ•°æ®ä¸ºç©ºï¼Œè¿”å›é›¶ç‰¹å¾")
                return np.zeros(60, dtype=float)
            
            features = []
            
            # åŸºç¡€é¢‘ç‡ç‰¹å¾ï¼ˆ10ç»´ï¼‰
            for tail in range(10):
                try:
                    count = 0
                    total_periods = min(15, len(data_list))
                    for i in range(total_periods):
                        if tail in data_list[i].get('tails', []):
                            count += 1
                    frequency = count / total_periods if total_periods > 0 else 0.0
                    
                    # ç¡®ä¿é¢‘ç‡åœ¨åˆç†èŒƒå›´å†…
                    if math.isnan(frequency) or math.isinf(frequency):
                        frequency = 0.0
                    else:
                        frequency = max(0.0, min(1.0, frequency))
                    
                    features.append(frequency)
                except Exception as e:
                    print(f"   âš ï¸ è®¡ç®—å°¾æ•°{tail}é¢‘ç‡å¤±è´¥: {e}")
                    features.append(0.0)
            
            # çŸ­æœŸé¢‘ç‡ç‰¹å¾ï¼ˆ10ç»´ï¼‰
            for tail in range(10):
                try:
                    count = 0
                    total_periods = min(5, len(data_list))
                    for i in range(total_periods):
                        if tail in data_list[i].get('tails', []):
                            count += 1
                    frequency = count / total_periods if total_periods > 0 else 0.0
                    
                    if math.isnan(frequency) or math.isinf(frequency):
                        frequency = 0.0
                    else:
                        frequency = max(0.0, min(1.0, frequency))
                    
                    features.append(frequency)
                except Exception as e:
                    print(f"   âš ï¸ è®¡ç®—å°¾æ•°{tail}çŸ­æœŸé¢‘ç‡å¤±è´¥: {e}")
                    features.append(0.0)
            
            # è¿ç»­æ€§ç‰¹å¾ï¼ˆ10ç»´ï¼‰
            for tail in range(10):
                try:
                    consecutive = 0
                    for period in data_list[:10]:
                        if tail in period.get('tails', []):
                            consecutive += 1
                        else:
                            break
                    
                    consecutive_ratio = consecutive / 10.0
                    if math.isnan(consecutive_ratio) or math.isinf(consecutive_ratio):
                        consecutive_ratio = 0.0
                    else:
                        consecutive_ratio = max(0.0, min(1.0, consecutive_ratio))
                    
                    features.append(consecutive_ratio)
                except Exception as e:
                    print(f"   âš ï¸ è®¡ç®—å°¾æ•°{tail}è¿ç»­æ€§å¤±è´¥: {e}")
                    features.append(0.0)
            
            # é—´éš”ç‰¹å¾ï¼ˆ10ç»´ï¼‰
            for tail in range(10):
                try:
                    last_seen = -1
                    for i, period in enumerate(data_list[:15]):
                        if tail in period.get('tails', []):
                            last_seen = i
                            break
                    
                    if last_seen >= 0:
                        interval_ratio = (last_seen + 1) / 15.0
                    else:
                        interval_ratio = 1.0  # å¾ˆä¹…æ²¡å‡ºç°
                    
                    if math.isnan(interval_ratio) or math.isinf(interval_ratio):
                        interval_ratio = 1.0
                    else:
                        interval_ratio = max(0.0, min(1.0, interval_ratio))
                    
                    features.append(interval_ratio)
                except Exception as e:
                    print(f"   âš ï¸ è®¡ç®—å°¾æ•°{tail}é—´éš”å¤±è´¥: {e}")
                    features.append(1.0)
            
            # è¶‹åŠ¿ç‰¹å¾ï¼ˆ10ç»´ï¼‰
            for tail in range(10):
                try:
                    if len(data_list) >= 10:
                        mid = 5
                        early_count = sum(1 for period in data_list[mid:10] if tail in period.get('tails', []))
                        late_count = sum(1 for period in data_list[:mid] if tail in period.get('tails', []))
                        trend = (late_count - early_count) / 5.0
                        
                        if math.isnan(trend) or math.isinf(trend):
                            trend = 0.0
                        else:
                            trend = max(-1.0, min(1.0, trend))
                    else:
                        trend = 0.0
                    
                    features.append(trend)
                except Exception as e:
                    print(f"   âš ï¸ è®¡ç®—å°¾æ•°{tail}è¶‹åŠ¿å¤±è´¥: {e}")
                    features.append(0.0)
            
            # ç»Ÿè®¡ç‰¹å¾ï¼ˆ10ç»´ï¼‰
            try:
                tail_counts_per_period = []
                for period in data_list[:15]:
                    count = len(period.get('tails', []))
                    tail_counts_per_period.append(count)
                
                if tail_counts_per_period:
                    mean_count = np.mean(tail_counts_per_period)
                    std_count = np.std(tail_counts_per_period)
                    min_count = np.min(tail_counts_per_period)
                    max_count = np.max(tail_counts_per_period)
                    median_count = np.median(tail_counts_per_period)
                    
                    # éªŒè¯ç»Ÿè®¡å€¼
                    stats = [mean_count, std_count, min_count, max_count, median_count]
                    for i, stat in enumerate(stats):
                        if math.isnan(stat) or math.isinf(stat):
                            stats[i] = 0.0
                        else:
                            stats[i] = float(stat)
                    
                    features.extend(stats)
                else:
                    features.extend([0.0] * 5)
            except Exception as e:
                print(f"   âš ï¸ è®¡ç®—ç»Ÿè®¡ç‰¹å¾å¤±è´¥: {e}")
                features.extend([0.0] * 5)
            
            # è¡¥å……ç‰¹å¾åˆ°60ç»´
            while len(features) < 60:
                features.append(0.0)
            
            # ç¡®ä¿åªæœ‰60ç»´
            features = features[:60]
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶éªŒè¯
            features_array = np.array(features, dtype=float)
            
            # æœ€ç»ˆéªŒè¯
            if np.any(np.isnan(features_array)) or np.any(np.isinf(features_array)):
                print(f"âš ï¸ å›é€€ç‰¹å¾æå–ä»åŒ…å«æ— æ•ˆå€¼ï¼Œä½¿ç”¨é›¶æ•°ç»„")
                features_array = np.zeros(60, dtype=float)
            
            print(f"âœ… å›é€€ç‰¹å¾æå–å®Œæˆï¼Œç»´åº¦: {features_array.shape}")
            return features_array
            
        except Exception as e:
            print(f"âŒ å›é€€ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(60, dtype=float)
    
    def create_model_specific_features(self, base_features, data_list):
        """ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºä¸“å±ç‰¹å¾"""
        model_features = {}
    
        # æå–å†å²æ•°æ®ç”¨äºç‰¹å¾å·¥ç¨‹
        latest_tails = set(data_list[0].get('tails', [])) if data_list else set()
    
        # ä¸ºæ¯ä¸ªRiveræ¨¡å‹åˆ›å»ºä¸“å±ç‰¹å¾
        model_configs = [
            ('hoeffding_tree', self._create_decision_tree_features),
            ('hoeffding_adaptive', self._create_adaptive_tree_features),
            ('logistic', self._create_logistic_features),
            ('naive_bayes', self._create_independence_features),
            ('naive_bayes_multinomial', self._create_count_features),
            ('naive_bayes_gaussian', self._create_distribution_features),
            ('naive_bayes_mixed', self._create_mixed_features),
            ('bagging', self._create_stability_features),
            ('adaboost', self._create_error_correction_features),
            ('bagging_nb', self._create_probabilistic_stability_features),
            ('bagging_lr', self._create_linear_stability_features),
            ('pattern_matcher_strict', self._create_pattern_matching_features)
        ]
    
        for model_key, feature_creator in model_configs:
            # ç›´æ¥ä½¿ç”¨æ¨¡å‹é”®åï¼Œä¸éœ€è¦å¤æ‚çš„åŒ¹é…é€»è¾‘
            try:
                features = feature_creator(base_features, data_list)
                # ä¸ºä¸åŒçš„å‰ç¼€ç‰ˆæœ¬éƒ½åˆ›å»ºç‰¹å¾
                model_features[f'local_{model_key}'] = features
                model_features[f'river_{model_key}'] = features
                model_features[model_key] = features
            except Exception as e:
                print(f"ä¸ºæ¨¡å‹ {model_key} åˆ›å»ºç‰¹å¾å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤ç‰¹å¾
                default_features = {f'feature_{i}': base_features[i] for i in range(min(30, len(base_features)))}
                model_features[f'local_{model_key}'] = default_features
                model_features[f'river_{model_key}'] = default_features
                model_features[model_key] = default_features
    
        return model_features
    
    def _create_decision_tree_features(self, base_features, data_list):
        """ä¸ºå†³ç­–æ ‘åˆ›å»ºåˆ†å‰²å¯¼å‘çš„ç‰¹å¾"""
        features = {}
        
        # åˆ›å»ºæ˜æ˜¾çš„åˆ†å‰²ç‰¹å¾
        for tail in range(10):
            # äºŒå…ƒåˆ†å‰²ç‰¹å¾ï¼šæ˜¯å¦åœ¨æœ€æ–°æœŸå‡ºç°
            features[f'is_in_latest_{tail}'] = 1.0 if (data_list and tail in data_list[0].get('tails', [])) else 0.0
            
            # é¢‘ç‡åˆ†å‰²ç‰¹å¾ï¼šé«˜é¢‘/ä¸­é¢‘/ä½é¢‘
            recent_count = sum(1 for i in range(min(10, len(data_list))) if tail in data_list[i].get('tails', []))
            if recent_count >= 6:
                features[f'freq_category_{tail}'] = 2.0  # é«˜é¢‘
            elif recent_count >= 3:
                features[f'freq_category_{tail}'] = 1.0  # ä¸­é¢‘
            else:
                features[f'freq_category_{tail}'] = 0.0  # ä½é¢‘
            
            # é—´éš”åˆ†å‰²ç‰¹å¾
            last_appearance = -1
            for i, period in enumerate(data_list):
                if tail in period.get('tails', []):
                    last_appearance = i
                    break
            
            if last_appearance == 0:
                features[f'gap_category_{tail}'] = 0.0  # åˆšå‡ºç°
            elif last_appearance <= 2:
                features[f'gap_category_{tail}'] = 1.0  # è¿‘æœŸå‡ºç°
            elif last_appearance <= 5:
                features[f'gap_category_{tail}'] = 2.0  # ä¸­æœŸå‡ºç°
            else:
                features[f'gap_category_{tail}'] = 3.0  # è¿œæœŸæˆ–æœªå‡ºç°
        
        return features
    
    def _create_adaptive_tree_features(self, base_features, data_list):
        """ä¸ºè‡ªé€‚åº”æ ‘åˆ›å»ºå˜åŒ–æ£€æµ‹ç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # è¶‹åŠ¿å˜åŒ–ç‰¹å¾
            recent_5 = [1 if tail in data_list[i].get('tails', []) else 0 for i in range(min(5, len(data_list)))]
            older_5 = [1 if tail in data_list[i].get('tails', []) else 0 for i in range(5, min(10, len(data_list)))]
            
            recent_avg = sum(recent_5) / len(recent_5) if recent_5 else 0
            older_avg = sum(older_5) / len(older_5) if older_5 else 0
            
            features[f'trend_change_{tail}'] = recent_avg - older_avg
            
            # æ³¢åŠ¨æ€§ç‰¹å¾
            if len(recent_5) > 1:
                variance = sum((x - recent_avg) ** 2 for x in recent_5) / len(recent_5)
                features[f'volatility_{tail}'] = variance
            else:
                features[f'volatility_{tail}'] = 0.0
            
            # é€‚åº”æ€§ä¿¡å·
            changes = sum(1 for i in range(1, len(recent_5)) if recent_5[i] != recent_5[i-1])
            features[f'adaptation_signal_{tail}'] = changes / max(1, len(recent_5) - 1)
        
        return features
    
    def _create_logistic_features(self, base_features, data_list):
        """ä¸ºé€»è¾‘å›å½’åˆ›å»ºçº¿æ€§å¯åˆ†ç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # çº¿æ€§è¶‹åŠ¿ç‰¹å¾
            appearances = []
            for i, period in enumerate(data_list[:15]):
                appearances.append(1 if tail in period.get('tails', []) else 0)
            
            if len(appearances) > 1:
                # è®¡ç®—çº¿æ€§è¶‹åŠ¿æ–œç‡
                n = len(appearances)
                x_sum = sum(range(n))
                y_sum = sum(appearances)
                xy_sum = sum(i * appearances[i] for i in range(n))
                x2_sum = sum(i * i for i in range(n))
                
                denominator = n * x2_sum - x_sum * x_sum
                if denominator != 0:
                    slope = (n * xy_sum - x_sum * y_sum) / denominator
                    features[f'linear_trend_{tail}'] = slope
                else:
                    features[f'linear_trend_{tail}'] = 0.0
            else:
                features[f'linear_trend_{tail}'] = 0.0
            
            # åŠ æƒé¢‘ç‡ï¼ˆçº¿æ€§æƒé‡ï¼‰
            weighted_sum = sum((15 - i) * (1 if tail in data_list[i].get('tails', []) else 0) 
                             for i in range(min(15, len(data_list))))
            weight_total = sum(15 - i for i in range(min(15, len(data_list))))
            features[f'weighted_freq_{tail}'] = weighted_sum / weight_total if weight_total > 0 else 0.0
        
        return features
    
    def _create_independence_features(self, base_features, data_list):
        """ä¸ºç‹¬ç«‹æ€§å‡è®¾åˆ›å»ºç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # æ¯ä¸ªå°¾æ•°ç‹¬ç«‹çš„å‡ºç°æ¦‚ç‡
            total_periods = len(data_list)
            appearances = sum(1 for period in data_list if tail in period.get('tails', []))
            features[f'independent_prob_{tail}'] = appearances / total_periods if total_periods > 0 else 0.1
            
            # æ¡ä»¶ç‹¬ç«‹ç‰¹å¾ï¼ˆå¿½ç•¥å…¶ä»–å°¾æ•°çš„å½±å“ï¼‰
            features[f'marginal_freq_{tail}'] = appearances / 10.0 if total_periods > 0 else 0.1
            
            # å…ˆéªŒæ¦‚ç‡ç‰¹å¾
            features[f'prior_belief_{tail}'] = 0.1  # å‡åŒ€å…ˆéªŒ
        
        return features
    
    def _create_count_features(self, base_features, data_list):
        """ä¸ºå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯åˆ›å»ºè®¡æ•°ç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # åŸå§‹è®¡æ•°ç‰¹å¾
            count_5 = sum(1 for i in range(min(5, len(data_list))) if tail in data_list[i].get('tails', []))
            count_10 = sum(1 for i in range(min(10, len(data_list))) if tail in data_list[i].get('tails', []))
            count_20 = sum(1 for i in range(min(20, len(data_list))) if tail in data_list[i].get('tails', []))
            
            features[f'count_5_{tail}'] = float(count_5)
            features[f'count_10_{tail}'] = float(count_10)
            features[f'count_20_{tail}'] = float(count_20)
            
            # è®¡æ•°æ¯”ä¾‹ç‰¹å¾
            features[f'count_ratio_{tail}'] = count_5 / max(1, count_10)
        
        return features
    
    def _create_distribution_features(self, base_features, data_list):
        """ä¸ºé«˜æ–¯æœ´ç´ è´å¶æ–¯åˆ›å»ºåˆ†å¸ƒç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # æ—¶åºåˆ†å¸ƒç‰¹å¾
            positions = []  # è¯¥å°¾æ•°å‡ºç°çš„ä½ç½®
            for i, period in enumerate(data_list[:20]):
                if tail in period.get('tails', []):
                    positions.append(i)
            
            if positions:
                mean_position = sum(positions) / len(positions)
                features[f'mean_position_{tail}'] = mean_position / 20.0
                
                if len(positions) > 1:
                    variance = sum((p - mean_position) ** 2 for p in positions) / len(positions)
                    features[f'position_variance_{tail}'] = variance / 400.0  # å½’ä¸€åŒ–
                else:
                    features[f'position_variance_{tail}'] = 0.5
            else:
                features[f'mean_position_{tail}'] = 1.0  # è¡¨ç¤ºå¾ˆä¹…æ²¡å‡ºç°
                features[f'position_variance_{tail}'] = 1.0
        
        return features
    
    def _create_mixed_features(self, base_features, data_list):
        """ä¸ºæ··åˆæœ´ç´ è´å¶æ–¯åˆ›å»ºæ··åˆç±»å‹ç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # æ··åˆäºŒå…ƒç‰¹å¾
            features[f'binary_recent_{tail}'] = 1.0 if (data_list and tail in data_list[0].get('tails', [])) else 0.0
            
            # æ··åˆè¿ç»­ç‰¹å¾
            recent_freq = sum(1 for i in range(min(8, len(data_list))) if tail in data_list[i].get('tails', []))
            features[f'continuous_freq_{tail}'] = recent_freq / 8.0
            
            # æ··åˆåˆ†ç±»ç‰¹å¾  
            if recent_freq >= 5:
                features[f'category_{tail}'] = 2.0  # çƒ­é—¨
            elif recent_freq >= 2:
                features[f'category_{tail}'] = 1.0  # æ™®é€š
            else:
                features[f'category_{tail}'] = 0.0  # å†·é—¨
        
        return features
    
    def _create_stability_features(self, base_features, data_list):
        """ä¸ºåŸºç¡€è£…è¢‹åˆ›å»ºç¨³å®šæ€§ç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # ç¨³å®šæ€§æŒ‡æ ‡ï¼šæ–¹å·®
            recent_appearances = [1 if tail in data_list[i].get('tails', []) else 0 
                                for i in range(min(12, len(data_list)))]
            if len(recent_appearances) > 1:
                mean_val = sum(recent_appearances) / len(recent_appearances)
                variance = sum((x - mean_val) ** 2 for x in recent_appearances) / len(recent_appearances)
                features[f'stability_{tail}'] = 1.0 - variance  # ä½æ–¹å·® = é«˜ç¨³å®šæ€§
            else:
                features[f'stability_{tail}'] = 0.5
            
            # ä¸€è‡´æ€§ç‰¹å¾
            features[f'consistency_{tail}'] = mean_val if len(recent_appearances) > 0 else 0.5
        
        return features
    
    def _create_error_correction_features(self, base_features, data_list):
        """ä¸ºAdaBooståˆ›å»ºé”™è¯¯ä¿®æ­£ç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # é”™è¯¯ä¿®æ­£ä¿¡å·ï¼šä¸é¢„æœŸåå·®
            expected_freq = 0.1  # ç†è®ºæœŸæœ›é¢‘ç‡
            actual_freq = sum(1 for period in data_list[:10] if tail in period.get('tails', [])) / min(10, len(data_list))
            
            features[f'error_signal_{tail}'] = abs(actual_freq - expected_freq)
            features[f'correction_need_{tail}'] = max(0, expected_freq - actual_freq)  # éœ€è¦å‘ä¸Šä¿®æ­£
            
            # ç´¯ç§¯é”™è¯¯
            features[f'cumulative_error_{tail}'] = (actual_freq - expected_freq) ** 2
        
        return features
    
    def _create_probabilistic_stability_features(self, base_features, data_list):
        """ä¸ºæœ´ç´ è´å¶æ–¯è£…è¢‹åˆ›å»ºæ¦‚ç‡ç¨³å®šæ€§ç‰¹å¾"""
        features = {}
        
        # ç»“åˆæ¦‚ç‡å’Œç¨³å®šæ€§
        for tail in range(10):
            # æ¦‚ç‡ç‰¹å¾
            prob = sum(1 for period in data_list[:8] if tail in period.get('tails', [])) / min(8, len(data_list))
            features[f'prob_{tail}'] = prob
            
            # ç¨³å®šæ¦‚ç‡
            segments = []
            for start in range(0, min(12, len(data_list)), 3):
                segment = data_list[start:start+3]
                segment_prob = sum(1 for period in segment if tail in period.get('tails', [])) / len(segment)
                segments.append(segment_prob)
            
            if len(segments) > 1:
                prob_variance = sum((p - prob) ** 2 for p in segments) / len(segments)
                features[f'prob_stability_{tail}'] = 1.0 - prob_variance
            else:
                features[f'prob_stability_{tail}'] = 0.5
        
        return features
    
    def _create_linear_stability_features(self, base_features, data_list):
        """ä¸ºé€»è¾‘å›å½’è£…è¢‹åˆ›å»ºçº¿æ€§ç¨³å®šæ€§ç‰¹å¾"""
        features = {}
        
        for tail in range(10):
            # çº¿æ€§ç‰¹å¾
            weights = [0.8, 0.6, 0.4, 0.2]  # é€’å‡æƒé‡  
            weighted_sum = 0
            weight_total = 0
            
            for i, weight in enumerate(weights):
                if i < len(data_list):
                    if tail in data_list[i].get('tails', []):
                        weighted_sum += weight
                    weight_total += weight
            
            features[f'linear_weighted_{tail}'] = weighted_sum / weight_total if weight_total > 0 else 0
            
            # çº¿æ€§ç¨³å®šæ€§
            features[f'linear_stability_{tail}'] = 1.0 - abs(0.5 - features[f'linear_weighted_{tail}'])
        
        return features
    
    def _create_pattern_matching_features(self, base_features, data_list):
        """ä¸ºå†å²æ¨¡å¼åŒ¹é…åˆ›å»ºç‰¹å¾"""
        features = {}
        
        # ä¿æŒåŸæœ‰ç‰¹å¾æ ¼å¼
        for i in range(len(base_features)):
            features[f'feature_{i}'] = base_features[i] if i < len(base_features) else 0.0
        
        return features
    
    def _apply_intelligent_feature_processing(self, features: np.ndarray) -> np.ndarray:
        """åº”ç”¨æ™ºèƒ½ç‰¹å¾å¤„ç†æµæ°´çº¿"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯ä¸€ç»´numpyæ•°ç»„
            if isinstance(features, (list, tuple)):
                # å…ˆæ¸…ç†åˆ—è¡¨ä¸­çš„æ— æ•ˆå€¼
                cleaned_list = []
                for item in features:
                    try:
                        if item is None:
                            cleaned_list.append(0.0)
                        elif isinstance(item, (int, float)):
                            if math.isnan(item) or math.isinf(item):
                                cleaned_list.append(0.0)
                            else:
                                cleaned_list.append(float(item))
                        else:
                            cleaned_list.append(float(item))
                    except (ValueError, TypeError):
                        cleaned_list.append(0.0)
                features = np.array(cleaned_list, dtype=float)
            else:
                try:
                    features = np.array(features, dtype=float)
                except (ValueError, TypeError):
                    print(f"âš ï¸ æ— æ³•è½¬æ¢ç‰¹å¾ä¸ºnumpyæ•°ç»„ï¼Œä½¿ç”¨é›¶æ•°ç»„: {type(features)}")
                    features = np.zeros(60, dtype=float)
        
            # ç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„
            if features.ndim > 1:
                features = features.flatten()
        
            # æ¸…ç†æ•°ç»„ä¸­çš„æ— æ•ˆå€¼
            if len(features) > 0:
                # æ›¿æ¢NaNå’Œæ— ç©·å¤§å€¼
                nan_mask = np.isnan(features)
                inf_mask = np.isinf(features)
                features[nan_mask] = 0.0
                features[inf_mask] = 0.0
                
                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                features = features.astype(float)
                
                # æœ€ç»ˆéªŒè¯
                if np.any(np.isnan(features)) or np.any(np.isinf(features)):
                    print(f"âš ï¸ ç‰¹å¾æ•°ç»„ä»åŒ…å«æ— æ•ˆå€¼ï¼Œé‡æ–°åˆå§‹åŒ–")
                    features = np.zeros(len(features), dtype=float)
            else:
                print(f"âš ï¸ ç‰¹å¾æ•°ç»„ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é•¿åº¦60")
                features = np.zeros(60, dtype=float)
        
            print(f"ğŸ”§ è¾“å…¥ç‰¹å¾ç»´åº¦: {features.shape}, ç±»å‹: {features.dtype}, æœ‰æ•ˆå€¼æ•°é‡: {np.sum(~np.isnan(features))}")
        
            # 1. æ—¶åºç‰¹å¾å¢å¼º
            if self.timeseries_enhancer:
                enhanced_features = self.timeseries_enhancer.enhance_features(features)
            else:
                enhanced_features = features
        
            # æ£€æŸ¥å’Œä¿®å¤å¢å¼ºç‰¹å¾
            if not isinstance(enhanced_features, np.ndarray):
                enhanced_features = np.array(enhanced_features, dtype=float)
            if enhanced_features.ndim > 1:
                enhanced_features = enhanced_features.flatten()
            enhanced_features = enhanced_features.astype(float)
        
            print(f"ğŸ”§ å¢å¼ºåç‰¹å¾ç»´åº¦: {enhanced_features.shape}")
        
            # 2. ç‰¹å¾äº¤äº’ç»„åˆ
            if self.feature_combiner:
                combined_features = self.feature_combiner.create_interaction_features(enhanced_features)
            else:
                combined_features = enhanced_features
        
            # æ£€æŸ¥å’Œä¿®å¤ç»„åˆç‰¹å¾
            if not isinstance(combined_features, np.ndarray):
                combined_features = np.array(combined_features, dtype=float)
            if combined_features.ndim > 1:
                combined_features = combined_features.flatten()
            combined_features = combined_features.astype(float)
        
            print(f"ğŸ”§ ç»„åˆåç‰¹å¾ç»´åº¦: {combined_features.shape}")
        
            # 3. åŠ¨æ€ç‰¹å¾é€‰æ‹©
            if self.feature_selector:
                selected_features, selected_indices = self.feature_selector.select_features(combined_features)
            else:
                selected_features = combined_features
        
            # æ£€æŸ¥å’Œä¿®å¤é€‰æ‹©ç‰¹å¾
            if not isinstance(selected_features, np.ndarray):
                selected_features = np.array(selected_features, dtype=float)
            if selected_features.ndim > 1:
                selected_features = selected_features.flatten()
            selected_features = selected_features.astype(float)
        
            print(f"ğŸ”§ é€‰æ‹©åç‰¹å¾ç»´åº¦: {selected_features.shape}")
        
            # 4. è‡ªé€‚åº”ç‰¹å¾åŠ æƒ
            if self.feature_weighter:
                weighted_features = self.feature_weighter.apply_weights(selected_features)
            else:
                weighted_features = selected_features
        
            # æ£€æŸ¥å’Œä¿®å¤åŠ æƒç‰¹å¾
            if not isinstance(weighted_features, np.ndarray):
                weighted_features = np.array(weighted_features, dtype=float)
            if weighted_features.ndim > 1:
                weighted_features = weighted_features.flatten()
            weighted_features = weighted_features.astype(float)
        
            print(f"ğŸ”§ åŠ æƒåç‰¹å¾ç»´åº¦: {weighted_features.shape}")
        
            # 5. ç‰¹å¾è´¨é‡è¯„ä¼°ï¼ˆæ¯10æ¬¡æ‰§è¡Œä¸€æ¬¡ï¼‰
            self.feature_processing_stats['enhancement_count'] += 1
            if self.feature_processing_stats['enhancement_count'] % 10 == 0 and self.feature_assessor:
                current_accuracy = 0.5  # é»˜è®¤å‡†ç¡®ç‡ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦ä¼ å…¥
                quality_report = self.feature_assessor.assess_feature_quality(weighted_features, current_accuracy)
                self.feature_processing_stats['last_quality_report'] = quality_report
                self.feature_processing_stats['quality_assessments'] += 1
            
                # æ ¹æ®è´¨é‡æŠ¥å‘Šè°ƒæ•´å‚æ•°
                if quality_report['overall_quality'] < 0.5:
                    print(f"âš ï¸ ç‰¹å¾è´¨é‡è¾ƒä½({quality_report['overall_quality']:.3f})ï¼Œå»ºè®®: {quality_report['recommendations'][:2]}")
        
            # ç¡®ä¿è¾“å‡ºç»´åº¦ä¸€è‡´ï¼ˆå¡«å……æˆ–æˆªæ–­åˆ°ç›®æ ‡ç»´åº¦ï¼‰
            target_dim = 60  # ä¿æŒä¸åŸå§‹ç³»ç»Ÿå…¼å®¹
            if len(weighted_features) > target_dim:
                final_features = weighted_features[:target_dim]
            else:
                padding_size = target_dim - len(weighted_features)
                if padding_size > 0:
                    padding = np.zeros(padding_size, dtype=float)
                    final_features = np.concatenate([weighted_features, padding])
                else:
                    final_features = weighted_features
        
            # æœ€ç»ˆæ£€æŸ¥
            final_features = final_features.astype(float)
            print(f"ğŸ”§ æœ€ç»ˆç‰¹å¾ç»´åº¦: {final_features.shape}")
        
            return final_features
        
        except Exception as e:
            print(f"æ™ºèƒ½ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
            # å›é€€åˆ°åŸºç¡€ç‰¹å¾
            try:
                if len(features) >= 60:
                    return features[:60].astype(float)
                else:
                    padding_size = 60 - len(features)
                    padding = np.zeros(padding_size, dtype=float)
                    return np.concatenate([features.astype(float), padding])
            except Exception as fallback_error:
                print(f"å›é€€å¤„ç†ä¹Ÿå¤±è´¥: {fallback_error}")
                return np.zeros(60, dtype=float)
    
    def extract_tail_specific_features(self, data_list, tail):
        """æå–ç‰¹å®šå°¾æ•°çš„ç‰¹å¾"""
        if not data_list:
            return {}
        
        features = {}
        
        # æœ€è¿‘5æœŸå‡ºç°æƒ…å†µ
        recent_5_appearances = []
        for i, period in enumerate(data_list[:5]):
            appeared = tail in period.get('tails', [])
            recent_5_appearances.append(appeared)
        features['recent_5_pattern'] = recent_5_appearances
        features['recent_5_count'] = sum(recent_5_appearances)
        
        # æœ€è¿‘10æœŸç»Ÿè®¡
        recent_10_count = sum(1 for period in data_list[:10] if tail in period.get('tails', []))
        features['recent_10_count'] = recent_10_count
        features['recent_10_frequency'] = recent_10_count / min(10, len(data_list))
        
        # è¿ç»­æ€§åˆ†æ
        consecutive_count = 0
        for period in data_list:
            if tail in period.get('tails', []):
                consecutive_count += 1
            else:
                break
        features['consecutive_appearances'] = consecutive_count
        
        # è·ç¦»ä¸Šæ¬¡å‡ºç°çš„é—´éš”
        last_appearance = -1
        for i, period in enumerate(data_list):
            if tail in period.get('tails', []):
                last_appearance = i
                break
        features['last_appearance_distance'] = last_appearance
        
        # åœ¨æœ€æ–°æœŸä¸­çš„çŠ¶æ€
        features['in_latest_period'] = tail in data_list[0].get('tails', [])
        
        return features
    
    def update_components_with_learning_result(self, features, accuracy):
        """ä½¿ç”¨å­¦ä¹ ç»“æœæ›´æ–°æ™ºèƒ½ç‰¹å¾å¤„ç†ç»„ä»¶"""
        try:
            # æ›´æ–°ç‰¹å¾é€‰æ‹©å™¨
            if self.feature_selector:
                self.feature_selector.update_feature_importance(features, accuracy)
            
            # æ›´æ–°ç‰¹å¾äº¤äº’ç»„åˆå™¨
            if self.feature_combiner:
                self.feature_combiner.update_interaction_scores(features, accuracy)
            
            # æ›´æ–°è‡ªé€‚åº”æƒé‡å™¨
            if self.feature_weighter:
                self.feature_weighter.update_weights(features, accuracy)
            
            # ç»Ÿè®¡æ›´æ–°
            self.feature_processing_stats['selection_updates'] += 1
            
        except Exception as e:
            print(f"æ›´æ–°æ™ºèƒ½ç‰¹å¾å¤„ç†ç»„ä»¶å¤±è´¥: {e}")