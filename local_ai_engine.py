# ultimate_online_ai_engine.py - ç»ˆæåœ¨çº¿å­¦ä¹ AIå¼•æ“ï¼ˆå®Œæ•´å®ç°ç‰ˆï¼‰

import json
import sqlite3
import numpy as np
import os
import torch.nn.functional as F
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import pickle
import os
import warnings
warnings.filterwarnings('ignore')
import traceback
import math
from data.data_analysis_tools import DataAnalysisTools
from ai.deep_learning_models import LSTMPredictor, TransformerPredictor, PositionalEncoding
from ai.deep_learning_module import DeepLearningModule
from core.config_manager import ConfigManager
from ai_engine.prediction.anti_manipulation_models import BankerBehaviorAnalyzer
from ai_engine.prediction.reverse_psychology_models import ReversePsychologyPredictor
from ai_engine.prediction.unpopular_digger_models import UnpopularDigger
from ai_engine.prediction.money_flow_analyzer import MoneyFlowAnalyzer
from ai_engine.prediction.game_theory_strategist import GameTheoryStrategist
from ai_engine.prediction.manipulation_timing_detector import ManipulationTimingDetector
from ai_engine.prediction.anti_trend_hunter import AntiTrendHunter
from ai_engine.prediction.crowd_psychology_analyzer import CrowdPsychologyAnalyzer

# å¯¼å…¥AIé…ç½®æ¨¡å—
from ai.ai_config import (
    ai_config, PYTORCH_AVAILABLE, DEVICE, RIVER_AVAILABLE, 
    SKMULTIFLOW_AVAILABLE, DEEP_RIVER_AVAILABLE, LOCAL_MODELS_AVAILABLE, 
    ENABLED_SPECIAL_MODELS
)
from ai.feature_engineering import FeatureEngineer
from analysis.fundamental_laws import FundamentalLaws
from investment.investment_system import InvestmentSystem
from data.database_manager import DatabaseManager
from ai.prediction_analyzer import PredictionAnalyzer
from ai.drift_detection import DriftDetectionManager
from analysis.statistics_manager import StatisticsManager
    
class UltimateOnlineAIEngine:
    """ç»ˆæåœ¨çº¿å­¦ä¹ AIå¼•æ“ - èåˆå¤šä¸ªåº“çš„ä¼˜åŠ¿"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager()

        # è·å–å¯ç”¨æ¨¡å‹é…ç½®
        try:
            self.enabled_models = self.config_manager.get_enabled_models_config()
            print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹é…ç½®åŠ è½½å®Œæˆ")
            print(f"   Riveræ¨¡å‹: {self.enabled_models['river_models']}")
            print(f"   sklearnæ¨¡å‹: {self.enabled_models['sklearn_models']}")
            print(f"   PyTorchæ¨¡å‹: {self.enabled_models['pytorch_models']}")
            print(f"   ç‰¹æ®Šæ¨¡å‹: {self.enabled_models['special_models']}")
        except:
            # å¦‚æœconfig_manageræ²¡æœ‰è¯¥æ–¹æ³•ï¼Œä½¿ç”¨ai_configçš„é…ç½®
            self.enabled_models = {
                'river_models': ['local_bagging', 'local_pattern_matcher_strict'],
                'sklearn_models': ['extremely_fast_tree', 'skm_hoeffding_adaptive'],
                'pytorch_models': ['lstm', 'transformer'],
                'special_models': ai_config.ENABLED_SPECIAL_MODELS
            }
            print(f"ğŸ“‹ ä½¿ç”¨ai_configçš„ç‰¹æ®Šæ¨¡å‹é…ç½®")
            print(f"   ç‰¹æ®Šæ¨¡å‹: {self.enabled_models['special_models']}")

        # æå–å„ç±»å‹çš„å¯ç”¨æ¨¡å‹é…ç½®ï¼ˆä¾›åç»­ä½¿ç”¨ï¼‰
        enabled_special = self.enabled_models.get('special_models', [])
        enabled_pytorch = self.enabled_models.get('pytorch_models', [])
        enabled_river = self.enabled_models.get('river_models', [])
        enabled_sklearn = self.enabled_models.get('sklearn_models', [])

        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨ï¼Œå¹¶æ·»åŠ é”™è¯¯å¤„ç†
        try:
            self.data_dir.mkdir(parents=True, exist_ok=True)
            print(f"âœ… AIæ•°æ®ç›®å½•å·²ç¡®ä¿å­˜åœ¨: {self.data_dir}")
            
            # éªŒè¯ç›®å½•æ˜¯å¦å¯å†™
            test_file = self.data_dir / ".write_test"
            try:
                test_file.write_text("test")
                test_file.unlink()
                print(f"âœ… AIæ•°æ®ç›®å½•å†™å…¥æƒé™éªŒè¯æˆåŠŸ")
            except Exception as write_e:
                print(f"âŒ AIæ•°æ®ç›®å½•å†™å…¥æƒé™éªŒè¯å¤±è´¥: {write_e}")
                raise Exception(f"AIæ•°æ®ç›®å½•æ— å†™å…¥æƒé™: {write_e}")
                
        except Exception as dir_e:
            print(f"âŒ åˆ›å»ºAIæ•°æ®ç›®å½•å¤±è´¥: {dir_e}")
            raise Exception(f"æ— æ³•åˆ›å»ºæˆ–è®¿é—®AIæ•°æ®ç›®å½•: {dir_e}")
        
        # æ•°æ®åº“å’Œæ¨¡å‹è·¯å¾„
        db_config = self.config_manager.get_database_config()
        self.db_path = self.data_dir / db_config['db_name']
        self.model_path = self.data_dir / db_config['model_state_name']
        
        # åœ¨çº¿å­¦ä¹ æ¨¡å‹ç»„ä»¶
        self.river_models = {}        # Riveræ¨¡å‹
        self.sklearn_models = {}      # scikit-multiflowæ¨¡å‹
        self.drift_detectors = {}     # æ¦‚å¿µæ¼‚ç§»æ£€æµ‹å™¨
        self.metrics_trackers = {}    # æ€§èƒ½åº¦é‡è·Ÿè¸ªå™¨
        self.ensemble_weights = {}    # é›†æˆå­¦ä¹ æƒé‡
        
        # å­¦ä¹ çŠ¶æ€
        self.total_samples_seen = 0
        self.correct_predictions = 0
        self.model_performance = {}
        self.is_initialized = False
        self.last_prediction_result = None
        
        # è°ƒè¯•ä¿¡æ¯æ§åˆ¶å¼€å…³
        self.debug_pattern_matching = False  # è®¾ç½®ä¸ºFalseæ¥å…³é—­å†å²æ¨¡å¼åŒ¹é…çš„è°ƒè¯•ä¿¡æ¯

        # åœ¨çº¿å­¦ä¹ é…ç½®
        self.learning_config = self.config_manager.get_learning_config()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.init_database()
        # é¢„å…ˆåˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å—ï¼ˆé¿å…åœ¨init_online_modelsä¸­è®¿é—®æœªåˆå§‹åŒ–çš„å±æ€§ï¼‰
        self.deep_learning_module = None
        if ai_config.PYTORCH_AVAILABLE:
            try:
                print("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å—...")
                dl_config = self.config_manager.get_deep_learning_config()
                self.deep_learning_module = DeepLearningModule(
                    input_size=dl_config['input_size'],
                    device=ai_config.DEVICE
                )
                
                # ç¡®ä¿PyTorchæ¨¡å‹ç›®å½•å­˜åœ¨
                pytorch_model_dir = self.ensure_pytorch_model_directory()
                if pytorch_model_dir and hasattr(self.deep_learning_module, 'models_dir'):
                    self.deep_learning_module.models_dir = pytorch_model_dir
                    print(f"âœ… æ¨¡å‹ç›®å½•è·¯å¾„å·²ä¼ é€’ç»™æ·±åº¦å­¦ä¹ æ¨¡å—")
                else:
                    print(f"âŒ æ— æ³•ä¼ é€’æ¨¡å‹ç›®å½•è·¯å¾„ç»™æ·±åº¦å­¦ä¹ æ¨¡å—")
                    self.deep_learning_module = None
                    raise Exception("æ— æ³•åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æ¨¡å—")
                

                # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
                if hasattr(self.deep_learning_module, 'models') and self.deep_learning_module.models:
                    print(f"ğŸ¤– æ·±åº¦å­¦ä¹ æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼ŒåŒ…å« {len(self.deep_learning_module.models)} ä¸ªæ¨¡å‹")
                else:
                    print("âš ï¸ æ·±åº¦å­¦ä¹ æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œä½†æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
                    self.deep_learning_module = None
                    
            except Exception as e:
                print(f"âŒ æ·±åº¦å­¦ä¹ æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
                print(f"ğŸ“‹ è¯¦ç»†é”™è¯¯: {str(e)}")
                import traceback
                traceback.print_exc()
                self.deep_learning_module = None
        else:
            print("âš ï¸ PyTorchä¸å¯ç”¨ï¼Œè·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å—")
        self.init_online_models()
        self.load_saved_state()
        
        # å¯åŠ¨æ—¶è¿›è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        self.check_data_consistency_on_startup()
        
        # æ·»åŠ æ•°æ®è´¨é‡å’Œéšæœºæ€§åˆ†æå·¥å…·
        self.data_analysis_tools = DataAnalysisTools()
        print("ğŸ“Š æ•°æ®åˆ†æå·¥å…·åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–é«˜çº§æ¼‚ç§»æ£€æµ‹å™¨
        self.advanced_drift_detector = ai_config.EnsembleDriftDetector()
        print("ğŸ” é«˜çº§é›†æˆæ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–æ™ºèƒ½ç‰¹å¾ç»„åˆå™¨
        self.feature_selector = ai_config.DynamicFeatureSelector(feature_count=60, selection_ratio=0.9)
        self.feature_combiner = ai_config.FeatureInteractionCombiner(original_features=60, max_interactions=15)
        self.timeseries_enhancer = ai_config.TimeSeriesFeatureEnhancer(history_length=10)
        self.feature_weighter = ai_config.AdaptiveFeatureWeighter(feature_count=75)  # 60åŸå§‹+15äº¤äº’
        self.feature_assessor = ai_config.FeatureQualityAssessor(assessment_window=50)
        print("ğŸ¯ æ™ºèƒ½ç‰¹å¾å¤„ç†ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        self.feature_engineer = FeatureEngineer(
            ai_config=ai_config,
            feature_selector=self.feature_selector,
            feature_combiner=self.feature_combiner,
            timeseries_enhancer=self.timeseries_enhancer,
            feature_weighter=self.feature_weighter,
            feature_assessor=self.feature_assessor
        )
        print("ğŸ”§ ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–å®šå¾‹è§„åˆ™å¤„ç†å™¨
        self.fundamental_laws = FundamentalLaws()
        print("âš–ï¸ å®šå¾‹è§„åˆ™å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–æŠ•èµ„ç®¡ç†ç³»ç»Ÿ
        self.investment_system = InvestmentSystem()
        print("ğŸ’° æŠ•èµ„ç®¡ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = DatabaseManager(self.db_path)
        print("ğŸ—„ï¸ æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–é¢„æµ‹åˆ†æå™¨
        self.prediction_analyzer = PredictionAnalyzer(self.fundamental_laws)
        self.prediction_analyzer._engine_ref = self  # æ·»åŠ å¼•æ“å¼•ç”¨
        print("ğŸ”¬ é¢„æµ‹åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–æ¼‚ç§»æ£€æµ‹ç®¡ç†å™¨
        self.drift_manager = DriftDetectionManager(ai_config, self.db_manager)
        print("ğŸš¨ æ¼‚ç§»æ£€æµ‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–ç»Ÿè®¡åˆ†æç®¡ç†å™¨
        self.statistics_manager = StatisticsManager(ai_config, self.db_manager, self.data_analysis_tools)
        print("ğŸ“Š ç»Ÿè®¡åˆ†æç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–åæ“æ§åˆ†æå™¨ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        print(f"ğŸ” æ£€æŸ¥åæ“æ§æ¨¡å‹é…ç½®...")
        print(f"   enabled_special = {enabled_special}")
        print(f"   'anti_manipulation_banker_behavior' in enabled_special = {'anti_manipulation_banker_behavior' in enabled_special}")

        if 'anti_manipulation_banker_behavior' in enabled_special:
            try:
                print("ğŸ” å¼€å§‹å¯¼å…¥ BankerBehaviorAnalyzer...")
                from ai_engine.prediction.anti_manipulation_models import BankerBehaviorAnalyzer
                print("âœ… BankerBehaviorAnalyzer å¯¼å…¥æˆåŠŸ")
        
                print("ğŸ” å¼€å§‹åˆå§‹åŒ– BankerBehaviorAnalyzer...")
                self.banker_behavior_analyzer = BankerBehaviorAnalyzer()
                print("ğŸ¯ åº„å®¶è¡Œä¸ºåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
            except ImportError as ie:
                print(f"âŒ å¯¼å…¥ BankerBehaviorAnalyzer å¤±è´¥: {ie}")
                print(f"ğŸ“‹ å¯¼å…¥é”™è¯¯è¯¦æƒ…: {str(ie)}")
                self.banker_behavior_analyzer = None
            except Exception as e:
                print(f"âŒ åº„å®¶è¡Œä¸ºåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                print(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {str(e)}")
                import traceback
                traceback.print_exc()
                self.banker_behavior_analyzer = None
        else:
            self.banker_behavior_analyzer = None
            print("â„¹ï¸ åæ“æ§åˆ†æå™¨å·²ç¦ç”¨")

        # åˆå§‹åŒ–åå‘å¿ƒç†å­¦é¢„æµ‹æ¨¡å‹ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        if 'reverse_psychology_predictor' in enabled_special:
            try:
                self.reverse_psychology_predictor = ReversePsychologyPredictor()
                print("ğŸ”„ åå‘å¿ƒç†å­¦é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ åå‘å¿ƒç†å­¦é¢„æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.reverse_psychology_predictor = None
        else:
            self.reverse_psychology_predictor = None
            print("â„¹ï¸ åå‘å¿ƒç†å­¦é¢„æµ‹å™¨å·²ç¦ç”¨")

        # åˆå§‹åŒ–å†·é—¨æŒ–æ˜å™¨ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        if 'unpopular_digger' in enabled_special:
            try:
                self.unpopular_digger = UnpopularDigger()
                print("ğŸ” å†·é—¨æŒ–æ˜å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ å†·é—¨æŒ–æ˜å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.unpopular_digger = None
        else:
            self.unpopular_digger = None
            print("â„¹ï¸ å†·é—¨æŒ–æ˜å™¨å·²ç¦ç”¨")

        # åˆå§‹åŒ–èµ„é‡‘æµå‘åˆ†æå™¨ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        if 'money_flow_analyzer' in enabled_special:
            try:
                from ai_engine.prediction.money_flow_analyzer import MoneyFlowAnalyzer
                self.money_flow_analyzer = MoneyFlowAnalyzer()
                print("ğŸ’° èµ„é‡‘æµå‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ èµ„é‡‘æµå‘åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.money_flow_analyzer = None
        else:
            self.money_flow_analyzer = None
            print("â„¹ï¸ èµ„é‡‘æµå‘åˆ†æå™¨å·²ç¦ç”¨")

        # åˆå§‹åŒ–åšå¼ˆè®ºç­–ç•¥å™¨ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        if 'game_theory_strategist' in enabled_special:
            try:
                self.game_theory_strategist = GameTheoryStrategist()
                print("ğŸ® åšå¼ˆè®ºç­–ç•¥å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ åšå¼ˆè®ºç­–ç•¥å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.game_theory_strategist = None
        else:
            self.game_theory_strategist = None
            print("â„¹ï¸ åšå¼ˆè®ºç­–ç•¥å™¨å·²ç¦ç”¨")

        # åˆå§‹åŒ–æ“æ§æ—¶æœºæ£€æµ‹å™¨ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        if 'manipulation_timing_detector' in enabled_special:
            try:
                self.manipulation_timing_detector = ManipulationTimingDetector()
                print("ğŸ¯ æ“æ§æ—¶æœºæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ æ“æ§æ—¶æœºæ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.manipulation_timing_detector = None
        else:
            self.manipulation_timing_detector = None
            print("â„¹ï¸ æ“æ§æ—¶æœºæ£€æµ‹å™¨å·²ç¦ç”¨")

        # åˆå§‹åŒ–åè¶‹åŠ¿çŒæ‰‹ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        if 'anti_trend_hunter' in enabled_special:
            try:
                self.anti_trend_hunter = AntiTrendHunter()
                print("ğŸ¯ åè¶‹åŠ¿çŒæ‰‹åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ åè¶‹åŠ¿çŒæ‰‹åˆå§‹åŒ–å¤±è´¥: {e}")
                self.anti_trend_hunter = None
        else:
            self.anti_trend_hunter = None
            print("â„¹ï¸ åè¶‹åŠ¿çŒæ‰‹å·²ç¦ç”¨")

        # åˆå§‹åŒ–ç¾¤ä½“å¿ƒç†åˆ†æå™¨ï¼ˆå—å¯ç”¨é…ç½®æ§åˆ¶ï¼‰
        if 'crowd_psychology_analyzer' in enabled_special:
            try:
                self.crowd_psychology_analyzer = CrowdPsychologyAnalyzer()
                print("ğŸ§  ç¾¤ä½“å¿ƒç†åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âŒ ç¾¤ä½“å¿ƒç†åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.crowd_psychology_analyzer = None
        else:
            self.crowd_psychology_analyzer = None
            print("â„¹ï¸ ç¾¤ä½“å¿ƒç†åˆ†æå™¨å·²ç¦ç”¨")

        # åœ¨æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆåï¼Œåˆå§‹åŒ–åœ¨çº¿æ¨¡å‹å’Œé›†æˆæƒé‡
        self.init_online_models()
        self.load_saved_state()

        # ç‰¹å¾å¤„ç†ç»Ÿè®¡
        self.feature_processing_stats = self.config_manager.get_feature_processing_config()
        
        # æƒé‡è°ƒæ•´ç»Ÿè®¡
        self.weight_adjustment_stats = self.config_manager.get_weight_adjustment_config()

        # æƒé‡æ± ç®¡ç†
        self.investment_manager = self.investment_system
        
        # æŠ•èµ„ç»Ÿè®¡
        self.investment_stats = self.investment_system.investment_stats

        # æ·»åŠ ä¸»åº”ç”¨å¼•ç”¨ï¼ˆç”¨äºæ•°æ®è´¨é‡åˆ†æï¼‰
        self._main_app_ref = None

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå·²å§”æ‰˜ç»™æ•°æ®åº“ç®¡ç†å™¨ï¼‰"""
        # æ•°æ®åº“åˆå§‹åŒ–å·²å§”æ‰˜ç»™æ•°æ®åº“ç®¡ç†å™¨
        pass
    
    def init_online_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰åœ¨çº¿å­¦ä¹ æ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç»ˆæåœ¨çº¿å­¦ä¹ æ¨¡å‹...")
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥åº“å¯ç”¨æ€§
        print(f"ğŸ” åº“å¯ç”¨æ€§æ£€æŸ¥ï¼š")
        print(f"   - LOCAL_MODELS_AVAILABLE: {LOCAL_MODELS_AVAILABLE}")
        print(f"   - RIVER_AVAILABLE: {RIVER_AVAILABLE}")
        print(f"   - SKMULTIFLOW_AVAILABLE: {SKMULTIFLOW_AVAILABLE}")
        print(f"   - PYTORCH_AVAILABLE: {PYTORCH_AVAILABLE}")

        # === Riveræ¨¡å‹ ===
        if RIVER_AVAILABLE:
            self._init_river_models()
        
        # === scikit-multiflowæ¨¡å‹ ===
        if SKMULTIFLOW_AVAILABLE:
            self._init_sklearn_models()
        
        # === æ¦‚å¿µæ¼‚ç§»æ£€æµ‹å™¨ ===
        self._init_drift_detectors()
        
        # === æ€§èƒ½åº¦é‡è·Ÿè¸ªå™¨ ===
        self._init_metrics_trackers()
        
        # === é›†æˆå­¦ä¹ æƒé‡åˆå§‹åŒ– ===
        self._init_ensemble_weights()
        
        self.is_initialized = True
        print(f"âœ… ç»ˆæåœ¨çº¿å­¦ä¹ å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")
        print(f"   - Riveræ¨¡å‹: {len(self.river_models)} ä¸ª")
        print(f"   - scikit-multiflowæ¨¡å‹: {len(self.sklearn_models)} ä¸ª")
        print(f"   - æ¼‚ç§»æ£€æµ‹å™¨: {len(self.drift_detectors)} ä¸ª")
        print(f"   - æ€»è®¡: {len(self.river_models) + len(self.sklearn_models)} ä¸ªä¸“ä¸šåœ¨çº¿å­¦ä¹ æ¨¡å‹")
    
    def _init_river_models(self):
        """åˆå§‹åŒ–Riveræ¨¡å‹ï¼ˆåªåˆ›å»ºå¯ç”¨çš„æ¨¡å‹ï¼‰"""
        enabled_river = self.enabled_models.get('river_models', [])
        models_initialized = 0

        if not enabled_river:
            print("   â„¹ï¸ æœªå¯ç”¨ä»»ä½•Riveræ¨¡å‹")
            return

        print(f"   ğŸ¯ å‡†å¤‡åˆ›å»º {len(enabled_river)} ä¸ªå¯ç”¨çš„Riveræ¨¡å‹")

        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹å®ç°
        if ai_config.LOCAL_MODELS_AVAILABLE:
            print("   ğŸ  ä½¿ç”¨æœ¬åœ°åœ¨çº¿å­¦ä¹ æ¨¡å‹å®ç°")

            # æœ¬åœ°è£…è¢‹åˆ†ç±»å™¨
            if 'local_bagging' in enabled_river:
                try:
                    self.river_models['local_bagging'] = ai_config.LocalBaggingClassifier(
                        model_class=ai_config.LocalHoeffdingTree, 
                        n_models=5
                    )
                    models_initialized += 1
                    print(f"   âœ“ æœ¬åœ°è£…è¢‹åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"   âœ— æœ¬åœ°è£…è¢‹åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

            # å†å²æ¨¡å¼åŒ¹é…ç®—æ³•ï¼ˆ100%ç›¸ä¼¼åº¦ï¼‰
            if 'local_pattern_matcher_strict' in enabled_river:
                try:
                    self.river_models['local_pattern_matcher_strict'] = ai_config.LocalHistoricalPatternMatcher(
                        pattern_length=10, 
                        min_similarity=1.0  # 100%ç›¸ä¼¼åº¦ï¼Œå®Œå…¨åŒ¹é…
                    )
                    models_initialized += 1
                    print(f"   âœ“ å†å²æ¨¡å¼åŒ¹é…ç®—æ³•åˆå§‹åŒ–æˆåŠŸï¼ˆ100%ç›¸ä¼¼åº¦ï¼‰")
                except Exception as e:
                    print(f"   âœ— å†å²æ¨¡å¼åŒ¹é…ç®—æ³•åˆå§‹åŒ–å¤±è´¥: {e}")

        # å¦‚æœæœ¬åœ°æ¨¡å‹å¯ç”¨ä¸”æˆåŠŸåˆå§‹åŒ–äº†è¶³å¤Ÿçš„æ¨¡å‹ï¼Œå°±ä¸å†å°è¯•åŸå§‹Riveræ¨¡å‹
        if models_initialized >= len(enabled_river):
            print(f"   âœ… æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå…± {models_initialized} ä¸ªæ¨¡å‹")
            print(f"   âœ“ Riveræ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.river_models)} ä¸ªæ¨¡å‹")
            return

        # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•åŸå§‹Riveræ¨¡å‹
        if ai_config.RIVER_AVAILABLE:
            print("   ğŸŒ å°è¯•ä½¿ç”¨åŸå§‹Riveråº“æ¨¡å‹")

            # è‡ªé€‚åº”éšæœºæ£®æ—
            if 'adaptive_forest' in enabled_river and 'AdaptiveRandomForestClassifier' in ai_config.ensemble_models:
                try:
                    self.river_models['adaptive_forest'] = ai_config.ensemble_models['AdaptiveRandomForestClassifier'](
                        n_models=10
                    )
                    models_initialized += 1
                    print(f"   âœ“ è‡ªé€‚åº”éšæœºæ£®æ—åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"   âœ— è‡ªé€‚åº”éšæœºæ£®æ—åˆå§‹åŒ–å¤±è´¥: {e}")

            # åœ¨çº¿è£…è¢‹åˆ†ç±»å™¨
            if 'online_bagging' in enabled_river:
                if 'OnlineBaggingClassifier' in ai_config.ensemble_models:
                    try:
                        base_model = ai_config.HoeffdingTreeClassifier()
                        self.river_models['online_bagging'] = ai_config.ensemble_models['OnlineBaggingClassifier'](
                            model=base_model,
                            n_models=5
                        )
                        models_initialized += 1
                        print(f"   âœ“ åœ¨çº¿è£…è¢‹åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ")
                    except Exception as e:
                        print(f"   âœ— åœ¨çº¿è£…è¢‹åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                elif 'BaggingClassifier' in ai_config.ensemble_models:
                    try:
                        base_model = ai_config.HoeffdingTreeClassifier()
                        self.river_models['online_bagging'] = ai_config.ensemble_models['BaggingClassifier'](
                            model=base_model,
                            n_models=5
                        )
                        models_initialized += 1
                        print(f"   âœ“ è£…è¢‹åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ")
                    except Exception as e:
                        print(f"   âœ— è£…è¢‹åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        print(f"   âœ“ Riveræ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.river_models)} ä¸ªæ¨¡å‹")
    
    def _init_sklearn_models(self):
        """åˆå§‹åŒ–scikit-multiflowæ¨¡å‹ï¼ˆåªåˆ›å»ºå¯ç”¨çš„æ¨¡å‹ï¼‰"""
        if not SKMULTIFLOW_AVAILABLE:
            print("   âš ï¸ scikit-multiflowåº“ä¸å¯ç”¨ï¼Œè·³è¿‡sklearnæ¨¡å‹åˆå§‹åŒ–")
            return

        enabled_sklearn = self.enabled_models.get('sklearn_models', [])
        if not enabled_sklearn:
            print("   â„¹ï¸ æœªå¯ç”¨ä»»ä½•sklearnæ¨¡å‹")
            return

        print(f"   ğŸ¯ å‡†å¤‡åˆ›å»º {len(enabled_sklearn)} ä¸ªå¯ç”¨çš„sklearnæ¨¡å‹")

        # æå¿«å†³ç­–æ ‘
        if 'extremely_fast_tree' in enabled_sklearn:
            try:
                self.sklearn_models['extremely_fast_tree'] = ai_config.ExtremelyFastDecisionTreeClassifier()
                print("   âœ“ æå¿«å†³ç­–æ ‘åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"   âœ— æå¿«å†³ç­–æ ‘åˆå§‹åŒ–å¤±è´¥: {e}")

        # Hoeffdingè‡ªé€‚åº”æ ‘ï¼ˆSKMç‰ˆæœ¬ï¼‰
        if 'skm_hoeffding_adaptive' in enabled_sklearn:
            try:
                self.sklearn_models['skm_hoeffding_adaptive'] = ai_config.SKMHoeffdingAdaptive()
                print("   âœ“ SKM Hoeffdingè‡ªé€‚åº”æ ‘åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"   âœ— SKM Hoeffdingè‡ªé€‚åº”æ ‘åˆå§‹åŒ–å¤±è´¥: {e}")

        # åœ¨çº¿AdaBoost
        if 'online_adaboost' in enabled_sklearn:
            try:
                self.sklearn_models['online_adaboost'] = ai_config.OnlineAdaBoostClassifier(
                    n_estimators=10
                )
                print("   âœ“ åœ¨çº¿AdaBooståˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"   âœ— åœ¨çº¿AdaBooståˆå§‹åŒ–å¤±è´¥: {e}")

        # æµå¼éšæœºè¡¥ä¸ï¼ˆSKMç‰ˆæœ¬ï¼‰
        if 'skm_random_patches' in enabled_sklearn:
            try:
                self.sklearn_models['skm_random_patches'] = ai_config.SKMRandomPatches(
                n_estimators=10
                )
                print("   âœ“ SKMæµå¼éšæœºè¡¥ä¸åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"   âœ— SKMæµå¼éšæœºè¡¥ä¸åˆå§‹åŒ–å¤±è´¥: {e}")

        print(f"   âœ“ scikit-multiflowæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.sklearn_models)} ä¸ªæ¨¡å‹")
    
    def _init_drift_detectors(self):
        """åˆå§‹åŒ–æ¦‚å¿µæ¼‚ç§»æ£€æµ‹å™¨"""
    
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¼‚ç§»æ£€æµ‹å™¨
        if LOCAL_MODELS_AVAILABLE:
            try:
                self.drift_detectors['local_drift_detector'] = ai_config.LocalDriftDetector(
                    window_size=100, 
                    threshold=0.1
                )
                print("   âœ“ æœ¬åœ°æ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"   âœ— æœ¬åœ°æ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")

        try:
            self.drift_detectors['local_drift_detector_sensitive'] = ai_config.LocalDriftDetector(
                window_size=50, 
                threshold=0.05
            )
            print("   âœ“ æœ¬åœ°æ•æ„Ÿæ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"   âœ— æœ¬åœ°æ•æ„Ÿæ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
        if RIVER_AVAILABLE and ai_config.drift_detectors:
            for name, detector_class in ai_config.drift_detectors.items():
                try:
                    if name == 'ADWIN':
                        self.drift_detectors[name.lower()] = detector_class(delta=self.learning_config['drift_sensitivity'])
                    elif name == 'PageHinkley':
                        self.drift_detectors[name.lower()] = detector_class(min_instances=30, delta=0.005, threshold=50)
                    elif name == 'KSWIN':
                        self.drift_detectors[name.lower()] = detector_class(alpha=0.005, window_size=100)
                    elif name in ['HDDM_A', 'HDDM_W']:
                        self.drift_detectors[name.lower()] = detector_class(drift_confidence=0.001, warning_confidence=0.005)
                except Exception as e:
                    print(f"   æ¼‚ç§»æ£€æµ‹å™¨ {name} åˆå§‹åŒ–å¤±è´¥: {e}")
    
        print(f"   âœ“ æ¦‚å¿µæ¼‚ç§»æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.drift_detectors)} ä¸ª")
    
    def _init_metrics_trackers(self):
        """åˆå§‹åŒ–æ€§èƒ½åº¦é‡è·Ÿè¸ªå™¨"""
        if RIVER_AVAILABLE:
            all_models = list(self.river_models.keys()) + list(self.sklearn_models.keys())
        
            for model_name in all_models:
                # åŸºç¡€åº¦é‡å™¨
                model_metrics = {}
            
                # å°è¯•åˆå§‹åŒ–å„ç§åº¦é‡å™¨
                try:
                    model_metrics['accuracy'] = ai_config.metrics.Accuracy()
                except Exception:
                    pass
            
                try:
                    model_metrics['precision'] = ai_config.metrics.Precision()
                except Exception:
                    pass
            
                try:
                    model_metrics['recall'] = ai_config.metrics.Recall()
                except Exception:
                    pass
            
                try:
                    model_metrics['f1'] = ai_config.metrics.F1()
                except Exception:
                    pass
            
                # å°è¯•æ»šåŠ¨çª—å£åº¦é‡å™¨ï¼ˆå¤„ç†ç‰ˆæœ¬å…¼å®¹æ€§ï¼‰
                try:
                    # æ–°ç‰ˆæœ¬å¯èƒ½ä½¿ç”¨ utils.Rolling
                    from river.utils import Rolling
                    model_metrics['rolling_accuracy'] = Rolling(ai_config.metrics.Accuracy(), window_size=self.learning_config['performance_tracking_window'])
                except ImportError:
                    try:
                        # æ—§ç‰ˆæœ¬ä½¿ç”¨ metrics.Rolling
                        model_metrics['rolling_accuracy'] = ai_config.metrics.Rolling(ai_config.metrics.Accuracy(), window_size=self.learning_config['performance_tracking_window'])
                    except (ImportError, AttributeError):
                        # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„å‡†ç¡®ç‡åº¦é‡å™¨
                        try:
                            model_metrics['rolling_accuracy'] = ai_config.metrics.Accuracy()
                        except Exception:
                            pass
            
                self.metrics_trackers[model_name] = model_metrics
    
        print(f"   âœ“ æ€§èƒ½åº¦é‡è·Ÿè¸ªå™¨åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.metrics_trackers)} ä¸ª")
    
    def _init_ensemble_weights(self):
        """åˆå§‹åŒ–é›†æˆå­¦ä¹ æƒé‡ï¼ˆåªä¸ºå¯ç”¨çš„æ¨¡å‹åˆ†é…æƒé‡ï¼‰"""
        all_models = []
        enabled_pytorch = self.enabled_models.get('pytorch_models', [])
        enabled_special = self.enabled_models.get('special_models', [])

        # æ·»åŠ Riveræ¨¡å‹ï¼ˆå¸¦å‰ç¼€ï¼‰
        for model_name in self.river_models.keys():
            all_models.append(f'river_{model_name}')

        # æ·»åŠ sklearnæ¨¡å‹ï¼ˆå¸¦å‰ç¼€ï¼‰
        for model_name in self.sklearn_models.keys():
            all_models.append(f'sklearn_{model_name}')

        # æ·»åŠ å¯ç”¨çš„PyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¸¦å‰ç¼€ï¼‰
        if PYTORCH_AVAILABLE and self.deep_learning_module and self.deep_learning_module.models:
            pytorch_model_names = list(self.deep_learning_module.models.keys())
            for model_name in pytorch_model_names:
                if model_name in enabled_pytorch:
                    all_models.append(f'pytorch_{model_name}')

        # æ·»åŠ å¯ç”¨çš„ç‰¹æ®Šæ¨¡å‹
        if 'anti_manipulation_banker_behavior' in enabled_special and hasattr(self, 'banker_behavior_analyzer') and self.banker_behavior_analyzer:
            all_models.append('anti_manipulation_banker_behavior')

        if 'reverse_psychology_predictor' in enabled_special and hasattr(self, 'reverse_psychology_predictor') and self.reverse_psychology_predictor:
            all_models.append('reverse_psychology_predictor')

        if 'unpopular_digger' in enabled_special and hasattr(self, 'unpopular_digger') and self.unpopular_digger:
            all_models.append('unpopular_digger')

        if 'money_flow_analyzer' in enabled_special and hasattr(self, 'money_flow_analyzer') and self.money_flow_analyzer:
            all_models.append('money_flow_analyzer')

        if 'game_theory_strategist' in enabled_special and hasattr(self, 'game_theory_strategist') and self.game_theory_strategist:
            all_models.append('game_theory_strategist')

        if 'manipulation_timing_detector' in enabled_special and hasattr(self, 'manipulation_timing_detector') and self.manipulation_timing_detector:
            all_models.append('manipulation_timing_detector')

        if 'anti_trend_hunter' in enabled_special and hasattr(self, 'anti_trend_hunter') and self.anti_trend_hunter:
            all_models.append('anti_trend_hunter')

        if 'crowd_psychology_analyzer' in enabled_special and hasattr(self, 'crowd_psychology_analyzer') and self.crowd_psychology_analyzer:
            all_models.append('crowd_psychology_analyzer')

        # ç»™æ¯ä¸ªå¯ç”¨çš„æ¨¡å‹åˆ†é…åˆå§‹æƒé‡
        initial_weight = 1.0 / len(all_models) if all_models else 0.0

        for model_name in all_models:
            self.ensemble_weights[model_name] = {
                'weight': float(initial_weight),          # å½“å‰å¯ç”¨æƒé‡
                'frozen_weight': 0.0,                     # å†»ç»“æƒé‡
                'total_weight': float(initial_weight),    # æ€»æƒé‡ï¼ˆæ˜¾ç¤ºç”¨ï¼‰
                'is_frozen': False,                       # å†»ç»“çŠ¶æ€
                'frozen_timestamp': None,                 # å†»ç»“æ—¶é—´
                'weight_investments': {},                 # æƒé‡æŠ•èµ„ {tail: weight_amount}
                'confidence': 0.5,
                'last_update': datetime.now(),
                'performance_history': [],
                'investment_history': []                  # æŠ•èµ„å†å²è®°å½•
            }

        print("   âœ“ é›†æˆå­¦ä¹ æƒé‡åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š æ€»å¯ç”¨æ¨¡å‹æ•°: {len(all_models)}")
        print(f"   ğŸ“Š æ¯ä¸ªæ¨¡å‹åˆå§‹æƒé‡: {initial_weight:.4f}")
    
        # æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        for model_name in all_models:
            model_type = "ç‰¹æ®Šæ¨¡å‹" if not model_name.startswith(('river_', 'sklearn_', 'pytorch_')) else model_name.split('_')[0].upper()
            print(f"      âœ“ {model_name} ({model_type})")
    
    def get_detailed_extremely_hot_analysis(self, data_list: List[Dict]) -> Dict:
        """è·å–è¯¦ç»†çš„æçƒ­å°¾æ•°åˆ†æ"""
        analysis = {
            'hot_30_20': set(),  # 30æœŸâ‰¥20æ¬¡
            'hot_10_8': set(),   # 10æœŸâ‰¥8æ¬¡
            'combined': set()    # åˆå¹¶ç»“æœ
        }
        
        if len(data_list) < 10:
            return analysis
        
        # æ£€æŸ¥30æœŸâ‰¥20æ¬¡çš„æƒ…å†µ
        if len(data_list) >= 30:
            recent_30_data = data_list[:30]
            for tail in range(10):
                count_30 = sum(1 for period in recent_30_data if tail in period.get('tails', []))
                if count_30 >= 20:
                    analysis['hot_30_20'].add(tail)
        
        # æ£€æŸ¥10æœŸâ‰¥8æ¬¡çš„æƒ…å†µ
        recent_10_data = data_list[:10]
        for tail in range(10):
            count_10 = sum(1 for period in recent_10_data if tail in period.get('tails', []))
            if count_10 >= 8:
                analysis['hot_10_8'].add(tail)
        
        # åˆå¹¶ç»“æœ
        analysis['combined'] = analysis['hot_30_20'].union(analysis['hot_10_8'])
        
        return analysis
    
    def apply_strict_fundamental_laws(self, probabilities: Dict[int, float], data_list: List[Dict]) -> List[int]:
        """åº”ç”¨ä¸¥æ ¼çš„åº•å±‚å®šå¾‹ï¼ˆå·²åˆ é™¤å®šå¾‹1ï¼‰"""
        if not data_list:
            return []
    
        # åˆå§‹å€™é€‰å°¾æ•°ï¼šæ‰€æœ‰å°¾æ•°0-9
        all_tails = set(range(10))
        print(f"ğŸ¯ åˆå§‹å€™é€‰å°¾æ•°ï¼ˆæ‰€æœ‰å°¾æ•°ï¼‰ï¼š{sorted(all_tails)}")
    
        # å®šå¾‹2ï¼šæ’é™¤é™·é˜±å°¾æ•°
        trap_tails = self.fundamental_laws.identify_comprehensive_trap_tails(data_list)
        print(f"ğŸš« å®šå¾‹2 - è¯†åˆ«å‡ºé™·é˜±å°¾æ•°ï¼š{sorted(trap_tails)}")
    
        # å®šå¾‹3ï¼šæ’é™¤åœ¨10æœŸå’Œ30æœŸä¸­åŒæ—¶è¡¨ç°ä¸ºæœ€å°‘å‡ºç°æ¬¡æ•°çš„å°¾æ•°
        dual_minimum_tails = self.fundamental_laws.identify_dual_minimum_tails(data_list)
        print(f"âš¡ å®šå¾‹3 - 10æœŸå’Œ30æœŸåŒæ—¶æœ€å°‘çš„å°¾æ•°ï¼š{sorted(dual_minimum_tails)}")
    
        # å®šå¾‹4ï¼šæ’é™¤æçƒ­å°¾æ•°ï¼ˆæœ€è¿‘30æœŸå‡ºç°â‰¥20æ¬¡æˆ–10æœŸâ‰¥8æ¬¡çš„å°¾æ•°ï¼‰
        extremely_hot_tails_v2 = self.fundamental_laws.identify_extremely_hot_tails(data_list, periods=30, threshold=20)
        print(f"ğŸ”¥ å®šå¾‹4 - æçƒ­å°¾æ•°ï¼ˆ30æœŸâ‰¥20æ¬¡æˆ–10æœŸâ‰¥8æ¬¡ï¼‰ï¼š{sorted(extremely_hot_tails_v2)}")
    
        # åº”ç”¨å‰©ä½™ä¸‰å¤§å®šå¾‹ï¼Œå¾—åˆ°å€™é€‰å°¾æ•°
        candidates = all_tails - trap_tails - dual_minimum_tails - extremely_hot_tails_v2
        print(f"âœ… åº”ç”¨ä¸‰å¤§å®šå¾‹åçš„å€™é€‰å°¾æ•°ï¼š{sorted(candidates)}")
        
        if not candidates:
            print("âš ï¸ åº”ç”¨ä¸‰å¤§å®šå¾‹åæ— å€™é€‰å°¾æ•°ï¼Œé€‰æ‹©æ‰€æœ‰å°¾æ•°ä¸­æ¦‚ç‡æœ€é«˜çš„")
            best_tail = max(all_tails, key=lambda t: probabilities.get(t, 0))
            return [best_tail]
        
        # ä»å€™é€‰ä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å°¾æ•°
        best_candidate = max(candidates, key=lambda t: probabilities.get(t, 0))
        print(f"ğŸ¯ æœ€ç»ˆæ¨èå°¾æ•°ï¼š{best_candidate}ï¼ˆæ¦‚ç‡ï¼š{probabilities.get(best_candidate, 0):.3f}ï¼‰")
        
        return [best_candidate]
    
    def identify_comprehensive_trap_tails(self, data_list: List[Dict]) -> set:
        """è¯†åˆ«æ‰€æœ‰ç±»å‹çš„é™·é˜±å°¾æ•°"""
        trap_tails = set()
        
        if len(data_list) < 6:
            return trap_tails
        
        for tail in range(10):
            # æ£€æŸ¥è¿ç»­3æœŸä»¥ä¸Šæ²¡æœ‰å‡ºç°ï¼Œåœ¨æœ€æ–°ä¸€æœŸçªç„¶å‡ºç°
            if self.is_sudden_appearance_trap(data_list, tail):
                trap_tails.add(tail)
                continue
            
            # æ£€æŸ¥å„ç§è§„å¾‹æ€§æ¨¡å¼
            if self.has_regular_patterns(data_list, tail):
                trap_tails.add(tail)
        
        return trap_tails
    
    def is_sudden_appearance_trap(self, data_list: List[Dict], tail: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè¿ç»­3æœŸä»¥ä¸Šæ²¡æœ‰å‡ºç°åçªç„¶å‡ºç°çš„é™·é˜±"""
        if len(data_list) < 4:
            return False
        
        if tail not in data_list[0].get('tails', []):
            return False
        
        consecutive_absent = 0
        for i in range(1, len(data_list)):
            if tail not in data_list[i].get('tails', []):
                consecutive_absent += 1
            else:
                break
        
        return consecutive_absent >= 3
    
    def has_regular_patterns(self, data_list: List[Dict], tail: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è§„å¾‹æ€§å‡ºç°æ¨¡å¼"""
        if len(data_list) < 6:
            return False
        
        appearances = []
        check_periods = min(10, len(data_list))
        
        for i in range(check_periods):
            appearances.append(1 if tail in data_list[i].get('tails', []) else 0)
        
        # æ£€æŸ¥å„ç§è§„å¾‹æ¨¡å¼
        patterns = [
            self.check_pattern_1010(appearances),
            self.check_pattern_110011(appearances),
            self.check_pattern_100100(appearances),
            self.check_pattern_110110(appearances),
        ]
        
        return any(patterns)
    
    def check_pattern_1010(self, appearances: List[int]) -> bool:
        if len(appearances) < 4:
            return False
        return (appearances[0] == 1 and appearances[1] == 0 and appearances[2] == 1 and appearances[3] == 0)
    
    def check_pattern_110011(self, appearances: List[int]) -> bool:
        if len(appearances) < 6:
            return False
        return (appearances[0] == 1 and appearances[1] == 1 and 
                appearances[2] == 0 and appearances[3] == 0 and 
                appearances[4] == 1 and appearances[5] == 1)
    
    def check_pattern_100100(self, appearances: List[int]) -> bool:
        if len(appearances) < 6:
            return False
        return (appearances[0] == 1 and appearances[1] == 0 and appearances[2] == 0 and
                appearances[3] == 1 and appearances[4] == 0 and appearances[5] == 0)
    
    def check_pattern_110110(self, appearances: List[int]) -> bool:
        if len(appearances) < 6:
            return False
        return (appearances[0] == 1 and appearances[1] == 1 and appearances[2] == 0 and
                appearances[3] == 1 and appearances[4] == 1 and appearances[5] == 0)
    
    def identify_dual_minimum_tails(self, data_list: List[Dict]) -> set:
        """è¯†åˆ«åœ¨10æœŸå’Œ30æœŸä¸­åŒæ—¶è¡¨ç°ä¸ºæœ€å°‘å‡ºç°æ¬¡æ•°çš„å°¾æ•°"""
        dual_minimum = set()
        
        if len(data_list) < 10:
            return dual_minimum
        
        # è®¡ç®—æœ€è¿‘10æœŸå„å°¾æ•°å‡ºç°æ¬¡æ•°
        counts_10 = {}
        for tail in range(10):
            counts_10[tail] = sum(1 for i in range(min(10, len(data_list))) 
                                if tail in data_list[i].get('tails', []))
        
        min_count_10 = min(counts_10.values())
        min_tails_10 = {tail for tail, count in counts_10.items() if count == min_count_10}
        
        if len(data_list) >= 30:
            counts_30 = {}
            for tail in range(10):
                counts_30[tail] = sum(1 for i in range(min(30, len(data_list))) 
                                    if tail in data_list[i].get('tails', []))
            
            min_count_30 = min(counts_30.values())
            min_tails_30 = {tail for tail, count in counts_30.items() if count == min_count_30}
            
            dual_minimum = min_tails_10 & min_tails_30
        
        return dual_minimum
    
    def identify_extremely_hot_tails(self, data_list: List[Dict], periods=30, threshold=20):
        """è¯†åˆ«æçƒ­å°¾æ•° - åœ¨æœ€è¿‘NæœŸä¸­å‡ºç°æ¬¡æ•°è¶…è¿‡é˜ˆå€¼çš„å°¾æ•°ï¼ˆæ ¸å¿ƒå®šå¾‹ï¼‰"""
        if len(data_list) < periods:
            # å¦‚æœæ•°æ®ä¸è¶³30æœŸï¼Œä½¿ç”¨ç°æœ‰æ•°æ®ä½†è°ƒæ•´é˜ˆå€¼
            available_periods = len(data_list)
            if available_periods < 10:  # æ•°æ®å¤ªå°‘ï¼Œä¸åº”ç”¨æ­¤å®šå¾‹
                return set()
            # æŒ‰æ¯”ä¾‹è°ƒæ•´é˜ˆå€¼ï¼š20/30 = 0.67ï¼Œå³å‡ºç°æ¬¡æ•°è¶…è¿‡67%çš„æœŸæ•°
            adjusted_threshold = int(available_periods * 0.67)
            periods = available_periods
            threshold = adjusted_threshold

        extremely_hot_tails = set()
        recent_data = data_list[:periods]  # å–æœ€è¿‘NæœŸæ•°æ®

        for tail in range(10):
            # è®¡ç®—è¯¥å°¾æ•°åœ¨æœ€è¿‘NæœŸä¸­çš„å‡ºç°æ¬¡æ•°
            count = sum(1 for period in recent_data if tail in period.get('tails', []))
    
            # å¦‚æœå‡ºç°æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæ ‡è®°ä¸ºæçƒ­é—¨
            if count >= threshold:
                extremely_hot_tails.add(tail)

        # æ–°å¢ï¼šæ£€æŸ¥æœ€è¿‘10æœŸä¸­å‡ºç°æ¬¡æ•°â‰¥8æœŸçš„å°¾æ•°
        if len(data_list) >= 10:
            recent_10_data = data_list[:10]  # å–æœ€è¿‘10æœŸæ•°æ®
            for tail in range(10):
                # è®¡ç®—è¯¥å°¾æ•°åœ¨æœ€è¿‘10æœŸä¸­çš„å‡ºç°æ¬¡æ•°
                count_10 = sum(1 for period in recent_10_data if tail in period.get('tails', []))
                
                # å¦‚æœæœ€è¿‘10æœŸå‡ºç°æ¬¡æ•°â‰¥8æœŸï¼Œä¹Ÿæ ‡è®°ä¸ºæçƒ­é—¨
                if count_10 >= 8:
                    extremely_hot_tails.add(tail)

        return extremely_hot_tails

    def predict_online(self, data_list: List[Dict]) -> Dict:
        """åœ¨çº¿é¢„æµ‹ - çœŸæ­£çš„å¤šæ¨¡å‹é›†æˆé¢„æµ‹"""
        if not self.is_initialized:
            return {'success': False, 'message': 'æ¨¡å‹æœªåˆå§‹åŒ–'}
            
        # éªŒè¯é¢„æµ‹æ•°æ®é¡ºåº
        if data_list and len(data_list) > 0:
            print(f"ğŸ” AIé¢„æµ‹æ•°æ®é¡ºåºéªŒè¯ï¼š")
            print(f"   åŸºäºå†å²æ•°æ®é‡ï¼š{len(data_list)}æœŸ")
            print(f"   æœ€æ–°æœŸï¼ˆindex 0ï¼‰ï¼š{data_list[0].get('numbers', [])[:3] if 'numbers' in data_list[0] else 'æ— å·ç '}")
            if len(data_list) > 1:
                print(f"   å†å²æœŸï¼ˆindex 1ï¼‰ï¼š{data_list[1].get('numbers', [])[:3] if 'numbers' in data_list[1] else 'æ— å·ç '}")
            print(f"   é¢„æµ‹é€»è¾‘ï¼šåŸºäºå…¨éƒ¨{len(data_list)}æœŸå†å²æ•°æ®è¿›è¡Œé¢„æµ‹")

        try:
            # ä¸ºæ¯ä¸ªæ¨¡å‹æå–ä¸“å±ç‰¹å¾
            base_features = self.feature_engineer.extract_enhanced_features(data_list)
            model_specific_features = self.feature_engineer.create_model_specific_features(base_features, data_list)
            
            # åœ¨é¢„æµ‹é˜¶æ®µï¼Œä½¿ç”¨å†å²æ€§èƒ½æ•°æ®æ›´æ–°é«˜çº§æ¼‚ç§»æ£€æµ‹å™¨
            historical_accuracy = self.get_current_accuracy()
            error_rate = 1.0 - historical_accuracy  # ä½¿ç”¨å†å²é”™è¯¯ç‡
            
            # æ›´æ–°é«˜çº§æ¼‚ç§»æ£€æµ‹å™¨
            performance_metrics = {
                'accuracy': historical_accuracy,
                'confidence': self.last_prediction_result.get('confidence', 0.5) if self.last_prediction_result else 0.5
            }
            self.advanced_drift_detector.update(error_rate, performance_metrics)
            
            # æ£€æŸ¥é«˜çº§æ¼‚ç§»æ£€æµ‹ç»“æœ
            advanced_drift_info = self.advanced_drift_detector.get_detailed_report()
            if advanced_drift_info['ensemble_detected']:
                print(f"ğŸš¨ é«˜çº§æ¼‚ç§»æ£€æµ‹å™¨æŠ¥å‘Šæ¦‚å¿µæ¼‚ç§»!")
                print(f"   æ£€æµ‹å™¨è¯¦æƒ…: {advanced_drift_info['individual_detectors']}")
                
                # å¤„ç†é«˜çº§æ¦‚å¿µæ¼‚ç§»
                self._handle_advanced_concept_drift(advanced_drift_info)

            # ä¸ºRiveræ¨¡å‹å‡†å¤‡ç‰¹å¾å­—å…¸
            X_river = {f'feature_{i}': base_features[i] for i in range(len(base_features))}
            
            # ä¸ºä¸åŒæ¨¡å‹åˆ›å»ºå®Œå…¨ä¸åŒçš„æ•°æ®è§†è§’å’Œå†³ç­–é€»è¾‘
            diversified_features = {}
            model_decision_strategies = {}
            
            # å®šä¹‰æ¯ä¸ªæ¨¡å‹çš„ç‹¬ç‰¹æ•°æ®è§†è§’å’Œå†³ç­–ç­–ç•¥
            model_strategies = {
                'hoeffding_tree': {
                    'focus': 'trend_continuation',  # å…³æ³¨è¶‹åŠ¿å»¶ç»­
                    'time_window': 5,  # å…³æ³¨æœ€è¿‘5æœŸ
                    'weight_recent': 2.0,  # åŠ é‡æœ€è¿‘æœŸæƒé‡
                    'decision_threshold': 0.45
                },
                'hoeffding_adaptive': {
                    'focus': 'trend_reversal',  # å…³æ³¨è¶‹åŠ¿åè½¬
                    'time_window': 8,  # å…³æ³¨æœ€è¿‘8æœŸ
                    'weight_recent': 1.5,
                    'decision_threshold': 0.55
                },
                'logistic': {
                    'focus': 'frequency_analysis',  # å…³æ³¨é¢‘ç‡åˆ†æ
                    'time_window': 10,  # å…³æ³¨æœ€è¿‘10æœŸ
                    'weight_recent': 1.0,  # å‡ç­‰æƒé‡
                    'decision_threshold': 0.5
                },
                'naive_bayes': {
                    'focus': 'independence_assumption',  # å…³æ³¨ç‹¬ç«‹æ€§å‡è®¾
                    'time_window': 15,  # å…³æ³¨æ›´é•¿æœŸ
                    'weight_recent': 0.8,  # é™ä½æœ€è¿‘æœŸæƒé‡
                    'decision_threshold': 0.6
                },
                'naive_bayes_multinomial': {
                    'focus': 'categorical_patterns',  # å…³æ³¨åˆ†ç±»æ¨¡å¼
                    'time_window': 6,
                    'weight_recent': 1.8,
                    'decision_threshold': 0.35
                },
                'naive_bayes_gaussian': {
                    'focus': 'continuous_distribution',  # å…³æ³¨è¿ç»­åˆ†å¸ƒ
                    'time_window': 12,
                    'weight_recent': 1.2,
                    'decision_threshold': 0.65
                },
                'naive_bayes_mixed': {
                    'focus': 'hybrid_approach',  # æ··åˆæ–¹æ³•
                    'time_window': 9,
                    'weight_recent': 1.3,
                    'decision_threshold': 0.4
                },
                'bagging': {
                    'focus': 'ensemble_voting',  # å…³æ³¨é›†æˆæŠ•ç¥¨
                    'time_window': 7,
                    'weight_recent': 1.6,
                    'decision_threshold': 0.3
                },
                'adaboost': {
                    'focus': 'error_correction',  # å…³æ³¨é”™è¯¯ä¿®æ­£
                    'time_window': 11,
                    'weight_recent': 1.1,
                    'decision_threshold': 0.7
                },
                'bagging_nb': {
                    'focus': 'probabilistic_ensemble',  # æ¦‚ç‡é›†æˆ
                    'time_window': 4,
                    'weight_recent': 2.2,
                    'decision_threshold': 0.25
                },
                'bagging_lr': {
                    'focus': 'linear_ensemble',  # çº¿æ€§é›†æˆ
                    'time_window': 13,
                    'weight_recent': 0.9,
                    'decision_threshold': 0.75
                },
                'pattern_matcher_strict': {
                    'focus': 'historical_matching',  # å†å²æ¨¡å¼åŒ¹é…
                    'time_window': 20,  # éœ€è¦æ›´å¤šå†å²æ•°æ®
                    'weight_recent': 0.5,  # å†å²æƒé‡æ›´é‡è¦
                    'decision_threshold': 0.8  # é«˜é˜ˆå€¼ï¼Œåªåœ¨é«˜åŒ¹é…åº¦æ—¶é¢„æµ‹
                }
            }
            
            for model_name in self.river_models.keys():
                # è·å–æ¨¡å‹ç­–ç•¥é…ç½®
                config_key = model_name.replace('local_', '').replace('river_', '')
                strategy = model_strategies.get(config_key, model_strategies['logistic'])  # é»˜è®¤ç­–ç•¥
                
                # æ ¹æ®ç­–ç•¥åˆ›å»ºè¯¥æ¨¡å‹çš„ä¸“å±ç‰¹å¾å’Œå†³ç­–é€»è¾‘
                model_features = {}
                model_decision_strategies[model_name] = strategy
                
                # æ ¹æ®ä¸åŒçš„å…³æ³¨ç‚¹åˆ›å»ºç‰¹å¾
                if strategy['focus'] == 'trend_continuation':
                    # è¶‹åŠ¿å»¶ç»­ï¼šå¼ºåŒ–è¿ç»­æ€§ç‰¹å¾
                    for tail in range(10):
                        # è¿ç»­å‡ºç°æ¬¡æ•°æƒé‡
                        consecutive_weight = 3.0
                        recent_appearance_weight = 2.5
                        frequency_weight = 1.0
                        
                        model_features[f'consecutive_{tail}'] = consecutive_weight
                        model_features[f'recent_appear_{tail}'] = recent_appearance_weight
                        model_features[f'frequency_{tail}'] = frequency_weight
                
                elif strategy['focus'] == 'trend_reversal':
                    # è¶‹åŠ¿åè½¬ï¼šå¼ºåŒ–åè½¬ä¿¡å·ç‰¹å¾
                    for tail in range(10):
                        # é—´éš”è·ç¦»æƒé‡
                        gap_weight = 2.8
                        reversal_signal_weight = 2.2
                        cold_tail_weight = 1.8
                        
                        model_features[f'gap_distance_{tail}'] = gap_weight
                        model_features[f'reversal_signal_{tail}'] = reversal_signal_weight
                        model_features[f'cold_tail_{tail}'] = cold_tail_weight
                
                elif strategy['focus'] == 'frequency_analysis':
                    # é¢‘ç‡åˆ†æï¼šå‡è¡¡å„ç§é¢‘ç‡ç‰¹å¾
                    for tail in range(10):
                        short_freq_weight = 1.5
                        medium_freq_weight = 1.5
                        long_freq_weight = 1.0
                        
                        model_features[f'short_freq_{tail}'] = short_freq_weight
                        model_features[f'medium_freq_{tail}'] = medium_freq_weight
                        model_features[f'long_freq_{tail}'] = long_freq_weight
                
                elif strategy['focus'] == 'independence_assumption':
                    # ç‹¬ç«‹æ€§å‡è®¾ï¼šæ¯ä¸ªå°¾æ•°ç‹¬ç«‹åˆ†æ
                    for tail in range(10):
                        independent_prob_weight = 2.0
                        prior_prob_weight = 1.5
                        
                        model_features[f'independent_prob_{tail}'] = independent_prob_weight
                        model_features[f'prior_prob_{tail}'] = prior_prob_weight
                
                elif strategy['focus'] == 'categorical_patterns':
                    # åˆ†ç±»æ¨¡å¼ï¼šå¼ºåŒ–åˆ†ç±»ç‰¹å¾
                    for tail in range(10):
                        category_weight = 2.5
                        pattern_weight = 2.0
                        
                        model_features[f'category_{tail}'] = category_weight
                        model_features[f'pattern_{tail}'] = pattern_weight
                
                elif strategy['focus'] == 'continuous_distribution':
                    # è¿ç»­åˆ†å¸ƒï¼šå…³æ³¨æ•°å€¼åˆ†å¸ƒ
                    for tail in range(10):
                        distribution_weight = 2.2
                        variance_weight = 1.8
                        
                        model_features[f'distribution_{tail}'] = distribution_weight
                        model_features[f'variance_{tail}'] = variance_weight
                
                elif strategy['focus'] == 'hybrid_approach':
                    # æ··åˆæ–¹æ³•ï¼šå¤šç§ç‰¹å¾ç»„åˆ
                    for tail in range(10):
                        hybrid_weight = 1.8
                        combined_weight = 1.6
                        
                        model_features[f'hybrid_{tail}'] = hybrid_weight
                        model_features[f'combined_{tail}'] = combined_weight
                
                elif strategy['focus'] == 'ensemble_voting':
                    # é›†æˆæŠ•ç¥¨ï¼šæŠ•ç¥¨æœºåˆ¶ç‰¹å¾
                    for tail in range(10):
                        vote_weight = 2.1
                        consensus_weight = 1.7
                        
                        model_features[f'vote_{tail}'] = vote_weight
                        model_features[f'consensus_{tail}'] = consensus_weight
                
                elif strategy['focus'] == 'error_correction':
                    # é”™è¯¯ä¿®æ­£ï¼šå¼ºåŒ–ä¿®æ­£ç‰¹å¾
                    for tail in range(10):
                        error_signal_weight = 2.4
                        correction_weight = 2.0
                        
                        model_features[f'error_signal_{tail}'] = error_signal_weight
                        model_features[f'correction_{tail}'] = correction_weight
                
                elif strategy['focus'] == 'probabilistic_ensemble':
                    # æ¦‚ç‡é›†æˆï¼šæ¦‚ç‡ç‰¹å¾
                    for tail in range(10):
                        prob_ensemble_weight = 2.3
                        likelihood_weight = 1.9
                        
                        model_features[f'prob_ensemble_{tail}'] = prob_ensemble_weight
                        model_features[f'likelihood_{tail}'] = likelihood_weight
                
                elif strategy['focus'] == 'linear_ensemble':
                    # çº¿æ€§é›†æˆï¼šçº¿æ€§ç»„åˆç‰¹å¾
                    for tail in range(10):
                        linear_comb_weight = 1.4
                        weighted_sum_weight = 1.6
                        
                        model_features[f'linear_comb_{tail}'] = linear_comb_weight
                        model_features[f'weighted_sum_{tail}'] = weighted_sum_weight
                
                elif strategy['focus'] == 'historical_matching':
                    # å†å²åŒ¹é…ï¼šä¿æŒåŸæœ‰é€»è¾‘ï¼Œä¸æ”¹å˜
                    for i in range(60):
                        model_features[f'feature_{i}'] = base_features[i] if i < len(base_features) else 0.0
                
                else:
                    # é»˜è®¤ç‰¹å¾é›†
                    for i in range(30):
                        model_features[f'feature_{i}'] = base_features[i] if i < len(base_features) else 0.0
                
                diversified_features[model_name] = model_features

            # ä¸ºsklearnæ¨¡å‹å‡†å¤‡ç‰¹å¾æ•°ç»„
            X_sklearn = base_features.reshape(1, -1)
            
            # ä¸ºDeep-Riveræ¨¡å‹å‡†å¤‡ç‰¹å¾å¼ é‡å’Œå­—å…¸
            if DEEP_RIVER_AVAILABLE and len(self.deep_models) > 0:
                X_deep = ai_config.torch.FloatTensor(base_features).unsqueeze(0)
                print(f"   ğŸ“Š ä¸ºDeep-Riverå‡†å¤‡ç‰¹å¾: ç»´åº¦{X_deep.shape}, æ•°æ®ç±»å‹{X_deep.dtype}")
            
            # ä¸ºDeep-Riveræ¨¡å‹å‡†å¤‡ç‰¹å¾å¼ é‡å’Œå­—å…¸
            if DEEP_RIVER_AVAILABLE and len(self.deep_models) > 0:
                X_deep = ai_config.torch.FloatTensor(base_features).unsqueeze(0)
                print(f"   ğŸ“Š ä¸ºDeep-Riverå‡†å¤‡ç‰¹å¾: ç»´åº¦{X_deep.shape}, æ•°æ®ç±»å‹{X_deep.dtype}")
    
                # å¢å¼ºçš„è¾“å…¥æ•°æ®éªŒè¯å’Œæ¸…ç†
                print("   ğŸ” æ‰§è¡ŒDeep-Riverè¾“å…¥æ•°æ®å…¨é¢éªŒè¯...")
    
                # éªŒè¯featuresæ•°ç»„
                if base_features is None:
                    print("   âŒ featuresä¸ºNoneï¼ŒDeep-Riveré¢„æµ‹å°†è·³è¿‡")
                    X_deep = None
                elif len(base_features) == 0:
                    print("   âŒ featuresä¸ºç©ºæ•°ç»„ï¼ŒDeep-Riveré¢„æµ‹å°†è·³è¿‡")
                    X_deep = None
                elif any(x is None for x in base_features):
                    print("   âš ï¸ featuresåŒ…å«Noneå€¼ï¼Œè¿›è¡Œæ¸…ç†")
                    cleaned_features = [0.0 if x is None else float(x) for x in base_features]
                    X_deep = ai_config.torch.FloatTensor(cleaned_features).unsqueeze(0)
                    print(f"   âœ“ æ¸…ç†åçš„featuresç»´åº¦: {X_deep.shape}")
                elif any(not isinstance(x, (int, float)) for x in base_features):
                    print("   âš ï¸ featuresåŒ…å«éæ•°å€¼ç±»å‹ï¼Œè¿›è¡Œè½¬æ¢")
                    try:
                        cleaned_features = []
                        for i, x in enumerate(base_features):
                            try:
                                cleaned_features.append(float(x))
                            except (ValueError, TypeError):
                                print(f"   âš ï¸ features[{i}]={x} æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ï¼Œä½¿ç”¨0.0")
                                cleaned_features.append(0.0)
                        X_deep = ai_config.torch.FloatTensor(cleaned_features).unsqueeze(0)
                        print(f"   âœ“ è½¬æ¢åçš„featuresç»´åº¦: {X_deep.shape}")
                    except Exception as clean_e:
                        print(f"   âŒ featuresæ¸…ç†å¤±è´¥: {clean_e}")
                        X_deep = None
    
                # éªŒè¯X_riverå­—å…¸
                print("   ğŸ” éªŒè¯X_riverå­—å…¸...")
                x_river_safe = {}
                invalid_keys = []
    
                for key, value in X_river.items():
                    if value is None:
                        print(f"   âš ï¸ X_river[{key}] ä¸ºNoneï¼Œæ›¿æ¢ä¸º0.0")
                        x_river_safe[key] = 0.0
                    elif isinstance(value, (int, float)):
                        if math.isnan(value) or math.isinf(value):
                            print(f"   âš ï¸ X_river[{key}] ä¸ºNaNæˆ–æ— ç©·ï¼Œæ›¿æ¢ä¸º0.0")
                            x_river_safe[key] = 0.0
                        else:
                            x_river_safe[key] = float(value)
                    else:
                        try:
                            converted_value = float(value)
                            if math.isnan(converted_value) or math.isinf(converted_value):
                                print(f"   âš ï¸ X_river[{key}] è½¬æ¢åä¸ºNaNæˆ–æ— ç©·ï¼Œæ›¿æ¢ä¸º0.0")
                                x_river_safe[key] = 0.0
                            else:
                                x_river_safe[key] = converted_value
                        except (ValueError, TypeError):
                            print(f"   âš ï¸ X_river[{key}]={value} æ— æ³•è½¬æ¢ä¸ºæ•°å€¼ï¼Œæ›¿æ¢ä¸º0.0")
                            x_river_safe[key] = 0.0
                            invalid_keys.append(key)
    
                if invalid_keys:
                    print(f"   ğŸ“Š å…±å‘ç°{len(invalid_keys)}ä¸ªæ— æ•ˆé”®å€¼å¯¹")
    
                print(f"   âœ… X_riveréªŒè¯å®Œæˆï¼Œå…±{len(x_river_safe)}ä¸ªæœ‰æ•ˆç‰¹å¾")
    
                # ç”¨å®‰å…¨çš„å­—å…¸æ›¿æ¢åŸæ¥çš„X_riverï¼ˆä»…ç”¨äºDeep-Riverï¼‰
                X_river_for_deep = x_river_safe

            # å¤šæ¨¡å‹é¢„æµ‹
            all_predictions = {}
            
            # åˆå§‹åŒ–æ‰€æœ‰å€™é€‰å°¾æ•°ï¼ˆ0-9ï¼‰
            all_tails = set(range(10))
            print(f"ğŸ¯ åˆå§‹å€™é€‰å°¾æ•°ï¼ˆæ‰€æœ‰å°¾æ•°ï¼‰ï¼š{sorted(all_tails)}")

            # === åæ“æ§åˆ†æ ===
            anti_manipulation_analysis = None
            if hasattr(self, 'banker_behavior_analyzer') and self.banker_behavior_analyzer:
                try:
                    # åˆ†æå½“å‰æœŸçš„æ“æ§ä¿¡å·
                    current_period = {
                        'tails': data_list[0].get('tails', []) if data_list else [],
                        'numbers': data_list[0].get('numbers', []) if data_list else [],
                        'timestamp': datetime.now()
                    }
                
                    anti_manipulation_analysis = self.banker_behavior_analyzer.analyze_period(
                        current_period, data_list[1:] if len(data_list) > 1 else []
                    )
                
                    print(f"ğŸ¯ åæ“æ§åˆ†æ: æ“æ§æ¦‚ç‡={anti_manipulation_analysis.get('manipulation_probability', 0):.2f}")
                
                    # è·å–åæ“æ§å»ºè®®
                    anti_recommendations = self.banker_behavior_analyzer.get_anti_manipulation_recommendations(data_list)
                    print(f"ğŸ¯ åæ“æ§å»ºè®®: æ¨è{anti_recommendations.get('recommended_tails', [])} é¿å¼€{anti_recommendations.get('avoid_tails', [])}")
                
                except Exception as e:
                    print(f"âš ï¸ åæ“æ§åˆ†æå¤±è´¥: {e}")
                    anti_manipulation_analysis = None

            # === åå‘å¿ƒç†å­¦åˆ†æ ===
            reverse_psychology_analysis = None
            if hasattr(self, 'reverse_psychology_predictor') and self.reverse_psychology_predictor:
                try:
                    reverse_psychology_analysis = self.reverse_psychology_predictor.predict(
                        period_data={'tails': data_list[0].get('tails', []) if data_list else []},
                        historical_context=data_list[1:] if len(data_list) > 1 else []
                    )
                    
                    recommended_tails = reverse_psychology_analysis.get('recommended_tails', [])
                    avoid_tails = reverse_psychology_analysis.get('avoid_tails', [])
                    confidence = reverse_psychology_analysis.get('confidence', 0.0)
                    strategy_type = reverse_psychology_analysis.get('strategy_type', 'unknown')
                    
                    print(f"ğŸ”„ åå‘å¿ƒç†å­¦åˆ†æ: ç­–ç•¥={strategy_type}, ç½®ä¿¡åº¦={confidence:.2f}")
                    print(f"ğŸ”„ åå‘å¿ƒç†å­¦å»ºè®®: æ¨è{recommended_tails} é¿å¼€{avoid_tails}")
                    
                except Exception as e:
                    print(f"âš ï¸ åå‘å¿ƒç†å­¦åˆ†æå¤±è´¥: {e}")
                    reverse_psychology_analysis = None

            # === å†·é—¨æŒ–æ˜å™¨åˆ†æ ===
            unpopular_digger_analysis = None
            if hasattr(self, 'unpopular_digger') and self.unpopular_digger:
                try:
                    # å†·é—¨æŒ–æ˜å™¨éœ€è¦å€™é€‰å°¾æ•°ä½œä¸ºè¾“å…¥
                    if 'valid_candidates' in locals() and valid_candidates:
                        candidate_tails_list = list(valid_candidates)
                    else:
                        candidate_tails_list = list(range(10))  # å¦‚æœæ²¡æœ‰å€™é€‰å°¾æ•°ï¼Œä½¿ç”¨æ‰€æœ‰å°¾æ•°
        
                    unpopular_digger_analysis = self.unpopular_digger.predict(
                        candidate_tails_list, 
                        data_list
                    )
        
                    if unpopular_digger_analysis.get('success'):
                        recommended_cold_tails = unpopular_digger_analysis.get('recommended_tails', [])
                        confidence = unpopular_digger_analysis.get('confidence', 0.0)
            
                        print(f"ğŸ” å†·é—¨æŒ–æ˜å™¨åˆ†æ: ç½®ä¿¡åº¦={confidence:.2f}")
                        print(f"ğŸ” å†·é—¨æŒ–æ˜å»ºè®®: æ¨èå†·é—¨å°¾æ•°{recommended_cold_tails}")
            
                        # å°†å†·é—¨æŒ–æ˜å™¨çš„å»ºè®®æ·»åŠ åˆ°é¢„æµ‹ä¸­
                        if recommended_cold_tails:
                            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´å¦‚ä½•ä½¿ç”¨å†·é—¨æŒ–æ˜å™¨çš„å»ºè®®
                            print(f"ğŸ” å†·é—¨æŒ–æ˜å™¨å‘ç°äº† {len(recommended_cold_tails)} ä¸ªå†·é—¨æœºä¼š")
                    else:
                        print(f"ğŸ” å†·é—¨æŒ–æ˜å™¨åˆ†æ: æ— æœ‰æ•ˆçš„å†·é—¨æŒ–æ˜æœºä¼š")
        
                except Exception as e:
                    print(f"âš ï¸ å†·é—¨æŒ–æ˜å™¨åˆ†æå¤±è´¥: {e}")
                    unpopular_digger_analysis = None

            # === åº”ç”¨ä¸‰å¤§å®šå¾‹ç­›é€‰å€™é€‰å°¾æ•° ===
            if data_list:
                # è¯†åˆ«éœ€è¦æ’é™¤çš„å°¾æ•°
                trap_tails = self.fundamental_laws.identify_comprehensive_trap_tails(data_list)
                dual_minimum_tails = self.fundamental_laws.identify_dual_minimum_tails(data_list)
                extremely_hot_tails = self.fundamental_laws.identify_extremely_hot_tails(data_list, periods=30, threshold=20)
                
                # è®¡ç®—é€šè¿‡ç­›é€‰çš„å€™é€‰å°¾æ•°
                excluded_tails = trap_tails.union(dual_minimum_tails).union(extremely_hot_tails)
                valid_candidates = all_tails - excluded_tails
                
                # æ˜¾ç¤ºç­›é€‰è¿‡ç¨‹
                print(f"ğŸ” ä¸‰å¤§å®šå¾‹ç­›é€‰è¿‡ç¨‹ï¼š")
                if trap_tails:
                    print(f"  â€¢ å®šå¾‹2æ’é™¤é™·é˜±å°¾æ•°ï¼š{sorted(trap_tails)}")
                if dual_minimum_tails:
                    print(f"  â€¢ å®šå¾‹3æ’é™¤åŒé‡æœ€å°‘å°¾æ•°ï¼š{sorted(dual_minimum_tails)}")
                if extremely_hot_tails:
                    print(f"  â€¢ å®šå¾‹4æ’é™¤æçƒ­å°¾æ•°ï¼š{sorted(extremely_hot_tails)}")
                print(f"  â€¢ é€šè¿‡ç­›é€‰çš„å€™é€‰å°¾æ•°ï¼š{sorted(valid_candidates)}")
                
                # å¦‚æœæ²¡æœ‰å€™é€‰å°¾æ•°ï¼Œè¿”å›æ— ç»“æœ
                if not valid_candidates:
                    print(f"âš ï¸ æ‰€æœ‰å°¾æ•°éƒ½è¢«ä¸‰å¤§å®šå¾‹æ’é™¤ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
                    return {
                        'success': True,
                        'recommended_tails': [],
                        'confidence': 0.0,
                        'ensemble_probabilities': {},
                        'model_count': len(all_predictions),
                        'message': 'æ‰€æœ‰å€™é€‰å°¾æ•°éƒ½è¢«ä¸‰å¤§å®šå¾‹æ’é™¤',
                        'exclusion_reason': {
                            'trap_tails': sorted(trap_tails),
                            'dual_minimum_tails': sorted(dual_minimum_tails),
                            'extremely_hot_tails': sorted(extremely_hot_tails)
                        }
                    }
                    
                # ä½¿ç”¨ç­›é€‰åçš„å€™é€‰å°¾æ•°ä½œä¸ºé¢„æµ‹ç›®æ ‡
                prediction_targets = valid_candidates
            else:
                prediction_targets = all_tails
                valid_candidates = all_tails

            print(f"ğŸ¯ æœ€ç»ˆé¢„æµ‹ç›®æ ‡å°¾æ•°ï¼š{sorted(prediction_targets)}")

            # === èµ„é‡‘æµå‘åˆ†æ ===
            money_flow_analysis = None
            if hasattr(self, 'money_flow_analyzer') and self.money_flow_analyzer:
                try:
                    # ä½¿ç”¨ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°è¿›è¡Œåˆ†æ
                    candidate_tails_list = list(prediction_targets)
        
                    money_flow_analysis = self.money_flow_analyzer.analyze_money_flow(
                        candidate_tails_list, 
                        data_list
                    )
        
                    if money_flow_analysis.get('success'):
                        recommended_flow_tails = money_flow_analysis.get('recommended_tails', [])
                        avoid_flow_tails = money_flow_analysis.get('avoid_tails', [])
                        confidence = money_flow_analysis.get('confidence', 0.0)
            
                        print(f"ğŸ’° èµ„é‡‘æµå‘åˆ†æ: ç½®ä¿¡åº¦={confidence:.2f}")
                        print(f"ğŸ’° èµ„é‡‘æµå‘å»ºè®®: æ¨è{recommended_flow_tails} é¿å¼€{avoid_flow_tails}")
            
                        # å°†èµ„é‡‘æµå‘åˆ†æçš„å»ºè®®æ·»åŠ åˆ°é¢„æµ‹ä¸­
                        if recommended_flow_tails:
                            print(f"ğŸ’° èµ„é‡‘æµå‘åˆ†æå‘ç°äº† {len(recommended_flow_tails)} ä¸ªæ¨èæœºä¼š")
                    else:
                        print(f"ğŸ’° èµ„é‡‘æµå‘åˆ†æ: æ— æœ‰æ•ˆçš„åˆ†æç»“æœ")
        
                except Exception as e:
                    print(f"âš ï¸ èµ„é‡‘æµå‘åˆ†æå¤±è´¥: {e}")
                    money_flow_analysis = None

            # === åº”ç”¨åæ“æ§é¿å¼€å»ºè®®è¿›ä¸€æ­¥ç­›é€‰ï¼ˆç²¾å‡†ç‰ˆï¼‰ ===
            if ('anti_manipulation_analysis' in locals() and anti_manipulation_analysis and 
                'anti_recommendations' in locals() and anti_recommendations and 
                anti_recommendations.get('avoid_tails')):
    
                # æ£€æŸ¥ä¸‰å¤§å®šå¾‹ç­›é€‰åçš„å€™é€‰å°¾æ•°æ•°é‡
                if len(prediction_targets) <= 1:
                    print(f"â„¹ï¸ åæ“æ§è·³è¿‡ï¼šä¸‰å¤§å®šå¾‹ç­›é€‰ååªå‰©{len(prediction_targets)}ä¸ªå€™é€‰å°¾æ•°ï¼Œä¸è¿›è¡Œæ“ä½œ")
                else:
                    avoid_tails_recommendations = set(anti_recommendations.get('avoid_tails', []))
                    original_candidates = prediction_targets.copy()
        
                    # åªè€ƒè™‘ä¸‰å¤§å®šå¾‹ç­›é€‰åçš„å€™é€‰å°¾æ•°ä¸­ä¸é¿å¼€å»ºè®®é‡åˆçš„éƒ¨åˆ†
                    overlapping_tails = prediction_targets.intersection(avoid_tails_recommendations)
        
                    if overlapping_tails:
                        # æ™ºèƒ½é€‰æ‹©æœ€åº”è¯¥é¿å¼€çš„ä¸€ä¸ªå°¾æ•°
                        if len(overlapping_tails) == 1:
                            target_to_exclude = list(overlapping_tails)[0]
                        else:
                            target_to_exclude = self._select_most_dangerous_tail(
                                overlapping_tails, anti_manipulation_analysis, data_list
                            )
            
                        # åªæ’é™¤é€‰ä¸­çš„ä¸€ä¸ªå°¾æ•°
                        prediction_targets = prediction_targets - {target_to_exclude}
                        valid_candidates = prediction_targets
            
                        print(f"ğŸš« åæ“æ§ç­›é€‰ï¼šæ’é™¤å°¾æ•°{target_to_exclude}ï¼Œä¿ç•™{sorted(prediction_targets)}")
            
                        # å¦‚æœç²¾å‡†ç­›é€‰åæ²¡æœ‰å€™é€‰å°¾æ•°äº†ï¼Œæ¢å¤æœ€å®‰å…¨çš„ä¸€ä¸ª
                        if not prediction_targets:
                            safest_tail = self._select_safest_tail(original_candidates, anti_recommendations)
                            prediction_targets = {safest_tail}
                            valid_candidates = prediction_targets
                            print(f"âš ï¸ ç­›é€‰åæ— å€™é€‰å°¾æ•°ï¼Œæ¢å¤æœ€å®‰å…¨å°¾æ•°ï¼š{safest_tail}")
                    else:
                        print(f"â„¹ï¸ åæ“æ§åˆ†æï¼šå€™é€‰å°¾æ•°ä¸é¿å¼€å»ºè®®æ— é‡åˆï¼Œè·³è¿‡ç­›é€‰")
            else:
                print(f"â„¹ï¸ åæ“æ§åˆ†ææ— é¿å¼€å»ºè®®æˆ–åˆ†æå¤±è´¥ï¼Œè·³è¿‡åæ“æ§ç­›é€‰")

            # åˆ†ææ¨¡å‹é¢„æµ‹å¤šæ ·æ€§
            if prediction_targets:
                target_tail = next(iter(prediction_targets))  # å–ç¬¬ä¸€ä¸ªå€™é€‰å°¾æ•°åˆ†æ
                tail_predictions = []
                for model_key, predictions in all_predictions.items():
                    if target_tail in predictions:
                        tail_predictions.append(predictions[target_tail])
                
                if len(tail_predictions) > 1:
                    import statistics
                    pred_mean = statistics.mean(tail_predictions)
                    pred_std = statistics.stdev(tail_predictions) if len(tail_predictions) > 1 else 0
                    print(f"ğŸ“Š å°¾æ•°{target_tail}é¢„æµ‹å¤šæ ·æ€§: å‡å€¼{pred_mean:.3f}, æ ‡å‡†å·®{pred_std:.3f}")
                    
                    # æ˜¾ç¤ºé¢„æµ‹åˆ†å¸ƒ
                    high_prob_models = [k for k, v in all_predictions.items() if v.get(target_tail, 0) > 0.6]
                    low_prob_models = [k for k, v in all_predictions.items() if v.get(target_tail, 0) < 0.4]
                    print(f"ğŸ“ˆ é«˜æ¦‚ç‡æ¨¡å‹({len(high_prob_models)}ä¸ª): {high_prob_models[:3]}" + ("..." if len(high_prob_models) > 3 else ""))
                    print(f"ğŸ“‰ ä½æ¦‚ç‡æ¨¡å‹({len(low_prob_models)}ä¸ª): {low_prob_models[:3]}" + ("..." if len(low_prob_models) > 3 else ""))

                    # æ˜¾ç¤ºä¸åŒå†³ç­–ç­–ç•¥çš„åˆ†å¸ƒ
                    strategy_distribution = {}
                    for model_key in all_predictions.keys():
                        if model_key.startswith('river_'):
                            river_model_name = model_key.replace('river_', '')
                            strategy = model_decision_strategies.get(river_model_name, {})
                            focus = strategy.get('focus', 'unknown')
                            if focus not in strategy_distribution:
                                strategy_distribution[focus] = []
                            if target_tail in all_predictions[model_key]:
                                strategy_distribution[focus].append(all_predictions[model_key][target_tail])
                    
                    print(f"ğŸ¯ ä¸åŒå†³ç­–ç­–ç•¥çš„é¢„æµ‹åˆ†å¸ƒ:")
                    for strategy, probs in strategy_distribution.items():
                        if probs:
                            avg_prob = sum(probs) / len(probs)
                            print(f"   â€¢ {strategy}: {len(probs)}ä¸ªæ¨¡å‹, å¹³å‡æ¦‚ç‡{avg_prob:.3f}")

            # éªŒè¯å’Œæ¸…ç†ç‰¹å¾æ•°æ®ï¼Œé¿å…math domain error
            def clean_features_for_river(features_dict):
                """æ¸…ç†ç‰¹å¾æ•°æ®ä»¥é¿å…æ•°å­¦åŸŸé”™è¯¯"""
                cleaned_features = {}
                for key, value in features_dict.items():
                    try:
                        if value is None:
                            cleaned_features[key] = 0.0
                        elif isinstance(value, (int, float)):
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°å€¼
                            if math.isnan(value) or math.isinf(value):
                                cleaned_features[key] = 0.0
                            elif value < 0:
                                cleaned_features[key] = 0.0  # ç¡®ä¿éè´Ÿ
                            else:
                                cleaned_features[key] = float(value)
                        elif isinstance(value, (list, tuple, np.ndarray)):
                            # å¦‚æœæ˜¯æ•°ç»„ç±»å‹ï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼æˆ–0
                            try:
                                if len(value) > 0:
                                    first_val = float(value[0])
                                    if math.isnan(first_val) or math.isinf(first_val):
                                        cleaned_features[key] = 0.0
                                    else:
                                        cleaned_features[key] = max(0.0, first_val)
                                else:
                                    cleaned_features[key] = 0.0
                            except (ValueError, TypeError, IndexError):
                                cleaned_features[key] = 0.0
                        else:
                            # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                            try:
                                converted_val = float(value)
                                if math.isnan(converted_val) or math.isinf(converted_val):
                                    cleaned_features[key] = 0.0
                                else:
                                    cleaned_features[key] = max(0.0, converted_val)
                            except (ValueError, TypeError):
                                cleaned_features[key] = 0.0
                    except Exception as e:
                        print(f"   âš ï¸ æ¸…ç†ç‰¹å¾ {key} æ—¶å‡ºé”™: {e}")
                        cleaned_features[key] = 0.0
                
                # éªŒè¯æ¸…ç†ç»“æœ
                for key, value in cleaned_features.items():
                    if not isinstance(value, (int, float)) or math.isnan(value) or math.isinf(value):
                        print(f"   âŒ æ¸…ç†åçš„ç‰¹å¾ {key} ä»ç„¶æ— æ•ˆ: {value}")
                        cleaned_features[key] = 0.0
                
                return cleaned_features
        
            # æ¸…ç†Riveræ¨¡å‹çš„è¾“å…¥ç‰¹å¾
            X_river_cleaned = clean_features_for_river(X_river)

            # === Riveræ¨¡å‹é¢„æµ‹ ===
            model_decision_records = {}  # è®°å½•æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†å†³ç­–è¿‡ç¨‹
            
            for model_name, model in self.river_models.items():
                try:
                    model_predictions = {}
                    decision_record = {
                        'model_type': 'river',
                        'model_name': model_name,
                        'detailed_reasons': {},
                        'internal_states': {},
                        'decision_process': []
                    }
                    
                    # ç‰¹æ®Šå¤„ç†å†å²æ¨¡å¼åŒ¹é…ç®—æ³•
                    if hasattr(model, 'update_historical_data'):
                        # æ›´æ–°å†å²æ•°æ®
                        model.update_historical_data(data_list)
                        
                        # è·å–è¯¦ç»†çš„åŒ¹é…åˆ†æ
                        if hasattr(model, 'get_detailed_matching_analysis'):
                            # ä½¿ç”¨å½“å‰æœŸçš„å°¾æ•°è¿›è¡ŒåŒ¹é…åˆ†æ
                            current_period_tails = data_list[0].get('tails', []) if data_list else []
                            matching_analysis = model.get_detailed_matching_analysis(list(current_period_tails))
                            decision_record['matching_analysis'] = matching_analysis
                            decision_record['decision_process'].append("å†å²æ¨¡å¼åŒ¹é…åˆ†æå®Œæˆ")
    
                            # ä¿å­˜åˆ†æç»“æœåˆ°æ¨¡å‹çš„ _last_detailed_analysis å±æ€§
                            model._last_detailed_analysis = matching_analysis
                        
                        # æ£€æŸ¥ç®—æ³•æ˜¯å¦åº”è¯¥å‚ä¸æœ¬è½®é¢„æµ‹
                        if hasattr(model, 'should_participate_in_ensemble'):
                            should_participate = model.should_participate_in_ensemble(list(prediction_targets))
                        else:
                            should_participate = True
                        
                        if should_participate:
                            # è·å–åŸºäºåŒ¹é…è´¨é‡çš„æ¦‚ç‡é¢„æµ‹
                            if hasattr(model, 'predict_probabilities_for_candidates'):
                                probabilities = model.predict_probabilities_for_candidates(list(prediction_targets))
                                
                                # è®°å½•è¯¦ç»†çš„é¢„æµ‹è¿‡ç¨‹
                                decision_record['prediction_probabilities'] = probabilities
                                decision_record['decision_process'].append(f"è®¡ç®—å‡º{len(probabilities)}ä¸ªå€™é€‰å°¾æ•°çš„æ¦‚ç‡")
                                
                                # ä½¿ç”¨è®¡ç®—å‡ºçš„æ¦‚ç‡
                                best_tail = None
                                best_probability = 0.0
                        
                                for tail in prediction_targets:
                                    probability = probabilities.get(tail, 0.5)
                                    model_predictions[tail] = probability
                            
                                    # è®°å½•æ¯ä¸ªå°¾æ•°çš„è¯¦ç»†å†³ç­–ç†ç”±
                                    if hasattr(model, 'get_tail_decision_reason'):
                                        detailed_reason = model.get_tail_decision_reason(tail, probabilities)
                                        decision_record['detailed_reasons'][tail] = detailed_reason
                            
                                    # æ‰¾åˆ°æœ€ä½³é¢„æµ‹å°¾æ•°
                                    if probability > best_probability:
                                        best_probability = probability
                                        best_tail = tail
                        
                                # æ”¶é›†é¢„æµ‹è®°å½•ï¼Œç¨åç»Ÿä¸€å¤„ç†ï¼ˆé¿å…é‡å¤è®°å½•ï¼‰
                                if best_tail is not None:
                                    predicted_class = 1 if best_probability > 0.5 else 0
                                    if 'predictions_to_record' not in locals():
                                        predictions_to_record = []
                                    predictions_to_record.append({
                                        'model_name': f'river_{model_name}',
                                        'predicted_class': predicted_class,
                                        'confidence': best_probability,
                                        'target_tail': best_tail
                                    })
                                
                                selected_tails = [t for t, p in probabilities.items() if p != 0.5]
                                decision_record['decision_process'].append(f"æœ€ç»ˆé€‰æ‹©å‚ä¸é¢„æµ‹çš„å°¾æ•°ï¼š{selected_tails}")
                                print(f"ğŸ¯ å†å²æ¨¡å¼åŒ¹é…(100%ç›¸ä¼¼åº¦) å‚ä¸é¢„æµ‹ï¼Œé€‰æ‹©äº†å°¾æ•°ï¼š{selected_tails}")
                            else:
                                # å›é€€åˆ°æŠ•ç¥¨æ¨¡å¼ï¼ˆå…¼å®¹æ€§ï¼‰
                                votes = model.predict_for_candidates(list(prediction_targets))
                                decision_record['votes'] = votes
                                decision_record['decision_process'].append("ä½¿ç”¨æŠ•ç¥¨æ¨¡å¼é¢„æµ‹")
                                
                                # è½¬æ¢ä¸ºæ¦‚ç‡æ ¼å¼ï¼Œæ‰¾åˆ°æœ€ä½³æŠ•ç¥¨ç»“æœ
                                best_tail = None
                                best_vote = -1
                                best_probability = 0.0
                                
                                for tail in prediction_targets:
                                    vote = votes.get(tail, 0)
                                    probability = 0.8 if vote > 0 else 0.2
                                    model_predictions[tail] = probability
                                    
                                    # æ‰¾åˆ°æœ€ä½³æŠ•ç¥¨ç»“æœ
                                    if vote > best_vote:
                                        best_vote = vote
                                        best_tail = tail
                                        best_probability = probability
                                
                                # åªè®°å½•æœ€ä½³æŠ•ç¥¨ç»“æœåˆ°æ•°æ®åº“
                                if best_tail is not None:
                                    predicted_class = 1 if best_vote > 0 else 0
                                    self._record_model_prediction(
                                        f'river_{model_name}', 
                                        predicted_class, 
                                        best_probability, 
                                        best_tail
                                    )
                        else:
                            # ç®—æ³•ä¸å‚ä¸æœ¬è½®é¢„æµ‹
                            decision_record['decision_process'].append("æœªæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„å†å²æ¨¡å¼ï¼Œä¸å‚ä¸æœ¬è½®é¢„æµ‹")
                            print(f"ğŸš« å†å²æ¨¡å¼åŒ¹é…(100%ç›¸ä¼¼åº¦) ä¸å‚ä¸æœ¬è½®é¢„æµ‹ï¼šæœªæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„å†å²æ¨¡å¼")
                            model_decision_records[f'river_{model_name}'] = decision_record
                            continue
                    else:
                        # æ™®é€šæ¨¡å‹é¢„æµ‹
                        decision_record['decision_process'].append("å¼€å§‹æ™®é€šæ¨¡å‹é¢„æµ‹")
                        
                        # è®°å½•æ¨¡å‹å†…éƒ¨çŠ¶æ€ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if hasattr(model, 'get_internal_state'):
                            try:
                                internal_state = model.get_internal_state()
                                decision_record['internal_states'] = internal_state
                            except:
                                pass
                        
                        # åªå¯¹é€šè¿‡å››å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°è¿›è¡Œé¢„æµ‹
                        for tail in prediction_targets:
                            prediction_details = {}
                            
                            # æå–æ›´å¤šç‰¹å¾ç”¨äºåˆ†æ
                            tail_features = self.feature_engineer.extract_tail_specific_features(data_list, tail)
                            prediction_details['tail_features'] = tail_features
                            
                            if hasattr(model, 'predict_proba_one'):
                                try:
                                    # ä½¿ç”¨è¯¥æ¨¡å‹çš„ä¸“å±ç‰¹å¾
                                    model_X_river = model_specific_features.get(model_name, X_river)
                            
                                    # å¯¹æ‰€æœ‰æ¨¡å‹éƒ½è¿›è¡Œç‰¹å¾æ¸…ç†
                                    if isinstance(model_X_river, dict):
                                        model_X_river_cleaned = clean_features_for_river(model_X_river)
                                    else:
                                        # å¦‚æœä¸æ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—å…¸
                                        if isinstance(model_X_river, (list, tuple, np.ndarray)):
                                            temp_dict = {}
                                            for i, val in enumerate(model_X_river[:60]):  # é™åˆ¶æœ€å¤š60ä¸ªç‰¹å¾
                                                temp_dict[f'feature_{i}'] = val
                                            model_X_river_cleaned = clean_features_for_river(temp_dict)
                                        else:
                                            model_X_river_cleaned = clean_features_for_river(X_river_cleaned)
                            
                                    # éªŒè¯æ¸…ç†åçš„ç‰¹å¾
                                    if not model_X_river_cleaned:
                                        print(f"   âš ï¸ æ¨¡å‹ {model_name} æ¸…ç†åç‰¹å¾ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
                                        model_X_river_cleaned = {f'feature_{i}': 0.0 for i in range(10)}
                            
                                    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾å€¼éƒ½æ˜¯æœ‰æ•ˆçš„æµ®ç‚¹æ•°
                                    validated_features = {}
                                    for key, value in model_X_river_cleaned.items():
                                        try:
                                            validated_value = float(value)
                                            if math.isnan(validated_value) or math.isinf(validated_value):
                                                validated_features[key] = 0.0
                                            else:
                                                validated_features[key] = validated_value
                                        except (ValueError, TypeError):
                                            validated_features[key] = 0.0
                            
                                    print(f"   ğŸ” æ¨¡å‹ {model_name} ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(validated_features)}")
                            
                                    # æ‰§è¡Œé¢„æµ‹
                                    proba = model.predict_proba_one(validated_features)
                                    
                                    # å¤„ç†ä¸åŒçš„æ¦‚ç‡è¾“å‡ºæ ¼å¼
                                    if isinstance(proba, dict):
                                        prob_1 = proba.get(1, proba.get(True, 0.5))
                                        
                                        # åº”ç”¨æ¨¡å‹ç‰¹å®šçš„å†³ç­–ç­–ç•¥
                                        strategy = model_decision_strategies.get(model_name, {})
                                        focus = strategy.get('focus', 'frequency_analysis')
                                        threshold = strategy.get('decision_threshold', 0.5)
                                        time_window = strategy.get('time_window', 10)
                                        weight_recent = strategy.get('weight_recent', 1.0)
                                        
                                        # æ ¹æ®ä¸åŒç­–ç•¥é‡æ–°è®¡ç®—æ¦‚ç‡
                                        if focus == 'trend_continuation':
                                            # è¶‹åŠ¿å»¶ç»­ç­–ç•¥ï¼šå¦‚æœå°¾æ•°åœ¨æœ€è¿‘æœŸå‡ºç°ï¼Œå¤§å¹…æå‡æ¦‚ç‡
                                            if tail_features.get('in_latest_period', False):
                                                consecutive = tail_features.get('consecutive_appearances', 0)
                                                if consecutive >= 2:
                                                    prob_1 = 0.85  # å¼ºçƒˆå»¶ç»­ä¿¡å·
                                                elif consecutive >= 1:
                                                    prob_1 = 0.72  # ä¸­ç­‰å»¶ç»­ä¿¡å·
                                                else:
                                                    prob_1 = 0.58  # å¼±å»¶ç»­ä¿¡å·
                                            else:
                                                prob_1 = 0.25  # ä¸ç¬¦åˆå»¶ç»­ç­–ç•¥
                                        
                                        elif focus == 'trend_reversal':
                                            # è¶‹åŠ¿åè½¬ç­–ç•¥ï¼šå¦‚æœå°¾æ•°é•¿æœŸæœªå‡ºç°ï¼Œæå‡æ¦‚ç‡
                                            last_distance = tail_features.get('last_appearance_distance', -1)
                                            if last_distance >= 5:
                                                prob_1 = 0.78  # å¼ºçƒˆåè½¬ä¿¡å·
                                            elif last_distance >= 3:
                                                prob_1 = 0.65  # ä¸­ç­‰åè½¬ä¿¡å·
                                            elif last_distance >= 1:
                                                prob_1 = 0.52  # å¼±åè½¬ä¿¡å·
                                            else:
                                                prob_1 = 0.28  # ä¸ç¬¦åˆåè½¬ç­–ç•¥
                                        
                                        elif focus == 'frequency_analysis':
                                            # é¢‘ç‡åˆ†æç­–ç•¥ï¼šåŸºäºé¢‘ç‡è®¡ç®—
                                            recent_freq = tail_features.get('recent_10_frequency', 0.5)
                                            if recent_freq >= 0.6:
                                                prob_1 = 0.68
                                            elif recent_freq >= 0.4:
                                                prob_1 = 0.55
                                            elif recent_freq >= 0.2:
                                                prob_1 = 0.42
                                            else:
                                                prob_1 = 0.32
                                        
                                        elif focus == 'independence_assumption':
                                            # ç‹¬ç«‹æ€§å‡è®¾ï¼šåŸºäºå…ˆéªŒæ¦‚ç‡
                                            if tail in data_list[0].get('tails', []):
                                                prob_1 = 0.35  # ç‹¬ç«‹æ€§å‡è®¾ä¸‹ï¼Œæœ€è¿‘å‡ºç°é™ä½æ¦‚ç‡
                                            else:
                                                prob_1 = 0.65  # æœªå‡ºç°åˆ™æå‡æ¦‚ç‡
                                        
                                        elif focus == 'categorical_patterns':
                                            # åˆ†ç±»æ¨¡å¼ï¼šåŸºäºå°¾æ•°ç±»åˆ«
                                            if tail in [0, 5]:  # 0å’Œ5ç»“å°¾
                                                prob_1 = 0.62
                                            elif tail in [1, 3, 7, 9]:  # å¥‡æ•°
                                                prob_1 = 0.58
                                            else:  # å¶æ•°
                                                prob_1 = 0.48
                                        
                                        elif focus == 'continuous_distribution':
                                            # è¿ç»­åˆ†å¸ƒï¼šåŸºäºæ•°å€¼åˆ†å¸ƒç‰¹æ€§
                                            recent_count = tail_features.get('recent_5_count', 0)
                                            if recent_count == 0:
                                                prob_1 = 0.70  # è¡¥ç¼º
                                            elif recent_count == 1:
                                                prob_1 = 0.55
                                            elif recent_count == 2:
                                                prob_1 = 0.45
                                            else:
                                                prob_1 = 0.30  # è¿‡çƒ­
                                        
                                        elif focus == 'hybrid_approach':
                                            # æ··åˆæ–¹æ³•ï¼šå¤šå› ç´ æƒè¡¡
                                            score = 0.5
                                            if tail_features.get('in_latest_period', False):
                                                score += 0.15
                                            if tail_features.get('recent_5_count', 0) >= 2:
                                                score += 0.1
                                            if tail_features.get('last_appearance_distance', 0) >= 3:
                                                score += 0.2
                                            prob_1 = min(0.85, max(0.15, score))
                                        
                                        elif focus == 'ensemble_voting':
                                            # é›†æˆæŠ•ç¥¨ï¼šæ¨¡æ‹Ÿå†…éƒ¨æŠ•ç¥¨
                                            votes = 0
                                            if tail_features.get('in_latest_period', False):
                                                votes += 2
                                            if tail_features.get('recent_10_count', 0) >= 4:
                                                votes += 1
                                            if tail_features.get('consecutive_appearances', 0) >= 1:
                                                votes += 1
                                            if tail_features.get('last_appearance_distance', 0) >= 4:
                                                votes += 2
                                            
                                            prob_1 = 0.2 + (votes / 6.0) * 0.6  # åŸºç¡€0.2 + æŠ•ç¥¨æƒé‡
                                        
                                        elif focus == 'error_correction':
                                            # é”™è¯¯ä¿®æ­£ï¼šåŸºäºå†å²å‡†ç¡®æ€§è°ƒæ•´
                                            base_prob = prob_1
                                            # æ¨¡æ‹Ÿé”™è¯¯ä¿®æ­£é€»è¾‘
                                            if tail_features.get('recent_5_count', 0) == 0:
                                                prob_1 = base_prob * 1.4  # ä¿®æ­£é¢„æµ‹
                                            else:
                                                prob_1 = base_prob * 0.8  # ä¿å®ˆé¢„æµ‹
                                        
                                        elif focus == 'probabilistic_ensemble':
                                            # æ¦‚ç‡é›†æˆï¼šåŸºäºæ¦‚ç‡ç»„åˆ
                                            prob_components = []
                                            prob_components.append(0.3 if tail_features.get('in_latest_period', False) else 0.7)
                                            prob_components.append(tail_features.get('recent_10_frequency', 0.5))
                                            prob_components.append(0.8 if tail_features.get('last_appearance_distance', 0) >= 3 else 0.2)
                                            
                                            prob_1 = sum(prob_components) / len(prob_components)
                                        
                                        elif focus == 'linear_ensemble':
                                            # çº¿æ€§é›†æˆï¼šçº¿æ€§ç»„åˆç‰¹å¾
                                            linear_score = 0
                                            linear_score += tail_features.get('recent_10_frequency', 0.5) * 0.3
                                            linear_score += (1.0 if tail_features.get('in_latest_period', False) else 0.0) * 0.4
                                            linear_score += min(1.0, tail_features.get('last_appearance_distance', 0) / 5.0) * 0.3
                                            
                                            prob_1 = linear_score
                                        
                                        elif focus == 'historical_matching':
                                            # å†å²åŒ¹é…ï¼šä¿æŒåŸæœ‰æ¦‚ç‡ï¼Œä¸åšè°ƒæ•´
                                            pass
                                        
                                        # ç¡®ä¿æ¦‚ç‡åœ¨åˆç†èŒƒå›´å†…
                                        prob_1 = max(0.01, min(0.99, prob_1))
                                        model_predictions[tail] = prob_1

                                        # é€»è¾‘å›å½’ï¼šå¢å¼ºé¢‘ç‡ç‰¹å¾æƒé‡
                                        recent_freq = tail_features.get('recent_10_frequency', 0.5)
                                        if recent_freq > 0.6:
                                            prob_1 *= 1.05
                                        elif recent_freq < 0.3:
                                            prob_1 *= 0.95
                                        elif 'naive_bayes' in model_name:
                                            # æœ´ç´ è´å¶æ–¯ï¼šåŸºäºç‹¬ç«‹æ€§å‡è®¾è°ƒæ•´
                                            if tail_features.get('in_latest_period', False):
                                                prob_1 *= 1.08
                                            else:
                                                prob_1 *= 0.92
                                        elif 'bagging' in model_name:
                                            # è£…è¢‹æ¨¡å‹ï¼šåŸºäºé›†æˆæŠ•ç¥¨è°ƒæ•´
                                            if tail_features.get('recent_5_count', 0) >= 3:
                                                prob_1 *= 1.03  # ç¡®å®šæ€§å¢å¼º
                                            elif tail_features.get('recent_5_count', 0) <= 1:
                                                prob_1 *= 0.97  # ç¡®å®šæ€§é™ä½
                                        
                                        # ç¡®ä¿æ¦‚ç‡åœ¨åˆç†èŒƒå›´å†…
                                        prob_1 = max(0.01, min(0.99, prob_1))
                                        model_predictions[tail] = prob_1
                                        prediction_details['final_probability'] = prob_1
                                        prediction_details['probability_source'] = "æ¨¡å‹æ¦‚ç‡è¾“å‡º"
                                        
                                        # ç”Ÿæˆè¯¦ç»†çš„å†³ç­–åˆ†æ
                                        detailed_reason = self.prediction_analyzer.generate_model_specific_reason(
                                            model_name, tail, prob_1, tail_features, data_list
                                        )
                                        prediction_details['detailed_analysis'] = detailed_reason
                                        
                                        # æš‚å­˜é¢„æµ‹ç»“æœï¼Œç¨åç»Ÿä¸€è®°å½•
                                        if 'predictions_to_record' not in locals():
                                            predictions_to_record = []
                        
                                        predicted_class = 1 if prob_1 > 0.5 else 0
                                        predictions_to_record.append({
                                            'model_name': f'river_{model_name}',
                                            'predicted_class': predicted_class,
                                            'confidence': prob_1,
                                            'target_tail': tail
                                        })
                                    else:
                                        model_predictions[tail] = 0.5
                                        prediction_details['final_probability'] = 0.5
                                        prediction_details['detailed_analysis'] = "æ¨¡å‹è¾“å‡ºæ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•è§£ææ¦‚ç‡"
                                        
                                except Exception as pred_e:
                                    model_predictions[tail] = 0.5
                                    prediction_details['error'] = str(pred_e)
                                    prediction_details['detailed_analysis'] = f"é¢„æµ‹è¿‡ç¨‹å‡ºé”™ï¼š{str(pred_e)}"
                            else:
                                model_predictions[tail] = 0.5
                                prediction_details['detailed_analysis'] = "æ¨¡å‹ä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹ï¼Œä½¿ç”¨é»˜è®¤æ¦‚ç‡0.5"
                            
                            decision_record['detailed_reasons'][tail] = prediction_details
                    
                    all_predictions[f'river_{model_name}'] = model_predictions

                    # æ¯ä¸ªæ¨¡å‹åªè®°å½•ä¸€æ¬¡é¢„æµ‹ï¼ˆé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„å°¾æ•°ï¼‰
                    if 'predictions_to_record' in locals() and predictions_to_record:
                        best_prediction = max(predictions_to_record, key=lambda x: x['confidence'])
                        self._record_model_prediction(
                            best_prediction['model_name'],
                            best_prediction['predicted_class'],
                            best_prediction['confidence'],
                            best_prediction['target_tail']
                        )
                        predictions_to_record = []  # æ¸…ç©ºè®°å½•åˆ—è¡¨
                    model_decision_records[f'river_{model_name}'] = decision_record
                
                except Exception as e:
                    error_msg = str(e)
                    print(f"âŒ Riveræ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {e}")
                    print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                    print(f"   é”™è¯¯è¯¦æƒ…: {error_msg}")
                    
                    # æ£€æŸ¥ç‰¹å®šé”™è¯¯ç±»å‹
                    if 'schema' in error_msg.lower():
                        print(f"   ğŸ” SchemaéªŒè¯å¤±è´¥ï¼Œæ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼")
                        print(f"   ç‰¹å¾æ•°é‡: {len(model_X_river) if isinstance(model_X_river, dict) else 'N/A'}")
                        if isinstance(model_X_river, dict):
                            sample_features = list(model_X_river.items())[:5]
                            print(f"   æ ·æœ¬ç‰¹å¾: {sample_features}")
                    elif 'math domain error' in error_msg:
                        print(f"   ğŸ” æ•°å­¦åŸŸé”™è¯¯ï¼Œç‰¹å¾åŒ…å«æ— æ•ˆæ•°å€¼")
                    elif 'nan' in error_msg.lower() or 'inf' in error_msg.lower():
                        print(f"   ğŸ” æ•°å€¼é”™è¯¯ï¼Œç‰¹å¾åŒ…å«NaNæˆ–æ— ç©·å¤§")
                    
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                    if hasattr(model, '__class__'):
                        print(f"   æ¨¡å‹ç±»å‹: {model.__class__.__name__}")
                    
                    all_predictions[f'river_{model_name}'] = {tail: 0.5 for tail in prediction_targets}
                    model_decision_records[f'river_{model_name}'] = {
                        'model_type': 'river',
                        'model_name': model_name,
                        'error': str(e),
                        'error_type': type(e).__name__,
                        'detailed_reasons': {tail: f"æ¨¡å‹é¢„æµ‹å¤±è´¥ï¼š{str(e)}" for tail in prediction_targets}
                    }
                    
                    model_decision_records[f'river_{model_name}'] = {
                        'model_type': 'river',
                        'model_name': model_name,
                        'error': str(e),
                        'detailed_reasons': {tail: f"æ¨¡å‹é¢„æµ‹å¤±è´¥ï¼š{str(e)}" for tail in prediction_targets}
                    }
            
            # ç»Ÿä¸€è®°å½•Riveræ¨¡å‹é¢„æµ‹ï¼ˆé¿å…é‡å¤ï¼‰
            if 'predictions_to_record' in locals() and predictions_to_record:
                # æ¯ä¸ªæ¨¡å‹åªè®°å½•ä¸€æ¬¡æœ€ä½³é¢„æµ‹
                recorded_models = set()
                for pred in predictions_to_record:
                    if pred['model_name'] not in recorded_models:
                        self._record_model_prediction(
                            pred['model_name'],
                            pred['predicted_class'],
                            pred['confidence'],
                            pred['target_tail']
                        )
                        recorded_models.add(pred['model_name'])
                print(f"ğŸ“Š å·²è®°å½• {len(recorded_models)} ä¸ªRiveræ¨¡å‹çš„é¢„æµ‹")

            # === scikit-multiflowæ¨¡å‹é¢„æµ‹ ===
            # ä¸ºsklearnæ¨¡å‹åˆ›å»ºå¤šæ ·åŒ–ç‰¹å¾
            sklearn_diversified_features = {}
            sklearn_feature_configs = {
                'extremely_fast_tree': {'feature_start': 0, 'feature_end': 48},
                'skm_hoeffding_adaptive': {'feature_start': 5, 'feature_end': 55},
                'online_adaboost': {'feature_start': 10, 'feature_end': 50},
                'skm_random_patches': {'feature_start': 8, 'feature_end': 53}
            }
            
            for model_name in self.sklearn_models.keys():
                config = sklearn_feature_configs.get(model_name, {'feature_start': 0, 'feature_end': len(base_features)})
                
                # ç¡®å®šæ€§åœ°é€‰æ‹©ç‰¹å¾å­é›†
                feature_start = config['feature_start']
                feature_end = min(config['feature_end'], len(base_features))
                
                # åˆ›å»ºè¯¥æ¨¡å‹çš„ç‰¹å¾å‘é‡
                model_features = np.zeros(len(base_features))
                for idx in range(feature_start, feature_end):
                    if idx < len(base_features):
                        base_value = base_features[idx]
                        # ç›´æ¥ä½¿ç”¨åŸºç¡€ç‰¹å¾å€¼ï¼Œæ— å™ªå£°
                        model_features[idx] = base_value
                
                sklearn_diversified_features[model_name] = model_features.reshape(1, -1)

            for model_name, model in self.sklearn_models.items():
                try:
                    model_predictions = {}
                    # ä½¿ç”¨è¯¥æ¨¡å‹çš„ä¸“å±ç‰¹å¾
                    model_X_sklearn = sklearn_diversified_features.get(model_name, X_sklearn)
                    
                    # åªå¯¹é€šè¿‡å››å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°è¿›è¡Œé¢„æµ‹
                    best_tail = None
                    best_confidence = 0.0
                    
                    for tail in prediction_targets:
                        if hasattr(model, 'predict_proba'):
                            proba = model.predict_proba(model_X_sklearn)
                            if proba is not None and len(proba) > 0 and len(proba[0]) > 1:
                                confidence = proba[0][1]
                                model_predictions[tail] = confidence
                                if confidence > best_confidence:
                                    best_confidence = confidence
                                    best_tail = tail
                            else:
                                model_predictions[tail] = 0.5
                        else:
                            model_predictions[tail] = 0.5
                    
                    # åªè®°å½•æœ€ä½³é¢„æµ‹
                    if best_tail is not None:
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            f'sklearn_{model_name}',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
                    
                    all_predictions[f'sklearn_{model_name}'] = model_predictions
                
                except Exception as e:
                    print(f"sklearnæ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {e}")
                    all_predictions[f'sklearn_{model_name}'] = {tail: 0.5 for tail in prediction_targets}

            # === PyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹ ===
            if PYTORCH_AVAILABLE and self.deep_learning_module and self.deep_learning_module.models:
                try:
                    # ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹
                    dl_predictions = self.deep_learning_module.predict_single(base_features)
                
                    for model_name, probabilities in dl_predictions.items():
                        model_predictions = {}
                    
                        # ä¸ºæ¯ä¸ªå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail < len(probabilities):
                                model_predictions[tail] = float(probabilities[tail])
                            else:
                                model_predictions[tail] = 0.5
                    
                        all_predictions[f'pytorch_{model_name}'] = model_predictions
                    
                        # åªè®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“ï¼ˆé¿å…é‡å¤è®°å½•ï¼‰
                        if model_predictions:
                            best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                            best_confidence = model_predictions[best_tail]
                            predicted_class = 1 if best_confidence > 0.5 else 0
                            self._record_model_prediction(
                                f'pytorch_{model_name}',
                                predicted_class,
                                best_confidence,
                                best_tail
                            )
            
                except Exception as e:
                    print(f"PyTorchæ·±åº¦å­¦ä¹ é¢„æµ‹å¤±è´¥: {e}")
                    # ä¸ºå€™é€‰å°¾æ•°æ·»åŠ é»˜è®¤é¢„æµ‹
                    for model_name in ['lstm', 'transformer']:
                        all_predictions[f'pytorch_{model_name}'] = {tail: 0.5 for tail in prediction_targets}

            # === åå‘å¿ƒç†å­¦æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'reverse_psychology_predictor') and self.reverse_psychology_predictor:
                try:
                    model_predictions = {}
                    
                    # ä¼˜å…ˆä½¿ç”¨åˆ†æç»“æœï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸºç¡€é¢„æµ‹
                    if reverse_psychology_analysis:
                        recommended_tails = reverse_psychology_analysis.get('recommended_tails', [])
                        avoid_tails = reverse_psychology_analysis.get('avoid_tails', [])
                        confidence = reverse_psychology_analysis.get('confidence', 0.5)
                        
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_tails:
                                # æ¨èçš„å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + confidence * 0.3)
                            elif tail in avoid_tails:
                                # é¿å¼€çš„å°¾æ•°ç»™äºˆä½æ¦‚ç‡
                                model_predictions[tail] = max(0.1, 0.4 - confidence * 0.3)
                            else:
                                # ä¸­æ€§å°¾æ•°ç»™äºˆé»˜è®¤æ¦‚ç‡
                                model_predictions[tail] = 0.5
                        
                        print(f"ğŸ”„ åå‘å¿ƒç†å­¦æ¨¡å‹é¢„æµ‹å®Œæˆï¼ˆåŸºäºåˆ†æç»“æœï¼‰ï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # åˆ†æå¤±è´¥æ—¶çš„å¤‡ç”¨é¢„æµ‹ç­–ç•¥
                        print(f"âš ï¸ åå‘å¿ƒç†å­¦åˆ†æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é¢„æµ‹ç­–ç•¥")
                        
                        # åŸºäºå†å²æ•°æ®çš„ç®€å•åå‘é€»è¾‘
                        if data_list and len(data_list) >= 5:
                            recent_5_data = data_list[:5]
                            tail_frequencies = {}
                            
                            # è®¡ç®—æœ€è¿‘5æœŸå„å°¾æ•°é¢‘ç‡
                            for tail in range(10):
                                count = sum(1 for period in recent_5_data if tail in period.get('tails', []))
                                tail_frequencies[tail] = count / 5.0
                            
                            # åå‘é€»è¾‘ï¼šé¢‘ç‡é«˜çš„ç»™ä½æ¦‚ç‡ï¼Œé¢‘ç‡ä½çš„ç»™é«˜æ¦‚ç‡
                            for tail in prediction_targets:
                                freq = tail_frequencies.get(tail, 0.5)
                                # åå‘æ¦‚ç‡ï¼šé¢‘ç‡è¶Šé«˜ï¼Œé¢„æµ‹æ¦‚ç‡è¶Šä½
                                reverse_prob = max(0.2, min(0.8, 0.8 - freq * 0.6))
                                model_predictions[tail] = reverse_prob
                            
                            print(f"ğŸ”„ åå‘å¿ƒç†å­¦æ¨¡å‹ä½¿ç”¨å¤‡ç”¨ç­–ç•¥ï¼šåŸºäºé¢‘ç‡åå‘")
                        else:
                            # æ•°æ®ä¸è¶³æ—¶ä½¿ç”¨ä¸­æ€§é¢„æµ‹
                            for tail in prediction_targets:
                                model_predictions[tail] = 0.5
                            print(f"ğŸ”„ åå‘å¿ƒç†å­¦æ¨¡å‹ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šæ•°æ®ä¸è¶³")
                    
                    all_predictions['reverse_psychology_predictor'] = model_predictions
                    
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'reverse_psychology_predictor',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
                    
                except Exception as e:
                    print(f"âŒ åå‘å¿ƒç†å­¦æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['reverse_psychology_predictor'] = model_predictions
                    
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'reverse_psychology_predictor',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )
            
            # === åæ“æ§åˆ†æå™¨æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'banker_behavior_analyzer') and self.banker_behavior_analyzer and anti_manipulation_analysis:
                try:
                    model_predictions = {}
        
                    if anti_manipulation_analysis and anti_recommendations:
                        recommended_anti_tails = anti_recommendations.get('recommended_tails', [])
                        avoid_anti_tails = anti_recommendations.get('avoid_tails', [])
                        manipulation_prob = anti_manipulation_analysis.get('manipulation_probability', 0.5)
            
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_anti_tails:
                                # æ¨èçš„å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + manipulation_prob * 0.3)
                            elif tail in avoid_anti_tails:
                                # é¿å¼€çš„å°¾æ•°ç»™äºˆä½æ¦‚ç‡
                                model_predictions[tail] = max(0.1, 0.4 - manipulation_prob * 0.3)
                            else:
                                # ä¸­æ€§å°¾æ•°ç»™äºˆé»˜è®¤æ¦‚ç‡
                                model_predictions[tail] = 0.5
            
                        print(f"ğŸ¯ åæ“æ§åˆ†æå™¨æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåˆ†æï¼Œä½¿ç”¨ä¸­æ€§é¢„æµ‹
                        for tail in prediction_targets:
                            model_predictions[tail] = 0.5
                        print(f"ğŸ¯ åæ“æ§åˆ†æå™¨æ¨¡å‹ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šæ— æœ‰æ•ˆåˆ†æç»“æœ")
        
                    all_predictions['anti_manipulation_banker_behavior'] = model_predictions
        
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'anti_manipulation_banker_behavior',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
        
                except Exception as e:
                    print(f"âŒ åæ“æ§åˆ†æå™¨æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['anti_manipulation_banker_behavior'] = model_predictions
        
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'anti_manipulation_banker_behavior',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )

            # === å†·é—¨æŒ–æ˜å™¨æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'unpopular_digger') and self.unpopular_digger and unpopular_digger_analysis:
                try:
                    model_predictions = {}
        
                    if unpopular_digger_analysis.get('success') and unpopular_digger_analysis.get('recommended_tails'):
                        recommended_cold_tails = unpopular_digger_analysis.get('recommended_tails', [])
                        confidence = unpopular_digger_analysis.get('confidence', 0.5)
            
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_cold_tails:
                                # æ¨èçš„å†·é—¨å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + confidence * 0.3)
                            else:
                                # éå†·é—¨å°¾æ•°ç»™äºˆè¾ƒä½æ¦‚ç‡
                                model_predictions[tail] = max(0.1, 0.4 - confidence * 0.2)
            
                        print(f"ğŸ” å†·é—¨æŒ–æ˜å™¨æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # å¦‚æœæ²¡æœ‰å†·é—¨å‘ç°ï¼Œä½¿ç”¨ä¸­æ€§é¢„æµ‹
                        for tail in prediction_targets:
                            model_predictions[tail] = 0.5
                        print(f"ğŸ” å†·é—¨æŒ–æ˜å™¨æ¨¡å‹ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šæ— æ˜æ˜¾å†·é—¨æœºä¼š")
        
                    all_predictions['unpopular_digger'] = model_predictions
        
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'unpopular_digger',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
        
                except Exception as e:
                    print(f"âŒ å†·é—¨æŒ–æ˜å™¨æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['unpopular_digger'] = model_predictions
        
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'unpopular_digger',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )

            # === èµ„é‡‘æµå‘åˆ†æå™¨æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'money_flow_analyzer') and self.money_flow_analyzer and money_flow_analysis:
                try:
                    model_predictions = {}
        
                    if money_flow_analysis.get('success') and money_flow_analysis.get('recommended_tails'):
                        recommended_flow_tails = money_flow_analysis.get('recommended_tails', [])
                        avoid_flow_tails = money_flow_analysis.get('avoid_tails', [])
                        confidence = money_flow_analysis.get('confidence', 0.5)
            
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_flow_tails:
                                # æ¨èçš„å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + confidence * 0.3)
                            elif tail in avoid_flow_tails:
                                # é¿å¼€çš„å°¾æ•°ç»™äºˆä½æ¦‚ç‡
                                model_predictions[tail] = max(0.1, 0.4 - confidence * 0.3)
                            else:
                                # ä¸­æ€§å°¾æ•°ç»™äºˆé»˜è®¤æ¦‚ç‡
                                model_predictions[tail] = 0.5
            
                        print(f"ğŸ’° èµ„é‡‘æµå‘åˆ†æå™¨æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåˆ†æï¼Œä½¿ç”¨ä¸­æ€§é¢„æµ‹
                        for tail in prediction_targets:
                            model_predictions[tail] = 0.5
                        print(f"ğŸ’° èµ„é‡‘æµå‘åˆ†æå™¨æ¨¡å‹ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šæ— æœ‰æ•ˆåˆ†æç»“æœ")
        
                    all_predictions['money_flow_analyzer'] = model_predictions
        
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'money_flow_analyzer',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
        
                except Exception as e:
                    print(f"âŒ èµ„é‡‘æµå‘åˆ†æå™¨æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['money_flow_analyzer'] = model_predictions
        
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'money_flow_analyzer',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )

            # === åšå¼ˆè®ºç­–ç•¥å™¨æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'game_theory_strategist') and self.game_theory_strategist:
                try:
                    # ä½¿ç”¨ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°è¿›è¡Œåšå¼ˆè®ºåˆ†æ
                    candidate_tails_list = list(prediction_targets)
        
                    game_theory_analysis = self.game_theory_strategist.predict(
                        candidate_tails_list, 
                        data_list
                    )
        
                    model_predictions = {}
        
                    if game_theory_analysis.get('success'):
                        recommended_game_tails = game_theory_analysis.get('recommended_tails', [])
                        confidence = game_theory_analysis.get('confidence', 0.0)
                        strategy_type = game_theory_analysis.get('strategy_type', 'balanced')
            
                        print(f"ğŸ® åšå¼ˆè®ºç­–ç•¥å™¨åˆ†æ: ç­–ç•¥={strategy_type}, ç½®ä¿¡åº¦={confidence:.2f}")
                        print(f"ğŸ® åšå¼ˆè®ºç­–ç•¥å»ºè®®: æ¨è{recommended_game_tails}")
            
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_game_tails:
                                # æ¨èçš„å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + confidence * 0.3)
                            else:
                                # éæ¨èå°¾æ•°ç»™äºˆè¾ƒä½æ¦‚ç‡
                                base_prob = 0.4 - confidence * 0.2
                                model_predictions[tail] = max(0.1, base_prob)
            
                        print(f"ğŸ® åšå¼ˆè®ºç­–ç•¥å™¨æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # å¦‚æœåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¸­æ€§é¢„æµ‹
                        for tail in prediction_targets:
                            model_predictions[tail] = 0.5
                        print(f"ğŸ® åšå¼ˆè®ºç­–ç•¥å™¨æ¨¡å‹ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šåˆ†æå¤±è´¥")
        
                    all_predictions['game_theory_strategist'] = model_predictions
        
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'game_theory_strategist',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
        
                    # å­˜å‚¨åšå¼ˆè®ºåˆ†æç»“æœä¾›å­¦ä¹ ä½¿ç”¨
                    if 'last_prediction_result' not in locals():
                        self.last_prediction_result = {}
                    self.last_prediction_result['game_theory_analysis'] = game_theory_analysis
        
                except Exception as e:
                    print(f"âŒ åšå¼ˆè®ºç­–ç•¥å™¨æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['game_theory_strategist'] = model_predictions
        
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'game_theory_strategist',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )

            # === æ“æ§æ—¶æœºæ£€æµ‹å™¨æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'manipulation_timing_detector') and self.manipulation_timing_detector:
                try:
                    # ä½¿ç”¨ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°è¿›è¡Œæ“æ§æ—¶æœºæ£€æµ‹
                    candidate_tails_list = list(prediction_targets)
        
                    manipulation_timing_analysis = self.manipulation_timing_detector.detect_manipulation_timing(
                        candidate_tails_list, 
                        data_list
                    )
        
                    model_predictions = {}
        
                    if manipulation_timing_analysis.get('success'):
                        recommended_timing_tails = manipulation_timing_analysis.get('recommended_tails', [])
                        avoid_timing_tails = manipulation_timing_analysis.get('avoid_tails', [])
                        confidence = manipulation_timing_analysis.get('confidence', 0.0)
                        timing_type = manipulation_timing_analysis.get('timing_type', 'natural')
            
                        print(f"ğŸ¯ æ“æ§æ—¶æœºæ£€æµ‹åˆ†æ: æ—¶æœºç±»å‹={timing_type}, ç½®ä¿¡åº¦={confidence:.2f}")
                        print(f"ğŸ¯ æ“æ§æ—¶æœºå»ºè®®: æ¨è{recommended_timing_tails} é¿å¼€{avoid_timing_tails}")
            
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_timing_tails:
                                # æ¨èçš„å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + confidence * 0.3)
                            elif tail in avoid_timing_tails:
                                # é¿å¼€çš„å°¾æ•°ç»™äºˆä½æ¦‚ç‡
                                model_predictions[tail] = max(0.1, 0.4 - confidence * 0.3)
                            else:
                                # ä¸­æ€§å°¾æ•°ç»™äºˆé»˜è®¤æ¦‚ç‡
                                model_predictions[tail] = 0.5
            
                        print(f"ğŸ¯ æ“æ§æ—¶æœºæ£€æµ‹å™¨æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # å¦‚æœåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¸­æ€§é¢„æµ‹
                        for tail in prediction_targets:
                            model_predictions[tail] = 0.5
                        print(f"ğŸ¯ æ“æ§æ—¶æœºæ£€æµ‹å™¨æ¨¡å‹ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šåˆ†æå¤±è´¥")
        
                    all_predictions['manipulation_timing_detector'] = model_predictions
        
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'manipulation_timing_detector',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
        
                    # å­˜å‚¨æ“æ§æ—¶æœºåˆ†æç»“æœä¾›å­¦ä¹ ä½¿ç”¨
                    if 'last_prediction_result' not in locals():
                        self.last_prediction_result = {}
                    self.last_prediction_result['manipulation_timing_analysis'] = manipulation_timing_analysis
        
                except Exception as e:
                    print(f"âŒ æ“æ§æ—¶æœºæ£€æµ‹å™¨æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['manipulation_timing_detector'] = model_predictions
        
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'manipulation_timing_detector',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )

            # === åè¶‹åŠ¿çŒæ‰‹æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'anti_trend_hunter') and self.anti_trend_hunter:
                try:
                    # ä½¿ç”¨ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°è¿›è¡Œåè¶‹åŠ¿åˆ†æ
                    candidate_tails_list = list(prediction_targets)
        
                    anti_trend_analysis = self.anti_trend_hunter.predict(
                        candidate_tails_list, 
                        data_list
                    )
        
                    model_predictions = {}
        
                    if anti_trend_analysis.get('success'):
                        recommended_trend_tails = anti_trend_analysis.get('recommended_tails', [])
                        confidence = anti_trend_analysis.get('confidence', 0.0)
                        strategy_type = anti_trend_analysis.get('strategy_type', 'anti_trend_analysis')
            
                        print(f"ğŸ¯ åè¶‹åŠ¿çŒæ‰‹åˆ†æ: ç­–ç•¥={strategy_type}, ç½®ä¿¡åº¦={confidence:.2f}")
                        print(f"ğŸ¯ åè¶‹åŠ¿å»ºè®®: æ¨è{recommended_trend_tails}")
            
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_trend_tails:
                                # æ¨èçš„å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + confidence * 0.3)
                            else:
                                # éæ¨èå°¾æ•°ç»™äºˆè¾ƒä½æ¦‚ç‡
                                base_prob = 0.4 - confidence * 0.2
                                model_predictions[tail] = max(0.1, base_prob)
            
                        print(f"ğŸ¯ åè¶‹åŠ¿çŒæ‰‹æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # å¦‚æœåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¸­æ€§é¢„æµ‹
                        for tail in prediction_targets:
                            model_predictions[tail] = 0.5
                        print(f"ğŸ¯ åè¶‹åŠ¿çŒæ‰‹æ¨¡å‹ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šåˆ†æå¤±è´¥")
        
                    all_predictions['anti_trend_hunter'] = model_predictions
        
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'anti_trend_hunter',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
        
                    # å­˜å‚¨åè¶‹åŠ¿åˆ†æç»“æœä¾›å­¦ä¹ ä½¿ç”¨
                    if 'last_prediction_result' not in locals():
                        self.last_prediction_result = {}
                    self.last_prediction_result['anti_trend_analysis'] = anti_trend_analysis
        
                except Exception as e:
                    print(f"âŒ åè¶‹åŠ¿çŒæ‰‹æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['anti_trend_hunter'] = model_predictions
        
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'anti_trend_hunter',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )

            # === ç¾¤ä½“å¿ƒç†åˆ†æå™¨æ¨¡å‹é¢„æµ‹ ===
            if hasattr(self, 'crowd_psychology_analyzer') and self.crowd_psychology_analyzer:
                try:
                    # ä½¿ç”¨ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°è¿›è¡Œç¾¤ä½“å¿ƒç†åˆ†æ
                    candidate_tails_list = list(prediction_targets)
        
                    crowd_psychology_analysis = self.crowd_psychology_analyzer.predict(
                        candidate_tails_list, 
                        data_list
                    )
        
                    model_predictions = {}
        
                    if crowd_psychology_analysis.get('success'):
                        recommended_psychology_tails = crowd_psychology_analysis.get('recommended_tails', [])
                        confidence = crowd_psychology_analysis.get('confidence', 0.0)
                        strategy_type = crowd_psychology_analysis.get('strategy_type', 'crowd_psychology_analysis')
                        crowd_emotion = crowd_psychology_analysis.get('crowd_emotion', 'neutral')
                        herd_intensity = crowd_psychology_analysis.get('herd_intensity', 0.0)
            
                        print(f"ğŸ§  ç¾¤ä½“å¿ƒç†åˆ†æ: ç­–ç•¥={strategy_type}, ç½®ä¿¡åº¦={confidence:.2f}, ç¾¤ä½“æƒ…ç»ª={crowd_emotion}")
                        print(f"ğŸ§  ä»ä¼—å¼ºåº¦={herd_intensity:.2f}, æ¨è={recommended_psychology_tails}")
            
                        # ä¸ºå€™é€‰å°¾æ•°åˆ†é…é¢„æµ‹æ¦‚ç‡
                        for tail in prediction_targets:
                            if tail in recommended_psychology_tails:
                                # æ¨èçš„å°¾æ•°ç»™äºˆé«˜æ¦‚ç‡
                                model_predictions[tail] = min(0.9, 0.6 + confidence * 0.3)
                            else:
                                # éæ¨èå°¾æ•°ç»™äºˆè¾ƒä½æ¦‚ç‡
                                base_prob = 0.4 - confidence * 0.2
                                model_predictions[tail] = max(0.1, base_prob)
            
                        print(f"ğŸ§  ç¾¤ä½“å¿ƒç†åˆ†æå™¨é¢„æµ‹å®Œæˆï¼Œæ¨èæ¦‚ç‡: {[(t, f'{p:.3f}') for t, p in model_predictions.items() if p > 0.6]}")
                    else:
                        # å¦‚æœåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ä¸­æ€§é¢„æµ‹
                        for tail in prediction_targets:
                            model_predictions[tail] = 0.5
                        print(f"ğŸ§  ç¾¤ä½“å¿ƒç†åˆ†æå™¨ä½¿ç”¨ä¸­æ€§ç­–ç•¥ï¼šåˆ†æå¤±è´¥")
        
                    all_predictions['crowd_psychology_analyzer'] = model_predictions
        
                    # è®°å½•æœ€ä½³é¢„æµ‹åˆ°æ•°æ®åº“
                    if model_predictions:
                        best_tail = max(model_predictions.keys(), key=lambda t: model_predictions[t])
                        best_confidence = model_predictions[best_tail]
                        predicted_class = 1 if best_confidence > 0.5 else 0
                        self._record_model_prediction(
                            'crowd_psychology_analyzer',
                            predicted_class,
                            best_confidence,
                            best_tail
                        )
        
                    # å­˜å‚¨ç¾¤ä½“å¿ƒç†åˆ†æç»“æœä¾›å­¦ä¹ ä½¿ç”¨
                    if 'last_prediction_result' not in locals():
                        self.last_prediction_result = {}
                    self.last_prediction_result['crowd_psychology_analysis'] = crowd_psychology_analysis
        
                except Exception as e:
                    print(f"âŒ ç¾¤ä½“å¿ƒç†åˆ†æå™¨é¢„æµ‹å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤é¢„æµ‹ç¡®ä¿æ¨¡å‹å‚ä¸æƒé‡æŠ•èµ„
                    model_predictions = {tail: 0.5 for tail in prediction_targets}
                    all_predictions['crowd_psychology_analyzer'] = model_predictions
        
                    # è®°å½•é»˜è®¤é¢„æµ‹
                    if model_predictions and prediction_targets:
                        best_tail = list(prediction_targets)[0]
                        self._record_model_prediction(
                            'crowd_psychology_analyzer',
                            0,  # é»˜è®¤é¢„æµ‹ç±»åˆ«
                            0.5,  # é»˜è®¤ç½®ä¿¡åº¦
                            best_tail
                        )

            # === åŸºäºé¢„æµ‹æƒé‡çš„ç›´æ¥æŠ•èµ„ ===
            print(f"ğŸ’° å¼€å§‹åŸºäºé¢„æµ‹æƒé‡çš„ç›´æ¥æŠ•èµ„...")
            print(f"ğŸ“‹ æŠ•èµ„ç­–ç•¥: æ¯ä¸ªæ¨¡å‹å›ºå®šæŠ•èµ„30%é¢„æµ‹æƒé‡")
            
            # æ‰§è¡Œæƒé‡æŠ•èµ„
            model_weight_investments = {}
            
            for model_key in all_predictions.keys():
                if model_key in self.ensemble_weights:
                    # è·å–æ¨¡å‹çš„å½“å‰æƒé‡
                    current_weight = self.ensemble_weights[model_key]['weight']
                    
                    # è·å–è¯¥æ¨¡å‹å¯¹å€™é€‰å°¾æ•°çš„é¢„æµ‹æ¦‚ç‡
                    model_tail_probabilities = {}
                    if model_key in all_predictions:
                        model_predictions = all_predictions[model_key]
                        # ä¿®æ”¹ï¼šè®©æ¯ä¸ªæ¨¡å‹ä»è‡ªå·±çš„å®Œæ•´é¢„æµ‹ç»“æœä¸­é€‰æ‹©ï¼Œè€Œä¸æ˜¯è¢«é™åˆ¶åœ¨prediction_targetsä¸­
                        for tail, prob in model_predictions.items():
                            model_tail_probabilities[tail] = prob
                    else:
                        # å¦‚æœæ²¡æœ‰é¢„æµ‹æ•°æ®ï¼Œåªè€ƒè™‘prediction_targets
                        for tail in prediction_targets:
                            model_tail_probabilities[tail] = 0.5

                    print(f"ğŸ” {model_key} å®Œæ•´ç½®ä¿¡åº¦æ•°æ®: {model_tail_probabilities}")

                    # æ‰€æœ‰æ¨¡å‹é‡‡ç”¨é›†ä¸­æŠ•èµ„ç­–ç•¥ï¼šé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„å•ä¸ªå°¾æ•°æŠ•èµ„
                    print(f"ğŸ¯ {model_key} é‡‡ç”¨é›†ä¸­æŠ•èµ„ç­–ç•¥")

                    # ä»æ¨¡å‹çš„å®Œæ•´é¢„æµ‹ç»“æœä¸­æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„å°¾æ•°
                    if model_tail_probabilities:
                        best_tail = max(model_tail_probabilities.keys(), 
                                    key=lambda t: model_tail_probabilities.get(t, 0.5))
                        best_confidence = model_tail_probabilities[best_tail]
    
                        print(f"   ğŸ¯ é€‰å®šæœ€é«˜ç½®ä¿¡åº¦å°¾æ•°: {best_tail} (ç½®ä¿¡åº¦: {best_confidence:.3f})")
                        
                        # è®¡ç®—æ€»æŠ•èµ„æƒé‡ï¼ˆ30%æŠ•èµ„ï¼‰
                        total_investment_weight = current_weight * 0.3
                        
                        # å°†æ‰€æœ‰æŠ•èµ„æƒé‡é›†ä¸­æŠ•å…¥åˆ°æœ€é«˜ç½®ä¿¡åº¦çš„å°¾æ•°
                        weight_investments = {best_tail: total_investment_weight}
                        
                        print(f"   ğŸ’° é›†ä¸­æŠ•èµ„: å°¾æ•°{best_tail} æŠ•èµ„æƒé‡{total_investment_weight:.4f} (100%é›†ä¸­)")
                        
                        # ä¸ºä¸åŒæ¨¡å‹æ˜¾ç¤ºé€‰æ‹©ç†ç”±
                        if model_key == 'reverse_psychology_predictor':
                            # åå‘å¿ƒç†å­¦æ¨¡å‹çš„ç‰¹æ®Šåˆ†æ
                            if reverse_psychology_analysis:
                                strategy_type = reverse_psychology_analysis.get('strategy_type', 'unknown')
                                reasoning = reverse_psychology_analysis.get('reasoning', 'æ— è¯¦ç»†ç†ç”±')
                                print(f"   ğŸ“‹ åå‘ç­–ç•¥: {strategy_type}")
                                print(f"   ğŸ“‹ é€‰æ‹©ç†ç”±: {reasoning}")
                                
                                # æ˜¾ç¤ºæ¨èå’Œé¿å¼€çš„å°¾æ•°å¯¹æ¯”
                                recommended_tails = reverse_psychology_analysis.get('recommended_tails', [])
                                avoid_tails = reverse_psychology_analysis.get('avoid_tails', [])
                                
                                if best_tail in recommended_tails:
                                    print(f"   âœ… é€‰ä¸­å°¾æ•°{best_tail}åœ¨æ¨èåˆ—è¡¨ä¸­: {recommended_tails}")
                                elif best_tail not in avoid_tails:
                                    print(f"   âšª é€‰ä¸­å°¾æ•°{best_tail}ä¸ºä¸­æ€§é€‰æ‹© (æ¨è:{recommended_tails}, é¿å¼€:{avoid_tails})")
                                else:
                                    print(f"   âš ï¸ é€‰ä¸­å°¾æ•°{best_tail}åœ¨é¿å¼€åˆ—è¡¨ä¸­: {avoid_tails} (å¯èƒ½æ˜¯å¤‡ç”¨ç­–ç•¥)")
                            else:
                                print(f"   ğŸ“‹ ä½¿ç”¨å¤‡ç”¨ç­–ç•¥: åŸºäºå†å²é¢‘ç‡çš„åå‘é€‰æ‹©")
                        else:
                            # å…¶ä»–æ¨¡å‹çš„é€‰æ‹©ç†ç”±
                            model_display_name = model_key.replace('_', ' ').replace('river ', '').replace('sklearn ', '').replace('pytorch ', '').title()
                            
                            if 'hoeffding' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: å†³ç­–æ ‘åˆ¤æ–­å°¾æ•°{best_tail}ç‰¹å¾ç»„åˆæœ€ä¼˜")
                            elif 'logistic' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: é€»è¾‘å›å½’è®¡ç®—å°¾æ•°{best_confidence:.3f}æ¦‚ç‡æœ€é«˜")
                            elif 'naive_bayes' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: è´å¶æ–¯æ¨ç†è®¤ä¸ºå°¾æ•°{best_tail}åéªŒæ¦‚ç‡æœ€å¤§")
                            elif 'bagging' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: é›†æˆæŠ•ç¥¨æ˜¾ç¤ºå°¾æ•°{best_tail}æ”¯æŒåº¦æœ€é«˜")
                            elif 'adaboost' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: è‡ªé€‚åº”æå‡ç®—æ³•è®¤ä¸ºå°¾æ•°{best_tail}æœ€å¯èƒ½")
                            elif 'pattern_matcher' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: å†å²æ¨¡å¼åŒ¹é…æ˜¾ç¤ºå°¾æ•°{best_tail}ç›¸ä¼¼åº¦æœ€é«˜")
                            elif 'reverse_psychology' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: åå‘å¿ƒç†å­¦åˆ†æè®¤ä¸ºå°¾æ•°{best_tail}æ˜¯æœ€ä½³é€‰æ‹©")
                            elif 'unpopular_digger' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: å†·é—¨æŒ–æ˜ç®—æ³•å‘ç°å°¾æ•°{best_tail}å­˜åœ¨å¤å‡ºæœºä¼š")
                            elif 'anti_manipulation' in model_key.lower():
                                print(f"   ğŸ“‹ {model_display_name}: åæ“æ§åˆ†æè®¤ä¸ºå°¾æ•°{best_tail}å®‰å…¨æ€§æœ€é«˜")
                            elif 'pytorch' in model_key.lower():
                                if 'lstm' in model_key.lower():
                                    print(f"   ğŸ“‹ {model_display_name}: LSTMæ—¶åºåˆ†æé¢„æµ‹å°¾æ•°{best_tail}æœ€å¯èƒ½")
                                elif 'transformer' in model_key.lower():
                                    print(f"   ğŸ“‹ {model_display_name}: Transformeræ³¨æ„åŠ›æœºåˆ¶é”å®šå°¾æ•°{best_tail}")
                                else:
                                    print(f"   ğŸ“‹ {model_display_name}: æ·±åº¦å­¦ä¹ ç½‘ç»œè¾“å‡ºå°¾æ•°{best_tail}æ¦‚ç‡æœ€é«˜")
                            else:
                                print(f"   ğŸ“‹ {model_display_name}: ç®—æ³•è®¡ç®—è®¤ä¸ºå°¾æ•°{best_tail}ç½®ä¿¡åº¦æœ€é«˜")
                            
                            # æ˜¾ç¤ºç½®ä¿¡åº¦ç­‰çº§
                            if best_confidence > 0.8:
                                confidence_level = "æé«˜ç½®ä¿¡åº¦"
                            elif best_confidence > 0.7:
                                confidence_level = "é«˜ç½®ä¿¡åº¦"
                            elif best_confidence > 0.6:
                                confidence_level = "ä¸­é«˜ç½®ä¿¡åº¦"
                            elif best_confidence > 0.5:
                                confidence_level = "ä¸­ç­‰ç½®ä¿¡åº¦"
                            else:
                                confidence_level = "ä½ç½®ä¿¡åº¦"
                            
                            print(f"   ğŸ“Š ç½®ä¿¡åº¦è¯„ä¼°: {confidence_level} ({best_confidence:.3f})")
                    else:
                        print(f"   âŒ æ— æœ‰æ•ˆçš„ç½®ä¿¡åº¦æ•°æ®ï¼Œè·³è¿‡æŠ•èµ„")
                        weight_investments = {}
                    
                    if weight_investments:
                        # è®°å½•æƒé‡æŠ•èµ„
                        model_weight_investments[model_key] = weight_investments
                        self.ensemble_weights[model_key]['weight_investments'] = weight_investments.copy()
                        
                        total_investment_weight = sum(weight_investments.values())
                        investment_percentage = (total_investment_weight / current_weight) * 100 if current_weight > 0 else 0
                        print(f"   ğŸ’¼ {model_key}: æ¨¡å‹æƒé‡{current_weight:.4f}, æŠ•èµ„æƒé‡{total_investment_weight:.4f}({investment_percentage:.1f}%)")
                        
                        # æ˜¾ç¤ºæŠ•èµ„è¯¦æƒ…
                        model_predictions = all_predictions.get(model_key, {})
                        for tail, weight_amount in weight_investments.items():
                            tail_prob = model_predictions.get(tail, 0.5)
                            investment_ratio = (weight_amount / total_investment_weight) * 100 if total_investment_weight > 0 else 0
                            print(f"      ğŸ¯ å°¾æ•°{tail}: æŠ•èµ„æƒé‡{weight_amount:.4f}({investment_ratio:.1f}%) - æ¨¡å‹æ¦‚ç‡{tail_prob:.3f}")
            
            # å†»ç»“æŠ•èµ„çš„æƒé‡
            if model_weight_investments:
                self.freeze_investment_weights(model_weight_investments)
            
            print(f"ğŸ’° æƒé‡æŠ•èµ„åˆ†é…å®Œæˆï¼Œç­‰å¾…å¼€å¥–ç»“ç®—")
            
            # éªŒè¯æƒé‡æŠ•èµ„æ¯”ä¾‹
            print(f"ğŸ“Š æƒé‡æŠ•èµ„æ¯”ä¾‹éªŒè¯ï¼š")
            for model_key, weight_investments in model_weight_investments.items():
                if model_key in self.ensemble_weights:
                    original_weight = self.ensemble_weights[model_key]['weight'] + sum(weight_investments.values())  # æŠ•èµ„å‰çš„åŸå§‹æƒé‡
                    total_investment_weight = sum(weight_investments.values())
                    investment_ratio = (total_investment_weight / original_weight) * 100 if original_weight > 0 else 0
                    
                    status = "âœ…" if investment_ratio >= 25 else "âš ï¸"  # è°ƒæ•´ä¸º25%å› ä¸ºæˆ‘ä»¬ç°åœ¨æ˜¯30%æŠ•èµ„
                    investment_details = []
                    model_predictions = all_predictions.get(model_key, {})
                    
                    for tail, weight_amount in weight_investments.items():
                        tail_prob = model_predictions.get(tail, 0.5)
                        tail_ratio = (weight_amount / total_investment_weight) * 100 if total_investment_weight > 0 else 0
                        investment_details.append(f"å°¾æ•°{tail}({tail_prob:.2f}):{tail_ratio:.0f}%")
                    
                    print(f"   {status} {model_key}: æƒé‡æŠ•èµ„æ¯”ä¾‹ {investment_ratio:.1f}% (æŠ•èµ„æƒé‡{total_investment_weight:.4f}/åŸæƒé‡{original_weight:.4f})")
                    print(f"      ğŸ’° æŠ•èµ„åˆ†å¸ƒ: {' | '.join(investment_details)}")

            # åŸºäºæƒé‡æŠ•èµ„è®¡ç®—é›†æˆæ¦‚ç‡
            ensemble_probabilities = {}
            
            # é¦–å…ˆæ”¶é›†æ¯ä¸ªå°¾æ•°çš„æŠ•èµ„æƒé‡ä¿¡æ¯
            tail_investment_weights = {}
            for tail in prediction_targets:
                tail_investment_weights[tail] = 0.0
                
                # ç»Ÿè®¡æŠ•èµ„è¯¥å°¾æ•°çš„æ€»æƒé‡
                for model_key, weight_investments in model_weight_investments.items():
                    if tail in weight_investments:
                        tail_investment_weights[tail] += weight_investments[tail]
            
            # è®¡ç®—é›†æˆæ¦‚ç‡ï¼ˆåŸºäºæŠ•èµ„æƒé‡ï¼‰
            for tail in prediction_targets:
                tail_total_weight = tail_investment_weights[tail]
                
                if tail_total_weight > 0:
                    # ä½¿ç”¨æŠ•èµ„æƒé‡ä½œä¸ºè¯¥å°¾æ•°çš„æ”¯æŒåº¦
                    # æ¦‚ç‡ç­‰äºè¯¥å°¾æ•°çš„æŠ•èµ„æƒé‡å æ‰€æœ‰å°¾æ•°æŠ•èµ„æƒé‡çš„æ¯”ä¾‹
                    total_all_investments = sum(tail_investment_weights.values())
                    if total_all_investments > 0:
                        ensemble_probabilities[tail] = tail_total_weight / total_all_investments
                    else:
                        ensemble_probabilities[tail] = 0.5
                else:
                    ensemble_probabilities[tail] = 0.0
                
                # ç»Ÿè®¡æŠ•èµ„è¯¥å°¾æ•°çš„æ¨¡å‹ä¿¡æ¯
                investors = []
                for model_key, weight_investments in model_weight_investments.items():
                    if tail in weight_investments:
                        investment_weight = weight_investments[tail]
                        model_prob = all_predictions.get(model_key, {}).get(tail, 0.5)
                        investors.append(f"{model_key}(æŠ•èµ„æƒé‡{investment_weight:.4f},æ¦‚ç‡{model_prob:.3f})")
                
                investor_info = " | ".join(investors[:3]) + ("..." if len(investors) > 3 else "")
                print(f"   ğŸ¯ å°¾æ•°{tail}: æŠ•èµ„æƒé‡{tail_total_weight:.4f}, é›†æˆæ¦‚ç‡{ensemble_probabilities[tail]:.3f}")
                print(f"      ğŸ’¼ æŠ•èµ„æ–¹: {investor_info}")
            
            # ä¿å­˜æƒé‡æŠ•èµ„ä¿¡æ¯ä»¥ä¾¿åç»­ç»“ç®—
            self.current_weight_investments = model_weight_investments
            
            # ç¡®ä¿æŠ•èµ„æƒé‡æœºåˆ¶æ­£å¸¸å·¥ä½œ
            if not model_weight_investments:
                print("âš ï¸ æ²¡æœ‰æ¨¡å‹è¿›è¡Œæƒé‡æŠ•èµ„ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
                return {
                    'success': False,
                    'recommended_tails': [],
                    'confidence': 0.0,
                    'ensemble_probabilities': {},
                    'model_count': len(all_predictions),
                    'message': 'æ²¡æœ‰æ¨¡å‹è¿›è¡Œæƒé‡æŠ•èµ„ï¼Œæ— æ³•ç”Ÿæˆé¢„æµ‹ç»“æœ'
                }

            # === é€‰æ‹©æœ€ä½³å€™é€‰å°¾æ•° ===
            if ensemble_probabilities:
                # ä»å€™é€‰ä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å°¾æ•°
                best_candidate = max(ensemble_probabilities.keys(), key=lambda t: ensemble_probabilities.get(t, 0))
                recommended_tails = [best_candidate]
                print(f"ğŸ¯ ä»å€™é€‰å°¾æ•°ä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ï¼š{best_candidate}ï¼ˆæ¦‚ç‡ï¼š{ensemble_probabilities.get(best_candidate, 0):.3f}ï¼‰")
            else:
                recommended_tails = []
            
            # === è®¡ç®—ç½®ä¿¡åº¦ ===
            confidence = self.calculate_ensemble_confidence(recommended_tails, ensemble_probabilities, all_predictions)
            
            # ä¿å­˜é¢„æµ‹ç»“æœï¼ˆåŒ…å«æ­£ç¡®çš„æƒé‡ä¿¡æ¯ï¼‰
            self.last_prediction_result = {
                'recommended_tails': recommended_tails,
                'confidence': confidence,
                'ensemble_probabilities': ensemble_probabilities,
                'model_predictions': all_predictions,
                'features': base_features.tolist(),
                'model_weight_investments': getattr(self, 'current_weight_investments', {}),
                'model_weights': {k: v['weight'] for k, v in self.ensemble_weights.items()},
                'weight_pool_size': getattr(self, 'weight_pool', 0.0),
                'reverse_psychology_analysis': reverse_psychology_analysis,  # å­˜å‚¨åå‘å¿ƒç†å­¦åˆ†æç»“æœ
                'money_flow_analysis': money_flow_analysis  # å­˜å‚¨èµ„é‡‘æµå‘åˆ†æç»“æœ
            }
            
            # ç”Ÿæˆè¯¦ç»†çš„é¢„æµ‹åˆ†æ
            detailed_prediction_info = self.prediction_analyzer.generate_detailed_prediction_analysis(
                recommended_tails, confidence, ensemble_probabilities, all_predictions, data_list, model_decision_records
            )

            return {
                'success': True,
                'recommended_tails': recommended_tails,
                'confidence': confidence,
                'ensemble_probabilities': ensemble_probabilities,
                'model_count': len(all_predictions),
                'message': f'ç»ˆæåœ¨çº¿é¢„æµ‹å®Œæˆï¼Œä½¿ç”¨äº†{len(all_predictions)}ä¸ªæ¨¡å‹',
                'weight_details': detailed_prediction_info['weight_details'],
                'decision_summary': detailed_prediction_info['decision_summary'],
                'detailed_analysis': detailed_prediction_info['detailed_analysis'],
                'total_models_participated': len(all_predictions),
                'reverse_psychology_analysis': reverse_psychology_analysis  # å­˜å‚¨åˆ†æç»“æœä¾›å­¦ä¹ ä½¿ç”¨
            }
            
        except Exception as e:
            return {'success': False, 'message': f'åœ¨çº¿é¢„æµ‹å¤±è´¥: {str(e)}'}
    
    def learn_online(self, data_list: List[Dict], actual_tails: List[int]) -> Dict:
        """åœ¨çº¿å­¦ä¹  - çœŸæ­£çš„å¢é‡å­¦ä¹ """
        if not self.is_initialized:
            return {'success': False, 'message': 'æ¨¡å‹æœªåˆå§‹åŒ–'}
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤å­¦ä¹ åŒä¸€ä¸ªæ ·æœ¬
        current_sample_id = f"{len(data_list)}_{hash(str(actual_tails))}"
        if hasattr(self, '_last_sample_id') and self._last_sample_id == current_sample_id:
            print(f"âš ï¸ æ£€æµ‹åˆ°é‡å¤å­¦ä¹ åŒä¸€æ ·æœ¬ï¼Œè·³è¿‡è®¡æ•°å¢åŠ ")
            return {
                'success': True,
                'samples_processed': self.total_samples_seen,
                'prediction_correct': None,
                'message': 'é‡å¤æ ·æœ¬ï¼Œè·³è¿‡å­¦ä¹ '
            }
        
        self._last_sample_id = current_sample_id
        self.total_samples_seen += 1
        print(f"ğŸ“Š å¤„ç†æ ·æœ¬ #{self.total_samples_seen}")
        
        try:
            # æå–ç‰¹å¾
            base_features = self.feature_engineer.extract_enhanced_features(data_list)
            features = base_features  # ä¿æŒå…¼å®¹æ€§
            
            # ä¸ºä¸åŒæ¨¡å‹å‡†å¤‡æ•°æ®æ ¼å¼
            X_river = {f'feature_{i}': features[i] for i in range(len(features))}
            X_sklearn = base_features.reshape(1, -1)
            
            # æ¦‚å¿µæ¼‚ç§»æ£€æµ‹
            drift_detected_by = []
            prediction_correct = False
            
            # æ£€æŸ¥é¢„æµ‹å‡†ç¡®æ€§
            if self.last_prediction_result:
                predicted_tails = self.last_prediction_result['recommended_tails']
                prediction_correct = any(tail in actual_tails for tail in predicted_tails)
                
                if prediction_correct:
                    self.correct_predictions += 1
                
                # æ›´æ–°æ¼‚ç§»æ£€æµ‹å™¨
                error = 0.0 if prediction_correct else 1.0
                
                for detector_name, detector in self.drift_detectors.items():
                    try:
                        if hasattr(detector, 'update'):
                            detector.update(error)
                            if hasattr(detector, 'drift_detected') and detector.drift_detected:
                                drift_detected_by.append(detector_name)
                    except Exception as e:
                        print(f"æ¼‚ç§»æ£€æµ‹å™¨ {detector_name} æ›´æ–°å¤±è´¥: {e}")
            
            # === åœ¨çº¿å­¦ä¹ æ‰€æœ‰æ¨¡å‹ ===
            learning_results = {}

            # === åå‘å¿ƒç†å­¦æ¨¡å‹å­¦ä¹  ===
            if hasattr(self, 'reverse_psychology_predictor') and self.reverse_psychology_predictor:
                try:
                    # ä»ä¸Šæ¬¡é¢„æµ‹ç»“æœä¸­è·å–åå‘å¿ƒç†å­¦åˆ†æ
                    if self.last_prediction_result:
                        stored_reverse_analysis = self.last_prediction_result.get('reverse_psychology_analysis')
                        
                        if stored_reverse_analysis:
                            # è®©æ¨¡å‹ä»ç»“æœä¸­å­¦ä¹ 
                            learn_result = self.reverse_psychology_predictor.learn_from_outcome(
                                stored_reverse_analysis,
                                actual_tails
                            )
                            
                            if learn_result and learn_result.get('learning_success', False):
                                learning_results['reverse_psychology_predictor'] = f'success (accuracy: {learn_result.get("overall_accuracy", 0):.3f})'
                                print(f"âœ… åå‘å¿ƒç†å­¦æ¨¡å‹å­¦ä¹ å®Œæˆï¼Œæ€»ä½“å‡†ç¡®ç‡: {learn_result.get('overall_accuracy', 0):.3f}")
                            else:
                                learning_results['reverse_psychology_predictor'] = 'success (basic update)'
                                print("âœ… åå‘å¿ƒç†å­¦æ¨¡å‹åŸºç¡€å­¦ä¹ å®Œæˆ")
                        else:
                            # æ²¡æœ‰å­˜å‚¨çš„åˆ†æç»“æœï¼Œè¿›è¡ŒåŸºç¡€å­¦ä¹ 
                            print("âš ï¸ æ— å­˜å‚¨çš„åå‘å¿ƒç†å­¦åˆ†æï¼Œè¿›è¡ŒåŸºç¡€æƒé‡æ›´æ–°")
                            
                            # åŸºäºé¢„æµ‹æ­£ç¡®æ€§æ›´æ–°æ¨¡å‹ç½®ä¿¡åº¦
                            if 'reverse_psychology_predictor' in self.ensemble_weights:
                                weight_info = self.ensemble_weights['reverse_psychology_predictor']
                                if prediction_correct is not None:
                                    # æ›´æ–°æ€§èƒ½å†å²
                                    if 'performance_history' not in weight_info:
                                        weight_info['performance_history'] = []
                                    weight_info['performance_history'].append(prediction_correct)
                                    if len(weight_info['performance_history']) > 100:
                                        weight_info['performance_history'].pop(0)
                            
                            learning_results['reverse_psychology_predictor'] = 'success (basic weight update)'
                    else:
                        learning_results['reverse_psychology_predictor'] = 'skipped (no last prediction result)'
                        print("âš ï¸ åå‘å¿ƒç†å­¦æ¨¡å‹å­¦ä¹ è·³è¿‡ï¼šæ— ä¸Šæ¬¡é¢„æµ‹ç»“æœ")
                
                except Exception as e:
                    print(f"âŒ åå‘å¿ƒç†å­¦æ¨¡å‹å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['reverse_psychology_predictor'] = f'failed: {str(e)}'
            
            # === åæ“æ§åˆ†æå™¨å­¦ä¹  ===
            if hasattr(self, 'banker_behavior_analyzer') and self.banker_behavior_analyzer and self.last_prediction_result:
                try:
                    # æ„å»ºé¢„æµ‹ç»“æœç”¨äºå­¦ä¹ 
                    model_predictions_dict = self.last_prediction_result.get('model_predictions', {})
                    anti_manipulation_predictions = model_predictions_dict.get('anti_manipulation_banker_behavior', {})

                    if anti_manipulation_predictions:
                        # æ‰¾åˆ°é¢„æµ‹æ¦‚ç‡æœ€é«˜çš„å°¾æ•°ä½œä¸ºæ¨è
                        recommended_tails = [tail for tail, prob in anti_manipulation_predictions.items() if prob > 0.6]
                        avg_confidence = sum(anti_manipulation_predictions.values()) / len(anti_manipulation_predictions) if anti_manipulation_predictions else 0.5
                    else:
                        recommended_tails = []
                        avg_confidence = 0.5

                    anti_prediction = {
                        'recommended_tails': recommended_tails,
                        'confidence': avg_confidence,
                        'success': True
                    }
        
                    # åæ“æ§åˆ†æå™¨é€šå¸¸æ²¡æœ‰ä¸“é—¨çš„å­¦ä¹ æ–¹æ³•ï¼Œæ‰€ä»¥è¿›è¡ŒåŸºç¡€æ›´æ–°
                    learning_results['anti_manipulation_banker_behavior'] = 'success (basic update)'
                    print("âœ… åæ“æ§åˆ†æå™¨åŸºç¡€å­¦ä¹ å®Œæˆ")
        
                except Exception as e:
                    print(f"âŒ åæ“æ§åˆ†æå™¨å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['anti_manipulation_banker_behavior'] = f'failed: {str(e)}'

            # === å†·é—¨æŒ–æ˜å™¨å­¦ä¹  ===
            if hasattr(self, 'unpopular_digger') and self.unpopular_digger and self.last_prediction_result:
                try:
                    # æ„å»ºé¢„æµ‹ç»“æœç”¨äºå­¦ä¹ 
                    model_predictions_dict = self.last_prediction_result.get('model_predictions', {})
                    unpopular_digger_predictions = model_predictions_dict.get('unpopular_digger', {})

                    if unpopular_digger_predictions:
                        # æ‰¾åˆ°é¢„æµ‹æ¦‚ç‡æœ€é«˜çš„å°¾æ•°ä½œä¸ºæ¨è
                        recommended_tails = [tail for tail, prob in unpopular_digger_predictions.items() if prob > 0.6]
                        avg_confidence = sum(unpopular_digger_predictions.values()) / len(unpopular_digger_predictions) if unpopular_digger_predictions else 0.5
                    else:
                        recommended_tails = []
                        avg_confidence = 0.5

                    digger_prediction = {
                        'recommended_tails': recommended_tails,
                        'confidence': avg_confidence,
                        'success': True
                    }
        
                    learn_result = self.unpopular_digger.learn_from_outcome(
                        digger_prediction, 
                        actual_tails
                    )
        
                    if learn_result and learn_result.get('learning_success', False):
                        learning_results['unpopular_digger'] = f'success (accuracy: {learn_result.get("recommendation_accuracy", 0):.3f})'
                        print(f"âœ… å†·é—¨æŒ–æ˜å™¨å­¦ä¹ å®Œæˆï¼Œæ¨èå‡†ç¡®ç‡: {learn_result.get('recommendation_accuracy', 0):.3f}")
                        print(f"   å†·é—¨å¤å‡ºæˆåŠŸç‡: {learn_result.get('revival_success_rate', 0):.3f}")
                    else:
                        learning_results['unpopular_digger'] = 'success (basic update)'
                        print("âœ… å†·é—¨æŒ–æ˜å™¨åŸºç¡€å­¦ä¹ å®Œæˆ")
    
                except Exception as e:
                    print(f"âŒ å†·é—¨æŒ–æ˜å™¨å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['unpopular_digger'] = f'failed: {str(e)}'

            # === èµ„é‡‘æµå‘åˆ†æå™¨å­¦ä¹  ===
            if hasattr(self, 'money_flow_analyzer') and self.money_flow_analyzer and self.last_prediction_result:
                try:
                    # ä»ä¸Šæ¬¡é¢„æµ‹ç»“æœä¸­è·å–èµ„é‡‘æµå‘åˆ†æ
                    stored_flow_analysis = self.last_prediction_result.get('money_flow_analysis')
                    
                    if stored_flow_analysis:
                        # è®©æ¨¡å‹ä»ç»“æœä¸­å­¦ä¹ 
                        learn_result = self.money_flow_analyzer.learn_from_outcome(
                            stored_flow_analysis,
                            actual_tails
                        )
                        
                        if learn_result and learn_result.get('learning_success', False):
                            learning_results['money_flow_analyzer'] = f'success (accuracy: {learn_result.get("overall_accuracy", 0):.3f})'
                            print(f"âœ… èµ„é‡‘æµå‘åˆ†æå™¨å­¦ä¹ å®Œæˆï¼Œæ€»ä½“å‡†ç¡®ç‡: {learn_result.get('overall_accuracy', 0):.3f}")
                        else:
                            learning_results['money_flow_analyzer'] = 'success (basic update)'
                            print("âœ… èµ„é‡‘æµå‘åˆ†æå™¨åŸºç¡€å­¦ä¹ å®Œæˆ")
                    else:
                        learning_results['money_flow_analyzer'] = 'skipped (no stored analysis)'
                        print("âš ï¸ èµ„é‡‘æµå‘åˆ†æå™¨å­¦ä¹ è·³è¿‡ï¼šæ— å­˜å‚¨çš„åˆ†æç»“æœ")
                
                except Exception as e:
                    print(f"âŒ èµ„é‡‘æµå‘åˆ†æå™¨å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['money_flow_analyzer'] = f'failed: {str(e)}'

            # === åšå¼ˆè®ºç­–ç•¥å™¨å­¦ä¹  ===
            if hasattr(self, 'game_theory_strategist') and self.game_theory_strategist and self.last_prediction_result:
                try:
                    # ä»ä¸Šæ¬¡é¢„æµ‹ç»“æœä¸­è·å–åšå¼ˆè®ºåˆ†æ
                    stored_game_analysis = self.last_prediction_result.get('game_theory_analysis')
        
                    if stored_game_analysis:
                        # è®©æ¨¡å‹ä»ç»“æœä¸­å­¦ä¹ 
                        learn_result = self.game_theory_strategist.learn_from_outcome(
                            stored_game_analysis,
                            actual_tails
                        )
            
                        if learn_result and learn_result.get('learning_success', False):
                            performance_metrics = learn_result.get('performance_metrics', {})
                            accuracy = performance_metrics.get('accuracy', 0)
                            strategy_type = performance_metrics.get('strategy_type', 'unknown')
                
                            learning_results['game_theory_strategist'] = f'success (accuracy: {accuracy:.3f}, strategy: {strategy_type})'
                            print(f"âœ… åšå¼ˆè®ºç­–ç•¥å™¨å­¦ä¹ å®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.3f}, ç­–ç•¥ç±»å‹: {strategy_type}")
                
                            # æ˜¾ç¤ºç­–ç•¥è¡¨ç°è¯¦æƒ…
                            recent_accuracy = performance_metrics.get('recent_accuracy', 0)
                            total_predictions = performance_metrics.get('total_predictions', 0)
                            print(f"   ğŸ“Š æœ€è¿‘å‡†ç¡®ç‡: {recent_accuracy:.3f}, æ€»é¢„æµ‹æ¬¡æ•°: {total_predictions}")
                        else:
                            learning_results['game_theory_strategist'] = 'success (basic update)'
                            print("âœ… åšå¼ˆè®ºç­–ç•¥å™¨åŸºç¡€å­¦ä¹ å®Œæˆ")
                    else:
                        learning_results['game_theory_strategist'] = 'skipped (no stored analysis)'
                        print("âš ï¸ åšå¼ˆè®ºç­–ç•¥å™¨å­¦ä¹ è·³è¿‡ï¼šæ— å­˜å‚¨çš„åˆ†æç»“æœ")
    
                except Exception as e:
                    print(f"âŒ åšå¼ˆè®ºç­–ç•¥å™¨å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['game_theory_strategist'] = f'failed: {str(e)}'

            # === æ“æ§æ—¶æœºæ£€æµ‹å™¨å­¦ä¹  ===
            if hasattr(self, 'manipulation_timing_detector') and self.manipulation_timing_detector and self.last_prediction_result:
                try:
                    # ä»ä¸Šæ¬¡é¢„æµ‹ç»“æœä¸­è·å–æ“æ§æ—¶æœºåˆ†æ
                    stored_timing_analysis = self.last_prediction_result.get('manipulation_timing_analysis')
        
                    if stored_timing_analysis:
                        # è®©æ¨¡å‹ä»ç»“æœä¸­å­¦ä¹ 
                        learn_result = self.manipulation_timing_detector.learn_from_outcome(
                            stored_timing_analysis,
                            actual_tails
                        )
            
                        if learn_result and learn_result.get('learning_success', False):
                            detection_accuracy = learn_result.get('detection_accuracy', 0)
                            total_predictions = learn_result.get('total_predictions', 0)
                
                            learning_results['manipulation_timing_detector'] = f'success (accuracy: {detection_accuracy:.3f}, total: {total_predictions})'
                            print(f"âœ… æ“æ§æ—¶æœºæ£€æµ‹å™¨å­¦ä¹ å®Œæˆï¼Œæ£€æµ‹å‡†ç¡®ç‡: {detection_accuracy:.3f}, æ€»é¢„æµ‹æ¬¡æ•°: {total_predictions}")
                        else:
                            learning_results['manipulation_timing_detector'] = 'success (basic update)'
                            print("âœ… æ“æ§æ—¶æœºæ£€æµ‹å™¨åŸºç¡€å­¦ä¹ å®Œæˆ")
                    else:
                        learning_results['manipulation_timing_detector'] = 'skipped (no stored analysis)'
                        print("âš ï¸ æ“æ§æ—¶æœºæ£€æµ‹å™¨å­¦ä¹ è·³è¿‡ï¼šæ— å­˜å‚¨çš„åˆ†æç»“æœ")
    
                except Exception as e:
                    print(f"âŒ æ“æ§æ—¶æœºæ£€æµ‹å™¨å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['manipulation_timing_detector'] = f'failed: {str(e)}'

            # === åè¶‹åŠ¿çŒæ‰‹å­¦ä¹  ===
            if hasattr(self, 'anti_trend_hunter') and self.anti_trend_hunter and self.last_prediction_result:
                try:
                    # ä»ä¸Šæ¬¡é¢„æµ‹ç»“æœä¸­è·å–åè¶‹åŠ¿åˆ†æ
                    stored_anti_trend_analysis = self.last_prediction_result.get('anti_trend_analysis')
        
                    if stored_anti_trend_analysis:
                        # è®©æ¨¡å‹ä»ç»“æœä¸­å­¦ä¹ 
                        learn_result = self.anti_trend_hunter.learn_from_outcome(
                            stored_anti_trend_analysis,
                            actual_tails
                        )
            
                        if learn_result and learn_result.get('learning_success', False):
                            current_accuracy = learn_result.get('current_accuracy', 0)
                            strategy_performance = learn_result.get('strategy_performance', {})
                            total_predictions = learn_result.get('total_predictions', 0)
                
                            learning_results['anti_trend_hunter'] = f'success (accuracy: {current_accuracy:.3f}, total: {total_predictions})'
                            print(f"âœ… åè¶‹åŠ¿çŒæ‰‹å­¦ä¹ å®Œæˆï¼Œæ€»ä½“å‡†ç¡®ç‡: {current_accuracy:.3f}, æ€»é¢„æµ‹æ¬¡æ•°: {total_predictions}")
                
                            # æ˜¾ç¤ºç­–ç•¥è¡¨ç°è¯¦æƒ…
                            trend_break_rate = strategy_performance.get('trend_break_rate', 0)
                            reversal_rate = strategy_performance.get('reversal_rate', 0)
                            print(f"   ğŸ“Š è¶‹åŠ¿ç»ˆç»“æˆåŠŸç‡: {trend_break_rate:.3f}")
                            print(f"   ğŸ“Š åè½¬æˆåŠŸç‡: {reversal_rate:.3f}")
                        else:
                            learning_results['anti_trend_hunter'] = 'success (basic update)'
                            print("âœ… åè¶‹åŠ¿çŒæ‰‹åŸºç¡€å­¦ä¹ å®Œæˆ")
                    else:
                        learning_results['anti_trend_hunter'] = 'skipped (no stored analysis)'
                        print("âš ï¸ åè¶‹åŠ¿çŒæ‰‹å­¦ä¹ è·³è¿‡ï¼šæ— å­˜å‚¨çš„åˆ†æç»“æœ")
    
                except Exception as e:
                    print(f"âŒ åè¶‹åŠ¿çŒæ‰‹å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['anti_trend_hunter'] = f'failed: {str(e)}'

            # === ç¾¤ä½“å¿ƒç†åˆ†æå™¨å­¦ä¹  ===
            if hasattr(self, 'crowd_psychology_analyzer') and self.crowd_psychology_analyzer and self.last_prediction_result:
                try:
                    # ä»ä¸Šæ¬¡é¢„æµ‹ç»“æœä¸­è·å–ç¾¤ä½“å¿ƒç†åˆ†æ
                    stored_crowd_psychology_analysis = self.last_prediction_result.get('crowd_psychology_analysis')
        
                    if stored_crowd_psychology_analysis:
                        # è®©æ¨¡å‹ä»ç»“æœä¸­å­¦ä¹ 
                        learn_result = self.crowd_psychology_analyzer.learn_from_outcome(
                            stored_crowd_psychology_analysis,
                            actual_tails
                        )
            
                        if learn_result and learn_result.get('learning_success', False):
                            current_accuracy = learn_result.get('current_accuracy', 0)
                            contrarian_success_rate = learn_result.get('contrarian_success_rate', 0)
                            psychology_insights = learn_result.get('psychology_insights', {})
                            total_predictions = learn_result.get('total_predictions', 0)
                
                            learning_results['crowd_psychology_analyzer'] = f'success (accuracy: {current_accuracy:.3f}, contrarian: {contrarian_success_rate:.3f}, total: {total_predictions})'
                            print(f"âœ… ç¾¤ä½“å¿ƒç†åˆ†æå™¨å­¦ä¹ å®Œæˆï¼Œæ€»ä½“å‡†ç¡®ç‡: {current_accuracy:.3f}, æ€»é¢„æµ‹æ¬¡æ•°: {total_predictions}")
                            print(f"   ğŸ“Š åå‘ç­–ç•¥æˆåŠŸç‡: {contrarian_success_rate:.3f}")
                
                            # æ˜¾ç¤ºå¿ƒç†å­¦æ´å¯Ÿ
                            key_insights = psychology_insights.get('key_insights', [])
                            if key_insights:
                                print(f"   ğŸ§  å¿ƒç†å­¦æ´å¯Ÿ: {'; '.join(key_insights[:2])}")
                
                            crowd_behavior_accuracy = psychology_insights.get('crowd_behavior_accuracy', 0)
                            emotion_prediction_accuracy = psychology_insights.get('emotion_prediction_accuracy', 0)
                            print(f"   ğŸ“ˆ ç¾¤ä½“è¡Œä¸ºé¢„æµ‹å‡†ç¡®ç‡: {crowd_behavior_accuracy:.3f}")
                            print(f"   ğŸ˜¨ æƒ…ç»ªé¢„æµ‹å‡†ç¡®ç‡: {emotion_prediction_accuracy:.3f}")
                        else:
                            learning_results['crowd_psychology_analyzer'] = 'success (basic update)'
                            print("âœ… ç¾¤ä½“å¿ƒç†åˆ†æå™¨åŸºç¡€å­¦ä¹ å®Œæˆ")
                    else:
                        learning_results['crowd_psychology_analyzer'] = 'skipped (no stored analysis)'
                        print("âš ï¸ ç¾¤ä½“å¿ƒç†åˆ†æå™¨å­¦ä¹ è·³è¿‡ï¼šæ— å­˜å‚¨çš„åˆ†æç»“æœ")
    
                except Exception as e:
                    print(f"âŒ ç¾¤ä½“å¿ƒç†åˆ†æå™¨å­¦ä¹ å¤±è´¥: {e}")
                    learning_results['crowd_psychology_analyzer'] = f'failed: {str(e)}'
                    
            # Riveræ¨¡å‹å­¦ä¹ 
            for model_name, model in self.river_models.items():
                try:
                    # å†å²æ¨¡å¼åŒ¹é…ç®—æ³•ä¸éœ€è¦ä¼ ç»Ÿçš„å­¦ä¹ ï¼Œåªéœ€è¦æ›´æ–°å†å²æ•°æ®
                    if hasattr(model, 'update_historical_data'):
                        model.update_historical_data(data_list)
                        learning_results[f'river_{model_name}'] = 'success (pattern updated)'
                    else:
                        # æ™®é€šæ¨¡å‹å­¦ä¹ 
                        for tail in range(10):
                            y = 1 if tail in actual_tails else 0
                            if hasattr(model, 'learn_one'):
                                model.learn_one(X_river, y)
                            
                            # æ›´æ–°æ€§èƒ½åº¦é‡
                            if model_name in self.metrics_trackers:
                                # è¿™é‡Œå¯ä»¥è¿›è¡Œé¢„æµ‹æ¥æ›´æ–°åº¦é‡ï¼Œä½†ä¸ºäº†æ€§èƒ½è€ƒè™‘å¯ä»¥å®šæœŸæ›´æ–°
                                pass
                        
                        learning_results[f'river_{model_name}'] = 'success'
                
                except Exception as e:
                    print(f"Riveræ¨¡å‹ {model_name} å­¦ä¹ å¤±è´¥: {e}")
                    learning_results[f'river_{model_name}'] = f'failed: {e}'
            
            # scikit-multiflowæ¨¡å‹å­¦ä¹ 
            for model_name, model in self.sklearn_models.items():
                try:
                    for tail in range(10):
                        y = np.array([1 if tail in actual_tails else 0])
                        if hasattr(model, 'partial_fit'):
                            model.partial_fit(X_sklearn, y)
                    
                    learning_results[f'sklearn_{model_name}'] = 'success'
                
                except Exception as e:
                    print(f"sklearnæ¨¡å‹ {model_name} å­¦ä¹ å¤±è´¥: {e}")
                    learning_results[f'sklearn_{model_name}'] = f'failed: {e}'
            
            # PyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çº¿å­¦ä¹ 
            if PYTORCH_AVAILABLE and self.deep_learning_module and self.deep_learning_module.models:
                try:
                    print(f"ğŸ§  å¼€å§‹PyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨çº¿å­¦ä¹ ...")
                    dl_learning_results = self.deep_learning_module.learn_online_single(features, actual_tails)
                    
                    for model_name, result in dl_learning_results.items():
                        full_model_name = f'pytorch_{model_name}'
                        if result.get('status') == 'success':
                            learning_results[full_model_name] = f'success (loss: {result["loss"]:.4f}, acc: {result["accuracy"]:.3f})'
                            print(f"   âœ“ {model_name}: æŸå¤±={result['loss']:.4f}, å‡†ç¡®ç‡={result['accuracy']:.3f}")
                            
                            # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
                            if result['loss'] > 0.8:  # å¦‚æœæŸå¤±è¾ƒé«˜ï¼Œé™ä½å­¦ä¹ ç‡
                                self.deep_learning_module.update_learning_rate(model_name, factor=0.9)
                        else:
                            learning_results[full_model_name] = f'failed: {result.get("error", "unknown")}'
                            print(f"   âœ— {model_name}: å­¦ä¹ å¤±è´¥")
                    
                    print(f"âœ… PyTorchæ·±åº¦å­¦ä¹ åœ¨çº¿å­¦ä¹ å®Œæˆ")
                
                except Exception as e:
                    print(f"âŒ PyTorchæ·±åº¦å­¦ä¹ åœ¨çº¿å­¦ä¹ å¤±è´¥: {e}")
                    # ä¸ºæ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹æ·»åŠ å¤±è´¥è®°å½•
                    if hasattr(self, 'deep_learning_module') and self.deep_learning_module and self.deep_learning_module.models:
                        for model_name in self.deep_learning_module.models.keys():
                            learning_results[f'pytorch_{model_name}'] = f'failed: {str(e)}'
            
            # === å¤„ç†æƒé‡æŠ•èµ„ç»“ç®— ===
            if prediction_correct is not None and hasattr(self, 'current_weight_investments') and self.current_weight_investments:
                self.settle_weight_investments(actual_tails)
            elif prediction_correct is not None:
                # å¦‚æœæ²¡æœ‰æŠ•èµ„è®°å½•ï¼Œæ›´æ–°ä¼ ç»Ÿçš„æ€§èƒ½å†å²
                print("ğŸ“Š æ›´æ–°ä¼ ç»Ÿæ€§èƒ½å†å²ï¼ˆæ— æŠ•èµ„è®°å½•ï¼‰")
                for model_key in self.ensemble_weights.keys():
                    weight_info = self.ensemble_weights[model_key]
                    weight_info['performance_history'].append(prediction_correct)
                    if len(weight_info['performance_history']) > 100:
                        weight_info['performance_history'].pop(0)
            
            # === æ›´æ–°æ™ºèƒ½ç‰¹å¾å¤„ç†ç»„ä»¶ ===
            if prediction_correct is not None:
                accuracy = 1.0 if prediction_correct else 0.0
                
                # æ›´æ–°æ™ºèƒ½ç‰¹å¾å¤„ç†ç»„ä»¶
                self.feature_engineer.update_components_with_learning_result(features, accuracy)

            # === å¤„ç†æ¦‚å¿µæ¼‚ç§» ===
            if drift_detected_by:
                self.handle_concept_drift(drift_detected_by)
            
            # === ä¿å­˜å­¦ä¹ è®°å½• ===
            self.save_learning_record(base_features, actual_tails, prediction_correct, drift_detected_by)
            
            # === ä¿å­˜å„æ¨¡å‹çš„é¢„æµ‹è®°å½• ===
            self._save_model_predictions(actual_tails)
            
            # === å®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€ ===
            if self.total_samples_seen % 10 == 0:  # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ï¼Œç¡®ä¿åŠæ—¶ä¿å­˜
                self.save_model_state()
            elif self.total_samples_seen <= 50:  # å‰50ä¸ªæ ·æœ¬æ¯æ¬¡éƒ½ä¿å­˜ï¼Œç¡®ä¿æ—©æœŸæ•°æ®ä¸ä¸¢å¤±
                self.save_model_state()
            
            # è§¦å‘UIæ›´æ–°ï¼ˆå¦‚æœæœ‰ä¸»åº”ç”¨å¼•ç”¨ï¼‰
            if hasattr(self, '_main_app_ref') and self._main_app_ref is not None:
                try:
                    # åŒæ—¶è§¦å‘ ui_updates å’Œ ai_display_manager çš„æ›´æ–°
                    if hasattr(self._main_app_ref, 'ui_updates') and self._main_app_ref.ui_updates is not None:
                        # å…ˆæ›´æ–°åŸºç¡€UIç»„ä»¶
                        self._main_app_ref.root.after(200, self._main_app_ref.ui_updates.update_learning_progress_display)
                        print("ğŸ”„ å·²å®‰æ’åŸºç¡€UIæ›´æ–°ä»»åŠ¡")
                    
                    if hasattr(self._main_app_ref, 'ai_display_manager') and self._main_app_ref.ai_display_manager is not None:
                        # å†æ›´æ–°è¯¦ç»†çš„AIæ˜¾ç¤º
                        self._main_app_ref.root.after(500, self._main_app_ref.ai_display_manager.trigger_update_after_learning)
                        print("ğŸ”„ å·²å®‰æ’AIè¯¦ç»†æ˜¾ç¤ºæ›´æ–°ä»»åŠ¡")
                except Exception as e:
                    print(f"è§¦å‘AIå­¦ä¹ åæ›´æ–°å¤±è´¥: {e}")

            return {
                'success': True,
                'samples_processed': self.total_samples_seen,
                'prediction_correct': prediction_correct,
                'drift_detected': len(drift_detected_by) > 0,
                'drift_detectors': drift_detected_by,
                'learning_results': learning_results,
                'current_accuracy': self.get_current_accuracy(),
                'message': 'ç»ˆæåœ¨çº¿å­¦ä¹ å®Œæˆ'
            }
            
        except Exception as e:
            return {'success': False, 'message': f'åœ¨çº¿å­¦ä¹ å¤±è´¥: {str(e)}'}
    
    def batch_pretrain(self, data_list: List[Dict]) -> Dict:
        """æ‰¹é‡é¢„è®­ç»ƒæ¨¡å‹ - åŸºç¡€æ•°æ®é‡é˜ˆå€¼ç­–ç•¥"""
        if not self.is_initialized:
            return {'success': False, 'message': 'æ¨¡å‹æœªåˆå§‹åŒ–'}
    
        if len(data_list) < 5:
            return {'success': False, 'message': 'æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘5æœŸæ•°æ®'}
    
        try:
            # ğŸ”§ ä¿®å¤æ—¶é—´æƒé‡é¡ºåºé—®é¢˜ï¼šåè½¬æ•°æ®åˆ—è¡¨
            # ç”¨æˆ·çš„æ•°æ®æ˜¯æœ€æ–°åœ¨å‰(index 0)ï¼Œæœ€è€åœ¨å
            # ä½†å­¦ä¹ éœ€è¦ä»æœ€è€å¼€å§‹ï¼Œæ‰€ä»¥åè½¬åˆ—è¡¨
            print(f"ğŸ”„ ä¿®å¤æ•°æ®é¡ºåºï¼šåŸå§‹æ•°æ®æœ€æ–°åœ¨index 0ï¼Œåè½¬åæœ€è€æ•°æ®åœ¨index 0")
            print(f"   åè½¬å‰ï¼šdata_list[0] = æœ€æ–°æœŸ")
            print(f"   åè½¬åï¼šdata_list[0] = æœ€è€æœŸ") 
        
            reversed_data_list = list(reversed(data_list))
            data_list = reversed_data_list  # ä½¿ç”¨åè½¬åçš„æ•°æ®
        
            print(f"âœ… æ•°æ®é¡ºåºå·²ä¿®å¤ï¼Œç°åœ¨ä»æœ€è€æ•°æ®å¼€å§‹å­¦ä¹ ï¼Œæ—¶é—´æƒé‡æ­£ç¡®")
        
            # è®¾ç½®åŸºç¡€æ•°æ®é‡é˜ˆå€¼ï¼ˆä»é…ç½®ä¸­è¯»å–ï¼‰
            base_data_threshold = self.learning_config.get('base_training_data_threshold', 500)
            total_data_count = len(data_list)
        
            print(f"ğŸš€ å¼€å§‹æ‰¹é‡é¢„è®­ç»ƒï¼Œæ€»å†å²æ•°æ®ï¼š{total_data_count}æœŸ")
            print(f"ğŸ“Š åŸºç¡€æ•°æ®é‡é˜ˆå€¼ï¼š{base_data_threshold}æœŸ")
        
            # è®¡ç®—è®­ç»ƒèµ·å§‹ä½ç½®
            if total_data_count > base_data_threshold:
                # æœ‰è¶³å¤Ÿæ•°æ®ï¼Œä»T(total-base_threshold)å¼€å§‹è®­ç»ƒ
                start_training_index = total_data_count - base_data_threshold
                print(f"âœ… æ•°æ®å……è¶³ï¼Œä»ç¬¬{start_training_index}æœŸå¼€å§‹è®­ç»ƒï¼ˆç¡®ä¿æœ‰{base_data_threshold}æœŸåŸºç¡€æ•°æ®ï¼‰")
            else:
                # æ•°æ®ä¸è¶³é˜ˆå€¼ï¼Œä»èƒ½è®­ç»ƒçš„æœ€æ—©ä½ç½®å¼€å§‹
                start_training_index = total_data_count - 1
                actual_base_data = total_data_count - 1
                print(f"âš ï¸ æ•°æ®ä¸è¶³é˜ˆå€¼ï¼Œä»ç¬¬{start_training_index}æœŸå¼€å§‹è®­ç»ƒï¼ˆå®é™…åŸºç¡€æ•°æ®ï¼š{actual_base_data}æœŸï¼‰")
        
            successful_samples = 0
            total_samples = 0
        
            # ä»èµ·å§‹ä½ç½®å¼€å§‹è®­ç»ƒï¼Œé€æ­¥å‘T0è®­ç»ƒ
            print(f"ğŸ“š è®­ç»ƒç­–ç•¥ï¼šä»T{start_training_index} â†’ T0ï¼Œç¡®ä¿å……è¶³å†å²æ•°æ®")
        
            for i in range(start_training_index, 0, -1):  # ä»start_training_indexåˆ°1
                try:
                    total_samples += 1
                
                    # ä½¿ç”¨ä»ç¬¬iæœŸåˆ°æœ€æ–°çš„æ‰€æœ‰æ•°æ®ä½œä¸ºå†å²ç‰¹å¾æ•°æ®
                    feature_data = data_list[i:]  # T[i]åˆ°T0çš„æ‰€æœ‰æ•°æ®
                    # è¦å­¦ä¹ çš„ç›®æ ‡æ˜¯ç¬¬i-1æœŸçš„å°¾æ•°
                    actual_tails = data_list[i-1].get('tails', [])
                
                    # éªŒè¯æ•°æ®è´¨é‡
                    history_data_count = len(feature_data)
                    if history_data_count < 5 or not actual_tails:
                        print(f"   âš ï¸ è·³è¿‡è®­ç»ƒæ ·æœ¬T{i-1}ï¼šå†å²æ•°æ®{history_data_count}æœŸï¼Œç›®æ ‡å°¾æ•°{len(actual_tails)}ä¸ª")
                        continue
                
                    # æ‰§è¡Œè®­ç»ƒ
                    result = self.learn_online(feature_data, actual_tails)
                    if result.get('success', False):
                        successful_samples += 1
                
                    # å®šæœŸæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
                    if total_samples % 50 == 0:
                        current_position = start_training_index - total_samples + 1
                        progress_percent = (total_samples / start_training_index) * 100
                        print(f"   ğŸ“ˆ è®­ç»ƒè¿›åº¦ï¼šT{current_position} ({progress_percent:.1f}%) "
                            f"æˆåŠŸç‡:{successful_samples}/{total_samples} "
                            f"å†å²æ•°æ®:{history_data_count}æœŸ")
                
                except Exception as e:
                    print(f"   âŒ è®­ç»ƒæ ·æœ¬T{i-1}å¤±è´¥: {e}")
                    continue
        
            success_rate = successful_samples / total_samples if total_samples > 0 else 0
        
            print(f"âœ… æ‰¹é‡é¢„è®­ç»ƒå®Œæˆ")
            print(f"   ğŸ“Š è®­ç»ƒç»Ÿè®¡ï¼šæˆåŠŸ{successful_samples}/{total_samples} (æˆåŠŸç‡:{success_rate:.1%})")
            print(f"   ğŸ“Š æœ€ç»ˆå†å²æ•°æ®é‡ï¼š{len(data_list)}æœŸ")
        
            return {
                'success': successful_samples > 0,
                'total_samples': total_samples,
                'successful_samples': successful_samples,
                'success_rate': success_rate,
                'base_data_threshold': base_data_threshold,
                'actual_base_data': len(data_list[start_training_index:]) if start_training_index < len(data_list) else 0,
                'message': f'æ‰¹é‡é¢„è®­ç»ƒå®Œæˆï¼ŒæˆåŠŸè®­ç»ƒ {successful_samples}/{total_samples} ä¸ªæ ·æœ¬'
            }
        
        except Exception as e:
            return {'success': False, 'message': f'æ‰¹é‡é¢„è®­ç»ƒå¤±è´¥: {str(e)}'}
    
    def deep_learning_batch_train(self, data_list: List[Dict], epochs=50) -> Dict:
        """æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒ"""
        if not PYTORCH_AVAILABLE or not self.deep_learning_module:
            return {'success': False, 'message': 'PyTorchæ·±åº¦å­¦ä¹ æ¨¡å—ä¸å¯ç”¨'}
        
        if not self.is_initialized:
            return {'success': False, 'message': 'åŸºç¡€æ¨¡å‹æœªåˆå§‹åŒ–'}
        
        if len(data_list) < 30:
            return {'success': False, 'message': 'æ·±åº¦å­¦ä¹ éœ€è¦è‡³å°‘30æœŸæ•°æ®'}
        
        try:
            print(f"ğŸš€ å¼€å§‹æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒ...")
            
            # æ‰§è¡Œæ‰¹é‡è®­ç»ƒ
            training_result = self.deep_learning_module.batch_train(
                data_list, 
                epochs=epochs, 
                validation_split=0.2
            )
            
            if training_result['success']:
                print(f"âœ… æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒå®Œæˆ")
                
                # æ›´æ–°é›†æˆæƒé‡ä»¥åŒ…å«æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
                self._update_ensemble_weights_for_deep_learning()
                
                # ä¿å­˜æ¨¡å‹çŠ¶æ€
                self.save_model_state()
                
                return {
                    'success': True,
                    'message': 'æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒæˆåŠŸ',
                    'training_details': training_result,
                    'models_trained': list(self.deep_learning_module.models.keys())
                }
            else:
                return training_result
                
        except Exception as e:
            print(f"âŒ æ·±åº¦å­¦ä¹ æ‰¹é‡è®­ç»ƒå¤±è´¥: {e}")
            traceback.print_exc()
            return {'success': False, 'message': f'æ·±åº¦å­¦ä¹ è®­ç»ƒå¤±è´¥: {str(e)}'}
    
    def _update_ensemble_weights_for_deep_learning(self):
        """ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹æ›´æ–°é›†æˆæƒé‡"""
        if not PYTORCH_AVAILABLE or not self.deep_learning_module:
            return
    
        # ä¸ºæ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ·»åŠ æƒé‡
        for model_name in self.deep_learning_module.models.keys():
            pytorch_model_key = f'pytorch_{model_name}'
            if pytorch_model_key not in self.ensemble_weights:
                # è®¡ç®—å½“å‰æ‰€æœ‰æ¨¡å‹çš„å¹³å‡æƒé‡ä½œä¸ºæ–°æ¨¡å‹çš„åˆå§‹æƒé‡
                current_weights = [w['weight'] for w in self.ensemble_weights.values()]
                if current_weights:
                    avg_weight = sum(current_weights) / len(current_weights)
                    initial_weight = min(0.1, avg_weight)  # ä½¿ç”¨å¹³å‡æƒé‡ï¼Œä½†ä¸è¶…è¿‡0.1
                else:
                    initial_weight = 0.1
            
                self.ensemble_weights[pytorch_model_key] = {
                    'weight': initial_weight,          # æ´»è·ƒæƒé‡
                    'frozen_weight': 0.0,             # å†»ç»“æƒé‡
                    'total_weight': initial_weight,    # æ€»æƒé‡
                    'is_frozen': False,               # å†»ç»“çŠ¶æ€
                    'frozen_timestamp': None,         # å†»ç»“æ—¶é—´
                    'confidence': 0.6,
                    'last_update': datetime.now(),
                    'performance_history': []
                }
    
        # é‡æ–°å½’ä¸€åŒ–æ‰€æœ‰æƒé‡
        total_weight = sum(w['weight'] for w in self.ensemble_weights.values())
        if total_weight > 0:
            for model_key in list(self.ensemble_weights.keys()):
                self.ensemble_weights[model_key]['weight'] /= total_weight
    
        print(f"âœ… é›†æˆæƒé‡å·²æ›´æ–°ï¼ŒåŒ…å«æ·±åº¦å­¦ä¹ æ¨¡å‹")
    
    def get_deep_learning_stats(self) -> Dict:
        """è·å–æ·±åº¦å­¦ä¹ è®­ç»ƒç»Ÿè®¡"""
        if not PYTORCH_AVAILABLE or not self.deep_learning_module:
            return {'available': False, 'message': 'PyTorchæ·±åº¦å­¦ä¹ æ¨¡å—ä¸å¯ç”¨'}
        
        try:
            training_stats = self.deep_learning_module.get_training_stats()
            
            return {
                'available': True,
                'device': str(self.deep_learning_module.device),
                'models': list(self.deep_learning_module.models.keys()),
                'training_stats': training_stats,
                'batch_size': self.deep_learning_module.batch_size,
                'sequence_length': self.deep_learning_module.sequence_length
            }
            
        except Exception as e:
            return {'available': False, 'message': f'è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}'}
        
    def calculate_ensemble_confidence(self, recommended_tails: List[int], 
                                    ensemble_probabilities: Dict[int, float], 
                                    all_predictions: Dict) -> float:
        """è®¡ç®—é›†æˆç½®ä¿¡åº¦"""
        if not recommended_tails:
            return 0.0
        
        # åŸºäºé›†æˆæ¦‚ç‡
        tail_prob = ensemble_probabilities.get(recommended_tails[0], 0.5)
        
        # åŸºäºæ¨¡å‹ä¸€è‡´æ€§
        if recommended_tails:
            target_tail = recommended_tails[0]
            tail_predictions = [pred.get(target_tail, 0.5) for pred in all_predictions.values()]
            consistency = 1.0 - np.std(tail_predictions) if tail_predictions else 0.0
        else:
            consistency = 0.0
        
        # åŸºäºå†å²å‡†ç¡®ç‡
        historical_accuracy = self.get_current_accuracy()
        
        # åŸºäºæ ·æœ¬æ•°é‡
        sample_factor = min(1.0, self.total_samples_seen / 100)
        
        # ç»¼åˆç½®ä¿¡åº¦
        confidence = (tail_prob * 0.35 + consistency * 0.25 + 
                     historical_accuracy * 0.25 + sample_factor * 0.15)
        
        return min(max(confidence, 0.2), 0.95)
    
    def get_model_effective_weight(self, model_key):
        """è·å–æ¨¡å‹çš„æœ‰æ•ˆæƒé‡ï¼ˆåŒ…æ‹¬å†»ç»“æƒé‡ï¼‰"""
        if model_key in self.ensemble_weights:
            weight_info = self.ensemble_weights[model_key]
            active_weight = weight_info.get('weight', 0.0)
            frozen_weight = weight_info.get('frozen_weight', 0.0)
            return active_weight + frozen_weight
        return 0.0
    
    def settle_investments(self, actual_tails: List[int]):
        """ç»“ç®—æŠ•èµ„ç»“æœ - æ–°çš„ç®€åŒ–ç‰ˆæœ¬"""
        if not hasattr(self, 'current_investments'):
            return
    
    def freeze_investment_weights(self, model_weight_investments):
        """å†»ç»“æ¨¡å‹çš„æŠ•èµ„æƒé‡"""
        return self.investment_system.freeze_investment_weights(model_weight_investments, self.ensemble_weights)

    def settle_weight_investments(self, actual_tails: List[int]):
        """ç»“ç®—æƒé‡æŠ•èµ„ç»“æœ"""
        if not hasattr(self, 'current_weight_investments'):
            return
    
        # ä½¿ç”¨æŠ•èµ„ç®¡ç†ç³»ç»Ÿè¿›è¡Œç»“ç®—
        settlement_results = self.investment_system.settle_weight_investments(
            actual_tails, self.current_weight_investments, self.ensemble_weights
        )
    
        # è§£å†»æƒé‡å¹¶åº”ç”¨ç»“ç®—ç»“æœ
        unfrozen_count, final_rewards, final_penalties = self.investment_system.unfreeze_and_settle_weights(
            settlement_results, self.ensemble_weights
        )
    
        # æ›´æ–°æ¨¡å‹çš„æŠ•èµ„å†å²å’Œæ€§èƒ½å†å²
        for model_key, settlement in settlement_results.items():
            self.investment_system.update_investment_history(
                model_key, settlement, self.current_weight_investments, self.ensemble_weights
            )
    
        # æ­£ç¡®æ›´æ–°æŠ•èµ„ç»Ÿè®¡ï¼ˆæ¯æ¬¡ç»“ç®—åªå¢åŠ 1è½®ï¼‰
        if hasattr(self, 'current_weight_investments') and self.current_weight_investments:
            self.investment_stats['total_rounds'] += 1  # åªåœ¨æœ‰å®é™…æŠ•èµ„æ—¶å¢åŠ è½®æ•°
            print(f"ğŸ“Š æŠ•èµ„è½®æ•°ç»Ÿè®¡æ›´æ–°: å½“å‰è½®æ•° {self.investment_stats['total_rounds']}")
        self.investment_stats['total_rewards'] += final_rewards
        self.investment_stats['total_penalties'] += final_penalties
    
        # æƒé‡å½’ä¸€åŒ–
        self.investment_system.normalize_weights(self.ensemble_weights)
    
        print(f"ğŸ’° æƒé‡æŠ•èµ„ç»“ç®—å®Œæˆ:")
        print(f"   æ€»å¥–åŠ±æƒé‡: {final_rewards:.4f}")
        print(f"   æ€»æƒ©ç½šæƒé‡: {final_penalties:.4f}")
        print(f"   å‡€æƒé‡å˜åŒ–: {final_rewards - final_penalties:.4f}")
    
        # æ¸…ç©ºå½“å‰æƒé‡æŠ•èµ„è®°å½•
        delattr(self, 'current_weight_investments')
    
    def get_investment_strategy_analysis(self):
        """è·å–æŠ•èµ„ç­–ç•¥åˆ†æ"""
        current_investments = getattr(self, 'current_weight_investments', {})
        if not current_investments:
            return {'status': 'no_active_investments'}
    
        # è·å–æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆç”¨äºç­–ç•¥åˆ†æï¼‰
        all_predictions = getattr(self, 'last_prediction_result', {}).get('model_predictions', {})
    
        return self.investment_system.get_investment_strategy_analysis(current_investments, all_predictions)
    
    def clear_failed_investments(self):
        """æ¸…ç†å¤±è´¥çš„æŠ•èµ„çŠ¶æ€"""
        unfrozen_count = self.investment_system.clear_failed_investments(self.ensemble_weights)
    
        # æ¸…ç©ºæƒé‡æŠ•èµ„è®°å½•
        if hasattr(self, 'current_weight_investments'):
            self.current_weight_investments = {}
    
        return unfrozen_count > 0

    def _normalize_weights(self):
        """å½’ä¸€åŒ–æ‰€æœ‰æ¨¡å‹æƒé‡ï¼ˆåªå½’ä¸€åŒ–å¯ç”¨æƒé‡ï¼Œå†»ç»“æƒé‡ä¿æŒä¸å˜ï¼‰"""
        self.investment_system.normalize_weights(self.ensemble_weights)

    def handle_concept_drift(self, drift_detectors: List[str]):
        """å¤„ç†æ¦‚å¿µæ¼‚ç§»"""
        # ä½¿ç”¨æ¼‚ç§»æ£€æµ‹ç®¡ç†å™¨å¤„ç†æ¦‚å¿µæ¼‚ç§»
        self.drift_manager.handle_concept_drift(
            drift_detectors, 
            self.ensemble_weights, 
            self.learning_config, 
            self.deep_learning_module
        )
    
        # é‡ç½®æ¼‚ç§»æ£€æµ‹å™¨
        self.drift_manager.reset_drift_detectors(self.drift_detectors)
    
    def _handle_advanced_concept_drift(self, drift_info: Dict):
        """å¤„ç†é«˜çº§æ¦‚å¿µæ¼‚ç§»"""
        # ä½¿ç”¨æ¼‚ç§»æ£€æµ‹ç®¡ç†å™¨å¤„ç†é«˜çº§æ¦‚å¿µæ¼‚ç§»
        self.drift_manager.handle_advanced_concept_drift(
            drift_info,
            self.feature_selector,
            self.feature_weighter,
            self.ensemble_weights,
            self.deep_learning_module
        )
    
        # é‡ç½®é«˜çº§æ¼‚ç§»æ£€æµ‹å™¨
        if hasattr(self, 'advanced_drift_detector'):
            self.advanced_drift_detector.reset()

    def reset_poor_performing_models(self):
        """é‡ç½®è¡¨ç°è¾ƒå·®çš„æ¨¡å‹"""
        self.drift_manager.reset_poor_performing_models(self.ensemble_weights)
    
    def adjust_learning_parameters(self):
        """è°ƒæ•´å­¦ä¹ å‚æ•°"""
        self.drift_manager.adjust_learning_parameters(self.learning_config, self.deep_learning_module)
    
    def get_current_accuracy(self) -> float:
        """è·å–å½“å‰å‡†ç¡®ç‡"""
        if self.total_samples_seen == 0:
            return 0.5
        
        return self.correct_predictions / self.total_samples_seen
    
    def process_new_sample(self, data_list: List[Dict], actual_tails: List[int] = None) -> Dict:
        """å¤„ç†æ–°æ ·æœ¬ - é¢„æµ‹å¹¶å­¦ä¹ """
        # é¦–å…ˆè¿›è¡Œé¢„æµ‹
        prediction_result = self.predict_online(data_list)
        
        # å¦‚æœæœ‰å®é™…ç»“æœï¼Œè¿›è¡Œåœ¨çº¿å­¦ä¹ 
        if actual_tails is not None:
            learning_result = self.learn_online(data_list, actual_tails)
            
            return {
                'prediction': prediction_result,
                'learning': learning_result,
                'total_samples': self.total_samples_seen,
                'current_accuracy': self.get_current_accuracy(),
                'ensemble_weights': {k: v['weight'] for k, v in self.ensemble_weights.items()}
            }
        
        return {'prediction': prediction_result}
    
    def save_learning_record(self, features: np.ndarray, actual_tails: List[int], 
                            prediction_correct: Optional[bool], drift_detectors: List[str]):
        """ä¿å­˜å­¦ä¹ è®°å½•"""
        predicted_tails = self.last_prediction_result.get('recommended_tails', []) if self.last_prediction_result else []
        confidence = self.last_prediction_result.get('confidence', 0.0) if self.last_prediction_result else 0.0
    
        self.db_manager.save_learning_record(
            features=features,
            actual_tails=actual_tails,
            prediction_correct=prediction_correct,
            drift_detectors=drift_detectors,
            predicted_tails=predicted_tails,
            confidence=confidence,
            sample_number=self.total_samples_seen
        )
    
    def _record_model_prediction(self, model_name: str, predicted_class: int, confidence: float, target_tail: int):
        """è®°å½•å•ä¸ªæ¨¡å‹çš„é¢„æµ‹"""
        self.db_manager.record_model_prediction(
            model_name=model_name,
            predicted_class=predicted_class,
            confidence=confidence,
            target_tail=target_tail,
            sample_number=self.total_samples_seen + 1
        )
    
    def _save_model_predictions(self, actual_tails: List[int]):
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ"""
        try:
            # æ›´æ–°æ¨¡å‹é¢„æµ‹è®°å½•çš„å®é™…ç»“æœ
            self.db_manager.update_model_predictions_with_actual_results(
                actual_tails=actual_tails,
                sample_number=self.total_samples_seen
            )
        
        except Exception as e:
            print(f"ä¿å­˜æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
    
    def get_model_performance_stats(self) -> Dict:
        """è·å–æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†æ€§èƒ½ç»Ÿè®¡"""
        return self.db_manager.get_model_performance_stats()
        
    def save_drift_record(self, detector_name: str, drift_type: str, confidence: float, action: str):
        """ä¿å­˜æ¦‚å¿µæ¼‚ç§»è®°å½•"""
        self.db_manager.save_drift_record(detector_name, drift_type, confidence, action)
    
    def ensure_pytorch_model_directory(self, models_dir=None):
        """ç¡®ä¿PyTorchæ¨¡å‹ç›®å½•å­˜åœ¨"""
        if models_dir is None:
            # ä½¿ç”¨é»˜è®¤ç›®å½•
            models_dir = self.data_dir / "deep_learning" / "models"
    
        try:
            # ç¡®ä¿æ˜¯Pathå¯¹è±¡
            if isinstance(models_dir, str):
                models_dir = Path(models_dir)
        
            # åˆ›å»ºç›®å½•
            models_dir.mkdir(parents=True, exist_ok=True)
            print(f"âœ… PyTorchæ¨¡å‹ç›®å½•å·²åˆ›å»º: {models_dir}")
        
            # å°†ç›®å½•è·¯å¾„ä¿å­˜åˆ°æ·±åº¦å­¦ä¹ æ¨¡å—ä¸­
            if hasattr(self, 'deep_learning_module') and self.deep_learning_module:
                self.deep_learning_module.models_dir = str(models_dir)
                print(f"âœ… æ¨¡å‹ç›®å½•è·¯å¾„å·²ä¼ é€’ç»™æ·±åº¦å­¦ä¹ æ¨¡å—")
        
            return str(models_dir)
        except Exception as e:
            print(f"âŒ åˆ›å»ºPyTorchæ¨¡å‹ç›®å½•å¤±è´¥: {e}")
            return None
    
    def save_model_state(self):
        """ä¿å­˜æ¨¡å‹çŠ¶æ€"""
        try:
            # ä»æ•°æ®åº“è·å–æœ€æ–°çš„å‡†ç¡®è®¡æ•°
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
        
            # è·å–å®é™…çš„è®°å½•æ•°
            cursor.execute("SELECT COUNT(*) FROM online_predictions")
            db_count_result = cursor.fetchone()
            actual_total = db_count_result[0] if db_count_result else 0
        
            cursor.execute("SELECT COUNT(*) FROM online_predictions WHERE is_correct = 1")
            db_correct_result = cursor.fetchone()
            actual_correct = db_correct_result[0] if db_correct_result else 0
        
            conn.close()
        
            # ä½¿ç”¨æ•°æ®åº“ä¸­çš„å‡†ç¡®æ•°æ®
            state = {
                'total_samples_seen': actual_total,
                'correct_predictions': actual_correct,
                'ensemble_weights': self.ensemble_weights,
                'learning_config': self.learning_config,
                'last_update': datetime.now().isoformat(),
                'version': '3.0',  # æ›´æ–°ç‰ˆæœ¬å·ä»¥åŒ…å«æ·±åº¦å­¦ä¹ 
                'save_source': 'database_verified',  # æ ‡è®°æ•°æ®æ¥æº
                'deep_learning_available': PYTORCH_AVAILABLE,
                'deep_learning_models': list(self.deep_learning_module.models.keys()) if PYTORCH_AVAILABLE and self.deep_learning_module else []
            }
            
            # ä¿å­˜æ·±åº¦å­¦ä¹ æ¨¡å‹çŠ¶æ€
            if PYTORCH_AVAILABLE and self.deep_learning_module and self.deep_learning_module.models:
                try:
                    deep_learning_state = {}
                    online_learning_stats = {}
                    
                    # ç¡®ä¿PyTorchæ¨¡å‹ç›®å½•å­˜åœ¨
                    model_save_dir = self.ensure_pytorch_model_directory()
                    if not model_save_dir:
                        print(f"âŒ æ— æ³•åˆ›å»ºPyTorchæ¨¡å‹ç›®å½•ï¼Œè·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ä¿å­˜")
                        state['deep_learning_save_error'] = "æ— æ³•åˆ›å»ºæ¨¡å‹ç›®å½•"
                    else:
                        model_save_dir = Path(model_save_dir)
                    
                    # éªŒè¯ç›®å½•åˆ›å»ºæ˜¯å¦æˆåŠŸ
                    if not model_save_dir.exists():
                        print(f"âŒ æ¨¡å‹ä¿å­˜ç›®å½•åˆ›å»ºå¤±è´¥: {model_save_dir}")
                        raise Exception(f"æ— æ³•åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {model_save_dir}")
            
                    # æ£€æŸ¥ç›®å½•å†™å…¥æƒé™
                    try:
                        test_file = model_save_dir / "write_test.tmp"
                        test_file.write_text("test")
                        test_file.unlink()
                        print(f"âœ… æ¨¡å‹ä¿å­˜ç›®å½•å†™å…¥æƒé™éªŒè¯æˆåŠŸ")
                    except Exception as write_error:
                        print(f"âŒ æ¨¡å‹ä¿å­˜ç›®å½•å†™å…¥æƒé™éªŒè¯å¤±è´¥: {write_error}")
                        raise Exception(f"æ¨¡å‹ä¿å­˜ç›®å½•æ— å†™å…¥æƒé™: {write_error}")
            
                    for model_name, model in self.deep_learning_module.models.items():
                        model_state_path = model_save_dir / f"pytorch_{model_name}_state.pth"
                
                        # ç¡®ä¿æ¨¡å‹æ–‡ä»¶çš„ç›´æ¥çˆ¶ç›®å½•å­˜åœ¨
                        model_state_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # å†æ¬¡ç¡®ä¿æ–‡ä»¶çš„çˆ¶ç›®å½•å­˜åœ¨ï¼ˆåŒé‡ä¿é™©ï¼‰
                        model_state_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # ä¿å­˜æ¨¡å‹çŠ¶æ€å’Œåœ¨çº¿å­¦ä¹ ç»Ÿè®¡
                        training_history = self.deep_learning_module.training_history[model_name]
                        try:
                            # ä¿å­˜å‰å†æ¬¡éªŒè¯è·¯å¾„
                            print(f"ğŸ”§ å‡†å¤‡ä¿å­˜æ¨¡å‹ {model_name} åˆ°: {model_state_path}")
                
                            # å®‰å…¨åœ°å‡†å¤‡è®­ç»ƒå†å²æ•°æ®ï¼ˆç¡®ä¿å¯åºåˆ—åŒ–ï¼‰
                            safe_training_history = {}
                            try:
                                # è½¬æ¢æ‰€æœ‰æ•°æ®ä¸ºPythonåŸç”Ÿç±»å‹
                                if 'loss' in training_history:
                                    safe_training_history['loss'] = [float(x) for x in training_history['loss'] if x is not None and not (isinstance(x, float) and (math.isnan(x) or math.isinf(x)))]
                                else:
                                    safe_training_history['loss'] = []
                    
                                if 'accuracy' in training_history:
                                    safe_training_history['accuracy'] = [float(x) for x in training_history['accuracy'] if x is not None and not (isinstance(x, float) and (math.isnan(x) or math.isinf(x)))]
                                else:
                                    safe_training_history['accuracy'] = []
                    
                                safe_training_history['best_loss'] = float(training_history.get('best_loss', float('inf')))
                                safe_training_history['epochs_trained'] = int(training_history.get('epochs_trained', 0))
                    
                                # å¤„ç†æ— ç©·å¤§å€¼
                                if math.isinf(safe_training_history['best_loss']):
                                    safe_training_history['best_loss'] = 999999.0
                        
                            except Exception as history_e:
                                print(f"   âš ï¸ å¤„ç†è®­ç»ƒå†å²æ•°æ®å¤±è´¥: {history_e}")
                                safe_training_history = {
                                    'loss': [],
                                    'accuracy': [],
                                    'best_loss': 999999.0,
                                    'epochs_trained': 0
                                }
                
                            # å®‰å…¨åœ°å‡†å¤‡åœ¨çº¿å­¦ä¹ ç»Ÿè®¡æ•°æ®
                            safe_online_stats = {}
                            try:
                                safe_online_stats = {
                                    'total_online_updates': len(safe_training_history.get('loss', [])),
                                    'recent_loss': safe_training_history.get('loss', [])[-10:],
                                    'recent_accuracy': safe_training_history.get('accuracy', [])[-10:],
                                    'current_lr': float(self.deep_learning_module.optimizers[model_name].param_groups[0]['lr'])
                                }
                            except Exception as stats_e:
                                print(f"   âš ï¸ å¤„ç†åœ¨çº¿å­¦ä¹ ç»Ÿè®¡å¤±è´¥: {stats_e}")
                                safe_online_stats = {
                                    'total_online_updates': 0,
                                    'recent_loss': [],
                                    'recent_accuracy': [],
                                    'current_lr': 0.001
                                }
                
                            # å‡†å¤‡è¦ä¿å­˜çš„çŠ¶æ€å­—å…¸
                            save_dict = {}
                
                            # å®‰å…¨åœ°è·å–æ¨¡å‹çŠ¶æ€
                            try:
                                save_dict['model_state_dict'] = model.state_dict()
                                print(f"   âœ“ æ¨¡å‹çŠ¶æ€è·å–æˆåŠŸ")
                            except Exception as model_e:
                                print(f"   âŒ è·å–æ¨¡å‹çŠ¶æ€å¤±è´¥: {model_e}")
                                raise Exception(f"æ— æ³•è·å–æ¨¡å‹çŠ¶æ€: {model_e}")
                
                            # å®‰å…¨åœ°è·å–ä¼˜åŒ–å™¨çŠ¶æ€
                            try:
                                save_dict['optimizer_state_dict'] = self.deep_learning_module.optimizers[model_name].state_dict()
                                print(f"   âœ“ ä¼˜åŒ–å™¨çŠ¶æ€è·å–æˆåŠŸ")
                            except Exception as opt_e:
                                print(f"   âš ï¸ è·å–ä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {opt_e}")
                                # ä¼˜åŒ–å™¨çŠ¶æ€ä¸æ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥è·³è¿‡
                                save_dict['optimizer_state_dict'] = None
                
                            # å®‰å…¨åœ°è·å–è°ƒåº¦å™¨çŠ¶æ€
                            try:
                                save_dict['scheduler_state_dict'] = self.deep_learning_module.schedulers[model_name].state_dict()
                                print(f"   âœ“ è°ƒåº¦å™¨çŠ¶æ€è·å–æˆåŠŸ")
                            except Exception as sch_e:
                                print(f"   âš ï¸ è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {sch_e}")
                                # è°ƒåº¦å™¨çŠ¶æ€ä¸æ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥è·³è¿‡
                                save_dict['scheduler_state_dict'] = None
                
                            # æ·»åŠ å¤„ç†è¿‡çš„å®‰å…¨æ•°æ®
                            save_dict['training_history'] = safe_training_history
                            save_dict['online_learning_stats'] = safe_online_stats
                            save_dict['save_timestamp'] = datetime.now().isoformat()
                            save_dict['model_type'] = model_name
                
                            # æ‰§è¡Œä¿å­˜æ“ä½œ
                            ai_config.torch.save(save_dict, str(model_state_path))
                            print(f"âœ… æ¨¡å‹ {model_name} ä¿å­˜æˆåŠŸ")
                    
                        except Exception as save_error:
                            print(f"âŒ ä¿å­˜æ¨¡å‹ {model_name} å¤±è´¥: {save_error}")
                            print(f"ğŸ“ å°è¯•çš„è·¯å¾„: {model_state_path}")
                            print(f"ğŸ“ çˆ¶ç›®å½•å­˜åœ¨: {model_state_path.parent.exists()}")
                            print(f"ğŸ“ çˆ¶ç›®å½•å¯å†™: {os.access(model_state_path.parent, os.W_OK)}")
                            print(f"ğŸ” é”™è¯¯ç±»å‹: {type(save_error).__name__}")
                            print(f"ğŸ” è¯¦ç»†é”™è¯¯: {str(save_error)}")
                
                            # å°è¯•ç®€åŒ–ä¿å­˜ï¼ˆåªä¿å­˜æ¨¡å‹çŠ¶æ€ï¼‰
                            try:
                                print(f"   ğŸ”„ å°è¯•ç®€åŒ–ä¿å­˜æ¨¡å‹ {model_name}...")
                                simple_save_dict = {
                                    'model_state_dict': model.state_dict(),
                                    'save_timestamp': datetime.now().isoformat(),
                                    'model_type': model_name,
                                    'simplified_save': True
                                }
                    
                                simple_path = model_save_dir / f"pytorch_{model_name}_simple.pth"
                                ai_config.torch.save(simple_save_dict, str(simple_path))
                                print(f"   âœ… æ¨¡å‹ {model_name} ç®€åŒ–ä¿å­˜æˆåŠŸåˆ°: {simple_path}")
                    
                            except Exception as simple_save_error:
                               print(f"   âŒ ç®€åŒ–ä¿å­˜ä¹Ÿå¤±è´¥: {simple_save_error}")
                               print(f"   âš ï¸ æ¨¡å‹ {model_name} å®Œå…¨ä¿å­˜å¤±è´¥ï¼Œå°†è·³è¿‡")
                
                            # ç»§ç»­ä¿å­˜å…¶ä»–æ¨¡å‹ï¼Œä¸ä¸­æ–­æ•´ä¸ªä¿å­˜è¿‡ç¨‹
                            continue
                        
                        deep_learning_state[model_name] = str(model_state_path)
                        
                        # è®°å½•åœ¨çº¿å­¦ä¹ ç»Ÿè®¡
                        if training_history.get('loss'):
                            online_learning_stats[model_name] = {
                                'online_updates': len(training_history['loss']),
                                'avg_recent_loss': np.mean(training_history['loss'][-20:]) if len(training_history['loss']) >= 20 else 0,
                                'avg_recent_accuracy': np.mean(training_history['accuracy'][-20:]) if len(training_history['accuracy']) >= 20 else 0
                            }
                    
                    state['deep_learning_model_paths'] = deep_learning_state
                    state['online_learning_stats'] = online_learning_stats
                    print(f"âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹çŠ¶æ€å·²ä¿å­˜ï¼ˆåŒ…å«åœ¨çº¿å­¦ä¹ ç»Ÿè®¡ï¼‰")
                    
                    # æ˜¾ç¤ºåœ¨çº¿å­¦ä¹ ç»Ÿè®¡
                    for model_name, stats in online_learning_stats.items():
                        print(f"   ğŸ“Š {model_name}: åœ¨çº¿æ›´æ–°{stats.get('online_updates', 0)}æ¬¡, "
                              f"å¹³å‡æŸå¤±{stats.get('avg_recent_loss', 0):.4f}, "
                              f"å¹³å‡å‡†ç¡®ç‡{stats.get('avg_recent_accuracy', 0):.3f}")
                    
                except Exception as e:
                    print(f"âŒ ä¿å­˜æ·±åº¦å­¦ä¹ æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
                    print(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    state['deep_learning_save_error'] = str(e)
        
            # åŒæ­¥å†…å­˜ä¸­çš„è®¡æ•°å™¨
            if self.total_samples_seen != actual_total or self.correct_predictions != actual_correct:
                print(f"ğŸ”§ çŠ¶æ€ä¿å­˜æ—¶ä¿®æ­£è®¡æ•°å™¨ï¼šæ ·æœ¬æ•° {self.total_samples_seen} â†’ {actual_total}, æ­£ç¡®é¢„æµ‹ {self.correct_predictions} â†’ {actual_correct}")
                self.total_samples_seen = actual_total
                self.correct_predictions = actual_correct
        
            # åˆ›å»ºå¤‡ä»½
            if self.model_path.exists():
                backup_path = self.model_path.with_suffix('.pkl.backup')
                try:
                    import shutil
                    shutil.copy2(self.model_path, backup_path)
                except Exception as backup_e:
                    print(f"åˆ›å»ºçŠ¶æ€å¤‡ä»½å¤±è´¥: {backup_e}")

            # åˆ›å»ºæ•°æ®åº“å¤‡ä»½
            try:
                self.db_manager.backup_database()
            except Exception as db_backup_e:
                print(f"åˆ›å»ºæ•°æ®åº“å¤‡ä»½å¤±è´¥: {db_backup_e}")

            # ä¿å­˜çŠ¶æ€
            with open(self.model_path, 'wb') as f:
                pickle.dump(state, f)
        
            print(f"âœ… æ¨¡å‹çŠ¶æ€å·²ä¿å­˜ï¼šæ ·æœ¬æ•°={actual_total}, æ­£ç¡®é¢„æµ‹={actual_correct}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def safe_sort_models_by_weight(self, models_dict, limit=5):
        """å®‰å…¨åœ°æŒ‰æƒé‡æ’åºæ¨¡å‹"""
        try:
            # ç¡®ä¿æ•°æ®å®Œæ•´æ€§
            valid_models = []
            for model_key, weight_info in models_dict.items():
                if isinstance(weight_info, dict):
                    total_weight = weight_info.get('total_weight', 0.0)
                    if isinstance(total_weight, (int, float)):
                        valid_models.append((model_key, weight_info, float(total_weight)))
                    else:
                        valid_models.append((model_key, weight_info, 0.0))
                else:
                    valid_models.append((model_key, weight_info, 0.0))
            
            # å®‰å…¨æ’åº
            sorted_models = sorted(valid_models, key=lambda x: x[2], reverse=True)
            return [(model_key, weight_info) for model_key, weight_info, _ in sorted_models[:limit]]
            
        except Exception as e:
            print(f"æ¨¡å‹æƒé‡æ’åºå¤±è´¥: {e}")
            return list(models_dict.items())[:limit]

    def _get_sortable_ensemble_weights(self):
        """è·å–å¯æ’åºçš„æƒé‡æ•°æ®ï¼ˆä¸“é—¨ç”¨äºè§£å†³å­—å…¸æ¯”è¾ƒé—®é¢˜ï¼‰"""
        sortable_weights = {}
        try:
            # å…ˆåˆ›å»ºå¯æ’åºçš„æƒé‡åˆ—è¡¨
            weight_list = []
            for model_key, weight_info in self.ensemble_weights.items():
                if isinstance(weight_info, dict):
                    # æå–æ•°å€¼ç”¨äºæ’åº
                    total_weight = float(weight_info.get('total_weight', 0.0))
                    active_weight = float(weight_info.get('weight', 0.0))
                    frozen_weight = float(weight_info.get('frozen_weight', 0.0))
                    confidence = float(weight_info.get('confidence', 0.5))
                    is_frozen = bool(weight_info.get('is_frozen', False))
                    
                    # å¤„ç†æ—¶é—´æˆ³
                    last_update = weight_info.get('last_update')
                    if isinstance(last_update, datetime):
                        last_update_str = last_update.strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        last_update_str = str(last_update) if last_update else 'Unknown'
                    
                    # å¤„ç†å†»ç»“æ—¶é—´æˆ³
                    frozen_timestamp = weight_info.get('frozen_timestamp')
                    if isinstance(frozen_timestamp, datetime):
                        frozen_timestamp_str = frozen_timestamp.strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        frozen_timestamp_str = str(frozen_timestamp) if frozen_timestamp else None
                    
                    # åˆ›å»ºå¯æ’åºçš„æ•°æ®ç»“æ„
                    sortable_data = {
                        'model_name': str(model_key),
                        'active_weight': active_weight,
                        'frozen_weight': frozen_weight,
                        'total_weight': total_weight,
                        'is_frozen': is_frozen,
                        'confidence': confidence,
                        'last_update': last_update_str,
                        'frozen_timestamp': frozen_timestamp_str,
                        'performance_count': len(weight_info.get('performance_history', [])),
                        'sort_key': total_weight  # ä¸“é—¨ç”¨äºæ’åºçš„é”®
                    }
                    
                    weight_list.append((total_weight, model_key, sortable_data))
                else:
                    # å¤„ç†å¼‚å¸¸æƒ…å†µ
                    weight_list.append((0.0, str(model_key), {
                        'model_name': str(model_key),
                        'active_weight': 0.0,
                        'frozen_weight': 0.0,
                        'total_weight': 0.0,
                        'is_frozen': False,
                        'confidence': 0.5,
                        'last_update': 'Unknown',
                        'frozen_timestamp': None,
                        'performance_count': 0,
                        'sort_key': 0.0
                    }))
            
            # æŒ‰æƒé‡æ’åºï¼ˆé¿å…å­—å…¸æ¯”è¾ƒï¼‰
            weight_list.sort(key=lambda x: x[0], reverse=True)
            
            # è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
            for _, model_key, sortable_data in weight_list:
                sortable_weights[model_key] = sortable_data
            
            return sortable_weights
            
        except Exception as e:
            print(f"è·å–å¯æ’åºæƒé‡æ•°æ®å¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤æ•°æ®
            return {
                'error_model': {
                    'model_name': 'error',
                    'active_weight': 0.0,
                    'frozen_weight': 0.0,
                    'total_weight': 0.0,
                    'is_frozen': False,
                    'confidence': 0.5,
                    'last_update': 'Error',
                    'frozen_timestamp': None,
                    'performance_count': 0,
                    'sort_key': 0.0,
                    'error': str(e)
                }
            }

    def _clean_ensemble_weights_data(self):
        """æ¸…ç†æƒé‡æ•°æ®ï¼Œç¡®ä¿ç±»å‹ä¸€è‡´æ€§"""
        try:
            for model_key, weight_info in list(self.ensemble_weights.items()):
                if isinstance(weight_info, dict):
                    # ç¡®ä¿æ‰€æœ‰æ•°å€¼å­—æ®µéƒ½æ˜¯floatç±»å‹
                    weight_info['weight'] = float(weight_info.get('weight', 0.0))
                    weight_info['total_weight'] = float(weight_info.get('total_weight', weight_info['weight']))
                    weight_info['confidence'] = float(weight_info.get('confidence', 0.5))
                    
                    # ç¡®ä¿æŠ•èµ„å­—æ®µæ˜¯å­—å…¸ç±»å‹
                    if not isinstance(weight_info.get('invested_weights'), dict):
                        weight_info['invested_weights'] = {}
                    if not isinstance(weight_info.get('pending_investments'), dict):
                        weight_info['pending_investments'] = {}
                    if not isinstance(weight_info.get('investment_history'), list):
                        weight_info['investment_history'] = []
                    
                    # ç¡®ä¿æ—¶é—´å­—æ®µæ˜¯datetimeç±»å‹æˆ–None
                    last_update = weight_info.get('last_update')
                    if last_update is not None and not isinstance(last_update, datetime):
                        try:
                            # å°è¯•è§£ææ—¶é—´å­—ç¬¦ä¸²
                            if isinstance(last_update, str):
                                weight_info['last_update'] = datetime.fromisoformat(last_update.replace('Z', '+00:00'))
                            else:
                                weight_info['last_update'] = datetime.now()
                        except:
                            weight_info['last_update'] = datetime.now()
                    elif last_update is None:
                        weight_info['last_update'] = datetime.now()
                    
                    # å¤„ç†å†»ç»“æ—¶é—´æˆ³
                    frozen_timestamp = weight_info.get('frozen_timestamp')
                    if frozen_timestamp is not None and not isinstance(frozen_timestamp, datetime):
                        try:
                            if isinstance(frozen_timestamp, str):
                                weight_info['frozen_timestamp'] = datetime.fromisoformat(frozen_timestamp.replace('Z', '+00:00'))
                            else:
                                weight_info['frozen_timestamp'] = None
                        except:
                            weight_info['frozen_timestamp'] = None
                    
                    # ç¡®ä¿æ€§èƒ½å†å²æ˜¯åˆ—è¡¨
                    if not isinstance(weight_info.get('performance_history'), list):
                        weight_info['performance_history'] = []
            
            print("âœ… æƒé‡æ•°æ®ç±»å‹æ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âŒ æƒé‡æ•°æ®æ¸…ç†å¤±è´¥: {e}")

    def load_saved_state(self):
        """åŠ è½½ä¿å­˜çš„çŠ¶æ€"""
        try:
            # é¦–å…ˆä»æ•°æ®åº“æ¢å¤å‡†ç¡®çš„è®¡æ•°
            self.recover_state_from_database()
        
            if self.model_path.exists():
                with open(self.model_path, 'rb') as f:
                    state = pickle.load(f)
            
                saved_samples = state.get('total_samples_seen', 0)
                saved_correct = state.get('correct_predictions', 0)
                state_version = state.get('version', '1.0')
                save_source = state.get('save_source', 'unknown')
            
                print(f"ğŸ“‚ åŠ è½½çŠ¶æ€æ–‡ä»¶ï¼šç‰ˆæœ¬={state_version}, æ¥æº={save_source}")
                print(f"ğŸ“‚ çŠ¶æ€æ–‡ä»¶æ˜¾ç¤ºï¼šæ ·æœ¬æ•°={saved_samples}, æ­£ç¡®é¢„æµ‹={saved_correct}")
                print(f"ğŸ“‚ æ•°æ®åº“æ˜¾ç¤ºï¼šæ ·æœ¬æ•°={self.total_samples_seen}, æ­£ç¡®é¢„æµ‹={self.correct_predictions}")
            
                # å¦‚æœçŠ¶æ€æ–‡ä»¶æ•°æ®æ¯”æ•°æ®åº“æ–°ï¼Œä¸”æ ‡è®°ä¸ºå¯ä¿¡ï¼Œåˆ™ä½¿ç”¨çŠ¶æ€æ–‡ä»¶æ•°æ®
                if (save_source == 'database_verified' and 
                    state_version >= '2.0' and 
                    saved_samples >= self.total_samples_seen):
                    print(f"âœ… ä½¿ç”¨å¯ä¿¡çš„çŠ¶æ€æ–‡ä»¶æ•°æ®")
                    self.total_samples_seen = saved_samples
                    self.correct_predictions = saved_correct
                else:
                    print(f"âœ… ä½¿ç”¨æ•°æ®åº“éªŒè¯çš„æ•°æ®")
                    # ä¿æŒä»æ•°æ®åº“æ¢å¤çš„æ•°æ®
                    pass
            
                # æ¢å¤é›†æˆæƒé‡
                saved_weights = state.get('ensemble_weights', {})
                for model_key in self.ensemble_weights:
                    if model_key in saved_weights:
                        self.ensemble_weights[model_key].update(saved_weights[model_key])
            
                # æ¢å¤å­¦ä¹ é…ç½®
                saved_config = state.get('learning_config', {})
                self.learning_config.update(saved_config)
            
                print(f"ğŸ“‚ æœ€ç»ˆçŠ¶æ€ï¼šæ ·æœ¬æ•°={self.total_samples_seen}, æ­£ç¡®é¢„æµ‹={self.correct_predictions}")

                # åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹çŠ¶æ€
                if (PYTORCH_AVAILABLE and hasattr(self, 'deep_learning_module') and 
                    self.deep_learning_module and 'deep_learning_model_paths' in state):
                    try:
                        deep_learning_paths = state['deep_learning_model_paths']
                        loaded_models = 0
                        total_models = len(deep_learning_paths)
                        
                        for model_name, model_path in deep_learning_paths.items():
                            model_path_obj = Path(model_path)
                            
                            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                            if not model_path_obj.exists():
                                print(f"âš ï¸ æ·±åº¦å­¦ä¹ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                                continue
                                
                            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å½“å‰æ¨¡å‹å­—å…¸ä¸­
                            if model_name not in self.deep_learning_module.models:
                                print(f"âš ï¸ æ¨¡å‹ {model_name} ä¸åœ¨å½“å‰æ¨¡å‹åˆ—è¡¨ä¸­ï¼Œè·³è¿‡åŠ è½½")
                                continue
                            
                            try:
                                checkpoint = ai_config.torch.load(model_path, map_location=self.deep_learning_module.device)
                                
                                # åŠ è½½æ¨¡å‹çŠ¶æ€
                                self.deep_learning_module.models[model_name].load_state_dict(
                                    checkpoint['model_state_dict']
                                )
                                self.deep_learning_module.optimizers[model_name].load_state_dict(
                                    checkpoint['optimizer_state_dict']
                                )
                                self.deep_learning_module.schedulers[model_name].load_state_dict(
                                    checkpoint['scheduler_state_dict']
                                )
                                self.deep_learning_module.training_history[model_name] = checkpoint['training_history']
                                loaded_models += 1
                                print(f"âœ… æˆåŠŸåŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹: {model_name}")
                                
                            except Exception as model_e:
                                print(f"âŒ åŠ è½½æ¨¡å‹ {model_name} å¤±è´¥: {model_e}")
                                continue
                        
                        print(f"ğŸ“‚ æ·±åº¦å­¦ä¹ æ¨¡å‹çŠ¶æ€åŠ è½½å®Œæˆ: {loaded_models}/{total_models} ä¸ªæ¨¡å‹åŠ è½½æˆåŠŸ")
                        
                    except Exception as e:
                        print(f"âŒ åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
                        print(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {str(e)}")
                        import traceback
                        traceback.print_exc()
            else:
                print(f"ğŸ“‚ æœªæ‰¾åˆ°çŠ¶æ€æ–‡ä»¶ï¼Œä»…ä½¿ç”¨æ•°æ®åº“æ•°æ®")
                    
        except Exception as e:
            print(f"âŒ åŠ è½½ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•ä»æ•°æ®åº“æ¢å¤çŠ¶æ€
            self.recover_state_from_database()
    
    def recover_state_from_database(self):
        """ä»æ•°æ®åº“æ¢å¤çŠ¶æ€ä¿¡æ¯"""
        try:
            total_samples, correct_predictions = self.db_manager.get_sample_counts()
            self.total_samples_seen = total_samples
            self.correct_predictions = correct_predictions
        
            print(f"ğŸ”„ ä»æ•°æ®åº“æ¢å¤çŠ¶æ€ï¼šæ ·æœ¬æ•°={self.total_samples_seen}, æ­£ç¡®é¢„æµ‹={self.correct_predictions}")
        
        except Exception as e:
            print(f"ä»æ•°æ®åº“æ¢å¤çŠ¶æ€å¤±è´¥: {e}")
            self.total_samples_seen = 0
            self.correct_predictions = 0
    
    def check_data_consistency_on_startup(self):
        """å¯åŠ¨æ—¶æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        try:
            # ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨è·å–ä¸€è‡´æ€§æŠ¥å‘Š
            consistency_report = self.db_manager.check_data_consistency()
        
            if 'error' in consistency_report:
                print(f"âŒ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {consistency_report['error']}")
                return
        
            db_count = consistency_report['total_predictions']
            db_correct = consistency_report['completed_predictions']
        
            print(f"ğŸ” å¯åŠ¨æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ï¼š")
            print(f"   å†…å­˜ä¸­æ ·æœ¬æ•°ï¼š{self.total_samples_seen}")
            print(f"   æ•°æ®åº“è®°å½•æ•°ï¼š{db_count}")
            print(f"   å†…å­˜ä¸­æ­£ç¡®é¢„æµ‹ï¼š{self.correct_predictions}")
            print(f"   æ•°æ®åº“æ­£ç¡®è®°å½•ï¼š{db_correct}")
            print(f"   æ•°æ®å®Œæ•´æ€§ï¼š{consistency_report['data_integrity']}")
        
            # å¦‚æœå‘ç°ä¸ä¸€è‡´ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“ä¸­çš„æ•°æ®
            if self.total_samples_seen != db_count or self.correct_predictions != db_correct:
                print(f"âš ï¸ å‘ç°æ•°æ®ä¸ä¸€è‡´ï¼Œå°†ä½¿ç”¨æ•°æ®åº“ä¸­çš„å®é™…æ•°æ®")
                self.total_samples_seen = db_count
                self.correct_predictions = db_correct
                # ç«‹å³ä¿å­˜ä¿®æ­£åçš„çŠ¶æ€
                self.save_model_state()
                print(f"âœ… æ•°æ®å·²ä¿®å¤ï¼šæ ·æœ¬æ•°={self.total_samples_seen}, æ­£ç¡®é¢„æµ‹={self.correct_predictions}")
            else:
                print(f"âœ… æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
            
        except Exception as e:
            print(f"âŒ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")

    def _validate_ensemble_weights(self):
        """éªŒè¯å’Œä¿®å¤é›†æˆæƒé‡æ•°æ®"""
        try:
            for model_key, weight_info in list(self.ensemble_weights.items()):
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨ä¸”ä¸ºæ­£ç¡®ç±»å‹
                if not isinstance(weight_info, dict):
                    print(f"âš ï¸ ä¿®å¤æ¨¡å‹ {model_key} çš„æƒé‡ä¿¡æ¯")
                    self.ensemble_weights[model_key] = {
                        'weight': 0.1,
                        'frozen_weight': 0.0,
                        'total_weight': 0.1,
                        'is_frozen': False,
                        'frozen_timestamp': None,
                        'pending_investments': {},
                        'confidence': 0.5,
                        'last_update': datetime.now(),
                        'performance_history': [],
                        'investment_history': []
                    }
                    continue
                
                # éªŒè¯å’Œä¿®å¤æ•°å€¼å­—æ®µ
                weight_info['weight'] = float(weight_info.get('weight', 0.1))
                weight_info['total_weight'] = float(weight_info.get('total_weight', weight_info['weight']))
                weight_info['confidence'] = float(weight_info.get('confidence', 0.5))
                
                # éªŒè¯å†»ç»“æƒé‡ç›¸å…³å­—æ®µ
                weight_info['frozen_weight'] = float(weight_info.get('frozen_weight', 0.0))
                weight_info['is_frozen'] = bool(weight_info.get('is_frozen', False))
                if 'frozen_timestamp' not in weight_info:
                    weight_info['frozen_timestamp'] = None

                # éªŒè¯æŠ•èµ„ç›¸å…³å­—æ®µ
                if not isinstance(weight_info.get('invested_weights'), dict):
                    weight_info['invested_weights'] = {}
                if not isinstance(weight_info.get('pending_investments'), dict):
                    weight_info['pending_investments'] = {}
                if not isinstance(weight_info.get('investment_history'), list):
                    weight_info['investment_history'] = []
                if not isinstance(weight_info.get('performance_history'), list):
                    weight_info['performance_history'] = []
                
                # ç¡®ä¿last_updateæ˜¯datetimeå¯¹è±¡
                if not isinstance(weight_info.get('last_update'), datetime):
                    weight_info['last_update'] = datetime.now()
            
            print("âœ… é›†æˆæƒé‡æ•°æ®éªŒè¯å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ éªŒè¯é›†æˆæƒé‡æ•°æ®å¤±è´¥: {e}")

    def get_comprehensive_stats(self) -> Dict:
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯ï¼ˆå§”æ‰˜ç»™ç»Ÿè®¡åˆ†æç®¡ç†å™¨ï¼‰"""
        return self.statistics_manager.get_comprehensive_stats(self)
    
    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return self.statistics_manager.generate_performance_summary(self)

    def get_model_diversity_analysis(self) -> Dict:
        """è·å–æ¨¡å‹å¤šæ ·æ€§åˆ†æ"""
        return self.statistics_manager.analyze_model_diversity(self)

    def get_system_health_status(self) -> Dict:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        return self.statistics_manager.get_system_health_status(self)
        
    def reset_model(self):
        """é‡ç½®AIæ¨¡å‹ - å®Œå…¨é‡ç½®æ‰€æœ‰çŠ¶æ€å’Œæ•°æ®"""
        print("ğŸ”„ å¼€å§‹é‡ç½®AIæ¨¡å‹...")
    
        try:
            # 1. å¼ºåˆ¶å…³é—­æ‰€æœ‰å¯èƒ½çš„æ•°æ®åº“è¿æ¥
            self._force_close_database_connections()
        
            # 2. åˆ é™¤æ•°æ®åº“æ–‡ä»¶
            self._delete_database_files()

            # 2.5. é‡æ–°åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨å®ä¾‹ä»¥ç¡®ä¿è¡¨ç»“æ„è¢«æ­£ç¡®åˆ›å»º
            try:
                self.db_manager = DatabaseManager(self.db_path)
                print("âœ… æ•°æ®åº“ç®¡ç†å™¨å·²é‡æ–°åˆ›å»ºï¼Œè¡¨ç»“æ„å·²åˆå§‹åŒ–")
            except Exception as e:
                print(f"âŒ é‡æ–°åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨å¤±è´¥: {e}")
                return False

            # 3. é‡ç½®å­¦ä¹ çŠ¶æ€
            self.total_samples_seen = 0
            self.correct_predictions = 0
            self.last_prediction_result = None
            self.model_performance = {}
        
            # 4. æ¸…ç©ºæ‰€æœ‰æ¨¡å‹é›†åˆ
            self.river_models.clear()
            self.sklearn_models.clear()
            self.drift_detectors.clear()
            self.metrics_trackers.clear()
            self.ensemble_weights.clear()
        
            # 5. éªŒè¯æ•°æ®åº“è¡¨æ˜¯å¦æ­£ç¡®åˆ›å»º
            try:
                # æµ‹è¯•æ•°æ®åº“è¿æ¥å’Œè¡¨å­˜åœ¨æ€§
                import sqlite3
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
    
                # æ£€æŸ¥å¿…è¦çš„è¡¨æ˜¯å¦å­˜åœ¨
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
    
                required_tables = ['online_predictions', 'individual_model_predictions', 'drift_events']
                missing_tables = [table for table in required_tables if table not in tables]
    
                conn.close()
    
                if missing_tables:
                    print(f"âŒ ç¼ºå°‘æ•°æ®åº“è¡¨: {missing_tables}")
                    return False
                else:
                    print(f"âœ… æ•°æ®åº“è¡¨éªŒè¯é€šè¿‡: {tables}")
        
            except Exception as e:
                print(f"âŒ æ•°æ®åº“è¡¨éªŒè¯å¤±è´¥: {e}")
                return False
        
            # 6. é‡æ–°åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
            self.init_online_models()
        
            # 7. é‡ç½®æ‰€æœ‰ç»„ä»¶çŠ¶æ€
            self._reset_all_components()
        
            # 8. ä¿å­˜æ–°çš„åˆå§‹çŠ¶æ€
            self.save_model_state()
        
            print("âœ… AIæ¨¡å‹é‡ç½®å®Œæˆ")
            print(f"   - é‡æ–°åˆå§‹åŒ–äº† {len(self.river_models)} ä¸ªRiveræ¨¡å‹")
            print(f"   - é‡æ–°åˆå§‹åŒ–äº† {len(self.sklearn_models)} ä¸ªscikit-multiflowæ¨¡å‹")
        
            return True
        
        except Exception as e:
            print(f"âŒ é‡ç½®AIæ¨¡å‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _force_close_database_connections(self):
        """å¼ºåˆ¶å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥"""
        try:
            if hasattr(self, 'db_manager') and self.db_manager:
                # å…³é—­æ•°æ®åº“ç®¡ç†å™¨çš„è¿æ¥
                if hasattr(self.db_manager, 'close_connection'):
                    self.db_manager.close_connection()
        
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
        
            print("   - æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except Exception as e:
            print(f"   âš ï¸ å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

    def _delete_database_files(self):
        """åˆ é™¤æ•°æ®åº“æ–‡ä»¶"""
        try:
            import time
            import os
        
            # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿è¿æ¥å®Œå…¨å…³é—­
            time.sleep(0.5)
        
            # åˆ é™¤ä¸»æ•°æ®åº“æ–‡ä»¶
            if self.db_path.exists():
                try:
                    os.remove(self.db_path)
                    print(f"   - åˆ é™¤æ•°æ®åº“æ–‡ä»¶: {self.db_path}")
                except Exception as e:
                    print(f"   âš ï¸ åˆ é™¤æ•°æ®åº“æ–‡ä»¶å¤±è´¥: {e}")
        
            # åˆ é™¤å¤‡ä»½æ–‡ä»¶
            backup_files = [self.db_path.with_suffix('.db.backup'), 
                        self.db_path.with_suffix('.backup')]
            for backup_file in backup_files:
                if backup_file.exists():
                    try:
                        os.remove(backup_file)
                        print(f"   - åˆ é™¤å¤‡ä»½æ–‡ä»¶: {backup_file}")
                    except Exception as e:
                        print(f"   âš ï¸ åˆ é™¤å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
        
            # åˆ é™¤æ¨¡å‹çŠ¶æ€æ–‡ä»¶
            if self.model_path.exists():
                try:
                    os.remove(self.model_path)
                    print(f"   - åˆ é™¤æ¨¡å‹çŠ¶æ€æ–‡ä»¶: {self.model_path}")
                except Exception as e:
                    print(f"   âš ï¸ åˆ é™¤æ¨¡å‹çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
                
        except Exception as e:
            print(f"åˆ é™¤æ•°æ®åº“æ–‡ä»¶å¤±è´¥: {e}")

    def _reset_all_components(self):
        """é‡ç½®æ‰€æœ‰ç»„ä»¶çŠ¶æ€"""
        try:
            # é‡ç½®ç‰¹å¾å·¥ç¨‹ç»„ä»¶
            if hasattr(self, 'feature_engineer'):
                self.feature_engineer = FeatureEngineer(
                    ai_config=ai_config,
                    feature_selector=ai_config.DynamicFeatureSelector(feature_count=60, selection_ratio=0.9),
                    feature_combiner=ai_config.FeatureInteractionCombiner(original_features=60, max_interactions=15),
                    timeseries_enhancer=ai_config.TimeSeriesFeatureEnhancer(history_length=10),
                    feature_weighter=ai_config.AdaptiveFeatureWeighter(feature_count=75),
                    feature_assessor=ai_config.FeatureQualityAssessor(assessment_window=50)
                )
        
            # é‡ç½®æŠ•èµ„ç³»ç»Ÿ
            if hasattr(self, 'investment_system'):
                self.investment_system = InvestmentSystem()
        
            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self.investment_stats = {
                'total_rounds': 0,
                'total_invested': 0.0,
                'total_rewards': 0.0,
                'total_penalties': 0.0,
                'current_pool_size': 0.0,
                'last_investment_details': {}
            }
        
            # é‡ç½®æ ·æœ¬ID
            if hasattr(self, '_last_sample_id'):
                self._last_sample_id = None
            
            print("   - æ‰€æœ‰ç»„ä»¶çŠ¶æ€å·²é‡ç½®")
        
        except Exception as e:
            print(f"é‡ç½®ç»„ä»¶çŠ¶æ€å¤±è´¥: {e}")
    
    def _add_fundamental_law_analysis(self, analysis_text, data_list, recommended_tails):
        """æ·»åŠ åº•å±‚å®šå¾‹åº”ç”¨åˆ†æï¼ˆå§”æ‰˜ç»™é¢„æµ‹åˆ†æå™¨ï¼‰"""
        self.prediction_analyzer._add_fundamental_law_analysis(analysis_text, data_list, recommended_tails)
    
    def get_last_prediction_details(self):
        """è·å–æœ€åä¸€æ¬¡é¢„æµ‹çš„è¯¦ç»†ä¿¡æ¯"""
        if self.last_prediction_result:
            # æ·»åŠ å½“å‰æƒé‡ä¿¡æ¯ç”¨äºè°ƒè¯•
            result = self.last_prediction_result.copy()
            result['current_model_weights'] = {k: v['weight'] for k, v in self.ensemble_weights.items()}
            result['debug_weight_info'] = f"æƒé‡æ€»æ•°: {len(self.ensemble_weights)}, æƒé‡æ± : {getattr(self, 'weight_pool', 0.0):.4f}"
            return result
        else:
            return {
                'success': False,
                'message': 'æš‚æ— é¢„æµ‹æ•°æ®',
                'weight_details': [],
                'decision_summary': 'è¯·å…ˆè¿›è¡ŒAIé¢„æµ‹',
                'detailed_analysis': 'æš‚æ— é¢„æµ‹åˆ†ææ•°æ®'
            }
        
    def _clear_database_records(self):
        """æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è®°å½•"""
        try:
            self.db_manager.clear_all_records()
        except Exception as e:
            print(f"æ¸…ç©ºæ•°æ®åº“è®°å½•å¤±è´¥: {e}")
            raise
    
    def _select_most_dangerous_tail(self, overlapping_tails: set, anti_manipulation_analysis: dict, data_list: List[Dict]) -> int:
        """ä»é‡åˆçš„å°¾æ•°ä¸­é€‰æ‹©æœ€åº”è¯¥é¿å¼€çš„ä¸€ä¸ª"""
        if len(overlapping_tails) == 1:
            return list(overlapping_tails)[0]
        
        tail_risk_scores = {}
        
        for tail in overlapping_tails:
            risk_score = 0.0
            
            # åŸºäºæœ€è¿‘é¢‘ç‡çš„é£é™©è¯„åˆ†
            if len(data_list) >= 5:
                recent_5_count = sum(1 for period in data_list[:5] if tail in period.get('tails', []))
                if recent_5_count >= 3:
                    risk_score += 0.4
                elif recent_5_count == 0:
                    risk_score += 0.3
            
            # åŸºäºæ“æ§åˆ†æè¯æ®çš„é£é™©è¯„åˆ†
            evidence = anti_manipulation_analysis.get('evidence', {})
            detection_results = evidence.get('detection_breakdown', {})
            
            psychological_score = detection_results.get('psychological_traps', {}).get('score', 0.0)
            if psychological_score > 0.7:
                risk_score += 0.3
            elif psychological_score > 0.5:
                risk_score += 0.2
            
            frequency_score = detection_results.get('frequency_deviation', {}).get('score', 0.0)
            if frequency_score > 0.6:
                risk_score += 0.25
            
            # åŸºäºå°¾æ•°ç‰¹æ€§çš„é£é™©è¯„åˆ†
            mirror_pairs = [(0, 9), (1, 8), (2, 7), (3, 6), (4, 5)]
            for pair in mirror_pairs:
                if tail in pair and any(other in overlapping_tails for other in pair if other != tail):
                    risk_score += 0.15
            
            if tail in [6, 8, 9]:
                risk_score += 0.1
            elif tail in [0, 5]:
                risk_score += 0.08
            
            tail_risk_scores[tail] = risk_score
        
        # é€‰æ‹©é£é™©åˆ†æ•°æœ€é«˜çš„å°¾æ•°
        most_dangerous_tail = max(tail_risk_scores.keys(), key=lambda t: tail_risk_scores[t])
        return most_dangerous_tail

    def _select_safest_tail(self, original_candidates: set, anti_recommendations: dict) -> int:
        """ä»åŸå§‹å€™é€‰ä¸­é€‰æ‹©æœ€å®‰å…¨çš„å°¾æ•°"""
        avoid_tails = set(anti_recommendations.get('avoid_tails', []))
        recommended_tails = set(anti_recommendations.get('recommended_tails', []))
        
        # ä¼˜å…ˆé€‰æ‹©æ¨èçš„å°¾æ•°
        safe_recommended = original_candidates.intersection(recommended_tails)
        if safe_recommended:
            return list(safe_recommended)[0]
        
        # å…¶æ¬¡é€‰æ‹©ä¸åœ¨é¿å¼€åˆ—è¡¨ä¸­çš„å°¾æ•°
        safe_neutral = original_candidates - avoid_tails
        if safe_neutral:
            return list(safe_neutral)[0]
        
        # æœ€åå…œåº•ï¼šé€‰æ‹©ä»»æ„ä¸€ä¸ªåŸå§‹å€™é€‰
        return list(original_candidates)[0] if original_candidates else 0
    
# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ç»ˆæåœ¨çº¿å­¦ä¹ AIå¼•æ“æµ‹è¯•...")
    
    # åˆ›å»ºå¼•æ“
    ultimate_ai = UltimateOnlineAIEngine("./ultimate_ai_data")
    
    if ultimate_ai.is_initialized:
        print("\nâœ… ç»ˆæåœ¨çº¿å­¦ä¹ AIå¼•æ“åˆå§‹åŒ–æˆåŠŸï¼")
        
        # æ¨¡æ‹ŸçœŸæ­£çš„åœ¨çº¿å­¦ä¹ è¿‡ç¨‹
        import random
        
        sample_data = []
        print("\nğŸ”„ å¼€å§‹åœ¨çº¿å­¦ä¹ æ¼”ç¤º...")
        
        for i in range(20):
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            num_tails = random.randint(4, 8)
            tails = random.sample(range(10), num_tails)
            numbers = [f"{random.randint(1, 49):02d}" for _ in range(7)]
            
            period_data = {
                'tails': sorted(tails),
                'numbers': numbers
            }
            sample_data.insert(0, period_data)  # æœ€æ–°æ•°æ®åœ¨å‰
            
            print(f"\nğŸ“Š æœŸæ•° {i+1}:")
            print(f"   å®é™…å°¾æ•°: {period_data['tails']}")
            
            if i == 0:
                # ç¬¬ä¸€æœŸåªè¿›è¡Œé¢„æµ‹
                result = ultimate_ai.process_new_sample(sample_data)
                print(f"   é¦–æ¬¡é¢„æµ‹: {result['prediction'].get('recommended_tails', [])}")
            else:
                # åç»­æœŸæ•°è¿›è¡Œé¢„æµ‹å’Œå­¦ä¹ 
                result = ultimate_ai.process_new_sample(sample_data, period_data['tails'])
                
                prediction = result.get('prediction', {})
                learning = result.get('learning', {})
                
                print(f"   AIé¢„æµ‹: {prediction.get('recommended_tails', [])}")
                print(f"   ç½®ä¿¡åº¦: {prediction.get('confidence', 0):.3f}")
                print(f"   é¢„æµ‹æ­£ç¡®: {learning.get('prediction_correct', False)}")
                print(f"   å½“å‰å‡†ç¡®ç‡: {learning.get('current_accuracy', 0):.3f}")
                print(f"   æ£€æµ‹åˆ°æ¼‚ç§»: {learning.get('drift_detected', False)}")
                
                if learning.get('drift_detected'):
                    print(f"   ğŸš¨ æ¼‚ç§»æ£€æµ‹å™¨: {learning.get('drift_detectors', [])}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        final_stats = ultimate_ai.get_comprehensive_stats()
        print(f"\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {final_stats['basic_stats']['total_samples_seen']}")
        print(f"   æ­£ç¡®é¢„æµ‹: {final_stats['basic_stats']['correct_predictions']}")
        print(f"   æ€»ä½“å‡†ç¡®ç‡: {final_stats['basic_stats']['current_accuracy']:.3f}")
        print(f"   ä½¿ç”¨æ¨¡å‹æ•°: {final_stats['model_stats']['total_models']}")
        
        print(f"\nğŸ¯ æƒé‡æœ€é«˜çš„æ¨¡å‹:")
        top_models = sorted(final_stats['ensemble_weights'].items(), 
                           key=lambda x: x[1], reverse=True)[:3]
        for model, weight in top_models:
            print(f"   {model}: {weight:.3f}")
        
        print("\nğŸ‰ ç»ˆæåœ¨çº¿å­¦ä¹ AIå¼•æ“æ¼”ç¤ºå®Œæˆï¼")
    else:
        print("âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–åº“å®‰è£…")