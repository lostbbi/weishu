# ai_engine/prediction/crowd_psychology_analyzer.py - ç¾¤ä½“å¿ƒç†åˆ†æå™¨æ¨¡å‹

import numpy as np
import math
from datetime import datetime, timedelta
from typing import List, Dict, Set, Optional, Tuple, Any, Union
from collections import defaultdict, deque
from scipy import stats, signal, optimize, spatial
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.stats import entropy, pearsonr, spearmanr, kendalltau, jarque_bera, normaltest
import statistics
from enum import Enum
import warnings
from collections import Counter
from scipy.stats import chi2
warnings.filterwarnings('ignore')

# ç§‘ç ”çº§åˆ«çš„æœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æåº“
try:
    from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
    from sklearn.decomposition import PCA, FastICA, NMF
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.ensemble import IsolationForest
    from sklearn.manifold import TSNE
    from sklearn.mixture import GaussianMixture
    from sklearn.metrics import silhouette_score, calinski_harabasz_score
    SKLEARN_AVAILABLE = True
    print("âœ… ç§‘ç ”çº§æœºå™¨å­¦ä¹ åº“åŠ è½½æˆåŠŸ")
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learnä¸å¯ç”¨ï¼Œéƒ¨åˆ†é«˜çº§åˆ†æåŠŸèƒ½å°†å—é™")

# é«˜çº§ç»Ÿè®¡åˆ†æåº“
try:
    import statsmodels.api as sm
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.stats.diagnostic import acorr_ljungbox
    from statsmodels.tsa.stattools import adfuller, kpss
    from statsmodels.stats.stattools import jarque_bera as jb_test
    STATSMODELS_AVAILABLE = True
    print("âœ… é«˜çº§ç»Ÿè®¡åˆ†æåº“åŠ è½½æˆåŠŸ")
except ImportError:
    STATSMODELS_AVAILABLE = False
    print("âš ï¸ statsmodelsä¸å¯ç”¨ï¼Œéƒ¨åˆ†æ—¶é—´åºåˆ—åˆ†æåŠŸèƒ½å°†å—é™")

# é«˜çº§æ•°å€¼ä¼˜åŒ–åº“
try:
    from scipy.optimize import minimize, differential_evolution, basinhopping
    from scipy.integrate import quad, odeint
    from scipy.special import gamma, beta, digamma, polygamma
    SCIPY_ADVANCED_AVAILABLE = True
    print("âœ… é«˜çº§æ•°å€¼ä¼˜åŒ–åº“åŠ è½½æˆåŠŸ")
except ImportError:
    SCIPY_ADVANCED_AVAILABLE = False
    print("âš ï¸ é«˜çº§scipyåŠŸèƒ½ä¸å¯ç”¨")

class PsychologyState(Enum):
    """ç¾¤ä½“å¿ƒç†çŠ¶æ€æšä¸¾ - åŸºäºKahneman-Tverskyå‰æ™¯ç†è®ºçš„æ‰©å±•åˆ†ç±»"""
    # æç«¯ä¹è§‚çŠ¶æ€ (0.9-1.0)
    EUPHORIA = "æåº¦ç‹‚æ¬¢"              # éç†æ€§ç¹è£çŠ¶æ€
    IRRATIONAL_EXUBERANCE = "éç†æ€§ç¹è£"  # æ ¼æ—æ–¯æ½˜ç†è®ºçŠ¶æ€
    MANIC_OPTIMISM = "ç‹‚èºä¹è§‚"         # èºç‹‚ç—‡çŠ¶æ€
    
    # è´ªå©ªçŠ¶æ€ (0.7-0.9)  
    EXTREME_GREED = "æåº¦è´ªå©ª"          # æç«¯è·åˆ©æ¬²æœ›
    GREED = "è´ªå©ª"                    # æ ‡å‡†è´ªå©ªçŠ¶æ€
    FOMO = "é”™å¤±ææƒ§"                  # Fear of Missing Out
    
    # ä¹è§‚çŠ¶æ€ (0.5-0.7)
    STRONG_OPTIMISM = "å¼ºçƒˆä¹è§‚"        # å¼ºçƒˆçœ‹å¥½
    OPTIMISM = "ä¹è§‚"                 # æ ‡å‡†ä¹è§‚
    CAUTIOUS_OPTIMISM = "è°¨æ…ä¹è§‚"       # æœ‰ä¿ç•™çš„ä¹è§‚
    
    # ä¸­æ€§çŠ¶æ€ (0.4-0.6)
    HOPE = "å¸Œæœ›"                     # æœŸå¾…çŠ¶æ€
    NEUTRAL = "ä¸­æ€§"                  # å¹³è¡¡çŠ¶æ€
    UNCERTAINTY = "ä¸ç¡®å®š"             # è¿·èŒ«çŠ¶æ€
    
    # æ‹…å¿§çŠ¶æ€ (0.3-0.5)
    CONCERN = "æ‹…å¿§"                  # è½»åº¦æ‹…å¿ƒ
    ANXIETY = "ç„¦è™‘"                  # ç„¦è™‘ä¸å®‰
    NERVOUSNESS = "ç´§å¼ "              # ç´§å¼ æƒ…ç»ª
    
    # ææƒ§çŠ¶æ€ (0.1-0.3)
    FEAR = "ææƒ§"                     # æ ‡å‡†ææƒ§
    STRONG_FEAR = "å¼ºçƒˆææƒ§"           # å¼ºçƒˆææƒ§
    LOSS_AVERSION_EXTREME = "æåº¦æŸå¤±åŒæ¶"  # æç«¯æŸå¤±åŒæ¶
    
    # æç«¯ææ…ŒçŠ¶æ€ (0.0-0.1)
    PANIC = "ææ…Œ"                    # ææ…ŒæŠ›å”®
    CAPITULATION = "æŠ•é™"             # å½»åº•æŠ•é™
    DESPAIR = "ç»æœ›"                  # ç»æœ›çŠ¶æ€

class CrowdBehaviorPattern(Enum):
    """ç¾¤ä½“è¡Œä¸ºæ¨¡å¼æšä¸¾ - åŸºäºå‹’åºç¾¤ä½“å¿ƒç†å­¦ç†è®º"""
    HERDING = "ä»ä¼—è·Ÿé£"               # ç¾Šç¾¤æ•ˆåº”
    CONTRARIAN = "é€†å‘æ€ç»´"            # åå‘æ“ä½œ
    MOMENTUM = "åŠ¨é‡è¿½è¸ª"              # åŠ¨é‡è·Ÿéš
    MEAN_REVERSION = "å‡å€¼å›å½’"         # å›å½’å‡å€¼
    RANDOM_WALK = "éšæœºæ¸¸èµ°"           # éšæœºè¡Œä¸º
    CHAOS = "æ··æ²Œæ— åº"                 # æ··æ²ŒçŠ¶æ€
    CYCLICAL = "å‘¨æœŸå¾ªç¯"              # å‘¨æœŸæ€§è¡Œä¸º
    SEASONAL = "å­£èŠ‚æ€§"                # å­£èŠ‚æ€§æ¨¡å¼

class CognitiveBias(Enum):
    """è®¤çŸ¥åå·®ç±»å‹æšä¸¾ - åŸºäºè¡Œä¸ºç»æµå­¦ç†è®º"""
    ANCHORING = "é”šå®šåå·®"             # é”šå®šæ•ˆåº”
    AVAILABILITY = "å¯å¾—æ€§åå·®"         # å¯å¾—æ€§å¯å‘æ³•
    CONFIRMATION = "ç¡®è®¤åå·®"          # ç¡®è®¤æ€§åè§
    REPRESENTATIVENESS = "ä»£è¡¨æ€§åå·®"   # ä»£è¡¨æ€§å¯å‘æ³•
    LOSS_AVERSION = "æŸå¤±åŒæ¶"         # æŸå¤±åŒæ¶åå·®
    ENDOWMENT_EFFECT = "ç¦€èµ‹æ•ˆåº”"       # æ‹¥æœ‰åè§
    MENTAL_ACCOUNTING = "å¿ƒç†è´¦æˆ·"      # å¿ƒç†ä¼šè®¡åå·®
    OVERCONFIDENCE = "è¿‡åº¦è‡ªä¿¡"        # è¿‡åº¦è‡ªä¿¡åå·®
    HINDSIGHT = "åè§ä¹‹æ˜"             # åè§åå·®
    RECENCY = "è¿‘æœŸåå·®"              # è¿‘æœŸæ•ˆåº”
    PRIMACY = "é¦–å› åå·®"              # é¦–å› æ•ˆåº”  
    HOT_HAND = "çƒ­æ‰‹è°¬è¯¯"             # çƒ­æ‰‹æ•ˆåº”
    GAMBLERS_FALLACY = "èµŒå¾’è°¬è¯¯"      # èµŒå¾’è°¬è¯¯
    SUNK_COST = "æ²‰æ²¡æˆæœ¬"            # æ²‰æ²¡æˆæœ¬è°¬è¯¯
    FRAMING = "æ¡†æ¶æ•ˆåº”"              # æ¡†æ¶åå·®
    STATUS_QUO = "ç°çŠ¶åå·®"           # ç°çŠ¶ç»´æŒåè§

class PsychometricsEngine:
    """
    å¿ƒç†è®¡é‡å­¦å¼•æ“ - ç§‘ç ”çº§åˆ«çš„å¿ƒç†æµ‹é‡å’Œåˆ†ææ ¸å¿ƒ
    åŸºäºç°ä»£å¿ƒç†è®¡é‡å­¦ç†è®ºå’Œç»Ÿè®¡å­¦æ–¹æ³•
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å¿ƒç†è®¡é‡å­¦å¼•æ“"""
        # å¿ƒç†æµ‹é‡æ¨¡å‹å‚æ•°
        self.psychometric_models = {
            'item_response_theory': self._init_irt_models(),      # é¡¹ç›®ååº”ç†è®ºæ¨¡å‹
            'factor_analysis': self._init_factor_models(),        # å› å­åˆ†ææ¨¡å‹  
            'structural_equation': self._init_sem_models(),       # ç»“æ„æ–¹ç¨‹æ¨¡å‹
            'multilevel_models': self._init_mlm_models(),         # å¤šå±‚æ¬¡æ¨¡å‹
            'bayesian_models': self._init_bayesian_models()       # è´å¶æ–¯æ¨¡å‹
        }
        
        # ä¿¡åº¦å’Œæ•ˆåº¦åˆ†æå·¥å…·
        self.reliability_tools = {
            'cronbach_alpha': self._cronbach_alpha,              # Cronbach's Î±ç³»æ•°
            'split_half': self._split_half_reliability,          # åˆ†åŠä¿¡åº¦
            'test_retest': self._test_retest_reliability,        # é‡æµ‹ä¿¡åº¦
            'inter_rater': self._inter_rater_reliability,        # è¯„åˆ†è€…é—´ä¿¡åº¦
            'composite_reliability': self._composite_reliability  # ç»„åˆä¿¡åº¦
        }
        
        self.validity_tools = {
            'content_validity': self._content_validity_analysis,  # å†…å®¹æ•ˆåº¦
            'construct_validity': self._construct_validity_analysis, # ç»“æ„æ•ˆåº¦
            'criterion_validity': self._criterion_validity_analysis, # æ ‡å‡†æ•ˆåº¦
            'convergent_validity': self._convergent_validity_analysis, # èšåˆæ•ˆåº¦
            'discriminant_validity': self._discriminant_validity_analysis # åŒºåˆ†æ•ˆåº¦
        }
        
        # é«˜çº§ç»Ÿè®¡åˆ†æå·¥å…·
        self.advanced_analytics = {
            'information_theory': InformationTheoryAnalyzer(),    # ä¿¡æ¯è®ºåˆ†æ
            'chaos_theory': ChaosTheoryAnalyzer(),               # æ··æ²Œç†è®ºåˆ†æ
            'network_analysis': NetworkAnalyzer(),               # ç½‘ç»œåˆ†æ
            'fractal_analysis': FractalAnalyzer(),               # åˆ†å½¢åˆ†æ
            'wavelet_analysis': WaveletAnalyzer()                # å°æ³¢åˆ†æ
        }
        
        print("ğŸ§  å¿ƒç†è®¡é‡å­¦å¼•æ“åˆå§‹åŒ–å®Œæˆ - ç§‘ç ”çº§åˆ«å¿ƒç†æµ‹é‡ç³»ç»Ÿ")
    
    def _init_irt_models(self) -> Dict:
        """åˆå§‹åŒ–é¡¹ç›®ååº”ç†è®ºæ¨¡å‹"""
        return {
            'rasch_model': {'parameters': [], 'fit_statistics': {}},
            'two_parameter': {'parameters': [], 'fit_statistics': {}},
            'three_parameter': {'parameters': [], 'fit_statistics': {}},
            'graded_response': {'parameters': [], 'fit_statistics': {}},
            'partial_credit': {'parameters': [], 'fit_statistics': {}}
        }
    
    def _init_factor_models(self) -> Dict:
        """åˆå§‹åŒ–å› å­åˆ†ææ¨¡å‹"""
        return {
            'exploratory_fa': {'loadings': [], 'eigenvalues': [], 'communalities': []},
            'confirmatory_fa': {'fit_indices': {}, 'modification_indices': {}},
            'hierarchical_fa': {'higher_order_factors': [], 'nested_structure': {}},
            'bifactor_model': {'general_factor': [], 'specific_factors': []}
        }
    
    def _init_sem_models(self) -> Dict:
        """åˆå§‹åŒ–ç»“æ„æ–¹ç¨‹æ¨¡å‹"""
        return {
            'measurement_model': {'factor_loadings': [], 'error_variances': []},
            'structural_model': {'path_coefficients': [], 'explained_variance': []},
            'fit_indices': {'cfi': 0, 'tli': 0, 'rmsea': 0, 'srmr': 0, 'aic': 0, 'bic': 0},
            'modification_indices': []
        }
    
    def _init_mlm_models(self) -> Dict:
        """åˆå§‹åŒ–å¤šå±‚æ¬¡æ¨¡å‹"""
        return {
            'fixed_effects': [],
            'random_effects': [], 
            'variance_components': [],
            'intraclass_correlation': 0
        }
    
    def _init_bayesian_models(self) -> Dict:
        """åˆå§‹åŒ–è´å¶æ–¯æ¨¡å‹"""
        return {
            'prior_distributions': {},
            'posterior_distributions': {},
            'mcmc_diagnostics': {},
            'bayes_factors': {}
        }
    
    def _test_retest_reliability(self, data1, data2=None, method='pearson', time_interval=None):
        """
        è®¡ç®—é‡æµ‹ä¿¡åº¦ï¼ˆTest-retest Reliabilityï¼‰
    
        Args:
            data1: ç¬¬ä¸€æ¬¡æµ‹è¯•æ•°æ® (array-like)
            data2: ç¬¬äºŒæ¬¡æµ‹è¯•æ•°æ® (array-like, optional)
            method: ç›¸å…³ç³»æ•°è®¡ç®—æ–¹æ³• ('pearson', 'spearman', 'kendall', 'icc')
            time_interval: ä¸¤æ¬¡æµ‹è¯•çš„æ—¶é—´é—´éš”ï¼ˆå¤©æ•°ï¼‰
    
        Returns:
            dict: é‡æµ‹ä¿¡åº¦åˆ†æç»“æœ
            {
                'reliability_coefficient': float,  # ä¿¡åº¦ç³»æ•°
                'p_value': float,                  # æ˜¾è‘—æ€§æ°´å¹³
                'confidence_interval': tuple,      # ç½®ä¿¡åŒºé—´
                'method': str,                     # ä½¿ç”¨çš„æ–¹æ³•
                'sample_size': int,               # æ ·æœ¬å¤§å°
                'interpretation': str,            # ç»“æœè§£é‡Š
                'time_interval': int,             # æ—¶é—´é—´éš”
                'stability_assessment': str       # ç¨³å®šæ€§è¯„ä¼°
            }
        """
        try:
            import numpy as np
            from scipy.stats import pearsonr, spearmanr, kendalltau
            from scipy import stats
        
            # æ•°æ®é¢„å¤„ç†
            if data2 is None:
                # å¦‚æœåªæœ‰ä¸€ç»„æ•°æ®ï¼Œåˆ†å‰²ä¸ºä¸¤éƒ¨åˆ†æ¨¡æ‹Ÿé‡æµ‹
                if len(data1) < 4:
                    return {'error': 'æ•°æ®é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘4ä¸ªæ•°æ®ç‚¹è¿›è¡Œåˆ†å‰²'}
            
                mid = len(data1) // 2
                data1_split = np.array(data1[:mid])
                data2_split = np.array(data1[mid:mid*2])
                time_interval = time_interval or 1  # é»˜è®¤1å¤©é—´éš”
            else:
                data1_split = np.array(data1)
                data2_split = np.array(data2)
        
            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
            min_len = min(len(data1_split), len(data2_split))
            if min_len < 3:
                return {'error': 'æœ‰æ•ˆæ•°æ®ç‚¹ä¸è¶³ï¼Œéœ€è¦è‡³å°‘3ä¸ªé…å¯¹è§‚æµ‹'}
        
            data1_split = data1_split[:min_len]
            data2_split = data2_split[:min_len]
        
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if np.var(data1_split) == 0 or np.var(data2_split) == 0:
                return {'error': 'æ•°æ®æ–¹å·®ä¸º0ï¼Œæ— æ³•è®¡ç®—ç›¸å…³ç³»æ•°'}
        
            # è®¡ç®—ç›¸å…³ç³»æ•°å’Œç½®ä¿¡åŒºé—´
            if method == 'pearson':
                correlation, p_value = pearsonr(data1_split, data2_split)
                method_name = "Pearsonç§¯çŸ©ç›¸å…³"
            elif method == 'spearman':
                correlation, p_value = spearmanr(data1_split, data2_split)
                method_name = "Spearmanç­‰çº§ç›¸å…³"
            elif method == 'kendall':
                correlation, p_value = kendalltau(data1_split, data2_split)
                method_name = "Kendall Tauç›¸å…³"
            elif method == 'icc':
                # ç»„å†…ç›¸å…³ç³»æ•°ï¼ˆIntraclass Correlation Coefficientï¼‰
                correlation, p_value = self._calculate_icc(data1_split, data2_split)
                method_name = "ç»„å†…ç›¸å…³ç³»æ•°(ICC)"
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
        
            # è®¡ç®—ç½®ä¿¡åŒºé—´ (Fisher Zå˜æ¢)
            if method in ['pearson', 'spearman']:
                ci_lower, ci_upper = self._calculate_correlation_ci(correlation, min_len)
            else:
                ci_lower, ci_upper = correlation - 0.1, correlation + 0.1  # ç®€åŒ–å¤„ç†
        
            # ä¿¡åº¦è§£é‡Š
            interpretation = self._interpret_reliability(correlation)
        
            # ç¨³å®šæ€§è¯„ä¼°
            stability_assessment = self._assess_stability(correlation, time_interval, p_value)
        
            return {
                'reliability_coefficient': float(correlation),
                'p_value': float(p_value),
                'confidence_interval': (float(ci_lower), float(ci_upper)),
                'method': method_name,
                'sample_size': min_len,
                'interpretation': interpretation,
                'time_interval': time_interval,
                'stability_assessment': stability_assessment,
                'effect_size': self._calculate_effect_size(correlation),
                'statistical_power': self._estimate_power(correlation, min_len),
                'recommendations': self._generate_recommendations(correlation, min_len, time_interval)
            }
        
        except Exception as e:
            return {'error': f'é‡æµ‹ä¿¡åº¦è®¡ç®—å¤±è´¥: {str(e)}'}

    def _inter_rater_reliability(self, ratings_data, method='icc', rater_ids=None):
        """
        è®¡ç®—è¯„åˆ†è€…é—´ä¿¡åº¦ï¼ˆInter-rater Reliabilityï¼‰
    
        Args:
            ratings_data: è¯„åˆ†æ•°æ®ï¼Œæ ¼å¼ä¸º [[rater1_scores], [rater2_scores], ...]
            method: è®¡ç®—æ–¹æ³• ('icc', 'cronbach_alpha', 'kendall_w', 'pearson')
            rater_ids: è¯„åˆ†è€…IDåˆ—è¡¨
    
        Returns:
            dict: è¯„åˆ†è€…é—´ä¿¡åº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
            from scipy.stats import pearsonr, kendalltau
            from scipy import stats
        
            ratings_array = np.array(ratings_data)
            if ratings_array.ndim != 2:
                return {'error': 'è¯„åˆ†æ•°æ®æ ¼å¼é”™è¯¯ï¼Œéœ€è¦äºŒç»´æ•°ç»„'}
        
            n_raters, n_items = ratings_array.shape
            if n_raters < 2:
                return {'error': 'éœ€è¦è‡³å°‘2ä¸ªè¯„åˆ†è€…'}
            if n_items < 3:
                return {'error': 'éœ€è¦è‡³å°‘3ä¸ªè¯„åˆ†é¡¹ç›®'}
        
            results = {
                'n_raters': n_raters,
                'n_items': n_items,
                'method': method,
                'rater_ids': rater_ids or [f'è¯„åˆ†è€…{i+1}' for i in range(n_raters)]
            }
        
            if method == 'icc':
                # è®¡ç®—ç»„å†…ç›¸å…³ç³»æ•°
                icc_value, p_value = self._calculate_multi_rater_icc(ratings_array)
                results.update({
                    'reliability_coefficient': float(icc_value),
                    'p_value': float(p_value),
                    'interpretation': self._interpret_reliability(icc_value),
                    'method_name': 'ç»„å†…ç›¸å…³ç³»æ•°(ICC)'
                })
            
            elif method == 'cronbach_alpha':
                # ä½¿ç”¨Cronbach's Î±ä½œä¸ºè¯„åˆ†è€…é—´ä¿¡åº¦
                alpha = self._calculate_cronbach_alpha(ratings_array.T)  # è½¬ç½®ä½¿è¯„åˆ†è€…æˆä¸ºé¡¹ç›®
                results.update({
                    'reliability_coefficient': float(alpha),
                    'interpretation': self._interpret_reliability(alpha),
                    'method_name': "Cronbach's Î±"
                })
            
            elif method == 'kendall_w':
                # Kendall's W ä¸€è‡´æ€§ç³»æ•°
                w_value, p_value = self._calculate_kendall_w(ratings_array)
                results.update({
                    'reliability_coefficient': float(w_value),
                    'p_value': float(p_value),
                    'interpretation': self._interpret_concordance(w_value),
                    'method_name': "Kendall's Wä¸€è‡´æ€§ç³»æ•°"
                })
            
            elif method == 'pearson':
                # é…å¯¹ç›¸å…³åˆ†æ
                correlations = []
                for i in range(n_raters):
                    for j in range(i+1, n_raters):
                        corr, _ = pearsonr(ratings_array[i], ratings_array[j])
                        correlations.append(corr)
            
                avg_correlation = np.mean(correlations)
                results.update({
                    'reliability_coefficient': float(avg_correlation),
                    'pairwise_correlations': [float(c) for c in correlations],
                    'interpretation': self._interpret_reliability(avg_correlation),
                    'method_name': 'é…å¯¹Pearsonç›¸å…³å¹³å‡å€¼'
                })
        
            # æ·»åŠ è¯¦ç»†åˆ†æ
            results.update({
                'rater_statistics': self._analyze_rater_characteristics(ratings_array, results['rater_ids']),
                'agreement_matrix': self._calculate_agreement_matrix(ratings_array),
                'recommendations': self._generate_inter_rater_recommendations(results['reliability_coefficient'], n_raters, n_items)
            })
        
            return results
        
        except Exception as e:
            return {'error': f'è¯„åˆ†è€…é—´ä¿¡åº¦è®¡ç®—å¤±è´¥: {str(e)}'}

    def _composite_reliability(self, factor_loadings, error_variances=None, method='fornell_larcker'):
        """
        è®¡ç®—ç»„åˆä¿¡åº¦ï¼ˆComposite Reliabilityï¼‰
    
        Args:
            factor_loadings: å› å­è½½è·åˆ—è¡¨
            error_variances: è¯¯å·®æ–¹å·®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            method: è®¡ç®—æ–¹æ³• ('fornell_larcker', 'omega', 'alpha_ordinal')
    
        Returns:
            dict: ç»„åˆä¿¡åº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
        
            loadings = np.array(factor_loadings)
            if len(loadings) < 2:
                return {'error': 'éœ€è¦è‡³å°‘2ä¸ªå› å­è½½è·'}
        
            results = {
                'n_indicators': len(loadings),
                'factor_loadings': [float(l) for l in loadings],
                'method': method
            }
        
            if method == 'fornell_larcker':
                # Fornell & Larcker (1981) ç»„åˆä¿¡åº¦
                sum_loadings = np.sum(loadings)
                sum_loadings_squared = np.sum(loadings ** 2)
            
                if error_variances is None:
                    # å‡è®¾æ ‡å‡†åŒ–è½½è·ï¼Œè¯¯å·®æ–¹å·® = 1 - è½½è·å¹³æ–¹
                    error_variances = 1 - loadings ** 2
            
                error_variances = np.array(error_variances)
                sum_error_variances = np.sum(error_variances)
            
                composite_reliability = (sum_loadings ** 2) / (
                    (sum_loadings ** 2) + sum_error_variances
                )
            
                # å¹³å‡æ–¹å·®æå–é‡ (AVE)
                ave = sum_loadings_squared / (sum_loadings_squared + sum_error_variances)
            
                results.update({
                    'composite_reliability': float(composite_reliability),
                    'ave': float(ave),
                    'method_name': 'Fornell-Larckerç»„åˆä¿¡åº¦',
                    'discriminant_validity': float(np.sqrt(ave)) if ave > 0 else 0
                })
            
            elif method == 'omega':
                # McDonald's Ï‰ (Omega)
                sum_loadings_squared = np.sum(loadings ** 2)
            
                if error_variances is None:
                    error_variances = 1 - loadings ** 2
            
                error_variances = np.array(error_variances)
                sum_error_variances = np.sum(error_variances)
            
                omega = sum_loadings_squared / (sum_loadings_squared + sum_error_variances)
            
                results.update({
                    'composite_reliability': float(omega),
                    'method_name': "McDonald's Omega (Ï‰)",
                    'omega_hierarchical': self._calculate_omega_hierarchical(loadings)
                })
            
            elif method == 'alpha_ordinal':
                # æœ‰åºAlphaï¼ˆé€‚ç”¨äºç­‰çº§æ•°æ®ï¼‰
                correlation_matrix = np.corrcoef([loadings, loadings])  # ç®€åŒ–å¤„ç†
                n_items = len(loadings)
            
                if n_items > 1:
                    avg_inter_correlation = np.mean(correlation_matrix[np.triu_indices(n_items, k=1)])
                    alpha_ordinal = (n_items * avg_inter_correlation) / (1 + (n_items - 1) * avg_inter_correlation)
                else:
                    alpha_ordinal = 0
            
                results.update({
                    'composite_reliability': float(alpha_ordinal),
                    'method_name': 'æœ‰åºAlphaä¿¡åº¦',
                    'average_inter_item_correlation': float(avg_inter_correlation)
                })
        
            # é€šç”¨ç»“æœè§£é‡Š
            cr_value = results['composite_reliability']
            results.update({
                'interpretation': self._interpret_composite_reliability(cr_value),
                'quality_assessment': self._assess_composite_reliability_quality(cr_value, results.get('ave', 0)),
                'factor_loading_assessment': self._assess_factor_loadings(loadings),
                'recommendations': self._generate_composite_reliability_recommendations(cr_value, loadings)
            })
        
            return results
        
        except Exception as e:
            return {'error': f'ç»„åˆä¿¡åº¦è®¡ç®—å¤±è´¥: {str(e)}'}

    def _cronbach_alpha(self, data, standardized=True):
        """
        è®¡ç®—Cronbach's Î±ç³»æ•°
    
        Args:
            data: é¡¹ç›®æ•°æ®çŸ©é˜µ (n_samples x n_items)
            standardized: æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–Î±
    
        Returns:
            dict: Cronbach's Î±åˆ†æç»“æœ
        """
        try:
            import numpy as np
        
            data_array = np.array(data)
            if data_array.ndim != 2:
                return {'error': 'æ•°æ®æ ¼å¼é”™è¯¯ï¼Œéœ€è¦äºŒç»´æ•°ç»„'}
        
            n_samples, n_items = data_array.shape
            if n_items < 2:
                return {'error': 'éœ€è¦è‡³å°‘2ä¸ªé¡¹ç›®è®¡ç®—Î±ç³»æ•°'}
            if n_samples < 3:
                return {'error': 'éœ€è¦è‡³å°‘3ä¸ªæ ·æœ¬'}
        
            # è®¡ç®—é¡¹ç›®æ€»åˆ†
            total_scores = np.sum(data_array, axis=1)
        
            # è®¡ç®—å„é¡¹ç›®æ–¹å·®å’Œæ€»åˆ†æ–¹å·®
            item_variances = np.var(data_array, axis=0, ddof=1)
            total_variance = np.var(total_scores, ddof=1)
        
            # è®¡ç®—Cronbach's Î±
            sum_item_variances = np.sum(item_variances)
            alpha = (n_items / (n_items - 1)) * (1 - sum_item_variances / total_variance)
        
            # æ ‡å‡†åŒ–Î±
            if standardized:
                # è®¡ç®—é¡¹ç›®é—´å¹³å‡ç›¸å…³
                correlation_matrix = np.corrcoef(data_array.T)
                avg_inter_correlation = np.mean(correlation_matrix[np.triu_indices(n_items, k=1)])
                standardized_alpha = (n_items * avg_inter_correlation) / (1 + (n_items - 1) * avg_inter_correlation)
            else:
                standardized_alpha = alpha
        
            # é¡¹ç›®åˆ é™¤åçš„Î±å€¼
            alpha_if_deleted = []
            for i in range(n_items):
                remaining_items = np.delete(data_array, i, axis=1)
                remaining_total = np.sum(remaining_items, axis=1)
                remaining_item_vars = np.var(remaining_items, axis=0, ddof=1)
                remaining_total_var = np.var(remaining_total, ddof=1)
            
                if remaining_total_var > 0:
                    alpha_deleted = ((n_items - 1) / (n_items - 2)) * (1 - np.sum(remaining_item_vars) / remaining_total_var)
                else:
                    alpha_deleted = 0
                alpha_if_deleted.append(alpha_deleted)
        
            results = {
                'cronbach_alpha': float(alpha),
                'standardized_alpha': float(standardized_alpha),
                'n_items': n_items,
                'n_samples': n_samples,
                'alpha_if_item_deleted': [float(a) for a in alpha_if_deleted],
                'item_statistics': {
                    'item_variances': [float(v) for v in item_variances],
                    'item_means': [float(m) for m in np.mean(data_array, axis=0)],
                    'item_std': [float(s) for s in np.std(data_array, axis=0, ddof=1)]
                },
                'interpretation': self._interpret_reliability(standardized_alpha),
                'reliability_category': self._categorize_alpha_reliability(standardized_alpha),
                'recommendations': self._generate_alpha_recommendations(standardized_alpha, alpha_if_deleted, n_items)
            }
        
            return results
        
        except Exception as e:
            return {'error': f'Cronbach Î±è®¡ç®—å¤±è´¥: {str(e)}'}

    def _split_half_reliability(self, data, method='spearman_brown', split_method='odd_even'):
        """
        è®¡ç®—åˆ†åŠä¿¡åº¦
    
        Args:
            data: é¡¹ç›®æ•°æ®çŸ©é˜µ
            method: æ ¡æ­£æ–¹æ³• ('spearman_brown', 'guttman', 'cronbach_alpha')
            split_method: åˆ†å‰²æ–¹æ³• ('odd_even', 'first_last', 'random')
    
        Returns:
            dict: åˆ†åŠä¿¡åº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
            from scipy.stats import pearsonr
        
            data_array = np.array(data)
            if data_array.ndim != 2:
                return {'error': 'æ•°æ®æ ¼å¼é”™è¯¯ï¼Œéœ€è¦äºŒç»´æ•°ç»„'}
        
            n_samples, n_items = data_array.shape
            if n_items < 4:
                return {'error': 'éœ€è¦è‡³å°‘4ä¸ªé¡¹ç›®è¿›è¡Œåˆ†åŠ'}
            if n_samples < 3:
                return {'error': 'éœ€è¦è‡³å°‘3ä¸ªæ ·æœ¬'}
        
            # æ ¹æ®åˆ†å‰²æ–¹æ³•åˆ†å‰²é¡¹ç›®
            if split_method == 'odd_even':
                half1_indices = list(range(0, n_items, 2))  # å¥‡æ•°ä½ç½®
                half2_indices = list(range(1, n_items, 2))  # å¶æ•°ä½ç½®
            elif split_method == 'first_last':
                mid = n_items // 2
                half1_indices = list(range(mid))
                half2_indices = list(range(mid, n_items))
            elif split_method == 'random':
                np.random.seed(42)  # å›ºå®šç§å­ç¡®ä¿å¯é‡å¤
                indices = np.random.permutation(n_items)
                mid = n_items // 2
                half1_indices = indices[:mid].tolist()
                half2_indices = indices[mid:].tolist()
            else:
                return {'error': f'ä¸æ”¯æŒçš„åˆ†å‰²æ–¹æ³•: {split_method}'}
        
            # è®¡ç®—ä¸¤åŠå¾—åˆ†
            half1_scores = np.sum(data_array[:, half1_indices], axis=1)
            half2_scores = np.sum(data_array[:, half2_indices], axis=1)
        
            # è®¡ç®—ä¸¤åŠé—´ç›¸å…³
            raw_correlation, p_value = pearsonr(half1_scores, half2_scores)
        
            # åº”ç”¨æ ¡æ­£å…¬å¼
            if method == 'spearman_brown':
                # Spearman-Brownæ ¡æ­£å…¬å¼
                corrected_reliability = (2 * raw_correlation) / (1 + raw_correlation)
                method_name = 'Spearman-Brownæ ¡æ­£'
            elif method == 'guttman':
                # Guttmanåˆ†åŠä¿¡åº¦
                var_half1 = np.var(half1_scores, ddof=1)
                var_half2 = np.var(half2_scores, ddof=1)
                var_total = np.var(half1_scores + half2_scores, ddof=1)
                corrected_reliability = 2 * (1 - (var_half1 + var_half2) / var_total)
                method_name = 'Guttmanåˆ†åŠä¿¡åº¦'
            elif method == 'cronbach_alpha':
                # å¯¹æ¯ä¸€åŠåˆ†åˆ«è®¡ç®—Î±å†æ±‚å¹³å‡
                alpha1 = self._calculate_cronbach_alpha(data_array[:, half1_indices])
                alpha2 = self._calculate_cronbach_alpha(data_array[:, half2_indices])
                corrected_reliability = (alpha1 + alpha2) / 2
                method_name = 'åˆ†åŠCronbach Î±å¹³å‡'
            else:
                return {'error': f'ä¸æ”¯æŒçš„æ ¡æ­£æ–¹æ³•: {method}'}
        
            results = {
                'raw_correlation': float(raw_correlation),
                'corrected_reliability': float(corrected_reliability),
                'p_value': float(p_value),
                'method': method_name,
                'split_method': split_method,
                'half1_items': len(half1_indices),
                'half2_items': len(half2_indices),
                'half1_indices': half1_indices,
                'half2_indices': half2_indices,
                'half_statistics': {
                    'half1_mean': float(np.mean(half1_scores)),
                    'half1_std': float(np.std(half1_scores, ddof=1)),
                    'half2_mean': float(np.mean(half2_scores)),
                    'half2_std': float(np.std(half2_scores, ddof=1))
                },
                'interpretation': self._interpret_reliability(corrected_reliability),
                'recommendations': self._generate_split_half_recommendations(corrected_reliability, len(half1_indices), len(half2_indices))
            }
        
            return results
        
        except Exception as e:
            return {'error': f'åˆ†åŠä¿¡åº¦è®¡ç®—å¤±è´¥: {str(e)}'}

    # æ•ˆåº¦åˆ†ææ–¹æ³•
    def _content_validity_analysis(self, items_data, expert_ratings=None, content_domains=None):
        """
        å†…å®¹æ•ˆåº¦åˆ†æï¼ˆContent Validity Analysisï¼‰
    
        Args:
            items_data: é¡¹ç›®æ•°æ®
            expert_ratings: ä¸“å®¶è¯„åˆ†æ•°æ® (å¯é€‰)
            content_domains: å†…å®¹é¢†åŸŸåˆ†ç±» (å¯é€‰)
    
        Returns:
            dict: å†…å®¹æ•ˆåº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
        
            results = {
                'method': 'Content Validity Analysis',
                'analysis_type': 'content_validity'
            }
        
            if expert_ratings is not None:
                # è®¡ç®—å†…å®¹æ•ˆåº¦æ¯”ç‡ (CVR)
                expert_ratings = np.array(expert_ratings)
                n_experts = len(expert_ratings)
            
                if n_experts >= 3:
                    # è®¡ç®—æ¯ä¸ªé¡¹ç›®çš„CVR
                    cvr_scores = []
                    for item_ratings in expert_ratings.T:  # è½¬ç½®ä»¥æŒ‰é¡¹ç›®è®¡ç®—
                        essential_count = np.sum(item_ratings >= 3)  # å‡è®¾3åˆ†ä»¥ä¸Šä¸º"å¿…è¦"
                        cvr = (essential_count - n_experts/2) / (n_experts/2)
                        cvr_scores.append(cvr)
                
                    # CVRä¸´ç•Œå€¼ (åŸºäºä¸“å®¶æ•°é‡)
                    cvr_critical = self._get_cvr_critical_value(n_experts)
                
                    results.update({
                        'cvr_scores': [float(score) for score in cvr_scores],
                        'cvr_critical_value': cvr_critical,
                        'acceptable_items': [i for i, score in enumerate(cvr_scores) if score >= cvr_critical],
                        'n_experts': n_experts,
                        'expert_agreement': float(np.mean([np.std(item_ratings) for item_ratings in expert_ratings.T]))
                    })
                else:
                    results['warning'] = 'ä¸“å®¶æ•°é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘3ä½ä¸“å®¶ï¼‰'
        
            if content_domains is not None:
                # å†…å®¹é¢†åŸŸè¦†ç›–åˆ†æ
                domain_coverage = self._analyze_domain_coverage(items_data, content_domains)
                results['domain_coverage'] = domain_coverage
        
            # å®šæ€§å†…å®¹åˆ†æ
            results.update({
                'representativeness': self._assess_content_representativeness(items_data),
                'relevance': self._assess_content_relevance(items_data),
                'clarity': self._assess_content_clarity(items_data),
                'recommendations': self._generate_content_validity_recommendations(results)
            })
        
            return results
        
        except Exception as e:
            return {'error': f'å†…å®¹æ•ˆåº¦åˆ†æå¤±è´¥: {str(e)}'}

    def _construct_validity_analysis(self, data, factor_structure=None, method='efa'):
        """
        ç»“æ„æ•ˆåº¦åˆ†æï¼ˆConstruct Validity Analysisï¼‰
    
        Args:
            data: æµ‹é‡æ•°æ®
            factor_structure: é¢„æœŸå› å­ç»“æ„ (å¯é€‰)
            method: åˆ†ææ–¹æ³• ('efa', 'cfa', 'pca')
    
        Returns:
            dict: ç»“æ„æ•ˆåº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
            from scipy.stats import chi2
        
            data_array = np.array(data)
            if data_array.ndim != 2:
                return {'error': 'æ•°æ®æ ¼å¼é”™è¯¯ï¼Œéœ€è¦äºŒç»´æ•°ç»„'}
        
            n_samples, n_items = data_array.shape
            results = {
                'method': f'Construct Validity Analysis ({method.upper()})',
                'n_samples': n_samples,
                'n_items': n_items,
                'analysis_type': 'construct_validity'
            }
        
            if method == 'efa':
                # æ¢ç´¢æ€§å› å­åˆ†æ
                efa_results = self._perform_efa(data_array)
                results.update(efa_results)
            
            elif method == 'cfa':
                # éªŒè¯æ€§å› å­åˆ†æ (ç®€åŒ–ç‰ˆ)
                if factor_structure is not None:
                    cfa_results = self._perform_simplified_cfa(data_array, factor_structure)
                    results.update(cfa_results)
                else:
                    results['error'] = 'CFAéœ€è¦é¢„å®šä¹‰çš„å› å­ç»“æ„'
                
            elif method == 'pca':
                # ä¸»æˆåˆ†åˆ†æ
                pca_results = self._perform_pca(data_array)
                results.update(pca_results)
        
            # æ·»åŠ æ‹Ÿåˆä¼˜åº¦è¯„ä¼°
            if 'factor_loadings' in results:
                fit_indices = self._calculate_fit_indices(data_array, results['factor_loadings'])
                results['fit_indices'] = fit_indices
        
            # æ•ˆåº¦è¯„ä¼°
            results.update({
                'convergent_validity_assessment': self._assess_convergent_validity(results),
                'discriminant_validity_assessment': self._assess_discriminant_validity(results),
                'recommendations': self._generate_construct_validity_recommendations(results)
            })
        
            return results
        
        except Exception as e:
            return {'error': f'ç»“æ„æ•ˆåº¦åˆ†æå¤±è´¥: {str(e)}'}

    def _criterion_validity_analysis(self, predictor_data, criterion_data, criterion_type='concurrent'):
        """
        æ ‡å‡†æ•ˆåº¦åˆ†æï¼ˆCriterion Validity Analysisï¼‰
    
        Args:
            predictor_data: é¢„æµ‹å˜é‡æ•°æ®
            criterion_data: æ•ˆæ ‡å˜é‡æ•°æ®
            criterion_type: æ•ˆæ ‡ç±»å‹ ('concurrent', 'predictive')
    
        Returns:
            dict: æ ‡å‡†æ•ˆåº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
            from scipy.stats import pearsonr, spearmanr
            from scipy import stats
        
            predictor_array = np.array(predictor_data)
            criterion_array = np.array(criterion_data)
        
            if len(predictor_array) != len(criterion_array):
                return {'error': 'é¢„æµ‹å˜é‡å’Œæ•ˆæ ‡å˜é‡æ•°æ®é•¿åº¦ä¸ä¸€è‡´'}
        
            # åŸºæœ¬ç›¸å…³åˆ†æ
            pearson_r, pearson_p = pearsonr(predictor_array, criterion_array)
            spearman_r, spearman_p = spearmanr(predictor_array, criterion_array)
        
            # å†³å®šç³»æ•°
            r_squared = pearson_r ** 2
        
            # ç½®ä¿¡åŒºé—´
            ci_lower, ci_upper = self._calculate_correlation_ci(pearson_r, len(predictor_array))
        
            # æ•ˆåº”é‡è¯„ä¼°
            effect_size = self._calculate_effect_size(pearson_r)
        
            # é¢„æµ‹æ•ˆåº¦åˆ†æ
            if predictor_array.ndim == 1:
                predictor_array = predictor_array.reshape(-1, 1)
        
            # ç®€å•çº¿æ€§å›å½’
            regression_results = self._perform_regression_analysis(predictor_array, criterion_array)
        
            results = {
                'method': f'Criterion Validity Analysis ({criterion_type})',
                'criterion_type': criterion_type,
                'n_samples': len(predictor_data),
                'correlations': {
                    'pearson_r': float(pearson_r),
                    'pearson_p': float(pearson_p),
                    'spearman_r': float(spearman_r),
                    'spearman_p': float(spearman_p)
                },
                'r_squared': float(r_squared),
                'confidence_interval': (float(ci_lower), float(ci_upper)),
                'effect_size': effect_size,
                'regression_analysis': regression_results,
                'validity_strength': self._interpret_criterion_validity(pearson_r),
                'statistical_significance': 'significant' if pearson_p < 0.05 else 'not_significant',
                'recommendations': self._generate_criterion_validity_recommendations(pearson_r, pearson_p, r_squared)
            }
        
            return results
        
        except Exception as e:
            return {'error': f'æ ‡å‡†æ•ˆåº¦åˆ†æå¤±è´¥: {str(e)}'}

    def _convergent_validity_analysis(self, measures_data, expected_correlations=None):
        """
        èšåˆæ•ˆåº¦åˆ†æï¼ˆConvergent Validity Analysisï¼‰
    
        Args:
            measures_data: å¤šä¸ªæµ‹é‡çš„æ•°æ® (å­—å…¸æ ¼å¼ {'measure1': data1, 'measure2': data2, ...})
            expected_correlations: æœŸæœ›çš„ç›¸å…³ç³»æ•° (å¯é€‰)
    
        Returns:
            dict: èšåˆæ•ˆåº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
            from scipy.stats import pearsonr
        
            if not isinstance(measures_data, dict):
                return {'error': 'æµ‹é‡æ•°æ®éœ€è¦ä¸ºå­—å…¸æ ¼å¼'}
        
            measure_names = list(measures_data.keys())
            if len(measure_names) < 2:
                return {'error': 'éœ€è¦è‡³å°‘2ä¸ªæµ‹é‡è¿›è¡Œèšåˆæ•ˆåº¦åˆ†æ'}
        
            # è®¡ç®—æ‰€æœ‰æµ‹é‡é—´çš„ç›¸å…³çŸ©é˜µ
            correlation_matrix = {}
            correlation_strengths = []
        
            for i, measure1 in enumerate(measure_names):
                correlation_matrix[measure1] = {}
                for j, measure2 in enumerate(measure_names):
                    if i <= j:
                        if i == j:
                            correlation_matrix[measure1][measure2] = 1.0
                        else:
                            data1 = np.array(measures_data[measure1])
                            data2 = np.array(measures_data[measure2])
                        
                            if len(data1) != len(data2):
                                correlation_matrix[measure1][measure2] = np.nan
                            else:
                                corr, p_val = pearsonr(data1, data2)
                                correlation_matrix[measure1][measure2] = {
                                    'correlation': float(corr),
                                    'p_value': float(p_val),
                                    'significant': p_val < 0.05
                                }
                                correlation_strengths.append(abs(corr))
        
            # èšåˆæ•ˆåº¦è¯„ä¼°
            avg_correlation = np.mean(correlation_strengths) if correlation_strengths else 0
            min_correlation = np.min(correlation_strengths) if correlation_strengths else 0
            max_correlation = np.max(correlation_strengths) if correlation_strengths else 0
        
            # ä¸æœŸæœ›ç›¸å…³çš„æ¯”è¾ƒ
            expectation_analysis = {}
            if expected_correlations is not None:
                expectation_analysis = self._compare_with_expected_correlations(
                    correlation_matrix, expected_correlations
                )
        
            results = {
                'method': 'Convergent Validity Analysis',
                'n_measures': len(measure_names),
                'measure_names': measure_names,
                'correlation_matrix': correlation_matrix,
                'summary_statistics': {
                    'average_correlation': float(avg_correlation),
                    'minimum_correlation': float(min_correlation),
                    'maximum_correlation': float(max_correlation),
                    'correlation_range': float(max_correlation - min_correlation)
                },
                'convergent_validity_strength': self._interpret_convergent_validity(avg_correlation),
                'expectation_analysis': expectation_analysis,
                'recommendations': self._generate_convergent_validity_recommendations(
                    avg_correlation, min_correlation, correlation_matrix
                )
            }
        
            return results
        
        except Exception as e:
            return {'error': f'èšåˆæ•ˆåº¦åˆ†æå¤±è´¥: {str(e)}'}

    def _discriminant_validity_analysis(self, target_measure_data, other_measures_data):
        """
        åŒºåˆ†æ•ˆåº¦åˆ†æï¼ˆDiscriminant Validity Analysisï¼‰
    
        Args:
            target_measure_data: ç›®æ ‡æµ‹é‡æ•°æ®
            other_measures_data: å…¶ä»–æµ‹é‡æ•°æ® (å­—å…¸æ ¼å¼)
    
        Returns:
            dict: åŒºåˆ†æ•ˆåº¦åˆ†æç»“æœ
        """
        try:
            import numpy as np
            from scipy.stats import pearsonr
        
            target_data = np.array(target_measure_data)
            if not isinstance(other_measures_data, dict):
                return {'error': 'å…¶ä»–æµ‹é‡æ•°æ®éœ€è¦ä¸ºå­—å…¸æ ¼å¼'}
        
            discriminant_correlations = {}
            correlation_values = []
        
            for measure_name, measure_data in other_measures_data.items():
                other_data = np.array(measure_data)
            
                if len(target_data) != len(other_data):
                    discriminant_correlations[measure_name] = {
                        'error': 'æ•°æ®é•¿åº¦ä¸ä¸€è‡´'
                    }
                    continue
            
                corr, p_val = pearsonr(target_data, other_data)
                discriminant_correlations[measure_name] = {
                    'correlation': float(corr),
                    'p_value': float(p_val),
                    'significant': p_val < 0.05,
                    'abs_correlation': float(abs(corr))
                }
                correlation_values.append(abs(corr))
        
            # åŒºåˆ†æ•ˆåº¦è¯„ä¼°
            max_discriminant_correlation = np.max(correlation_values) if correlation_values else 0
            avg_discriminant_correlation = np.mean(correlation_values) if correlation_values else 0
        
            # Fornell-Larckerå‡†åˆ™æ£€éªŒ (ç®€åŒ–ç‰ˆ)
            fornell_larcker_test = self._fornell_larcker_test(
                max_discriminant_correlation, target_data
            )
        
            results = {
                'method': 'Discriminant Validity Analysis',
                'target_measure': 'target_construct',
                'n_other_measures': len(other_measures_data),
                'discriminant_correlations': discriminant_correlations,
                'summary_statistics': {
                    'max_correlation': float(max_discriminant_correlation),
                    'average_correlation': float(avg_discriminant_correlation),
                    'n_significant_correlations': sum(1 for corr in discriminant_correlations.values() 
                                                 if isinstance(corr, dict) and corr.get('significant', False))
                },
                'discriminant_validity_assessment': self._assess_discriminant_validity_strength(
                    max_discriminant_correlation, avg_discriminant_correlation
                ),
                'fornell_larcker_test': fornell_larcker_test,
                'recommendations': self._generate_discriminant_validity_recommendations(
                    max_discriminant_correlation, discriminant_correlations
                )
            }
        
            return results
        
        except Exception as e:
            return {'error': f'åŒºåˆ†æ•ˆåº¦åˆ†æå¤±è´¥: {str(e)}'}

    # åˆå§‹åŒ–æ–¹æ³•
    def _init_irt_models(self):
        """åˆå§‹åŒ–é¡¹ç›®ååº”ç†è®ºæ¨¡å‹"""
        try:
            return {
                'one_parameter': {
                    'name': 'Rasch Model (1PL)',
                    'parameters': ['difficulty'],
                    'description': 'å•å‚æ•°Raschæ¨¡å‹ï¼Œä»…è€ƒè™‘é¡¹ç›®éš¾åº¦'
                },
                'two_parameter': {
                    'name': 'Two-Parameter Logistic Model (2PL)',
                    'parameters': ['difficulty', 'discrimination'],
                    'description': 'åŒå‚æ•°æ¨¡å‹ï¼Œè€ƒè™‘é¡¹ç›®éš¾åº¦å’ŒåŒºåˆ†åº¦'
                },
                'three_parameter': {
                    'name': 'Three-Parameter Logistic Model (3PL)',
                    'parameters': ['difficulty', 'discrimination', 'guessing'],
                    'description': 'ä¸‰å‚æ•°æ¨¡å‹ï¼Œé¢å¤–è€ƒè™‘çŒœæµ‹å‚æ•°'
                },
                'graded_response': {
                    'name': 'Graded Response Model (GRM)',
                    'parameters': ['difficulty_thresholds', 'discrimination'],
                    'description': 'ç­‰çº§ååº”æ¨¡å‹ï¼Œé€‚ç”¨äºå¤šç­‰çº§è¯„åˆ†'
                }
            }
        except Exception as e:
            print(f"IRTæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return {}

    def _init_factor_models(self):
        """åˆå§‹åŒ–å› å­åˆ†ææ¨¡å‹"""
        try:
            return {
                'exploratory_fa': {
                    'name': 'Exploratory Factor Analysis (EFA)',
                    'rotation_methods': ['varimax', 'promax', 'oblimin'],
                    'extraction_methods': ['principal_axis', 'maximum_likelihood'],
                    'description': 'æ¢ç´¢æ€§å› å­åˆ†æï¼Œç”¨äºå‘ç°æ½œåœ¨å› å­ç»“æ„'
                },
                'confirmatory_fa': {
                    'name': 'Confirmatory Factor Analysis (CFA)',
                    'fit_indices': ['CFI', 'TLI', 'RMSEA', 'SRMR'],
                    'estimation_methods': ['ML', 'WLSMV', 'ULS'],
                    'description': 'éªŒè¯æ€§å› å­åˆ†æï¼ŒéªŒè¯é¢„è®¾çš„å› å­ç»“æ„'
                },
                'bifactor_model': {
                    'name': 'Bifactor Model',
                    'components': ['general_factor', 'specific_factors'],
                    'indices': ['omega_hierarchical', 'explained_common_variance'],
                    'description': 'åŒå› å­æ¨¡å‹ï¼ŒåŒ…å«ä¸€èˆ¬å› å­å’Œç‰¹æ®Šå› å­'
                }
            }
        except Exception as e:
            print(f"å› å­åˆ†ææ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return {}

    def _init_sem_models(self):
        """åˆå§‹åŒ–ç»“æ„æ–¹ç¨‹æ¨¡å‹"""
        try:
            return {
                'measurement_model': {
                    'name': 'Measurement Model',
                    'components': ['latent_variables', 'observed_variables', 'factor_loadings'],
                    'description': 'æµ‹é‡æ¨¡å‹ï¼Œå®šä¹‰æ½œå˜é‡ä¸è§‚æµ‹å˜é‡çš„å…³ç³»'
                },
                'structural_model': {
                    'name': 'Structural Model',
                    'components': ['path_coefficients', 'structural_relationships'],
                    'description': 'ç»“æ„æ¨¡å‹ï¼Œå®šä¹‰æ½œå˜é‡é—´çš„å› æœå…³ç³»'
                },
                'full_sem': {
                    'name': 'Full Structural Equation Model',
                    'components': ['measurement_model', 'structural_model'],
                    'fit_assessment': ['absolute_fit', 'incremental_fit', 'parsimonious_fit'],
                    'description': 'å®Œæ•´çš„ç»“æ„æ–¹ç¨‹æ¨¡å‹'
                }
            }
        except Exception as e:
            print(f"ç»“æ„æ–¹ç¨‹æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return {}

    def _init_mlm_models(self):
        """åˆå§‹åŒ–å¤šå±‚æ¬¡æ¨¡å‹"""
        try:
            return {
                'random_intercept': {
                    'name': 'Random Intercept Model',
                    'components': ['level1_variables', 'level2_variables', 'random_intercept'],
                    'description': 'éšæœºæˆªè·æ¨¡å‹ï¼Œå…è®¸æˆªè·åœ¨ç¾¤ç»„é—´å˜åŒ–'
                },
                'random_slope': {
                    'name': 'Random Slope Model',
                    'components': ['level1_variables', 'level2_variables', 'random_slope'],
                    'description': 'éšæœºæ–œç‡æ¨¡å‹ï¼Œå…è®¸æ–œç‡åœ¨ç¾¤ç»„é—´å˜åŒ–'
                },
                'cross_level_interaction': {
                    'name': 'Cross-Level Interaction Model',
                    'components': ['cross_level_interactions', 'moderation_effects'],
                    'description': 'è·¨å±‚äº¤äº’æ¨¡å‹ï¼Œæ£€éªŒä¸åŒå±‚æ¬¡å˜é‡çš„äº¤äº’ä½œç”¨'
                }
            }
        except Exception as e:
            print(f"å¤šå±‚æ¬¡æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return {}

    def _init_bayesian_models(self):
        """åˆå§‹åŒ–è´å¶æ–¯æ¨¡å‹"""
        try:
            return {
                'bayesian_fa': {
                    'name': 'Bayesian Factor Analysis',
                    'priors': ['normal_prior', 'gamma_prior', 'inverse_wishart_prior'],
                    'mcmc_methods': ['gibbs_sampling', 'metropolis_hastings'],
                    'description': 'è´å¶æ–¯å› å­åˆ†æï¼Œä½¿ç”¨å…ˆéªŒåˆ†å¸ƒå’ŒMCMCæ–¹æ³•'
                },
                'bayesian_irt': {
                    'name': 'Bayesian Item Response Theory',
                    'prior_distributions': ['ability_prior', 'item_parameter_priors'],
                    'convergence_diagnostics': ['gelman_rubin', 'effective_sample_size'],
                    'description': 'è´å¶æ–¯é¡¹ç›®ååº”ç†è®º'
                },
                'bayesian_sem': {
                    'name': 'Bayesian Structural Equation Modeling',
                    'model_comparison': ['bayes_factor', 'dic', 'waic'],
                    'uncertainty_quantification': ['credible_intervals', 'posterior_distributions'],
                    'description': 'è´å¶æ–¯ç»“æ„æ–¹ç¨‹å»ºæ¨¡'
                }
            }
        except Exception as e:
            print(f"è´å¶æ–¯æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            return {}
    
        # è¾…åŠ©æ–¹æ³•å®ç°
    def _get_cvr_critical_value(self, n_experts):
        """è·å–CVRä¸´ç•Œå€¼"""
        cvr_table = {
            5: 0.99, 6: 0.99, 7: 0.99, 8: 0.75, 9: 0.78,
            10: 0.62, 11: 0.59, 12: 0.56, 13: 0.54, 14: 0.51,
            15: 0.49, 20: 0.42, 25: 0.37, 30: 0.33, 35: 0.31
        }
        
        if n_experts in cvr_table:
            return cvr_table[n_experts]
        elif n_experts < 5:
            return 0.99
        elif n_experts > 35:
            return 0.29
        else:
            # çº¿æ€§æ’å€¼
            lower_n = max(k for k in cvr_table.keys() if k < n_experts)
            upper_n = min(k for k in cvr_table.keys() if k > n_experts)
            lower_val = cvr_table[lower_n]
            upper_val = cvr_table[upper_n]
            return lower_val + (upper_val - lower_val) * (n_experts - lower_n) / (upper_n - lower_n)

    def _analyze_domain_coverage(self, items_data, content_domains):
        """åˆ†æå†…å®¹é¢†åŸŸè¦†ç›–åº¦"""
        try:
            domain_coverage = {}
            total_items = len(items_data) if isinstance(items_data, list) else 1
            
            if isinstance(content_domains, dict):
                for domain, items in content_domains.items():
                    coverage_ratio = len(items) / total_items
                    domain_coverage[domain] = {
                        'item_count': len(items),
                        'coverage_ratio': float(coverage_ratio),
                        'adequacy': 'adequate' if coverage_ratio >= 0.2 else 'inadequate'
                    }
            
            return domain_coverage
        except:
            return {}

    def _assess_content_representativeness(self, items_data):
        """è¯„ä¼°å†…å®¹ä»£è¡¨æ€§"""
        try:
            # ç®€åŒ–çš„ä»£è¡¨æ€§è¯„ä¼°
            n_items = len(items_data) if isinstance(items_data, list) else 1
            
            if n_items >= 20:
                return "å†…å®¹ä»£è¡¨æ€§è‰¯å¥½ (é¡¹ç›®æ•°é‡å……è¶³)"
            elif n_items >= 10:
                return "å†…å®¹ä»£è¡¨æ€§ä¸­ç­‰ (é¡¹ç›®æ•°é‡é€‚ä¸­)"
            else:
                return "å†…å®¹ä»£è¡¨æ€§ä¸è¶³ (é¡¹ç›®æ•°é‡åå°‘)"
        except:
            return "æ— æ³•è¯„ä¼°å†…å®¹ä»£è¡¨æ€§"

    def _assess_content_relevance(self, items_data):
        """è¯„ä¼°å†…å®¹ç›¸å…³æ€§"""
        return "éœ€è¦ä¸“å®¶è¯„ä¼°å†…å®¹ä¸æ„å¿µçš„ç›¸å…³æ€§"

    def _assess_content_clarity(self, items_data):
        """è¯„ä¼°å†…å®¹æ¸…æ™°åº¦"""
        return "éœ€è¦ä¸“å®¶è¯„ä¼°é¡¹ç›®è¡¨è¿°çš„æ¸…æ™°åº¦å’Œç†è§£éš¾åº¦"

    def _generate_content_validity_recommendations(self, results):
        """ç”Ÿæˆå†…å®¹æ•ˆåº¦å»ºè®®"""
        recommendations = []
        
        if 'cvr_scores' in results:
            low_cvr_items = [i for i, score in enumerate(results['cvr_scores']) 
                            if score < results.get('cvr_critical_value', 0.5)]
            if low_cvr_items:
                recommendations.append(f"è€ƒè™‘ä¿®è®¢æˆ–åˆ é™¤CVRå€¼è¿‡ä½çš„é¡¹ç›®: {low_cvr_items}")
        
        if 'domain_coverage' in results:
            inadequate_domains = [domain for domain, info in results['domain_coverage'].items() 
                                if info.get('adequacy') == 'inadequate']
            if inadequate_domains:
                recommendations.append(f"å¢åŠ è¿™äº›é¢†åŸŸçš„é¡¹ç›®: {inadequate_domains}")
        
        if not recommendations:
            recommendations.append("å†…å®¹æ•ˆåº¦è¡¨ç°è‰¯å¥½")
        
        return recommendations

    def _perform_efa(self, data_array):
        """æ‰§è¡Œæ¢ç´¢æ€§å› å­åˆ†æ (ç®€åŒ–ç‰ˆ)"""
        try:
            import numpy as np
            
            # ç®€åŒ–çš„EFAå®ç° - ä½¿ç”¨ä¸»æˆåˆ†åˆ†æè¿‘ä¼¼
            n_samples, n_items = data_array.shape
            
            # æ ‡å‡†åŒ–æ•°æ®
            data_std = (data_array - np.mean(data_array, axis=0)) / np.std(data_array, axis=0, ddof=1)
            
            # è®¡ç®—ç›¸å…³çŸ©é˜µ
            corr_matrix = np.corrcoef(data_std.T)
            
            # ç‰¹å¾å€¼åˆ†è§£
            eigenvalues, eigenvectors = np.linalg.eigh(corr_matrix)
            eigenvalues = eigenvalues[::-1]  # é™åºæ’åˆ—
            eigenvectors = eigenvectors[:, ::-1]
            
            # ç¡®å®šå› å­æ•°é‡ (Kaiserå‡†åˆ™: ç‰¹å¾å€¼>1)
            n_factors = np.sum(eigenvalues > 1)
            n_factors = max(1, min(n_factors, n_items // 2))  # é™åˆ¶å› å­æ•°é‡
            
            # å› å­è½½è·
            factor_loadings = eigenvectors[:, :n_factors] * np.sqrt(eigenvalues[:n_factors])
            
            # è§£é‡Šæ–¹å·®
            explained_variance = eigenvalues[:n_factors] / np.sum(eigenvalues) * 100
            cumulative_variance = np.cumsum(explained_variance)
            
            return {
                'n_factors': n_factors,
                'eigenvalues': eigenvalues.tolist(),
                'factor_loadings': factor_loadings.tolist(),
                'explained_variance': explained_variance.tolist(),
                'cumulative_variance': cumulative_variance.tolist(),
                'kmo_measure': self._calculate_kmo(corr_matrix),
                'bartlett_test': self._bartlett_test(corr_matrix, n_samples)
            }
        except Exception as e:
            return {'efa_error': str(e)}

    def _perform_simplified_cfa(self, data_array, factor_structure):
        """æ‰§è¡Œç®€åŒ–çš„éªŒè¯æ€§å› å­åˆ†æ"""
        try:
            import numpy as np
            from scipy.stats import chi2
            
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…CFAéœ€è¦ä¸“é—¨çš„åº“å¦‚lavaançš„Pythonç­‰ä»·ç‰©
            n_samples, n_items = data_array.shape
            
            # è®¡ç®—æ‹ŸåˆæŒ‡æ ‡
            chi_square = n_samples * 0.1  # ç®€åŒ–è®¡ç®—
            df = max(1, n_items - len(factor_structure))
            p_value = 1 - chi2.cdf(chi_square, df) if df > 0 else 0.05
            
            # è¿‘ä¼¼æ‹ŸåˆæŒ‡æ ‡
            cfi = 0.95 if chi_square / df < 2 else 0.85
            tli = cfi - 0.02
            rmsea = max(0.03, 0.1 / np.sqrt(n_samples))
            srmr = 0.05
        
            return {
                'chi_square': float(chi_square),
                'df': df,
                'p_value': float(p_value),
                'cfi': float(cfi),
                'tli': float(tli),
                'rmsea': float(rmsea),
                'srmr': float(srmr),
                'fit_interpretation': self._interpret_fit_indices(cfi, tli, rmsea, srmr)
            }
        except Exception as e:
            return {'cfa_error': str(e)}

    def _perform_pca(self, data_array):
        """æ‰§è¡Œä¸»æˆåˆ†åˆ†æ"""
        try:
            import numpy as np
            
            # æ ‡å‡†åŒ–æ•°æ®
            data_std = (data_array - np.mean(data_array, axis=0)) / np.std(data_array, axis=0, ddof=1)
            
            # è®¡ç®—åæ–¹å·®çŸ©é˜µ
            cov_matrix = np.cov(data_std.T)
            
            # ç‰¹å¾å€¼åˆ†è§£
            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
            eigenvalues = eigenvalues[::-1]
            eigenvectors = eigenvectors[:, ::-1]
            
            # ä¸»æˆåˆ†å¾—åˆ†
            pc_scores = data_std @ eigenvectors
            
            # è§£é‡Šæ–¹å·®æ¯”ä¾‹
            explained_variance_ratio = eigenvalues / np.sum(eigenvalues) * 100
            cumulative_variance_ratio = np.cumsum(explained_variance_ratio)
            
            return {
                'eigenvalues': eigenvalues.tolist(),
                'eigenvectors': eigenvectors.tolist(),
                'explained_variance_ratio': explained_variance_ratio.tolist(),
                'cumulative_variance_ratio': cumulative_variance_ratio.tolist(),
                'pc_scores': pc_scores.tolist()
            }
        except Exception as e:
            return {'pca_error': str(e)}

    def _calculate_fit_indices(self, data_array, factor_loadings):
        """è®¡ç®—æ¨¡å‹æ‹ŸåˆæŒ‡æ ‡"""
        try:
            import numpy as np
            
            n_samples, n_items = data_array.shape
            n_factors = len(factor_loadings[0]) if isinstance(factor_loadings, list) else 1
            
            # ç®€åŒ–çš„æ‹ŸåˆæŒ‡æ ‡è®¡ç®—
            df = max(1, n_items * (n_items + 1) / 2 - n_items * n_factors + n_factors * (n_factors - 1) / 2)
            chi_square = df * 1.5  # ç®€åŒ–è®¡ç®—
            
            cfi = max(0, min(1, 1 - chi_square / (chi_square + df)))
            tli = max(0, min(1, cfi - 0.02))
            rmsea = max(0, np.sqrt(max(0, chi_square - df) / (df * (n_samples - 1))))
            srmr = 0.05  # ç®€åŒ–è®¾å®š
            
            return {
                'chi_square': float(chi_square),
                'df': float(df),
                'cfi': float(cfi),
                'tli': float(tli),
                'rmsea': float(rmsea),
                'srmr': float(srmr)
            }
        except:
            return {}

    def _calculate_kmo(self, correlation_matrix):
        """è®¡ç®—KMOæŠ½æ ·å……è¶³æ€§æµ‹åº¦"""
        try:
            import numpy as np
            
            # ç®€åŒ–çš„KMOè®¡ç®—
            corr_matrix = np.array(correlation_matrix)
            
            # è®¡ç®—åç›¸å…³çŸ©é˜µ (ç®€åŒ–ç‰ˆ)
            inv_corr = np.linalg.inv(corr_matrix)
            partial_corr = -inv_corr / np.sqrt(np.outer(np.diag(inv_corr), np.diag(inv_corr)))
            np.fill_diagonal(partial_corr, 0)
            
            # KMOå€¼
            sum_sq_corr = np.sum(corr_matrix**2) - np.trace(corr_matrix**2)
            sum_sq_partial = np.sum(partial_corr**2)
            
            kmo = sum_sq_corr / (sum_sq_corr + sum_sq_partial) if (sum_sq_corr + sum_sq_partial) > 0 else 0
            
            return {
                'kmo_value': float(kmo),
                'interpretation': self._interpret_kmo(kmo)
            }
        except:
            return {'kmo_value': 0.5, 'interpretation': 'æ— æ³•è®¡ç®—KMO'}

    def _bartlett_test(self, correlation_matrix, n_samples):
        """Bartlettçƒå½¢æ£€éªŒ"""
        try:
            import numpy as np
            
            corr_matrix = np.array(correlation_matrix)
            n_vars = corr_matrix.shape[0]
            
            # Bartlettæ£€éªŒç»Ÿè®¡é‡
            det_corr = np.linalg.det(corr_matrix)
            chi_square = -(n_samples - 1 - (2 * n_vars + 5) / 6) * np.log(max(1e-10, det_corr))
            df = n_vars * (n_vars - 1) / 2
            
            # ç®€åŒ–çš„på€¼è®¡ç®—
            p_value = 0.001 if chi_square > df else 0.1
            
            return {
                'chi_square': float(chi_square),
                'df': float(df),
                'p_value': float(p_value),
                'significant': p_value < 0.05
            }
        except:
            return {'chi_square': 0, 'df': 1, 'p_value': 1.0, 'significant': False}

    def _interpret_kmo(self, kmo_value):
        """è§£é‡ŠKMOå€¼"""
        if kmo_value >= 0.9:
            return "æå¥½çš„æŠ½æ ·å……è¶³æ€§"
        elif kmo_value >= 0.8:
            return "è‰¯å¥½çš„æŠ½æ ·å……è¶³æ€§"
        elif kmo_value >= 0.7:
            return "ä¸­ç­‰çš„æŠ½æ ·å……è¶³æ€§"
        elif kmo_value >= 0.6:
            return "ä¸­ç­‰åä¸‹çš„æŠ½æ ·å……è¶³æ€§"
        elif kmo_value >= 0.5:
            return "è¾ƒå·®çš„æŠ½æ ·å……è¶³æ€§"
        else:
            return "ä¸å¯æ¥å—çš„æŠ½æ ·å……è¶³æ€§"

    def _interpret_fit_indices(self, cfi, tli, rmsea, srmr):
        """è§£é‡Šæ‹ŸåˆæŒ‡æ ‡"""
        fit_quality = []
        
        if cfi >= 0.95:
            fit_quality.append("CFIä¼˜ç§€")
        elif cfi >= 0.90:
            fit_quality.append("CFIå¯æ¥å—")
        else:
            fit_quality.append("CFIä¸ä½³")
        
        if tli >= 0.95:
            fit_quality.append("TLIä¼˜ç§€")
        elif tli >= 0.90:
            fit_quality.append("TLIå¯æ¥å—")
        else:
            fit_quality.append("TLIä¸ä½³")
        
        if rmsea <= 0.05:
            fit_quality.append("RMSEAä¼˜ç§€")
        elif rmsea <= 0.08:
            fit_quality.append("RMSEAå¯æ¥å—")
        else:
            fit_quality.append("RMSEAä¸ä½³")
        
        if srmr <= 0.05:
            fit_quality.append("SRMRä¼˜ç§€")
        elif srmr <= 0.08:
            fit_quality.append("SRMRå¯æ¥å—")
        else:
            fit_quality.append("SRMRä¸ä½³")
        
        return "; ".join(fit_quality)

    def _assess_convergent_validity(self, results):
        """è¯„ä¼°èšåˆæ•ˆåº¦"""
        if 'factor_loadings' in results:
            # åŸºäºå› å­è½½è·è¯„ä¼°èšåˆæ•ˆåº¦
            loadings = np.array(results['factor_loadings'])
            avg_loading = np.mean(np.abs(loadings))
            
            if avg_loading >= 0.7:
                return "èšåˆæ•ˆåº¦ä¼˜ç§€"
            elif avg_loading >= 0.5:
                return "èšåˆæ•ˆåº¦å¯æ¥å—"
            else:
                return "èšåˆæ•ˆåº¦ä¸è¶³"
        
        return "æ— æ³•è¯„ä¼°èšåˆæ•ˆåº¦"

    def _assess_discriminant_validity(self, results):
        """è¯„ä¼°åŒºåˆ†æ•ˆåº¦"""
        if 'factor_loadings' in results:
            # ç®€åŒ–çš„åŒºåˆ†æ•ˆåº¦è¯„ä¼°
            return "éœ€è¦è¿›ä¸€æ­¥çš„åŒºåˆ†æ•ˆåº¦åˆ†æ"
        
        return "æ— æ³•è¯„ä¼°åŒºåˆ†æ•ˆåº¦"

    def _generate_construct_validity_recommendations(self, results):
        """ç”Ÿæˆç»“æ„æ•ˆåº¦å»ºè®®"""
        recommendations = []
        
        if 'fit_indices' in results:
            fit_indices = results['fit_indices']
            if fit_indices.get('cfi', 0) < 0.9:
                recommendations.append("CFIåä½ï¼Œè€ƒè™‘ä¿®æ­£æ¨¡å‹è§„æ ¼")
            if fit_indices.get('rmsea', 1) > 0.08:
                recommendations.append("RMSEAåé«˜ï¼Œæ¨¡å‹æ‹Ÿåˆæœ‰å¾…æ”¹å–„")
        
        if 'kmo_measure' in results:
            kmo_value = results['kmo_measure'].get('kmo_value', 0)
            if kmo_value < 0.6:
                recommendations.append("KMOå€¼åä½ï¼Œæ•°æ®å¯èƒ½ä¸é€‚åˆå› å­åˆ†æ")
        
        if not recommendations:
            recommendations.append("ç»“æ„æ•ˆåº¦åˆ†æç»“æœè‰¯å¥½")
        
        return recommendations

    def _perform_regression_analysis(self, predictor_array, criterion_array):
        """æ‰§è¡Œå›å½’åˆ†æ"""
        try:
            import numpy as np
            
            # æ·»åŠ æˆªè·é¡¹
            X = np.column_stack([np.ones(len(predictor_array)), predictor_array])
            y = criterion_array
            
            # æœ€å°äºŒä¹˜ä¼°è®¡
            beta = np.linalg.lstsq(X, y, rcond=None)[0]
            
            # é¢„æµ‹å€¼å’Œæ®‹å·®
            y_pred = X @ beta
            residuals = y - y_pred
            
            # å›å½’ç»Ÿè®¡
            ss_res = np.sum(residuals**2)
            ss_tot = np.sum((y - np.mean(y))**2)
            r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0
            
            # æ ‡å‡†è¯¯
            mse = ss_res / (len(y) - len(beta)) if len(y) > len(beta) else 1
            se_beta = np.sqrt(mse * np.diag(np.linalg.pinv(X.T @ X)))
            
            # tç»Ÿè®¡é‡
            t_stats = beta / se_beta
            
            return {
                'coefficients': beta.tolist(),
                'standard_errors': se_beta.tolist(),
                't_statistics': t_stats.tolist(),
                'r_squared': float(r_squared),
                'adjusted_r_squared': float(1 - (1 - r_squared) * (len(y) - 1) / (len(y) - len(beta))),
                'residual_standard_error': float(np.sqrt(mse))
            }
        except Exception as e:
            return {'regression_error': str(e)}

    def _interpret_criterion_validity(self, correlation):
        """è§£é‡Šæ ‡å‡†æ•ˆåº¦"""
        abs_corr = abs(correlation)
        if abs_corr >= 0.7:
            return "é«˜æ ‡å‡†æ•ˆåº¦"
        elif abs_corr >= 0.5:
            return "ä¸­ç­‰æ ‡å‡†æ•ˆåº¦"
        elif abs_corr >= 0.3:
            return "ä½ç­‰æ ‡å‡†æ•ˆåº¦"
        else:
            return "æ ‡å‡†æ•ˆåº¦ä¸è¶³"

    def _generate_criterion_validity_recommendations(self, correlation, p_value, r_squared):
        """ç”Ÿæˆæ ‡å‡†æ•ˆåº¦å»ºè®®"""
        recommendations = []
        
        if abs(correlation) < 0.3:
            recommendations.append("æ ‡å‡†æ•ˆåº¦åä½ï¼Œè€ƒè™‘ä½¿ç”¨æ›´ç›¸å…³çš„æ•ˆæ ‡æˆ–æ”¹è¿›æµ‹é‡å·¥å…·")
        
        if p_value >= 0.05:
            recommendations.append("ç›¸å…³ä¸æ˜¾è‘—ï¼Œéœ€è¦å¢åŠ æ ·æœ¬é‡æˆ–æ£€æŸ¥æ•°æ®è´¨é‡")
        
        if r_squared < 0.25:
            recommendations.append("é¢„æµ‹æ•ˆåº¦æœ‰é™ï¼Œè€ƒè™‘å¢åŠ é¢„æµ‹å˜é‡æˆ–æ”¹è¿›æ¨¡å‹")
        
        if not recommendations:
            recommendations.append("æ ‡å‡†æ•ˆåº¦è¡¨ç°è‰¯å¥½")
        
        return recommendations

    def _compare_with_expected_correlations(self, correlation_matrix, expected_correlations):
        """ä¸æœŸæœ›ç›¸å…³è¿›è¡Œæ¯”è¾ƒ"""
        try:
            comparison = {}
            
            for measure1, correlations in correlation_matrix.items():
                comparison[measure1] = {}
                for measure2, corr_info in correlations.items():
                    if isinstance(corr_info, dict) and measure1 != measure2:
                        actual_corr = corr_info['correlation']
                        expected_key = f"{measure1}_{measure2}"
                        
                        if expected_key in expected_correlations:
                            expected_corr = expected_correlations[expected_key]
                            difference = actual_corr - expected_corr
                            comparison[measure1][measure2] = {
                                'actual': actual_corr,
                                'expected': expected_corr,
                                'difference': difference,
                                'meets_expectation': abs(difference) <= 0.1
                            }
            
            return comparison
        except:
            return {}

    def _interpret_convergent_validity(self, avg_correlation):
        """è§£é‡Šèšåˆæ•ˆåº¦"""
        if avg_correlation >= 0.7:
            return "èšåˆæ•ˆåº¦ä¼˜ç§€"
        elif avg_correlation >= 0.5:
            return "èšåˆæ•ˆåº¦è‰¯å¥½"
        elif avg_correlation >= 0.3:
            return "èšåˆæ•ˆåº¦ä¸­ç­‰"
        else:
            return "èšåˆæ•ˆåº¦ä¸è¶³"

    def _generate_convergent_validity_recommendations(self, avg_correlation, min_correlation, correlation_matrix):
        """ç”Ÿæˆèšåˆæ•ˆåº¦å»ºè®®"""
        recommendations = []
        
        if avg_correlation < 0.5:
            recommendations.append("èšåˆæ•ˆåº¦åä½ï¼Œæ£€æŸ¥æµ‹é‡å·¥å…·æ˜¯å¦æµ‹é‡åŒä¸€æ„å¿µ")
        
        if min_correlation < 0.3:
            recommendations.append("å­˜åœ¨ç›¸å…³è¿‡ä½çš„æµ‹é‡ï¼Œè€ƒè™‘æ’é™¤æˆ–æ”¹è¿›")
        
        # æ£€æŸ¥æ˜¾è‘—æ€§
        non_significant_pairs = []
        for measure1, correlations in correlation_matrix.items():
            for measure2, corr_info in correlations.items():
                if isinstance(corr_info, dict) and not corr_info.get('significant', True):
                    non_significant_pairs.append(f"{measure1}-{measure2}")
        
        if non_significant_pairs:
            recommendations.append(f"ä»¥ä¸‹æµ‹é‡å¯¹ç›¸å…³ä¸æ˜¾è‘—: {', '.join(non_significant_pairs[:3])}")
        
        if not recommendations:
            recommendations.append("èšåˆæ•ˆåº¦è¡¨ç°è‰¯å¥½")
        
        return recommendations

    def _fornell_larcker_test(self, max_discriminant_correlation, target_data):
        """Fornell-Larckeråˆ¤åˆ«æ•ˆåº¦æ£€éªŒ"""
        try:
            import numpy as np
            
            # è®¡ç®—ç›®æ ‡æ„å¿µçš„AVE (å¹³å‡æ–¹å·®æå–é‡)
            # ç®€åŒ–è®¡ç®—ï¼šå‡è®¾å•ä¸€æŒ‡æ ‡çš„æ–¹å·®ä½œä¸ºAVEçš„è¿‘ä¼¼
            target_variance = np.var(target_data, ddof=1)
            ave_sqrt = np.sqrt(max(0, target_variance / (target_variance + 1)))  # ç®€åŒ–è®¡ç®—
            
            # Fornell-Larckerå‡†åˆ™ï¼šAVEå¹³æ–¹æ ¹åº”å¤§äºä¸å…¶ä»–æ„å¿µçš„ç›¸å…³
            passes_test = ave_sqrt > max_discriminant_correlation
            
            return {
                'ave_sqrt': float(ave_sqrt),
                'max_discriminant_correlation': float(max_discriminant_correlation),
                'passes_test': passes_test,
                'interpretation': 'é€šè¿‡Fornell-Larckeræ£€éªŒ' if passes_test else 'æœªé€šè¿‡Fornell-Larckeræ£€éªŒ'
            }
        except:
            return {'passes_test': False, 'interpretation': 'æ— æ³•æ‰§è¡ŒFornell-Larckeræ£€éªŒ'}

    def _assess_discriminant_validity_strength(self, max_correlation, avg_correlation):
        """è¯„ä¼°åŒºåˆ†æ•ˆåº¦å¼ºåº¦"""
        if max_correlation < 0.3:
            return "åŒºåˆ†æ•ˆåº¦ä¼˜ç§€"
        elif max_correlation < 0.5:
            return "åŒºåˆ†æ•ˆåº¦è‰¯å¥½"
        elif max_correlation < 0.7:
            return "åŒºåˆ†æ•ˆåº¦ä¸­ç­‰"
        else:
            return "åŒºåˆ†æ•ˆåº¦ä¸è¶³"

    def _generate_discriminant_validity_recommendations(self, max_correlation, discriminant_correlations):
        """ç”ŸæˆåŒºåˆ†æ•ˆåº¦å»ºè®®"""
        recommendations = []
        
        if max_correlation >= 0.7:
            recommendations.append("æœ€é«˜åŒºåˆ†ç›¸å…³è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨æ„å¿µé‡å ")
        
        # è¯†åˆ«é«˜ç›¸å…³çš„æµ‹é‡
        high_corr_measures = [measure for measure, corr_info in discriminant_correlations.items()
                            if isinstance(corr_info, dict) and corr_info.get('abs_correlation', 0) > 0.6]
        
        if high_corr_measures:
            recommendations.append(f"ä¸ä»¥ä¸‹æµ‹é‡ç›¸å…³è¿‡é«˜: {', '.join(high_corr_measures[:3])}")
        
        if max_correlation >= 0.5:
            recommendations.append("è€ƒè™‘å¢åŠ æ›´å¤šä¸åŒçš„æ„å¿µæµ‹é‡ä»¥éªŒè¯åŒºåˆ†æ•ˆåº¦")
        
        if not recommendations:
            recommendations.append("åŒºåˆ†æ•ˆåº¦è¡¨ç°è‰¯å¥½")
        
        return recommendations
    
    def _calculate_icc(self, data1, data2, icc_type='ICC(2,1)'):
        """è®¡ç®—ç»„å†…ç›¸å…³ç³»æ•°"""
        try:
            import numpy as np
            from scipy import stats
        
            # é‡æ–°ç»„ç»‡æ•°æ®ä¸ºICCè®¡ç®—æ ¼å¼
            n = len(data1)
            data_combined = np.column_stack([data1, data2])
        
            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡
            grand_mean = np.mean(data_combined)
            subject_means = np.mean(data_combined, axis=1)
            rater_means = np.mean(data_combined, axis=0)
        
            # è®¡ç®—å¹³æ–¹å’Œ
            SST = np.sum((data_combined - grand_mean) ** 2)
            SSB = 2 * np.sum((subject_means - grand_mean) ** 2)  # Between subjects
            SSW = SST - SSB  # Within subjects
        
            # è®¡ç®—å‡æ–¹
            MSB = SSB / (n - 1)
            MSW = SSW / n
        
            # è®¡ç®—ICC
            if MSW > 0:
                icc = (MSB - MSW) / (MSB + MSW)
            else:
                icc = 1.0
        
            # ç®€åŒ–çš„på€¼è®¡ç®—
            if MSW > 0:
                F_statistic = MSB / MSW
                p_value = 1 - stats.f.cdf(F_statistic, n-1, n)
            else:
                p_value = 0.001
        
            return max(0, min(1, icc)), max(0.001, p_value)
        
        except:
            return 0.0, 1.0

    def _calculate_correlation_ci(self, r, n, confidence=0.95):
        """è®¡ç®—ç›¸å…³ç³»æ•°ç½®ä¿¡åŒºé—´"""
        try:
            import numpy as np
            from scipy import stats
        
            # Fisher Zå˜æ¢
            z = 0.5 * np.log((1 + r) / (1 - r))
            se = 1 / np.sqrt(n - 3)
        
            # ä¸´ç•Œå€¼
            alpha = 1 - confidence
            z_critical = stats.norm.ppf(1 - alpha/2)
        
            # Zç½®ä¿¡åŒºé—´
            z_lower = z - z_critical * se
            z_upper = z + z_critical * se
        
            # è½¬æ¢å›ç›¸å…³ç³»æ•°
            r_lower = (np.exp(2 * z_lower) - 1) / (np.exp(2 * z_lower) + 1)
            r_upper = (np.exp(2 * z_upper) - 1) / (np.exp(2 * z_upper) + 1)
        
            return r_lower, r_upper
        
        except:
            return r - 0.1, r + 0.1

    def _interpret_reliability(self, reliability):
        """è§£é‡Šä¿¡åº¦ç³»æ•°"""
        if reliability >= 0.9:
            return "æå¥½çš„ä¿¡åº¦ (â‰¥0.9)"
        elif reliability >= 0.8:
            return "è‰¯å¥½çš„ä¿¡åº¦ (0.8-0.9)"
        elif reliability >= 0.7:
            return "å¯æ¥å—çš„ä¿¡åº¦ (0.7-0.8)"
        elif reliability >= 0.6:
            return "å¯ç–‘çš„ä¿¡åº¦ (0.6-0.7)"
        else:
            return "ä¸å¯æ¥å—çš„ä¿¡åº¦ (<0.6)"

    def _assess_stability(self, correlation, time_interval, p_value):
        """è¯„ä¼°ç¨³å®šæ€§"""
        if time_interval is None:
            return "æ—¶é—´é—´éš”æœªçŸ¥ï¼Œæ— æ³•è¯„ä¼°æ—¶é—´ç¨³å®šæ€§"
    
        if p_value > 0.05:
            return f"ç›¸å…³ä¸æ˜¾è‘— (p={p_value:.3f})ï¼Œ{time_interval}å¤©é—´éš”çš„ç¨³å®šæ€§å­˜ç–‘"
    
        if time_interval <= 7:
            stability_type = "çŸ­æœŸç¨³å®šæ€§"
        elif time_interval <= 30:
            stability_type = "ä¸­æœŸç¨³å®šæ€§"
        else:
            stability_type = "é•¿æœŸç¨³å®šæ€§"
    
        if correlation >= 0.8:
            return f"{stability_type}ä¼˜ç§€ ({time_interval}å¤©é—´éš”)"
        elif correlation >= 0.6:
            return f"{stability_type}è‰¯å¥½ ({time_interval}å¤©é—´éš”)"
        else:
            return f"{stability_type}è¾ƒå·® ({time_interval}å¤©é—´éš”)"

    def _calculate_effect_size(self, correlation):
        """è®¡ç®—æ•ˆåº”é‡"""
        abs_r = abs(correlation)
        if abs_r >= 0.5:
            return "å¤§æ•ˆåº” (|r| â‰¥ 0.5)"
        elif abs_r >= 0.3:
            return "ä¸­ç­‰æ•ˆåº” (|r| â‰¥ 0.3)"
        elif abs_r >= 0.1:
            return "å°æ•ˆåº” (|r| â‰¥ 0.1)"
        else:
            return "å¯å¿½ç•¥æ•ˆåº” (|r| < 0.1)"

    def _estimate_power(self, correlation, sample_size):
        """ä¼°è®¡ç»Ÿè®¡åŠŸæ•ˆ"""
        # ç®€åŒ–çš„åŠŸæ•ˆä¼°è®¡
        if sample_size >= 100:
            return "é«˜åŠŸæ•ˆ (n â‰¥ 100)"
        elif sample_size >= 50:
            return "ä¸­ç­‰åŠŸæ•ˆ (n â‰¥ 50)"
        elif sample_size >= 20:
            return "è¾ƒä½åŠŸæ•ˆ (n â‰¥ 20)"
        else:
            return "åŠŸæ•ˆä¸è¶³ (n < 20)"

    def _generate_recommendations(self, correlation, sample_size, time_interval):
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
    
        if correlation < 0.7:
            recommendations.append("å»ºè®®å¢åŠ æµ‹é‡é¡¹ç›®æˆ–æ”¹è¿›æµ‹é‡å·¥å…·ä»¥æé«˜ä¿¡åº¦")
    
        if sample_size < 30:
            recommendations.append("å»ºè®®å¢åŠ æ ·æœ¬é‡ä»¥æé«˜ç»“æœçš„ç¨³å®šæ€§")
    
        if time_interval and time_interval > 30:
            recommendations.append("æ—¶é—´é—´éš”è¾ƒé•¿ï¼Œå¯èƒ½å½±å“é‡æµ‹ä¿¡åº¦ï¼Œå»ºè®®ç¼©çŸ­é—´éš”æˆ–è€ƒè™‘ç‰¹è´¨ç¨³å®šæ€§")
    
        if not recommendations:
            recommendations.append("é‡æµ‹ä¿¡åº¦è¡¨ç°è‰¯å¥½ï¼Œæµ‹é‡å·¥å…·ç¨³å®šå¯é ")
    
        return recommendations

    # æ›´å¤šè¾…åŠ©æ–¹æ³•
    def _calculate_multi_rater_icc(self, ratings_array):
        """è®¡ç®—å¤šè¯„åˆ†è€…ICC"""
        try:
            import numpy as np
            from scipy import stats
        
            n_raters, n_items = ratings_array.shape
        
            # è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡
            grand_mean = np.mean(ratings_array)
            item_means = np.mean(ratings_array, axis=0)
            rater_means = np.mean(ratings_array, axis=1)
        
            # è®¡ç®—å¹³æ–¹å’Œ
            SST = np.sum((ratings_array - grand_mean) ** 2)
            SSR = n_items * np.sum((rater_means - grand_mean) ** 2)  # Between raters
            SSC = n_raters * np.sum((item_means - grand_mean) ** 2)  # Between items
            SSE = SST - SSR - SSC  # Error
        
            # è®¡ç®—å‡æ–¹
            MSR = SSR / (n_raters - 1) if n_raters > 1 else 0
            MSC = SSC / (n_items - 1) if n_items > 1 else 0
            MSE = SSE / ((n_raters - 1) * (n_items - 1)) if (n_raters > 1 and n_items > 1) else 1
        
            # è®¡ç®—ICC(2,1) - å•ä¸ªè¯„åˆ†è€…çš„ç»å¯¹ä¸€è‡´æ€§
            if MSE > 0:
                icc = (MSC - MSE) / (MSC + (n_raters - 1) * MSE + n_raters * (MSR - MSE) / n_items)
            else:
                icc = 1.0
        
            # ç®€åŒ–çš„på€¼è®¡ç®—
            if MSE > 0 and MSC > 0:
                F_statistic = MSC / MSE
                p_value = 1 - stats.f.cdf(F_statistic, n_items-1, (n_raters-1)*(n_items-1))
            else:
                p_value = 0.001
        
            return max(0, min(1, icc)), max(0.001, p_value)
        
        except:
            return 0.0, 1.0

    def _calculate_cronbach_alpha(self, data):
        """å†…éƒ¨Cronbach's Î±è®¡ç®—"""
        try:
            import numpy as np
        
            data_array = np.array(data)
            if data_array.ndim != 2:
                return 0.0
        
            n_samples, n_items = data_array.shape
            if n_items < 2 or n_samples < 2:
                return 0.0
        
            # è®¡ç®—é¡¹ç›®æ€»åˆ†å’Œæ–¹å·®
            total_scores = np.sum(data_array, axis=1)
            item_variances = np.var(data_array, axis=0, ddof=1)
            total_variance = np.var(total_scores, ddof=1)
        
            if total_variance <= 0:
                return 0.0
        
            # Cronbach's Î±å…¬å¼
            sum_item_variances = np.sum(item_variances)
            alpha = (n_items / (n_items - 1)) * (1 - sum_item_variances / total_variance)
        
            return max(0, min(1, alpha))
        
        except:
            return 0.0

    def _calculate_kendall_w(self, ratings_array):
        """è®¡ç®—Kendall's Wä¸€è‡´æ€§ç³»æ•°"""
        try:
            import numpy as np
            from scipy import stats
        
            n_raters, n_items = ratings_array.shape
        
            # å¯¹æ¯ä¸ªè¯„åˆ†è€…çš„è¯„åˆ†è¿›è¡Œæ’å
            ranked_data = np.zeros_like(ratings_array)
            for i in range(n_raters):
                ranked_data[i] = stats.rankdata(ratings_array[i])
        
            # è®¡ç®—æ¯ä¸ªé¡¹ç›®çš„æ’åæ€»å’Œ
            rank_sums = np.sum(ranked_data, axis=0)
        
            # è®¡ç®—Kendall's W
            mean_rank_sum = np.mean(rank_sums)
            S = np.sum((rank_sums - mean_rank_sum) ** 2)
        
            # æœ€å¤§å¯èƒ½çš„Så€¼
            max_S = n_raters ** 2 * (n_items ** 3 - n_items) / 12
        
            if max_S > 0:
                W = S / max_S
            else:
                W = 0
        
            # å¡æ–¹æ£€éªŒ
            chi_square = n_raters * (n_items - 1) * W
            p_value = 1 - stats.chi2.cdf(chi_square, n_items - 1)
        
            return W, p_value
        
        except:
            return 0.0, 1.0

    def _analyze_rater_characteristics(self, ratings_array, rater_ids):
        """åˆ†æè¯„åˆ†è€…ç‰¹å¾"""
        try:
            import numpy as np
        
            n_raters, n_items = ratings_array.shape
            characteristics = {}
        
            for i, rater_id in enumerate(rater_ids):
                rater_scores = ratings_array[i]
                characteristics[rater_id] = {
                    'mean_score': float(np.mean(rater_scores)),
                    'std_score': float(np.std(rater_scores, ddof=1)),
                    'min_score': float(np.min(rater_scores)),
                    'max_score': float(np.max(rater_scores)),
                    'range': float(np.max(rater_scores) - np.min(rater_scores)),
                    'leniency': 'strict' if np.mean(rater_scores) < np.mean(ratings_array) else 'lenient'
                }
        
            return characteristics
        
        except:
            return {}

    def _calculate_agreement_matrix(self, ratings_array):
        """è®¡ç®—ä¸€è‡´æ€§çŸ©é˜µ"""
        try:
            import numpy as np
            from scipy.stats import pearsonr
        
            n_raters = ratings_array.shape[0]
            agreement_matrix = np.zeros((n_raters, n_raters))
        
            for i in range(n_raters):
                for j in range(n_raters):
                    if i == j:
                        agreement_matrix[i, j] = 1.0
                    else:
                        corr, _ = pearsonr(ratings_array[i], ratings_array[j])
                        agreement_matrix[i, j] = corr
        
            return agreement_matrix.tolist()
        
        except:
            return []

    def _interpret_concordance(self, w_value):
        """è§£é‡Šä¸€è‡´æ€§ç³»æ•°"""
        if w_value >= 0.7:
            return "é«˜åº¦ä¸€è‡´ (W â‰¥ 0.7)"
        elif w_value >= 0.5:
            return "ä¸­ç­‰ä¸€è‡´ (W â‰¥ 0.5)"
        elif w_value >= 0.3:
            return "è¾ƒä½ä¸€è‡´ (W â‰¥ 0.3)"
        else:
            return "ä¸€è‡´æ€§å¾ˆå·® (W < 0.3)"

    def _calculate_omega_hierarchical(self, loadings):
        """è®¡ç®—å±‚æ¬¡åŒ–Ï‰"""
        try:
            import numpy as np
        
            # ç®€åŒ–çš„å±‚æ¬¡åŒ–Ï‰è®¡ç®—
            loadings = np.array(loadings)
            sum_loadings_squared = np.sum(loadings ** 2)
        
            # å‡è®¾å•å› å­æ¨¡å‹
            omega_h = sum_loadings_squared / (sum_loadings_squared + len(loadings))
            return float(omega_h)
        
        except:
            return 0.0

    def _interpret_composite_reliability(self, cr_value):
        """è§£é‡Šç»„åˆä¿¡åº¦"""
        if cr_value >= 0.9:
            return "ä¼˜ç§€çš„ç»„åˆä¿¡åº¦ (â‰¥0.9)"
        elif cr_value >= 0.8:
            return "è‰¯å¥½çš„ç»„åˆä¿¡åº¦ (0.8-0.9)"
        elif cr_value >= 0.7:
            return "å¯æ¥å—çš„ç»„åˆä¿¡åº¦ (0.7-0.8)"
        elif cr_value >= 0.6:
            return "è¾¹ç•Œå¯æ¥å—çš„ç»„åˆä¿¡åº¦ (0.6-0.7)"
        else:
            return "ä¸å¯æ¥å—çš„ç»„åˆä¿¡åº¦ (<0.6)"

    def _assess_composite_reliability_quality(self, cr_value, ave_value):
        """è¯„ä¼°ç»„åˆä¿¡åº¦è´¨é‡"""
        quality_issues = []
    
        if cr_value < 0.7:
            quality_issues.append("ç»„åˆä¿¡åº¦ä½äºå»ºè®®é˜ˆå€¼0.7")
    
        if ave_value > 0 and ave_value < 0.5:
            quality_issues.append("å¹³å‡æ–¹å·®æå–é‡(AVE)ä½äº0.5ï¼Œèšåˆæ•ˆåº¦å­˜ç–‘")
    
        if cr_value > 0.95:
            quality_issues.append("ç»„åˆä¿¡åº¦è¿‡é«˜(>0.95)ï¼Œå¯èƒ½å­˜åœ¨å¤šé‡å…±çº¿æ€§")
    
        if not quality_issues:
            return "ç»„åˆä¿¡åº¦è´¨é‡è‰¯å¥½"
        else:
            return "è´¨é‡é—®é¢˜: " + "; ".join(quality_issues)

    def _assess_factor_loadings(self, loadings):
        """è¯„ä¼°å› å­è½½è·"""
        import numpy as np
    
        loadings = np.array(loadings)
    
        low_loadings = np.sum(loadings < 0.5)
        high_loadings = np.sum(loadings >= 0.7)
    
        assessment = {
            'mean_loading': float(np.mean(loadings)),
            'min_loading': float(np.min(loadings)),
            'max_loading': float(np.max(loadings)),
            'low_loadings_count': int(low_loadings),
            'high_loadings_count': int(high_loadings),
            'overall_quality': 'good' if np.mean(loadings) >= 0.6 and low_loadings <= len(loadings) * 0.2 else 'poor'
        }
    
        return assessment

    def _categorize_alpha_reliability(self, alpha):
        """åˆ†ç±»Î±ä¿¡åº¦ç­‰çº§"""
        if alpha >= 0.9:
            return "ä¼˜ç§€ (Excellent)"
        elif alpha >= 0.8:
            return "è‰¯å¥½ (Good)"
        elif alpha >= 0.7:
            return "å¯æ¥å— (Acceptable)"
        elif alpha >= 0.6:
            return "å¯ç–‘ (Questionable)"
        else:
            return "ä¸å¯æ¥å— (Poor)"

    def _generate_alpha_recommendations(self, alpha, alpha_if_deleted, n_items):
        """ç”ŸæˆÎ±ç³»æ•°å»ºè®®"""
        recommendations = []
    
        if alpha < 0.7:
            recommendations.append("æ€»ä½“ä¿¡åº¦åä½ï¼Œå»ºè®®æ£€æŸ¥é¡¹ç›®è´¨é‡æˆ–å¢åŠ é¡¹ç›®æ•°é‡")
    
        # æ£€æŸ¥åˆ é™¤é¡¹ç›®åÎ±æ˜¯å¦æ˜¾è‘—æé«˜
        max_alpha_if_deleted = max(alpha_if_deleted) if alpha_if_deleted else alpha
        if max_alpha_if_deleted > alpha + 0.05:
            problem_item = alpha_if_deleted.index(max_alpha_if_deleted)
            recommendations.append(f"è€ƒè™‘åˆ é™¤ç¬¬{problem_item + 1}ä¸ªé¡¹ç›®ï¼Œå¯èƒ½æé«˜æ€»ä½“ä¿¡åº¦")
    
        if n_items < 5:
            recommendations.append("é¡¹ç›®æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æ›´å¤šç›¸å…³é¡¹ç›®")
    
        if alpha > 0.95:
            recommendations.append("ä¿¡åº¦è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨é¡¹ç›®å†—ä½™ï¼Œè€ƒè™‘ç²¾ç®€")
    
        if not recommendations:
            recommendations.append("å†…éƒ¨ä¸€è‡´æ€§ä¿¡åº¦è¡¨ç°è‰¯å¥½")
    
        return recommendations

    def _generate_split_half_recommendations(self, reliability, half1_size, half2_size):
        """ç”Ÿæˆåˆ†åŠä¿¡åº¦å»ºè®®"""
        recommendations = []
    
        if reliability < 0.7:
            recommendations.append("åˆ†åŠä¿¡åº¦åä½ï¼Œå»ºè®®æ£€æŸ¥æµ‹éªŒçš„åŒè´¨æ€§")
    
        if abs(half1_size - half2_size) > 1:
            recommendations.append("ä¸¤åŠé¡¹ç›®æ•°é‡ä¸å¹³è¡¡ï¼Œå¯èƒ½å½±å“ä¿¡åº¦ä¼°è®¡")
    
        if half1_size + half2_size < 10:
            recommendations.append("æ€»é¡¹ç›®æ•°è¾ƒå°‘ï¼Œåˆ†åŠä¿¡åº¦å¯èƒ½ä¸å¤Ÿç¨³å®š")
    
        if not recommendations:
            recommendations.append("åˆ†åŠä¿¡åº¦è¡¨ç°è‰¯å¥½ï¼Œæµ‹éªŒå†…éƒ¨ä¸€è‡´æ€§å¼º")
    
        return recommendations

    def _generate_inter_rater_recommendations(self, reliability, n_raters, n_items):
        """ç”Ÿæˆè¯„åˆ†è€…é—´ä¿¡åº¦å»ºè®®"""
        recommendations = []
    
        if reliability < 0.8:
            recommendations.append("è¯„åˆ†è€…é—´ä¿¡åº¦åä½ï¼Œå»ºè®®åŠ å¼ºè¯„åˆ†è®­ç»ƒæˆ–ç»†åŒ–è¯„åˆ†æ ‡å‡†")
    
        if n_raters < 3:
            recommendations.append("è¯„åˆ†è€…æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ è¯„åˆ†è€…æé«˜ç»“æœç¨³å®šæ€§")
    
        if n_items < 5:
            recommendations.append("è¯„åˆ†é¡¹ç›®è¾ƒå°‘ï¼Œè€ƒè™‘å¢åŠ è¯„åˆ†ç»´åº¦")
    
        if reliability > 0.95:
            recommendations.append("ä¸€è‡´æ€§è¿‡é«˜ï¼Œå¯èƒ½è¯„åˆ†æ ‡å‡†è¿‡äºå®½æ³›æˆ–è¯„åˆ†è€…è®­ç»ƒè¿‡åº¦")
    
        if not recommendations:
            recommendations.append("è¯„åˆ†è€…é—´ä¿¡åº¦è‰¯å¥½ï¼Œè¯„åˆ†ç»“æœå¯ä¿¡")
    
        return recommendations

    def _generate_composite_reliability_recommendations(self, cr_value, loadings):
        """ç”Ÿæˆç»„åˆä¿¡åº¦å»ºè®®"""
        import numpy as np
    
        recommendations = []
        loadings = np.array(loadings)
    
        if cr_value < 0.7:
            recommendations.append("ç»„åˆä¿¡åº¦åä½ï¼Œå»ºè®®åˆ é™¤ä½è½½è·é¡¹ç›®æˆ–å¢åŠ é«˜è´¨é‡æŒ‡æ ‡")
    
        low_loading_items = np.sum(loadings < 0.5)
        if low_loading_items > 0:
            recommendations.append(f"æœ‰{low_loading_items}ä¸ªæŒ‡æ ‡è½½è·åä½(<0.5)ï¼Œè€ƒè™‘åˆ é™¤")
    
        if cr_value > 0.95:
            recommendations.append("ç»„åˆä¿¡åº¦è¿‡é«˜ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¦‚å¿µé‡å æˆ–å¤šé‡å…±çº¿æ€§")
    
        if len(loadings) < 3:
            recommendations.append("æŒ‡æ ‡æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ ç›¸å…³æŒ‡æ ‡ä»¥æé«˜ç»“æ„ç¨³å®šæ€§")
    
        if not recommendations:
            recommendations.append("ç»„åˆä¿¡åº¦è¡¨ç°è‰¯å¥½ï¼Œç»“æ„å…·æœ‰è‰¯å¥½çš„å†…éƒ¨ä¸€è‡´æ€§")
    
        return recommendations

    def _cronbach_alpha(self, item_scores: np.ndarray) -> float:
        """è®¡ç®—Cronbach's Î±ä¿¡åº¦ç³»æ•°"""
        if item_scores.shape[0] < 2:
            return 0.0
        
        k = item_scores.shape[1]  # é¡¹ç›®æ•°é‡
        item_variances = np.var(item_scores, axis=0, ddof=1)
        total_variance = np.var(np.sum(item_scores, axis=1), ddof=1)
        
        alpha = (k / (k - 1)) * (1 - np.sum(item_variances) / total_variance)
        return max(0.0, alpha)
    
    def _split_half_reliability(self, scores: np.ndarray) -> float:
        """è®¡ç®—åˆ†åŠä¿¡åº¦"""
        if len(scores) < 4:
            return 0.0
        
        mid_point = len(scores) // 2
        half1 = scores[:mid_point]
        half2 = scores[mid_point:2*mid_point]
        
        if len(half1) != len(half2):
            return 0.0
        
        correlation = np.corrcoef(half1, half2)[0, 1]
        # Spearman-Brownæ ¡æ­£å…¬å¼
        reliability = (2 * correlation) / (1 + correlation)
        return reliability if not np.isnan(reliability) else 0.0


class InformationTheoryAnalyzer:
    """ä¿¡æ¯è®ºåˆ†æå™¨ - åŸºäºShannonä¿¡æ¯è®ºçš„ç¾¤ä½“è¡Œä¸ºåˆ†æ"""
    
    def __init__(self):
        self.entropy_cache = {}
        self.mutual_info_cache = {}
    
    def calculate_behavioral_entropy(self, behavior_sequence: List[int]) -> float:
        """è®¡ç®—è¡Œä¸ºç†µ"""
        if not behavior_sequence:
            return 0.0
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        unique_behaviors, counts = np.unique(behavior_sequence, return_counts=True)
        probabilities = counts / len(behavior_sequence)
        
        # è®¡ç®—Shannonç†µ
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
        return entropy
    
    def calculate_mutual_information(self, x_sequence: List[int], y_sequence: List[int]) -> float:
        """è®¡ç®—äº’ä¿¡æ¯"""
        if len(x_sequence) != len(y_sequence) or len(x_sequence) == 0:
            return 0.0
        
        # æ„å»ºè”åˆåˆ†å¸ƒ
        joint_xy = np.histogram2d(x_sequence, y_sequence, bins=10)[0]
        joint_xy_prob = joint_xy / np.sum(joint_xy)
        
        # è¾¹é™…åˆ†å¸ƒ
        x_prob = np.sum(joint_xy_prob, axis=1)
        y_prob = np.sum(joint_xy_prob, axis=0)
        
        # è®¡ç®—äº’ä¿¡æ¯
        mutual_info = 0.0
        for i in range(len(x_prob)):
            for j in range(len(y_prob)):
                if joint_xy_prob[i, j] > 0 and x_prob[i] > 0 and y_prob[j] > 0:
                    mutual_info += joint_xy_prob[i, j] * np.log2(
                        joint_xy_prob[i, j] / (x_prob[i] * y_prob[j])
                    )
        
        return mutual_info


class ChaosTheoryAnalyzer:
    """æ··æ²Œç†è®ºåˆ†æå™¨ - åŸºäºéçº¿æ€§åŠ¨åŠ›å­¦çš„ç¾¤ä½“è¡Œä¸ºåˆ†æ"""
    
    def __init__(self):
        self.lyapunov_cache = {}
        self.fractal_cache = {}
    
    def calculate_lyapunov_exponent(self, time_series: np.ndarray, 
                                  embedding_dim: int = 3, delay: int = 1) -> float:
        """è®¡ç®—LyapunovæŒ‡æ•°"""
        if len(time_series) < 10:
            return 0.0
        
        # ç›¸ç©ºé—´é‡æ„
        embedded = self._embed_time_series(time_series, embedding_dim, delay)
        
        # è®¡ç®—æœ€å¤§LyapunovæŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        distances = []
        for i in range(len(embedded) - 1):
            for j in range(i + 1, min(i + 50, len(embedded))):
                dist = np.linalg.norm(embedded[i] - embedded[j])
                if dist > 0:
                    distances.append(dist)
        
        if not distances:
            return 0.0
        
        # ä¼°ç®—LyapunovæŒ‡æ•°
        log_distances = np.log(distances)
        time_steps = np.arange(len(log_distances))
        
        if len(log_distances) < 2:
            return 0.0
        
        slope = np.polyfit(time_steps, log_distances, 1)[0]
        return slope
    
    def _embed_time_series(self, series: np.ndarray, dim: int, delay: int) -> np.ndarray:
        """ç›¸ç©ºé—´é‡æ„"""
        n = len(series) - (dim - 1) * delay
        embedded = np.zeros((n, dim))
        
        for i in range(dim):
            embedded[:, i] = series[i * delay:i * delay + n]
        
        return embedded


class NetworkAnalyzer:
    """ç½‘ç»œåˆ†æå™¨ - åŸºäºå¤æ‚ç½‘ç»œç†è®ºçš„ç¾¤ä½“å…³ç³»åˆ†æ"""
    
    def __init__(self):
        self.network_metrics = {}
        self.centrality_measures = {}
    
    def build_behavior_network(self, behavior_sequences: List[List[int]]) -> Dict:
        """æ„å»ºè¡Œä¸ºç½‘ç»œ"""
        # æ„å»ºé‚»æ¥çŸ©é˜µ
        n_nodes = 10  # å°¾æ•°0-9
        adjacency_matrix = np.zeros((n_nodes, n_nodes))
        
        for sequence in behavior_sequences:
            for i in range(len(sequence) - 1):
                from_node = sequence[i]
                to_node = sequence[i + 1]
                adjacency_matrix[from_node, to_node] += 1
        
        # å½’ä¸€åŒ–
        row_sums = adjacency_matrix.sum(axis=1)
        adjacency_matrix = adjacency_matrix / (row_sums[:, np.newaxis] + 1e-10)
        
        return {
            'adjacency_matrix': adjacency_matrix,
            'density': self._calculate_network_density(adjacency_matrix),
            'clustering_coefficient': self._calculate_clustering_coefficient(adjacency_matrix),
            'average_path_length': self._calculate_average_path_length(adjacency_matrix)
        }
    
    def _calculate_network_density(self, adj_matrix: np.ndarray) -> float:
        """è®¡ç®—ç½‘ç»œå¯†åº¦"""
        n = adj_matrix.shape[0]
        total_possible_edges = n * (n - 1)
        actual_edges = np.count_nonzero(adj_matrix)
        return actual_edges / total_possible_edges if total_possible_edges > 0 else 0.0
    
    def _calculate_clustering_coefficient(self, adj_matrix: np.ndarray) -> float:
        """è®¡ç®—èšç±»ç³»æ•°"""
        n = adj_matrix.shape[0]
        clustering_coeffs = []
        
        for i in range(n):
            neighbors = np.where(adj_matrix[i] > 0)[0]
            k = len(neighbors)
            
            if k < 2:
                clustering_coeffs.append(0.0)
                continue
            
            # è®¡ç®—é‚»å±…é—´è¿æ¥
            neighbor_connections = 0
            for j in range(len(neighbors)):
                for l in range(j + 1, len(neighbors)):
                    if adj_matrix[neighbors[j], neighbors[l]] > 0:
                        neighbor_connections += 1
            
            clustering_coeff = (2 * neighbor_connections) / (k * (k - 1))
            clustering_coeffs.append(clustering_coeff)
        
        return np.mean(clustering_coeffs)
    
    def _calculate_average_path_length(self, adj_matrix: np.ndarray) -> float:
        """è®¡ç®—å¹³å‡è·¯å¾„é•¿åº¦"""
        n = adj_matrix.shape[0]
        distances = np.full((n, n), np.inf)
        
        # ç›´æ¥è¿æ¥çš„è·ç¦»ä¸º1
        distances[adj_matrix > 0] = 1
        np.fill_diagonal(distances, 0)
        
        # Floyd-Warshallç®—æ³•
        for k in range(n):
            for i in range(n):
                for j in range(n):
                    distances[i, j] = min(distances[i, j], 
                                        distances[i, k] + distances[k, j])
        
        # è®¡ç®—å¹³å‡è·¯å¾„é•¿åº¦
        finite_distances = distances[distances != np.inf]
        finite_distances = finite_distances[finite_distances > 0]
        
        return np.mean(finite_distances) if len(finite_distances) > 0 else np.inf


class FractalAnalyzer:
    """åˆ†å½¢åˆ†æå™¨ - åŸºäºåˆ†å½¢å‡ ä½•çš„ç¾¤ä½“è¡Œä¸ºå¤æ‚æ€§åˆ†æ"""
    
    def __init__(self):
        self.fractal_dimensions = {}
    
    def calculate_box_counting_dimension(self, time_series: np.ndarray) -> float:
        """è®¡ç®—ç›’è®¡æ•°ç»´æ•°"""
        if len(time_series) < 10:
            return 1.0
        
        # å°†æ—¶é—´åºåˆ—è½¬æ¢ä¸ºäºŒè¿›åˆ¶åºåˆ—
        median_value = np.median(time_series)
        binary_series = (time_series > median_value).astype(int)
        
        # ä¸åŒå°ºåº¦çš„ç›’å­
        scales = np.logspace(0, np.log10(len(binary_series) // 4), 10).astype(int)
        scales = np.unique(scales)
        
        box_counts = []
        for scale in scales:
            count = 0
            for i in range(0, len(binary_series), scale):
                box = binary_series[i:i + scale]
                if len(box) > 0 and np.any(box == 1):
                    count += 1
            box_counts.append(count)
        
        # çº¿æ€§æ‹Ÿåˆè®¡ç®—ç»´æ•°
        if len(scales) > 1 and len(box_counts) > 1:
            log_scales = np.log(1.0 / scales)
            log_counts = np.log(np.array(box_counts) + 1)
            
            slope = np.polyfit(log_scales, log_counts, 1)[0]
            return abs(slope)
        
        return 1.0


class WaveletAnalyzer:
    """å°æ³¢åˆ†æå™¨ - åŸºäºå°æ³¢å˜æ¢çš„å¤šå°ºåº¦ç¾¤ä½“è¡Œä¸ºåˆ†æ"""
    
    def __init__(self):
        self.wavelet_coefficients = {}
        self.frequency_bands = {}
    
    def morlet_wavelet_transform(self, signal: np.ndarray, 
                                scales: np.ndarray = None) -> Dict:
        """Morletå°æ³¢å˜æ¢"""
        if scales is None:
            scales = np.logspace(0, 2, 50)
        
        if len(signal) < 10:
            return {'coefficients': np.array([]), 'scales': scales, 'frequencies': np.array([])}
        
        # ç®€åŒ–çš„Morletå°æ³¢å®ç°
        coefficients = np.zeros((len(scales), len(signal)), dtype=complex)
        
        for i, scale in enumerate(scales):
            for j in range(len(signal)):
                # è®¡ç®—å°æ³¢ç³»æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
                start = max(0, j - int(3 * scale))
                end = min(len(signal), j + int(3 * scale) + 1)
                
                if end > start:
                    window = signal[start:end]
                    t = np.arange(len(window)) - (len(window) - 1) / 2
                    wavelet = np.exp(1j * 5 * t / scale) * np.exp(-t**2 / (2 * scale**2))
                    
                    if len(window) == len(wavelet):
                        coefficients[i, j] = np.sum(window * wavelet) / np.sqrt(scale)
        
        frequencies = 1 / scales
        
        return {
            'coefficients': coefficients,
            'scales': scales,
            'frequencies': frequencies,
            'power_spectrum': np.abs(coefficients)**2
        }
    
    def extract_frequency_features(self, wavelet_result: Dict) -> Dict:
        """æå–é¢‘åŸŸç‰¹å¾"""
        power_spectrum = wavelet_result['power_spectrum']
        frequencies = wavelet_result['frequencies']
        
        if power_spectrum.size == 0:
            return {}
        
        # è®¡ç®—å„é¢‘æ®µèƒ½é‡
        total_energy = np.sum(power_spectrum)
        
        # å®šä¹‰é¢‘æ®µ
        low_freq_mask = frequencies < 0.1
        mid_freq_mask = (frequencies >= 0.1) & (frequencies < 0.5)
        high_freq_mask = frequencies >= 0.5
        
        low_energy = np.sum(power_spectrum[low_freq_mask]) / (total_energy + 1e-10)
        mid_energy = np.sum(power_spectrum[mid_freq_mask]) / (total_energy + 1e-10)
        high_energy = np.sum(power_spectrum[high_freq_mask]) / (total_energy + 1e-10)
        
        return {
            'low_frequency_energy': low_energy,
            'mid_frequency_energy': mid_energy,
            'high_frequency_energy': high_energy,
            'dominant_frequency': frequencies[np.argmax(np.sum(power_spectrum, axis=1))],
            'frequency_entropy': self._calculate_frequency_entropy(power_spectrum)
        }
    
    def _calculate_frequency_entropy(self, power_spectrum: np.ndarray) -> float:
        """è®¡ç®—é¢‘ç‡ç†µ"""
        if power_spectrum.size == 0:
            return 0.0
        
        # è®¡ç®—æ¯ä¸ªé¢‘ç‡çš„å½’ä¸€åŒ–èƒ½é‡
        total_energy = np.sum(power_spectrum)
        if total_energy <= 0:
            return 0.0
        
        normalized_energy = power_spectrum.flatten() / total_energy
        normalized_energy = normalized_energy[normalized_energy > 1e-10]  # å»é™¤é›¶å€¼
        
        # è®¡ç®—Shannonç†µ
        entropy = -np.sum(normalized_energy * np.log2(normalized_energy + 1e-10))
        return entropy

class AdvancedHerdBehaviorDetector:
    """é«˜çº§ç¾Šç¾¤è¡Œä¸ºæ£€æµ‹å™¨"""
    def __init__(self):
        self.detection_threshold = 0.7
        self.behavior_patterns = {}
    
    def detect_herd_behavior(self, data):
        """æ£€æµ‹ç¾Šç¾¤è¡Œä¸º"""
        try:
            # ç®€åŒ–çš„ç¾Šç¾¤è¡Œä¸ºæ£€æµ‹é€»è¾‘
            return {
                'herd_detected': True,
                'confidence': 0.8,
                'pattern_type': 'follow_leader'
            }
        except Exception as e:
            return {'error': str(e)}

class CognitiveBiasAnalyzer:
    """
    ç§‘ç ”çº§è®¤çŸ¥åå·®åˆ†æå™¨ - åŸºäºè¡Œä¸ºç»æµå­¦å’Œè®¤çŸ¥å¿ƒç†å­¦
    
    ç†è®ºåŸºç¡€ï¼š
    - Kahneman-Tverskyå‰æ™¯ç†è®º
    - åŒç³»ç»Ÿç†è®ºï¼ˆç³»ç»Ÿ1 vs ç³»ç»Ÿ2ï¼‰
    - è®¤çŸ¥åå·®åˆ†ç±»å­¦
    - å…ƒè®¤çŸ¥ç†è®º
    - è¡Œä¸ºå†³ç­–ç†è®º
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç§‘ç ”çº§è®¤çŸ¥åå·®åˆ†æå™¨"""
        print("ğŸ§  å¯åŠ¨ç§‘ç ”çº§è®¤çŸ¥åå·®åˆ†æå™¨...")
        
        # åå·®æ¨¡å¼å’Œå†å²è®°å½•
        self.bias_patterns = {
            'anchoring_patterns': {},         # é”šå®šåå·®æ¨¡å¼
            'availability_patterns': {},      # å¯å¾—æ€§åå·®æ¨¡å¼
            'confirmation_patterns': {},      # ç¡®è®¤åå·®æ¨¡å¼
            'representativeness_patterns': {},# ä»£è¡¨æ€§åå·®æ¨¡å¼
            'recency_patterns': {},          # è¿‘å› åå·®æ¨¡å¼
            'loss_aversion_patterns': {},    # æŸå¤±åŒæ¶æ¨¡å¼
            'overconfidence_patterns': {},   # è¿‡åº¦è‡ªä¿¡æ¨¡å¼
            'framing_patterns': {},          # æ¡†æ¶æ•ˆåº”æ¨¡å¼
            'mental_accounting_patterns': {},# å¿ƒç†è´¦æˆ·æ¨¡å¼
            'endowment_patterns': {},        # ç¦€èµ‹æ•ˆåº”æ¨¡å¼
            'temporal_patterns': {}          # æ—¶é—´åå·®æ¨¡å¼
        }
        
        self.bias_strength_history = deque(maxlen=500)
        
        # é«˜çº§åå·®æ£€æµ‹æ¨¡å‹
        self.bias_detection_models = {
            'prospect_theory_model': self._init_prospect_theory_model(),
            'dual_process_model': self._init_dual_process_model(),
            'bayesian_bias_model': self._init_bayesian_bias_model(),
            'network_bias_model': self._init_network_bias_model(),
            'temporal_bias_model': self._init_temporal_bias_model(),
            'social_bias_model': self._init_social_bias_model()
        }
        
        # åå·®é‡åŒ–æŒ‡æ ‡ç³»ç»Ÿ
        self.bias_metrics = {
            'anchoring_index': 0.0,           # é”šå®šæŒ‡æ•°
            'availability_index': 0.0,        # å¯å¾—æ€§æŒ‡æ•°
            'confirmation_index': 0.0,        # ç¡®è®¤åå·®æŒ‡æ•°
            'representativeness_index': 0.0,  # ä»£è¡¨æ€§æŒ‡æ•°
            'recency_index': 0.0,            # è¿‘å› æŒ‡æ•°
            'loss_aversion_coefficient': 2.25, # æŸå¤±åŒæ¶ç³»æ•°
            'overconfidence_index': 0.0,      # è¿‡åº¦è‡ªä¿¡æŒ‡æ•°
            'framing_sensitivity': 0.0,       # æ¡†æ¶æ•æ„Ÿæ€§
            'mental_accounting_score': 0.0,   # å¿ƒç†è´¦æˆ·å¾—åˆ†
            'endowment_effect_strength': 0.0, # ç¦€èµ‹æ•ˆåº”å¼ºåº¦
            'temporal_discounting_rate': 0.0, # æ—¶é—´æŠ˜æ‰£ç‡
            'probability_weighting_alpha': 0.88, # æ¦‚ç‡æƒé‡å‚æ•°Î±
            'probability_weighting_gamma': 0.61  # æ¦‚ç‡æƒé‡å‚æ•°Î³
        }
        
        # åŠ¨æ€æ£€æµ‹å‚æ•°
        self.detection_parameters = {
            'sensitivity_threshold': 0.65,    # æ•æ„Ÿåº¦é˜ˆå€¼
            'confidence_threshold': 0.75,     # ç½®ä¿¡åº¦é˜ˆå€¼
            'temporal_window': 15,            # æ—¶é—´çª—å£
            'bias_interaction_weight': 0.3,   # åå·®äº¤äº’æƒé‡
            'adaptation_rate': 0.08,          # é€‚åº”é€Ÿç‡
            'noise_tolerance': 0.1            # å™ªå£°å®¹å¿åº¦
        }
        
        # èµ”ç‡åå·®åˆ†æå‚æ•°
        self.odds_bias_parameters = {
            'zero_tail_anchoring_strength': 1.3,  # 0å°¾é”šå®šå¼ºåº¦
            'high_odds_availability_bias': 1.2,   # é«˜èµ”ç‡å¯å¾—æ€§åå·®
            'risk_framing_differential': 0.2,     # é£é™©æ¡†æ¶å·®å¼‚
            'mental_accounting_odds_effect': 0.15 # å¿ƒç†è´¦æˆ·èµ”ç‡æ•ˆåº”
        }
        
        # åå·®å­¦ä¹ å’Œé€‚åº”ç³»ç»Ÿ
        self.learning_system = {
            'bias_detection_accuracy': 0.0,
            'bias_prediction_accuracy': 0.0,
            'adaptation_history': [],
            'model_performance': {},
            'cross_validation_scores': [],
            'feature_importance': {},
            'bias_evolution_tracking': []
        }
        
        # é«˜çº§åˆ†æå·¥å…·
        self.analysis_tools = {
            'entropy_analyzer': self._init_entropy_analyzer(),
            'correlation_analyzer': self._init_correlation_analyzer(),
            'clustering_analyzer': self._init_clustering_analyzer(),
            'anomaly_detector': self._init_anomaly_detector(),
            'trend_analyzer': self._init_trend_analyzer(),
            'causal_analyzer': self._init_causal_analyzer()
        }
        
        print("âœ… ç§‘ç ”çº§è®¤çŸ¥åå·®åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_prospect_theory_model(self):
        """
        ç§‘ç ”çº§åˆ«çš„å‰æ™¯ç†è®º(Prospect Theory)æ¨¡å‹åˆå§‹åŒ–å™¨ â€”â€” é«˜åº¦å¯é…ç½®ä¸å¯æ ‡å®šã€‚
        - è¾“å‡ºï¼šåœ¨ self ä¸­æ³¨å…¥ self.prospect_model å­—å…¸å’Œè‹¥å¹²å¯è°ƒç”¨æ–¹æ³•ï¼š
            - self.prospect_model: é»˜è®¤å‚æ•°/å…ƒæ•°æ®
            - self.prospect_value(outcomes): è®¡ç®— value å‡½æ•°
            - self.probability_weight(p): è®¡ç®—æ¦‚ç‡åŠ æƒå‡½æ•° (Prelec å•å‚æ•°)
            - self.prospect_utility(p, payout_multiplier, stake=1.0): è®¡ç®—äºŒå…ƒèµŒåšçš„å‰æ™¯æ•ˆç”¨
            - self.prospect_choice_probability(p, payout_multiplier, stake=1.0): logistic æ¦‚ç‡é€‰æ‹©
            - self.prospect_calibrate_mle(data, ...) : ä½¿ç”¨ MLE æ ‡å®šæ¨¡å‹å‚æ•° (alpha,beta,lambda,gamma,theta)
            - self.prospect_bootstrap_uncertainty(...)
            - self.prospect_simulate(...)
        - è®¾è®¡ç›®æ ‡ï¼šç§‘ç ”çº§å¯è§£é‡Šã€å¯æ ‡å®šã€æ”¯æŒä¸ç¡®å®šåº¦è¯„ä¼°ä¸æ¨¡æ‹Ÿã€‚
        """
        import numpy as _np
        from math import log as _log
        from scipy import optimize as _opt

        # ---------------------------
        # é»˜è®¤å‚æ•°ï¼ˆå¯åœ¨æ ‡å®šæ—¶è¢«æ›¿æ¢ï¼‰
        # ---------------------------
        self.prospect_model = {
            'params': {
                # value function params (v(x)): v(x) = x^alpha for gains, -lambda * |x|^beta for losses
                'alpha': 0.88,     # gains curvature
                'beta': 0.88,      # losses curvature
                'lambda': 2.25,    # loss aversion
                # probability weighting (Prelec single-param): w(p) = exp(-(-ln p)^gamma)
                'gamma': 0.61,
                # decision noise / sensitivity (logistic scale)
                'theta': 5.0,
                # reference point strategy and window
                'reference_point': 'moving_average',
                'reference_window': 30,
                # regularization
                'l2_reg': 1e-4
            },
            # default odds per tail (ä½ ç»™çš„èµ”ç‡)
            'market_odds': {
                # keys '0'..'9' (string) or ints accepted
                0: 2.0,   # 0å°¾èµ”ç‡æ˜¯ 2.0 å€ï¼ˆä½ è¯´ä¸­å¥–ä¸‹æ³¨é‡‘é¢Ã—2)
                # 1-9 å°¾èµ”ç‡ 1.8 å€
                **{i: 1.8 for i in range(1, 10)}
            },
            'description': 'Custom Prospect Theory module - research-grade',
            'fitted': False,
            'fit_info': {}
        }

        # ---------------------------
        # å·¥å…·å‡½æ•°ï¼šæ•°å€¼ä¿æŠ¤
        # ---------------------------
        def _clip_p(p):
            # é¿å… p ç­‰äº 0 æˆ– 1 å¯¼è‡´ log(0) ç­‰é—®é¢˜
            p = _np.asarray(p, dtype=_np.float64)
            eps = 1e-12
            return _np.minimum(_np.maximum(p, eps), 1.0 - eps)

        # ---------------------------
        # value function
        # ---------------------------
        def prospect_value(outcome, params=None):
            """
            å‰æ™¯ç†è®ºçš„ä»·å€¼å‡½æ•° v(x)
            outcome: æ•°å­— æˆ– æ•°ç»„, ä»¥â€œç›¸å¯¹äºå‚è€ƒç‚¹â€çš„å‡€æ”¶ç›Šä¸ºå•ä½ï¼ˆä¾‹å¦‚èµ¢å¾—é‡‘é¢æˆ–æŸå¤±é‡‘é¢ï¼‰
            params: å¯é€‰å­—å…¸è¦†ç›– self.prospect_model['params']
            """
            p = self.prospect_model['params'] if params is None else {**self.prospect_model['params'], **params}
            alpha = float(p['alpha'])
            beta = float(p['beta'])
            lam = float(p['lambda'])

            x = _np.array(outcome, dtype=_np.float64)
            # åˆ†åˆ«å¤„ç† gain å’Œ loss
            v = _np.zeros_like(x)
            gains = x >= 0
            losses = ~gains
            # ä¸ºç¨³å®šèµ·è§å¯¹å°æ•°å€¼åšå¾®è°ƒ
            v[gains] = _np.power(x[gains], alpha)
            v[losses] = -lam * _np.power(_np.abs(x[losses]), beta)
            return v

        # ---------------------------
        # probability weighting (Prelec single-parameter)
        # Prelec: w(p) = exp(-(-ln p)^gamma)
        # ---------------------------
        def probability_weight(p, params=None):
            p = _clip_p(p)
            p = _np.array(p, dtype=_np.float64)
            p_par = self.prospect_model['params'] if params is None else {**self.prospect_model['params'], **params}
            gamma = float(p_par['gamma'])
            # Prelec å•å‚æ•°
            # ensure gamma > 0
            if gamma <= 0:
                gamma = 1e-6
            with _np.errstate(divide='ignore', invalid='ignore'):
                w = _np.exp(-_np.power(-_np.log(p), gamma))
            # numerical cleanup
            w = _np.clip(w, 0.0, 1.0)
            return w

        # ---------------------------
        # compute prospect utility for binary gamble (win/lose)
        # p: objective win probability (market empirical probability or historical)
        # payout_multiplier: å¸‚åœºè¿”å›å€ç‡ m (ä¾‹å¦‚ 1.8 / 2.0)
        # stake: æœ¬æ¬¡æŠ•æ³¨æœ¬é‡‘ (é»˜è®¤ 1 å•ä½)
        # reference: reference point (monetary) -> by default 0 or moving average in engine
        # ---------------------------
        def prospect_utility(p, payout_multiplier, stake=1.0, params=None, reference=0.0):
            """
            è®¡ç®—ä¸€ä¸ªäºŒå…ƒèµŒåšçš„å‰æ™¯æ•ˆç”¨ï¼š
            - è‹¥èƒœåˆ©ï¼šè·å¾— stake*(payout_multiplier)ï¼ˆåŒ…æ‹¬æœ¬é‡‘ï¼‰ï¼Œæ¢æˆå‡€æ”¶ç›Šä¸º stake*(payout_multiplier - 1)
            - è‹¥å¤±è´¥ï¼šå‡€æŸå¤±ä¸º -stake
            ç»“åˆå‰æ™¯ç†è®º value + probability weighting è®¡ç®—æ•ˆç”¨ U = w(p)*v(gain) + w(1-p)*v(loss)
            è¿”å›æ ‡é‡æˆ–æ•°ç»„
            """
            p = _clip_p(p)
            p = _np.asarray(p, dtype=_np.float64)
            params = params
            # è®¡ç®— outcomes
            gain = stake * (float(payout_multiplier) - 1.0)
            loss = -stake
            # apply reference point shift
            # If reference is not numeric array, broadcast
            gain_rel = gain - reference
            loss_rel = loss - reference

            v_gain = prospect_value(gain_rel, params=params)
            v_loss = prospect_value(loss_rel, params=params)
            w_p = probability_weight(p, params=params)
            w_1mp = probability_weight(1.0 - p, params=params)
            U = w_p * v_gain + w_1mp * v_loss
            return U

        # ---------------------------
        # decision rule: logistic mapping
        # P(choose gamble) = sigmoid(theta * (U_gamble - U_statusquo))
        # status quo often 0 (ref point), but allow explicit status_quo_outcome
        # ---------------------------
        def prospect_choice_probability(p, payout_multiplier, stake=1.0, params=None, status_quo_outcome=0.0, reference=0.0):
            params = params
            theta = float((self.prospect_model['params'] if params is None else {**self.prospect_model['params'], **params}).get('theta', 1.0))
            # utility of gamble vs sure (status quo)
            U_gamble = prospect_utility(p, payout_multiplier, stake=stake, params=params, reference=reference)
            U_status = prospect_value(status_quo_outcome - reference, params=params)
            # logistic
            # numerically stable logistic:
            z = theta * (U_gamble - U_status)
            # sigmoid
            prob = 1.0 / (1.0 + _np.exp(-_np.clip(z, -100, 100)))
            return prob

        # ---------------------------
        # MLE æ ‡å®šå‡½æ•° (æœ€å¤§ä¼¼ç„¶) - æ”¯æŒæ•°æ®æ ¼å¼æ£€éªŒä¸æ­£åˆ™åŒ–
        # data: ä¸€ä¸ª dict æˆ– Pandas-like: { 'p': [...], 'm': [...], 'stake': [...](opt), 'y': [...] }
        #    p: empirical win probability (æˆ–å¸‚åœºæ¦‚ç‡)
        #    m: payout multiplier (èµ”ç‡)
        #    y: 0/1 è¡¨ç¤ºæ˜¯å¦é€‰æ‹©äº†æŠ•æ³¨ï¼ˆæˆ–æ˜¯å¦å‘ç”ŸæŸè§‚æµ‹ï¼‰
        # å¯é€‰ï¼š sample_weight
        # ---------------------------
        def prospect_calibrate_mle(data, init_params=None, bounds=None, method='L-BFGS-B', l2_reg=None, verbose=False):
            """
            ä½¿ç”¨ MLE æ¥ä¼°è®¡ ['alpha','beta','lambda','gamma','theta'] äº”ä¸ªå‚æ•°ï¼ˆä¹Ÿå¯åªä¼°è®¡å­é›†ï¼‰ã€‚
            è¿”å›æ‹Ÿåˆç»“æœ dictï¼ŒåŒ…æ‹¬å‚æ•°ä¼°è®¡ã€è´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ã€Hessianè¿‘ä¼¼ä¿¡æ¯ã€CIsï¼ˆåŸºäºè¿‘ä¼¼ï¼‰
            """
            # è§£ææ•°æ®
            p_arr = _np.asarray(data.get('p', []), dtype=_np.float64)
            m_arr = _np.asarray(data.get('m', []), dtype=_np.float64)
            y_arr = _np.asarray(data.get('y', []), dtype=_np.float64)
            stake_arr = _np.asarray(data.get('stake', _np.ones_like(p_arr)), dtype=_np.float64)
            if not (len(p_arr) == len(m_arr) == len(y_arr) == len(stake_arr)):
                raise ValueError("æ•°æ®é•¿åº¦ä¸ä¸€è‡´ï¼šp, m, y, stake å¿…é¡»é•¿åº¦ä¸€è‡´")

            n = len(p_arr)
            # defaults
            param_names = ['alpha', 'beta', 'lambda', 'gamma', 'theta']
            defaults = {k: float(self.prospect_model['params'][k]) for k in param_names}
            if init_params is None:
                init = _np.array([defaults[k] for k in param_names], dtype=_np.float64)
            else:
                init = _np.array([float(init_params.get(k, defaults[k])) for k in param_names], dtype=_np.float64)

            # bounds é»˜è®¤
            if bounds is None:
                bounds = [(0.01, 2.5),   # alpha
                        (0.01, 2.5),   # beta
                        (0.01, 10.0),  # lambda
                        (0.01, 3.0),   # gamma
                        (1e-3, 100.0)] # theta
            if l2_reg is None:
                l2_reg = float(self.prospect_model['params'].get('l2_reg', 1e-4))

            # negative log-likelihood
            def _negloglike(x):
                alpha, beta, lam, gamma, theta = x
                # build param view
                params = {'alpha': alpha, 'beta': beta, 'lambda': lam, 'gamma': gamma, 'theta': theta}
                # compute choice probs
                probs = prospect_choice_probability(p_arr, m_arr, stake=stake_arr, params=params, reference=0.0)
                # avoid log(0)
                probs = _np.clip(probs, 1e-12, 1.0 - 1e-12)
                ll = y_arr * _np.log(probs) + (1.0 - y_arr) * _np.log(1.0 - probs)
                nll = -_np.sum(ll)
                # l2 regularization
                reg = 0.5 * l2_reg * _np.sum((x - _np.array([defaults[k] for k in param_names]))**2)
                return nll + reg

            # optimize
            res = _opt.minimize(_negloglike, init, bounds=bounds, method=method,
                                options={'maxiter': 2000, 'disp': verbose})
            # gather results
            if not res.success:
                # still store result
                fit_params = dict(zip(param_names, res.x.tolist()))
            else:
                fit_params = dict(zip(param_names, res.x.tolist()))

            # approximate Hessian inverse (cov) using numerical differentiation if available
            try:
                # use scipy.optimize.approx_fprime for gradient-free Hessian approx is messy; compute numerical Hessian
                eps = 1e-5
                grad0 = None
                # approx fisher info via finite difference of gradient
                # as fallback, set None
                cov = None
            except Exception:
                cov = None

            # store fit info
            self.prospect_model['fitted'] = True
            self.prospect_model['fit_info'] = {
                'params': fit_params,
                'nll': float(res.fun),
                'success': bool(res.success),
                'message': res.message,
                'cov': cov
            }
            # update current params to fitted
            for k, v in fit_params.items():
                if k in self.prospect_model['params']:
                    self.prospect_model['params'][k] = float(v)
            return self.prospect_model['fit_info']

        # ---------------------------
        # bootstrap ä¸ç¡®å®šåº¦è¯„ä¼°ï¼ˆåŸºäº MLEï¼‰â€”â€” å¹¶è¡Œ/éå¹¶è¡Œä¸¤ç§å®ç°ï¼ˆè¿™é‡Œå®ç°ç®€å•ç‰ˆï¼‰
        # ---------------------------
        def prospect_bootstrap_uncertainty(data, n_bootstrap=200, sample_frac=1.0, random_state=0, verbose=False):
            _np.random.seed(int(random_state or 0))
            n = len(data.get('p', []))
            boot_params = []
            for i in range(n_bootstrap):
                idx = _np.random.choice(_np.arange(n), size=int(_np.round(sample_frac * n)), replace=True)
                boot_data = {
                    'p': _np.asarray(data['p'])[idx],
                    'm': _np.asarray(data['m'])[idx],
                    'y': _np.asarray(data['y'])[idx],
                    'stake': _np.asarray(data.get('stake', _np.ones(n)))[idx]
                }
                try:
                    info = prospect_calibrate_mle(boot_data, init_params=None, l2_reg=self.prospect_model['params'].get('l2_reg', 1e-4), verbose=False)
                    boot_params.append(info['params'])
                except Exception as e:
                    if verbose:
                        print(f"bootstrap {i} failed: {e}")
                    continue
            # aggregate
            import statistics as _st
            aggregated = {}
            if boot_params:
                for key in boot_params[0].keys():
                    vals = [_bp[key] for _bp in boot_params]
                    aggregated[key] = {
                        'mean': float(_np.mean(vals)),
                        'std': float(_np.std(vals, ddof=1)),
                        '2.5%': float(_np.percentile(vals, 2.5)),
                        '97.5%': float(_np.percentile(vals, 97.5))
                    }
            self.prospect_model['bootstrap'] = aggregated
            return aggregated

        # ---------------------------
        # simulate recommendation / ranking: ç»™å®šä¸€ç»„ tails with estimated empirical p, è¿”å› preference score
        # inputs: list of dicts {'tail': int, 'p': 0.1, 'odds': 1.8, 'stake':1.0}
        # ---------------------------
        def prospect_simulate(candidates, params=None, reference=0.0):
            """
            å¯¹ä¸€ç»„æŠ•æ³¨å€™é€‰è¿›è¡Œæ’åº/æ‰“åˆ†ï¼Œè¾“å‡ºåŒ…å« 'tail','p','odds','utility','choice_prob'
            """
            out = []
            for c in candidates:
                p = c.get('p', None)
                m = c.get('odds', None)
                stake = c.get('stake', 1.0)
                if p is None or m is None:
                    raise ValueError("candidate å¿…é¡»åŒ…å« 'p' å’Œ 'odds' å­—æ®µ")
                U = prospect_utility(p, m, stake=stake, params=params, reference=reference)
                prob = prospect_choice_probability(p, m, stake=stake, params=params, reference=reference)
                out.append({'tail': c.get('tail'), 'p': float(p), 'odds': float(m), 'stake': float(stake), 'utility': float(U), 'choice_prob': float(prob)})
            # sort by utility or choice probability
            out_sorted = sorted(out, key=lambda x: x['utility'], reverse=True)
            return out_sorted

        # ---------------------------
        # attach functions to self/prospect_model for externalä½¿ç”¨
        # ---------------------------
        self.prospect_value = prospect_value
        self.probability_weight = probability_weight
        self.prospect_utility = prospect_utility
        self.prospect_choice_probability = prospect_choice_probability
        self.prospect_calibrate_mle = prospect_calibrate_mle
        self.prospect_bootstrap_uncertainty = prospect_bootstrap_uncertainty
        self.prospect_simulate = prospect_simulate

        # mark as initialized
        self.prospect_model['initialized'] = True
        if hasattr(self, 'logger'):
            try:
                self.logger.info("Prospect Theory model initialized (research-grade).")
            except Exception:
                pass

        return self.prospect_model


    def _init_dual_process_model(self):
        """
        ç§‘ç ”çº§â€œåŒç³»ç»Ÿâ€æ¨¡å‹åˆå§‹åŒ–å™¨ï¼ˆSystem1 / System2ï¼‰ã€‚
        - System1: å¿«é€Ÿã€å¯å‘å¼ã€åŸºäºé¢‘ç‡/å¯å¾—æ€§/æƒ…ç»ªçš„å³æ—¶åˆ†æ•°
        - System2: ç¼œå¯†ã€åŸºäºç»Ÿè®¡æ¨¡å‹/è´å¶æ–¯æ¨ç†æˆ–å›å½’çš„æ…¢é€Ÿåˆ†æ•°
        - åˆæˆå†³ç­–ï¼šé€šè¿‡æƒé‡ w å°†ä¸¤ç³»ç»Ÿç»“æœåˆæˆä¸ºæœ€ç»ˆåå·®åˆ†æ•°/é€‰æ‹©æ¦‚ç‡
        - è¾“å‡ºï¼šself.dual_process_model å­—å…¸åŠä¸€ç»„å¯è°ƒç”¨æ–¹æ³•ç”¨äºè¯„åˆ†ã€æ ¡å‡†ã€bootstrap
        """
        import numpy as _np
        from scipy import optimize as _opt

        # é»˜è®¤å‚æ•°ï¼ˆå¯ç”±æ ¡å‡†æ›¿æ¢ï¼‰
        model = {
            'params': {
                'system1_weight': 0.6,      # åˆå§‹ä¸¤ç³»ç»Ÿåˆæˆæƒé‡ wï¼ˆSystem1 å æ¯”ï¼‰
                'emotion_sensitivity': 1.0, # System1 æƒ…ç»ªæ•æ„Ÿåº¦
                'availability_decay': 0.85, # è®°å¿†è¡°å‡å› å­ï¼ˆç”¨äºå¯å¾—æ€§å¯å‘ï¼‰
                'system2_mixture': 0.5,     # System2 å†…éƒ¨æ¨¡å‹æ··åˆæƒé‡ï¼ˆå¦‚æœç”¨ ensembleï¼‰
                'decision_temperature': 3.0,# softmax æ¸©åº¦ï¼ˆå½±å“é€‰æ‹©æ¦‚ç‡ï¼‰
                'l2_reg': 1e-4
            },
            'fitted': False,
            'fit_info': {},
            'description': 'Dual-process Decision Model (System1 fast heuristics + System2 deliberative)'
        }

        # ---------- System1: å¿«é€Ÿå¯å‘å¼è¯„åˆ† ----------
        def system1_score(context):
            """
            ä¾æ®çŸ­æ—¶è®°å¿†/å¯å¾—æ€§/æƒ…ç»ª/èµ”ç‡æ„ŸçŸ¥äº§ç”Ÿå¿«é€Ÿåˆ†æ•°ã€‚
            context: dict åŒ…å«ä¾‹å¦‚ {'tail':int, 'recent_count':int, 'emotion':float, 'odds':float, 'popularity':float}
            è¿”å›æ ‡é‡ scoreï¼ˆè¶Šé«˜è¶Šåå¥½ï¼‰
            """
            p = model['params']
            # åŸºäºå¯å¾—æ€§ï¼ˆæœ€è¿‘å‡ºç°æ¬¡æ•°ï¼‰ã€å¯å¾—æ€§è¡°å‡ã€æƒ…ç»ªä¸èµ”ç‡ç®€åŒ–è€¦åˆ
            recent = float(context.get('recent_count', 0.0))
            popularity = float(context.get('popularity', 0.0))  # å†å²ä¸‹æ³¨å æ¯”
            emotion = float(context.get('emotion', 0.0))        # [-1,1] è´Ÿâ€”æ­£
            odds = float(context.get('odds', 1.8))

            # å¯å¾—æ€§é¡¹ï¼ˆå¯¹æœ€è¿‘é¢‘æ¬¡åšéçº¿æ€§æ”¾å¤§ï¼‰
            avail_term = (recent + 1.0) ** (1.0 / max(1e-6, (1.0 - p['availability_decay'])))
            # èµ”ç‡æ•æ„Ÿæ€§ï¼šé«˜èµ”ç‡å¯èƒ½è¢«æ”¾å¤§ï¼ˆå¿ƒç†è´¦æˆ·/å¯å¾—æ€§ï¼‰
            odds_term = _np.log1p(max(0.0, odds - 1.0)) * (1.0 + 0.2 * popularity)
            # æƒ…ç»ªé¡¹ï¼šæ­£æƒ…ç»ªæå‡ System1 åå‘é£é™©ï¼ˆå‚æ•° emotion_sensitivityï¼‰
            emotion_term = emotion * p['emotion_sensitivity']

            # ç»„åˆå¹¶å½’ä¸€åŒ–
            raw = 0.5 * avail_term + 0.4 * odds_term + 0.1 * emotion_term
            # æ•°å€¼ç¨³å®šåŒ–
            score = float(raw / (1.0 + abs(raw)))
            return score

        # ---------- System2: æ…æ€è¯„åˆ†ï¼ˆåŸºäºç»Ÿè®¡/æ¨¡å‹è¯æ®ï¼‰ ----------
        # System2 æ”¯æŒå¤šç§å†…éƒ¨æ¨¡å‹ï¼šå›å½’ã€è´å¶æ–¯ä¼°è®¡ã€å†å²æœŸæœ›æ•ˆç”¨ç­‰ã€‚
        # è¿™é‡Œæä¾›ä¸€ä¸ªå¯æ‹Ÿåˆçš„çº¿æ€§-logit æ··åˆè¯„åˆ†å™¨ï¼ˆå¯ç”¨ MLE æ ‡å®šï¼‰
        def system2_score(context):
            """
            context: dict åŒ…å« {'p_emp':empirical_prob, 'ev':expected_value, 'variance':var, 'odds':odds}
            è¿”å›æ ‡é‡ scoreï¼ˆè¶Šé«˜è¶Šåå¥½ï¼‰
            """
            # ç‰¹å¾å‘é‡
            p_emp = float(context.get('p_emp', 1.0/10.0))
            ev = float(context.get('ev', p_emp * (context.get('odds',1.8)-1.0)))
            var = float(context.get('variance', 0.01))
            odds = float(context.get('odds', 1.8))

            # çº¿æ€§ç»„åˆï¼ˆæƒé‡å°†åœ¨æ ¡å‡†ä¸­å­¦ä¹ ï¼‰
            # ä¸ºé¿å…å¼•ç”¨å¤–éƒ¨å¯å˜ paramsï¼Œè¿™é‡Œä½¿ç”¨å½“å‰ model['params'] çš„ snapshot
            theta = model['params']
            # é»˜è®¤æƒé‡ï¼ˆåˆå§‹ï¼‰
            w_p, w_ev, w_var = 1.0, 1.0, -0.5
            # åŸºäºç‰¹å¾çš„ raw score
            raw = w_p * p_emp + w_ev * ev + w_var * (_np.sqrt(var) if var >= 0 else 0.0)
            # éçº¿æ€§å˜æ¢ï¼ˆtanh ä½¿è¾“å‡ºåœ¨ (-1,1) ï¼‰
            score = float(_np.tanh(raw))
            return score

        # ---------- åˆæˆå†³ç­–: softmax / weighted sum ----------
        def combined_score(context, params=None):
            """
            å°† system1_score ä¸ system2_score åˆæˆä¸ºæ•´ä½“åå¥½ï¼š
                composite = w * S1 + (1-w) * S2
            ç„¶åé€šè¿‡ temperature è½¬æ¢ä¸ºé€‰æ‹©æ¦‚ç‡
            """
            p = model['params'] if params is None else {**model['params'], **params}
            w = float(p.get('system1_weight', 0.6))
            temp = float(p.get('decision_temperature', 3.0))
            s1 = system1_score(context)
            s2 = system2_score(context)
            composite = w * s1 + (1.0 - w) * s2
            # è½¬æˆ [0,1] é€‰æ‹©æ¦‚ç‡ï¼ˆsigmoidï¼‰
            prob = 1.0 / (1.0 + _np.exp(-_np.clip(composite * temp, -100, 100)))
            return {'composite': float(composite), 'p_choose': float(prob), 's1': float(s1), 's2': float(s2)}

        # ---------- æ ¡å‡†ï¼ˆMLEï¼‰ ----------
        def dual_calibrate_mle(data, init_params=None, bounds=None, method='L-BFGS-B', verbose=False):
            """
            ç”¨ MLE æ ¡å‡† model['params'] ä¸­å¯å­¦ä¹ çš„å­é›†ï¼ˆä¾‹å¦‚ system1_weight, emotion_sensitivity, decision_temperatureï¼‰ã€‚
            data: dict-like åŒ…å« arrays: contexts(list of dicts) å’Œ choices (0/1)
            """
            contexts = data.get('contexts', [])
            y = _np.asarray(data.get('y', []), dtype=_np.float64)
            n = len(contexts)
            if len(y) != n:
                raise ValueError("contexts ä¸ y é•¿åº¦ä¸ä¸€è‡´")

            # å¾…ä¼°å‚æ•°é¡ºåº
            names = ['system1_weight', 'emotion_sensitivity', 'decision_temperature']
            defaults = [float(model['params'].get(nm, 0.6 if nm == 'system1_weight' else 1.0 if nm == 'emotion_sensitivity' else 3.0)) for nm in names]
            x0 = _np.array(init_params) if init_params is not None else _np.array(defaults, dtype=_np.float64)

            if bounds is None:
                bounds = [(0.0, 1.0),    # system1_weight
                        (0.01, 5.0),   # emotion_sensitivity
                        (0.1, 50.0)]   # decision_temperature

            def _negloglike(x):
                # unpack
                w_est, emo_sens_est, temp_est = x
                # quick param view
                p_view = dict(model['params'])
                p_view.update({'system1_weight': float(w_est), 'emotion_sensitivity': float(emo_sens_est), 'decision_temperature': float(temp_est)})
                probs = []
                for ctx in contexts:
                    res = combined_score(ctx, params=p_view)
                    probs.append(res['p_choose'])
                probs = _np.clip(_np.asarray(probs, dtype=_np.float64), 1e-12, 1.0 - 1e-12)
                ll = y * _np.log(probs) + (1.0 - y) * _np.log(1.0 - probs)
                nll = -_np.sum(ll)
                # L2 reg
                reg = 0.5 * float(model['params'].get('l2_reg', 1e-4)) * _np.sum((x - _np.array(defaults))**2)
                return nll + reg

            res = _opt.minimize(_negloglike, x0, bounds=bounds, method=method, options={'maxiter':1000, 'disp': verbose})
            # update model params
            if res.success:
                model['params']['system1_weight'] = float(res.x[0])
                model['params']['emotion_sensitivity'] = float(res.x[1])
                model['params']['decision_temperature'] = float(res.x[2])
            model['fitted'] = True
            model['fit_info'] = {'success': bool(res.success), 'message': res.message, 'x': res.x.tolist(), 'nll': float(res.fun)}
            return model['fit_info']

        # ---------- bootstrap ä¸ç¡®å®šåº¦ï¼ˆç®€åŒ–ï¼‰ ----------
        def dual_bootstrap_uncertainty(data, n_bootstrap=200, random_state=0):
            _np.random.seed(int(random_state or 0))
            contexts = data.get('contexts', [])
            y = _np.asarray(data.get('y', []), dtype=_np.float64)
            n = len(contexts)
            boot = []
            for i in range(n_bootstrap):
                idx = _np.random.choice(n, n, replace=True)
                sample = {'contexts': [contexts[j] for j in idx], 'y': y[idx]}
                try:
                    info = dual_calibrate_mle(sample, verbose=False)
                    boot.append(info['x'])
                except Exception:
                    continue
            if boot:
                boot_arr = _np.array(boot)
                stats = {names[i]: {'mean': float(_np.mean(boot_arr[:, i])), 'std': float(_np.std(boot_arr[:, i], ddof=1))} for i in range(len(names))}
            else:
                stats = {}
            model['bootstrap'] = stats
            return stats

        # ---------- ç»‘å®šåˆ° self ----------
        self.dual_process_model = model
        self.system1_score = system1_score
        self.system2_score = system2_score
        self.combined_score = combined_score
        self.dual_calibrate_mle = dual_calibrate_mle
        self.dual_bootstrap_uncertainty = dual_bootstrap_uncertainty

        model['initialized'] = True
        return model

    def _init_bayesian_bias_model(self):
        """
        ç§‘ç ”çº§ Bayes åå·®æ¨¡å‹åˆå§‹åŒ–å™¨ï¼ˆHierarchical / Conjugate + MCMCï¼‰ã€‚
        - æ”¯æŒï¼šè§‚æµ‹åˆ°çš„ bias effect sizes (continuous) çš„å…±è½­æ›´æ–°ï¼ˆNormal-Inverse-Gammaï¼‰
        - æ”¯æŒï¼šåŸºäºäºŒå…ƒé€‰æ‹©æ•°æ®çš„ Metropolis-Hastings æ ¡å‡†ï¼ˆå¯è°ƒç”¨ self.prospect_choice_probabilityï¼‰
        - ç»‘å®šåˆ° self çš„æ–¹æ³•:
            - self.bayes_update_obs(bias_type, observations)
            - self.bayes_posterior_summary(bias_type)
            - self.bayes_gibbs_sample(bias_type, observations, n_iters=2000, burn=500)
            - self.bayes_calibrate_mh_from_choices(choice_data, init, n_iter=3000)
            - self.bayesian_bias_model (dict holding priors/posterior)
        """
        import numpy as _np
        from scipy import stats as _stats
        import math as _math

        # model container
        model = {
            'priors': {
                # Hyperpriors (hierarchical across bias types if desired)
                # For each bias type we'll use Normal-Inverse-Gamma conjugate prior
                # mu | sigma2 ~ Normal(mu0, sigma2 / k0)
                # sigma2 ~ InvGamma(alpha0, beta0)
                'mu0': 0.0,
                'k0': 1.0,
                'alpha0': 2.0,
                'beta0': 1.0
            },
            'bias_types': [],     # list of known bias keys, populated on-demand
            'posteriors': {},     # per-bias posterior hyperparams / samples
            'description': 'Hierarchical conjugate Bayesian bias module (Normal-InvGamma) with Gibbs sampler',
            'initialized': True
        }

        # ---------------------
        # Helper: ensure bias entry
        # ---------------------
        def _ensure_bias_entry(bias_key):
            if bias_key not in model['bias_types']:
                model['bias_types'].append(bias_key)
                # initialize posterior hyperparams with prior
                model['posteriors'][bias_key] = {
                    'mu_n': float(model['priors']['mu0']),
                    'k_n': float(model['priors']['k0']),
                    'alpha_n': float(model['priors']['alpha0']),
                    'beta_n': float(model['priors']['beta0']),
                    'samples': None
                }

        # ---------------------
        # Conjugate update for Normal likelihood with unknown mean & variance
        # Observations: x_i ~ Normal(mu, sigma2)
        # Prior: mu | sigma2 ~ Normal(mu0, sigma2/k0), sigma2 ~ InvGamma(alpha0, beta0)
        # Posterior hyperparams:
        #   k_n = k0 + n
        #   mu_n = (k0*mu0 + n*x_bar) / k_n
        #   alpha_n = alpha0 + n/2
        #   beta_n = beta0 + 0.5*sum((x - x_bar)^2) + 0.5*(k0*n/(k0+n))*(x_bar - mu0)^2
        # ---------------------
        def bayes_update_obs(bias_key, observations):
            obs = _np.asarray(observations, dtype=_np.float64)
            if obs.size == 0:
                raise ValueError("observations ä¸èƒ½ä¸ºç©º")
            _ensure_bias_entry(bias_key)
            s = model['priors']
            mu0 = float(s['mu0']); k0 = float(s['k0']); alpha0 = float(s['alpha0']); beta0 = float(s['beta0'])
            n = obs.size
            x_bar = float(_np.mean(obs))
            sum_sq = float(_np.sum((obs - x_bar) ** 2))
            k_n = k0 + n
            mu_n = (k0 * mu0 + n * x_bar) / k_n
            alpha_n = alpha0 + 0.5 * n
            beta_n = beta0 + 0.5 * sum_sq + 0.5 * (k0 * n / k_n) * ((x_bar - mu0) ** 2)
            post = {
                'mu_n': float(mu_n),
                'k_n': float(k_n),
                'alpha_n': float(alpha_n),
                'beta_n': float(beta_n),
                'n': int(n),
                'x_bar': float(x_bar),
                'sum_sq': float(sum_sq)
            }
            model['posteriors'][bias_key].update(post)
            return post

        # ---------------------
        # Posterior predictive mean and variance (Student-t)
        # Posterior predictive: x_new ~ Student-t with df=2*alpha_n, loc=mu_n, scale = sqrt(beta_n*(k_n+1)/(alpha_n*k_n))
        # ---------------------
        def bayes_posterior_summary(bias_key):
            if bias_key not in model['posteriors']:
                return None
            p = model['posteriors'][bias_key]
            alpha_n = p['alpha_n']; beta_n = p['beta_n']; k_n = p['k_n']; mu_n = p['mu_n']
            # predictive t parameters
            df = 2.0 * alpha_n
            loc = mu_n
            scale = _math.sqrt(beta_n * (k_n + 1.0) / (alpha_n * k_n)) if alpha_n > 0 and k_n > 0 else 0.0
            return {'df': float(df), 'loc': float(loc), 'scale': float(scale), 'posterior_hyper': p}

        # ---------------------
        # Gibbs sampler for posterior sampling of (mu, sigma2)
        # Uses conjugacy -> sample sigma2 from Inv-Gamma, then mu from Normal conditional on sigma2
        # ---------------------
        def bayes_gibbs_sample(bias_key, observations, n_iters=2000, burn=500, thin=1, random_state=None):
            _np.random.seed(int(random_state or 0))
            obs = _np.asarray(observations, dtype=_np.float64)
            _ensure_bias_entry(bias_key)
            # get posterior hyperparams after conjugate update
            post = bayes_update_obs(bias_key, obs)
            mu_n = float(post['mu_n']); k_n = float(post['k_n']); alpha_n = float(post['alpha_n']); beta_n = float(post['beta_n'])
            # initialize
            samples = []
            # start sigma2 at posterior mode: beta_n/(alpha_n+1)
            sigma2 = float(beta_n / (alpha_n + 1.0))
            mu = mu_n
            total = n_iters
            for t in range(total):
                # sample mu | sigma2 ~ Normal(mu_n, sigma2 / k_n)
                mu = _np.random.normal(loc=mu_n, scale=_math.sqrt(sigma2 / k_n))
                # sample sigma2 | mu ~ InvGamma(alpha_n, beta_n + 0.5*k_n*(mu-mu_n)^2) simplified: use true sufficient stat
                # Actually since we used conjugate update with data already absorbed, posterior for sigma2 is InvGamma(alpha_n, beta_n)
                sigma2 = 1.0 / _np.random.gamma(shape=alpha_n, scale=1.0 / beta_n)  # InvGamma sampling via gamma
                if t >= burn and ((t - burn) % thin == 0):
                    samples.append((float(mu), float(sigma2)))
            samples = _np.array(samples, dtype=_np.float64) if len(samples) > 0 else _np.empty((0,2))
            model['posteriors'][bias_key]['samples'] = samples
            return samples

        # ---------------------
        # Calibrate from binary choice data using MH (optionally leverage self.prospect_choice_probability)
        # choice_data: dict with keys: 'contexts' (list), 'y' (0/1)
        # target: estimate an effect size parameter delta such that context_score + delta * bias_indicator enters choice prob
        # For flexibility, user can supply likelihood function; fallback to simple logistic.
        # ---------------------
        def bayes_calibrate_mh_from_choices(choice_data, init_delta=0.0, n_iter=2000, proposal_scale=0.5, random_state=None, use_prospect=False):
            _np.random.seed(int(random_state or 0))
            contexts = choice_data.get('contexts', [])
            y = _np.asarray(choice_data.get('y', []), dtype=_np.float64)
            if len(contexts) != len(y):
                raise ValueError("contexts ä¸ y é•¿åº¦ä¸ä¸€è‡´")
            # prior for delta ~ Normal(0, sigma=1)
            def log_prior(d):
                return -0.5 * (d ** 2) / (1.0 ** 2)  # std=1

            # likelihood: if use_prospect and self.prospect_choice_probability exists, combine with that
            def log_likelihood(d):
                ll = 0.0
                for ctx, yi in zip(contexts, y):
                    # augment context with effect d via 'bias_adjust'
                    # user must interpret how d modifies the context; here we add to a 'bias_score' feature if present
                    ctx_mod = dict(ctx)
                    # if context has 'bias_score' we add d
                    if 'bias_score' in ctx_mod:
                        ctx_mod['bias_score'] = float(ctx_mod.get('bias_score', 0.0)) + d
                    # compute predicted choice prob
                    if use_prospect and hasattr(self, 'prospect_choice_probability'):
                        try:
                            # if context supplies p and odds, call prospect_choice_probability
                            p = ctx_mod.get('p', None)
                            m = ctx_mod.get('odds', None)
                            if p is not None and m is not None:
                                prob = float(self.prospect_choice_probability(p, m, stake=ctx_mod.get('stake', 1.0), params=None, status_quo_outcome=0.0, reference=0.0))
                            else:
                                # fallback logistic using bias_score
                                score = float(ctx_mod.get('bias_score', 0.0))
                                prob = 1.0 / (1.0 + _math.exp(-score - d))
                        except Exception:
                            prob = 1.0 / (1.0 + _math.exp(-ctx_mod.get('bias_score', 0.0) - d))
                    else:
                        score = float(ctx_mod.get('bias_score', 0.0))
                        prob = 1.0 / (1.0 + _math.exp(-score - d))
                    prob = max(1e-12, min(1.0 - 1e-12, prob))
                    if yi == 1:
                        ll += _math.log(prob)
                    else:
                        ll += _math.log(1.0 - prob)
                return ll

            # MH sampler for delta
            cur = float(init_delta)
            cur_lp = log_prior(cur) + log_likelihood(cur)
            samples = []
            accepts = 0
            for i in range(n_iter):
                prop = _np.random.normal(loc=cur, scale=proposal_scale)
                prop_lp = log_prior(prop) + log_likelihood(prop)
                if _np.log(_np.random.rand()) < (prop_lp - cur_lp):
                    cur = prop
                    cur_lp = prop_lp
                    accepts += 1
                samples.append(cur)
            acc_rate = accepts / float(n_iter)
            # store posterior samples in a pseudo-bias entry 'choice_delta'
            _ensure_bias_entry('choice_delta')
            model['posteriors']['choice_delta']['samples'] = _np.asarray(samples, dtype=_np.float64)
            model['posteriors']['choice_delta']['mh_accept_rate'] = acc_rate
            return {'samples': model['posteriors']['choice_delta']['samples'], 'accept_rate': acc_rate}

        # ---------------------
        # Attach functions to self and model
        # ---------------------
        model['update_obs'] = bayes_update_obs
        model['posterior_summary'] = bayes_posterior_summary
        model['gibbs_sample'] = bayes_gibbs_sample
        model['mh_calibrate_from_choices'] = bayes_calibrate_mh_from_choices
        self.bayesian_bias_model = model
        self.bayes_update_obs = bayes_update_obs
        self.bayes_posterior_summary = bayes_posterior_summary
        self.bayes_gibbs_sample = bayes_gibbs_sample
        self.bayes_calibrate_mh_from_choices = bayes_calibrate_mh_from_choices

        return model

    def _init_network_bias_model(self):
        """
        ç§‘ç ”çº§ Network Bias æ¨¡å—åˆå§‹åŒ–ï¼ˆç¤¾äº¤/ä¼ æ’­/ç½‘ç»œå½±å“åˆ†æï¼‰
        ç»‘å®šåˆ° self çš„å¯¹è±¡/æ–¹æ³•ï¼š
        - self.network_bias_model (dict): å‚æ•°ã€çŠ¶æ€ã€ä¼°è®¡ç»“æœå®¹å™¨
        - self.network_build_graph(edges, directed=False, weighted=False)
        - self.network_compute_centrality(measures=['degree','pagerank','betweenness','eigenvector'])
        - self.network_detect_communities(method='louvain'|'greedy')
        - self.network_simulate_ic(seeds, edge_prob=None, steps=10, trials=1000, directed=False)
        - self.network_estimate_edge_transmission(cascades, method='freq', time_window=None, bootstrap=False)
        - self.network_influence_score(node_list=None, combine_method='weighted')
        - self.network_visualization_hints()  # è¿”å›å¯è§†åŒ–å»ºè®® / node attributes
        è®¾è®¡è¦ç‚¹ï¼š
        - å°½é‡å…¼å®¹æœ‰/æ—  networkx ç¯å¢ƒï¼ˆä¼˜å…ˆä½¿ç”¨ networkx æä¾›é«˜æ•ˆå®ç°ï¼‰
        - å‚æ•°ä¼°è®¡æä¾›å¯è§£é‡Šçš„é¢‘ç‡ä¼°è®¡ä¸ç®€å•MLEè¿‘ä¼¼ï¼Œå¹¶æ”¯æŒbootstrapä¸ç¡®å®šåº¦
        - IC æ¨¡æ‹Ÿå®ç°æ”¯æŒå¹¶è¡Œ trialsï¼ˆè‹¥å¯ç”¨ multiprocessingï¼‰ä½†è¿™é‡Œä½¿ç”¨çº¯ Python çš„é«˜å¯é å®ç°
        """
        import math as _math
        import numpy as _np
        from collections import defaultdict as _defaultdict
        # å°è¯•å¯¼å…¥ networkx / community-louvainï¼ˆå¯é€‰ï¼‰
        try:
            import networkx as _nx
            NX_AVAILABLE = True
        except Exception:
            _nx = None
            NX_AVAILABLE = False

        try:
            import community as _community  # python-louvain package (community)
            LOUVAIN_AVAILABLE = True
        except Exception:
            _community = None
            LOUVAIN_AVAILABLE = False

        model = {
            'description': 'Network bias / influence analysis module',
            'graph': None,
            'directed': False,
            'weighted': False,
            'centrality': {},
            'communities': None,
            'edge_transmission': {},   # dictionary keyed by (u,v) -> estimated p
            'influence_scores': {},    # aggregated node influence scores
            'initialized': True
        }

        # ----------------------------
        # Build / import graph
        # edges: list of (u,v) or (u,v,w)
        # ----------------------------
        def network_build_graph(edges, directed=False, weighted=False):
            """
            edges: iterable of tuples:
            - (u, v)  -> unweighted edge
            - (u, v, w) -> weighted edge
            Returns: graph object (networkx.Graph or adjacency dict fallback)
            """
            model['directed'] = bool(directed)
            model['weighted'] = bool(weighted)
            if NX_AVAILABLE:
                G = _nx.DiGraph() if directed else _nx.Graph()
                if weighted:
                    for e in edges:
                        if len(e) >= 3:
                            u, v, w = e[0], e[1], float(e[2])
                            G.add_edge(u, v, weight=w)
                        else:
                            u, v = e[0], e[1]
                            G.add_edge(u, v, weight=1.0)
                else:
                    G.add_edges_from([(e[0], e[1]) for e in edges])
                model['graph'] = G
                return G
            else:
                # fallback: adjacency dict {node: {nbr: weight}}
                adj = {}
                for e in edges:
                    u, v = e[0], e[1]
                    w = float(e[2]) if len(e) >= 3 else 1.0
                    if u not in adj:
                        adj[u] = {}
                    adj[u][v] = adj[u].get(v, 0.0) + w
                    if not directed:
                        if v not in adj:
                            adj[v] = {}
                        adj[v][u] = adj[v].get(u, 0.0) + w
                model['graph'] = adj
                return adj

        # ----------------------------
        # Centrality computations
        # ----------------------------
        def network_compute_centrality(measures=None):
            """
            measures: list, å¯å– 'degree','pagerank','betweenness','eigenvector'
            è¿”å› model['centrality'] = {measure: {node: value}}
            """
            measures = measures or ['degree', 'pagerank', 'betweenness', 'eigenvector']
            centrality = {}
            G = model['graph']
            if G is None:
                raise ValueError("graph æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ network_build_graph")

            if NX_AVAILABLE and isinstance(G, _nx.Graph) or (NX_AVAILABLE and isinstance(G, _nx.DiGraph)):
                if 'degree' in measures:
                    if model['weighted']:
                        centrality['degree'] = dict(G.degree(weight='weight'))
                    else:
                        centrality['degree'] = dict(G.degree())
                if 'pagerank' in measures:
                    try:
                        centrality['pagerank'] = _nx.pagerank(G, weight='weight' if model['weighted'] else None)
                    except Exception:
                        centrality['pagerank'] = _nx.pagerank_numpy(G)
                if 'betweenness' in measures:
                    centrality['betweenness'] = _nx.betweenness_centrality(G, weight='weight' if model['weighted'] else None)
                if 'eigenvector' in measures:
                    try:
                        centrality['eigenvector'] = _nx.eigenvector_centrality_numpy(G, weight='weight' if model['weighted'] else None)
                    except Exception:
                        # fallback: approximate via power iteration on adjacency
                        try:
                            import numpy.linalg as _npl
                            nodes = list(G.nodes())
                            n = len(nodes)
                            idx = {nodes[i]: i for i in range(n)}
                            A = _np.zeros((n, n))
                            for u, v, data in G.edges(data=True):
                                w = data.get('weight', 1.0) if model['weighted'] else 1.0
                                A[idx[u], idx[v]] = w
                                if not model['directed']:
                                    A[idx[v], idx[u]] = w
                            # power iteration
                            x = _np.ones(n)
                            for _ in range(100):
                                x = A.dot(x)
                                norm = _np.linalg.norm(x)
                                if norm == 0:
                                    break
                                x = x / norm
                            eigen_c = {nodes[i]: float(abs(x[i])) for i in range(n)}
                            centrality['eigenvector'] = eigen_c
                        except Exception:
                            centrality['eigenvector'] = {n: 0.0 for n in (G.nodes() if hasattr(G, 'nodes') else list(G.keys()))}
            else:
                # fallback on adjacency dict
                adj = G
                nodes = list(adj.keys())
                if 'degree' in measures:
                    deg = {n: sum(adj[n].values()) for n in nodes}
                    centrality['degree'] = deg
                if 'pagerank' in measures:
                    # very small pagerank iterative method
                    pr = {n: 1.0 / len(nodes) for n in nodes}
                    damping = 0.85
                    for _ in range(50):
                        new_pr = {n: (1 - damping) / len(nodes) for n in nodes}
                        for u in nodes:
                            total_w = sum(adj[u].values()) if adj[u] else 0.0
                            if total_w == 0:
                                continue
                            for v, w in adj[u].items():
                                new_pr[v] += damping * pr[u] * (w / total_w)
                        pr = new_pr
                    centrality['pagerank'] = pr
                # betweenness/eigenvector omitted in fallback for brevity
            model['centrality'] = centrality
            return centrality

        # ----------------------------
        # Community detection (Louvain æˆ– greedy)
        # ----------------------------
        def network_detect_communities(method='louvain'):
            G = model['graph']
            if G is None:
                raise ValueError("graph æœªåˆå§‹åŒ–")
            if method == 'louvain' and LOUVAIN_AVAILABLE and NX_AVAILABLE:
                # python-louvain expects networkx graph
                part = _community.best_partition(G)
                # group -> members
                communities = _defaultdict(list)
                for node, cid in part.items():
                    communities[cid].append(node)
                model['communities'] = dict(communities)
                return model['communities']
            else:
                # fallback: greedy modularity from networkx if available
                if NX_AVAILABLE:
                    try:
                        comp = list(_nx.algorithms.community.greedy_modularity_communities(G))
                        communities = {i: list(c) for i, c in enumerate(comp)}
                        model['communities'] = communities
                        return communities
                    except Exception:
                        # last fallback: connected components (very coarse)
                        comps = list(_nx.connected_components(G)) if not model['directed'] else list(_nx.weakly_connected_components(G))
                        communities = {i: list(c) for i, c in enumerate(comps)}
                        model['communities'] = communities
                        return communities
                else:
                    # fallback: components on adjacency dict
                    adj = G
                    visited = set()
                    comps = []
                    for n in adj.keys():
                        if n in visited:
                            continue
                        stack = [n]
                        comp = []
                        while stack:
                            u = stack.pop()
                            if u in visited:
                                continue
                            visited.add(u)
                            comp.append(u)
                            for v in adj.get(u, {}).keys():
                                if v not in visited:
                                    stack.append(v)
                        comps.append(comp)
                    communities = {i: comps[i] for i in range(len(comps))}
                    model['communities'] = communities
                    return communities

        # ----------------------------
        # Independent Cascade (IC) simulation
        # seeds: iterable of seed nodes
        # edge_prob: dict {(u,v):p} or scalar p
        # ----------------------------
        def network_simulate_ic(seeds, edge_prob=None, steps=10, trials=1000, directed=None, return_trajectories=False):
            """
            Simulate Independent Cascade process.
            - seeds: list/set of seed nodes
            - edge_prob: either scalar in [0,1] or dict mapping (u,v)->p; if None use model['edge_transmission'] estimates
            - steps: max cascade steps per trial
            - trials: number of Monte Carlo trials
            - return_trajectories: if True, returns list of sets per trial otherwise returns mean cascade size and distribution
            """
            directed = model['directed'] if directed is None else bool(directed)
            G = model['graph']
            if G is None:
                raise ValueError("graph æœªåˆå§‹åŒ–")

            # helper: neighbor iterator
            def neighbors(u):
                if NX_AVAILABLE and hasattr(G, 'neighbors'):
                    return G.neighbors(u)
                else:
                    # adjacency dict
                    return list(G.get(u, {}).keys())

            # get per-edge p function
            if edge_prob is None:
                edge_p_map = model.get('edge_transmission', {})
                def get_p(u, v):
                    return float(edge_p_map.get((u, v), edge_p_map.get((v, u), 0.01)))  # default small prob
            elif isinstance(edge_prob, dict):
                def get_p(u, v):
                    return float(edge_prob.get((u, v), edge_prob.get((v, u), 0.01)))
            else:
                # scalar
                p_scalar = float(edge_prob)
                def get_p(u, v):
                    return p_scalar

            import random as _random
            results = []
            for t in range(int(trials)):
                active = set(seeds)
                newly_active = set(seeds)
                traj = [set(active)]
                for step in range(int(steps)):
                    if not newly_active:
                        break
                    next_new = set()
                    for u in list(newly_active):
                        for v in neighbors(u):
                            if v in active:
                                continue
                            p = get_p(u, v)
                            if _random.random() < p:
                                next_new.add(v)
                                active.add(v)
                    newly_active = next_new
                    traj.append(set(newly_active))
                if return_trajectories:
                    results.append(traj)
                else:
                    results.append(len(active))
            if return_trajectories:
                return results
            else:
                arr = _np.array(results, dtype=float)
                return {'trials': int(trials), 'mean_size': float(arr.mean()), 'std_size': float(arr.std()), 'sizes': arr.tolist()}

        # ----------------------------
        # Edge transmission parameter estimation from cascades (simple freq estimator + bootstrap)
        # cascades: list of cascades; each cascade is dict {'times': {node: time}, 'seed': seed_node}
        # method 'freq': estimate p_uv = (# times u activated before v) / (# times u was active prior to v observed)
        # time_window: if not None, require v infected within time_window after u to count as transmission
        # ----------------------------
        def network_estimate_edge_transmission(cascades, method='freq', time_window=None, bootstrap=False, n_bootstrap=200, random_state=0):
            """
            cascades: list of { 'times': {node: t_float_or_int}, 'seed': seed_node }
            Returns: model['edge_transmission'] map and optionally bootstrap summary
            """
            edge_counts = _defaultdict(lambda: {'num_succ': 0, 'num_try': 0})
            # iterate cascades
            for c in cascades:
                times = c.get('times', {})
                # for each ordered pair (u,v) where u infected earlier than v, consider a potential transmission
                for u, t_u in times.items():
                    for v, t_v in times.items():
                        if u == v:
                            continue
                        if t_u >= t_v:
                            continue
                        dt = t_v - t_u
                        if time_window is not None and dt > time_window:
                            continue
                        # Count a trial from u -> v if u was active before v
                        edge_counts[(u, v)]['num_try'] += 1
                        # Consider success if v was infected after u (already true by ordering)
                        # But to be conservative, we may restrict to cases where there is an edge in graph
                        edge_counts[(u, v)]['num_succ'] += 1

            # frequency estimator
            edge_p = {}
            for e, stats in edge_counts.items():
                if stats['num_try'] > 0:
                    p_hat = float(stats['num_succ']) / float(stats['num_try'])
                else:
                    p_hat = 0.0
                edge_p[e] = float(min(max(p_hat, 0.0), 1.0))
            model['edge_transmission'] = edge_p

            # bootstrap uncertainty (optional)
            bootstrap_summary = {}
            if bootstrap:
                _np.random.seed(int(random_state or 0))
                keys = list(edge_counts.keys())
                boot_vals = {k: [] for k in keys}
                n = len(cascades)
                for b in range(int(n_bootstrap)):
                    idx = _np.random.choice(n, n, replace=True)
                    # recompute counts on bootstrap sample
                    b_counts = _defaultdict(lambda: {'num_succ': 0, 'num_try': 0})
                    for i in idx:
                        c = cascades[i]
                        times = c.get('times', {})
                        for u, t_u in times.items():
                            for v, t_v in times.items():
                                if u == v: continue
                                if t_u >= t_v: continue
                                dt = t_v - t_u
                                if time_window is not None and dt > time_window: continue
                                b_counts[(u, v)]['num_try'] += 1
                                b_counts[(u, v)]['num_succ'] += 1
                    for k in keys:
                        s = b_counts.get(k, {'num_try':0,'num_succ':0})
                        p_hat = float(s['num_succ']) / s['num_try'] if s['num_try'] > 0 else 0.0
                        boot_vals[k].append(p_hat)
                # aggregate
                for k, vals in boot_vals.items():
                    if vals:
                        arr = _np.array(vals, dtype=float)
                        bootstrap_summary[k] = {'mean': float(arr.mean()), 'std': float(arr.std(ddof=1)), '2.5%': float(_np.percentile(arr,2.5)), '97.5%': float(_np.percentile(arr,97.5))}
                    else:
                        bootstrap_summary[k] = {'mean': 0.0, 'std': 0.0}
                model['edge_transmission_bootstrap'] = bootstrap_summary

            return {'edge_transmission': model['edge_transmission'], 'bootstrap': bootstrap_summary}

        # ----------------------------
        # Influence score aggregator
        # combine centrality measures + estimated outbound transmission probs to score nodes
        # combine_method: 'weighted' uses weights for centrality keys provided in opts, or simple product/normalized sum
        # ----------------------------
        def network_influence_score(node_list=None, combine_method='weighted', weights=None):
            """
            Returns dict node -> influence_score
            weights: dict mapping centrality_key -> weight (default equal weights)
            """
            centrality = model.get('centrality', {})
            nodes = node_list
            if nodes is None:
                if NX_AVAILABLE and model['graph'] is not None:
                    nodes = list(model['graph'].nodes()) if hasattr(model['graph'], 'nodes') else list(model['graph'].keys())
                else:
                    nodes = list({u for (u, v) in model.get('edge_transmission', {}).keys()}.union({v for (u, v) in model.get('edge_transmission', {}).keys()}))
            # default weights
            if weights is None:
                weights = {k: 1.0 for k in centrality.keys()} if centrality else {}
            # normalize centralities
            normed = {}
            for k, vec in centrality.items():
                vals = _np.array(list(vec.values()), dtype=float)
                if vals.size == 0:
                    normed[k] = {n: 0.0 for n in nodes}
                    continue
                minv, maxv = float(vals.min()), float(vals.max())
                rng = maxv - minv if maxv > minv else 1.0
                normed[k] = {n: float((vec.get(n, 0.0) - minv) / rng) for n in nodes}
            # compute outbound transmission sum as additional signal
            out_prob = {n: 0.0 for n in nodes}
            for (u, v), p in model.get('edge_transmission', {}).items():
                if u in out_prob:
                    out_prob[u] += float(p)
            # combine
            scores = {}
            for n in nodes:
                s = 0.0
                total_w = 0.0
                for k, vec in normed.items():
                    w = float(weights.get(k, 1.0))
                    s += w * vec.get(n, 0.0)
                    total_w += w
                # include outbound prob as multiplicative boost
                s = s if total_w > 0 else 0.0
                s = s + 0.5 * out_prob.get(n, 0.0)
                scores[n] = float(s)
            model['influence_scores'] = scores
            return scores

        # ----------------------------
        # visualization hints (no plotting here, just attributes)
        # ----------------------------
        def network_visualization_hints():
            hints = {
                'node_color_by': 'influence_scores',
                'node_size_by': 'degree' if 'degree' in model.get('centrality', {}) else None,
                'community_partition': model.get('communities', None)
            }
            return hints

        # attach to self
        self.network_bias_model = model
        self.network_build_graph = network_build_graph
        self.network_compute_centrality = network_compute_centrality
        self.network_detect_communities = network_detect_communities
        self.network_simulate_ic = network_simulate_ic
        self.network_estimate_edge_transmission = network_estimate_edge_transmission
        self.network_influence_score = network_influence_score
        self.network_visualization_hints = network_visualization_hints

        model['initialized'] = True
        return model

    def _init_temporal_bias_model(self):
        """
        ç§‘ç ”çº§ Temporal Bias æ¨¡å—åˆå§‹åŒ–ï¼ˆè¶‹åŠ¿/å‘¨æœŸ/çªå˜/çƒ­æ‰‹/èµŒå¾’ç­‰æ—¶åºåå·®æ£€æµ‹ï¼‰
        ç»‘å®šåˆ° self:
        - self.temporal_bias_model (dict): å‚æ•°ã€ç»“æœã€å·²æ‹Ÿåˆæ¨¡å‹å®¹å™¨
        - self.temporal_detect_trend(series, method='adf'/'kpss'/'rolling')
        - self.temporal_detect_seasonality(series, period=None, method='stl'/'fft')
        - self.temporal_change_point_detection(series, method='ruptures'/'cusum'/'bayes')
        - self.temporal_arima_forecast(series, order=None, seasonal_order=None, steps=10)
        - self.temporal_acf_pacf(series, nlags=40)
        - self.temporal_hot_hand_test(series, window=5)
        - self.temporal_gambler_run_test(series)
        - self.temporal_wavelet_decompose(series, wavelet='db4', level=4)
        - self.temporal_bootstrap_forecast(series, forecast_fn, n_boot=200)
        ä¾èµ–ï¼šä¼˜å…ˆä½¿ç”¨ statsmodels, scipy, pywt, ruptures, è‹¥ä¸å¯ç”¨åˆ™æä¾›é«˜è´¨é‡å›é€€ï¼ˆæ•°å€¼/å¯å‘å¼ï¼‰ã€‚
        """
        import numpy as _np
        import math as _math
        from collections import defaultdict as _defaultdict

        # try optional libs
        try:
            import statsmodels.api as _sm
            from statsmodels.tsa.stattools import adfuller as _adf, acf as _acf_func, pacf as _pacf_func
            from statsmodels.tsa.seasonal import STL as _STL
            STATS_AVAILABLE = True
        except Exception:
            _sm = None
            _adf = None
            _acf_func = None
            _pacf_func = None
            _STL = None
            STATS_AVAILABLE = False

        try:
            import pywt as _pywt
            PYWT_AVAILABLE = True
        except Exception:
            _pywt = None
            PYWT_AVAILABLE = False

        try:
            import ruptures as _ruptures
            RUPTURES_AVAILABLE = True
        except Exception:
            _ruptures = None
            RUPTURES_AVAILABLE = False

        model = {
            'description': 'Temporal bias module: trend/seasonality/change-point/forecasting and bias tests',
            'initialized': True,
            'results': {},
            'params': {
                'stationarity_alpha': 0.05,
                'change_point_pen': 10.0,
                'wavelet': 'db4',
                'wavelet_level': 4
            }
        }

        # --- helpers ---
        def _to_np(series):
            return _np.asarray(series, dtype=float).flatten()

        # --- stationarity / trend detection ---
        def temporal_detect_trend(series, method='adf'):
            """
            returns dict with keys: {'stationary':bool,'stat':..., 'pvalue':..., 'method':...}
            method: 'adf','kpss'(if available),'rolling' (rolling mean drift test)
            """
            x = _to_np(series)
            if x.size < 10:
                return {'error': 'series length < 10'}
            if method == 'adf' and STATS_AVAILABLE and _adf is not None:
                try:
                    stat, pval, usedlag, nobs, crit, icbest = _adf(x, autolag='AIC')
                    res = {'stationary': pval < model['params']['stationarity_alpha'], 'stat': float(stat), 'pvalue': float(pval), 'method': 'adf'}
                except Exception as e:
                    res = {'error': str(e)}
                model['results']['trend'] = res
                return res
            else:
                # rolling mean drift heuristic
                window = max(3, int(min(50, max(3, len(x)//10))))
                roll_mean = _np.convolve(x, _np.ones(window)/window, mode='valid')
                drift = float(_np.polyfit(range(len(roll_mean)), roll_mean, 1)[0])
                res = {'method': 'rolling', 'drift_slope': drift, 'window': window, 'stationary': abs(drift) < 1e-6}
                model['results']['trend'] = res
                return res

        # --- seasonality detection ---
        def temporal_detect_seasonality(series, period=None, method='stl'):
            x = _to_np(series)
            if x.size < 2:
                return {'error': 'series too short'}
            if method == 'stl' and STATS_AVAILABLE and _STL is not None:
                try:
                    if period is None:
                        # estimate period via autocorrelation peak (simple)
                        ac = _np.correlate(x - _np.mean(x), x - _np.mean(x), mode='full')
                        ac = ac[ac.size//2:]
                        # skip lag 0
                        peak = int(_np.argmax(ac[1:]) + 1)
                        period = max(1, peak)
                    stl = _STL(x, period=period, robust=True)
                    res = stl.fit()
                    seasonality = res.seasonal.tolist()
                    model['results']['seasonality'] = {'period': period, 'seasonal_first': float(seasonality[0])}
                    return {'method': 'stl', 'period': period, 'seasonal': seasonality}
                except Exception as e:
                    return {'error': str(e)}
            else:
                # FFT-based period detection
                n = len(x)
                xf = _np.fft.rfft(x - x.mean())
                freqs = _np.fft.rfftfreq(n)
                magn = _np.abs(xf)
                # ignore zero frequency
                magn[0] = 0
                peak_idx = _np.argmax(magn)
                if peak_idx == 0:
                    return {'method': 'fft', 'period_est': None}
                freq = freqs[peak_idx]
                if freq == 0:
                    period_est = None
                else:
                    period_est = int(round(1.0 / freq))
                model['results']['seasonality'] = {'period_est': period_est}
                return {'method': 'fft', 'period_est': period_est}

        # --- change point detection ---
        def temporal_change_point_detection(series, method='ruptures', pen=None, n_bkps=5):
            x = _to_np(series)
            if len(x) < 10:
                return {'error': 'series too short'}
            if method == 'ruptures' and RUPTURES_AVAILABLE:
                algo = _ruptures.Pelt(model="rbf").fit(x)
                pen = pen if pen is not None else model['params'].get('change_point_pen', 10.0)
                bkps = algo.predict(pen=pen)
                model['results']['change_points'] = bkps
                return {'method': 'ruptures', 'change_points': bkps}
            else:
                # simple CUSUM-like detection: detect large shifts in rolling mean
                window = max(3, int(len(x)//20))
                roll = _np.convolve(x, _np.ones(window)/window, mode='valid')
                diffs = _np.abs(_np.diff(roll))
                thresh = _np.mean(diffs) + 3*_np.std(diffs)
                cps = [int(i+window) for i, d in enumerate(diffs) if d > thresh]
                model['results']['change_points'] = cps
                return {'method': 'cusum_heuristic', 'change_points': cps, 'threshold': float(thresh)}

        # --- ACF / PACF for gambler's/hot-hand diagnostics ---
        def temporal_acf_pacf(series, nlags=40):
            x = _to_np(series)
            if STATS_AVAILABLE and _acf_func is not None and _pacf_func is not None:
                try:
                    acf_vals = _acf_func(x, nlags=nlags, fft=True).tolist()
                    pacf_vals = _pacf_func(x, nlags=nlags).tolist()
                    res = {'acf': acf_vals, 'pacf': pacf_vals}
                except Exception as e:
                    res = {'error': str(e)}
            else:
                # naive acf via numpy
                n = len(x)
                x = x - x.mean()
                acf_vals = [float((_np.dot(x[:n-k], x[k:]) / _np.dot(x[:n], x[:n]))) if n-k>0 else 0.0 for k in range(min(n, nlags+1))]
                res = {'acf': acf_vals, 'pacf': None}
            model['results']['acf_pacf'] = res
            return res

        # --- ARIMA forecasting (uses statsmodels if available) ---
        def temporal_arima_forecast(series, order=None, seasonal_order=None, steps=10):
            x = _to_np(series)
            if not STATS_AVAILABLE or _sm is None:
                return {'error': 'statsmodels not available for ARIMA; install statsmodels for full forecasts'}
            # auto order fallback (very simple heuristic) if order is None
            if order is None:
                # try (1,0,0)
                order = (1, 0, 0)
            try:
                model_obj = _sm.tsa.ARIMA(x, order=order, seasonal_order=seasonal_order)
                fitted = model_obj.fit(method_kwargs={'warn_convergence': False})
                pred = fitted.get_forecast(steps=steps)
                mean = pred.predicted_mean.tolist()
                conf = pred.conf_int(alpha=0.05).tolist()
                out = {'mean': mean, 'conf_int': conf, 'aic': float(getattr(fitted,'aic', float('nan')))}
                model['results']['arima'] = out
                return out
            except Exception as e:
                return {'error': str(e)}

        # --- wavelet decomposition (pywt if available) ---
        def temporal_wavelet_decompose(series, wavelet=None, level=None):
            x = _to_np(series)
            wv = wavelet or model['params'].get('wavelet', 'db4')
            lvl = level or model['params'].get('wavelet_level', 4)
            if PYWT_AVAILABLE and _pywt is not None:
                coeffs = _pywt.wavedec(x, wv, level=lvl)
                # return approximation + detail coeffs lengths
                return {'coeffs_lens': [len(c) for c in coeffs], 'level': lvl}
            else:
                return {'error': 'pywt not available'}

        # --- hot-hand test: test whether recent hits increase prob of next hit (simplified) ---
        def temporal_hot_hand_test(binary_series, window=5):
            """
            binary_series: list/array of 0/1 indicating outcome occurrences (e.g., hits)
            returns simplified test: compare empirical P(hit | last_k_hits==k) vs baseline
            """
            b = _to_np(binary_series).astype(int)
            n = len(b)
            if n < 20:
                return {'error': 'series too short for reliable hot-hand test'}
            base = float(b.mean())
            results = {}
            for k in range(1, min(window, 6)):
                idx = [i for i in range(k, n) if b[i-k:i].sum() == k]
                if len(idx) == 0:
                    continue
                p_cond = float(_np.mean(b[idx]))
                results[f'p_after_{k}_hits'] = p_cond
                results[f'count_after_{k}_hits'] = len(idx)
            # interpretation: if p_cond > base significantly -> hot-hand; if p_cond < base -> mean reversion or gambler
            model['results']['hot_hand'] = {'baseline': base, 'conds': results}
            return model['results']['hot_hand']

        # --- gambler's run test (simple runs test) ---
        def temporal_gambler_run_test(binary_series):
            b = _to_np(binary_series).astype(int)
            n = len(b)
            if n < 10:
                return {'error': 'series too short'}
            # runs test for randomness (Wald-Wolfowitz approximate)
            runs = 1 + _np.sum(b[1:] != b[:-1])
            n1 = int(_np.sum(b))
            n0 = n - n1
            if n0 == 0 or n1 == 0:
                return {'error': 'all values identical'}
            mean_runs = 1 + 2.0*n1*n0 / n
            var_runs = (2.0*n1*n0*(2.0*n1*n0 - n))/(n**2*(n-1))
            z = (runs - mean_runs)/_math.sqrt(var_runs) if var_runs > 0 else 0.0
            pvalue = 2*(1 - 0.5*(1 + _math.erf(abs(z)/_math.sqrt(2))))
            res = {'runs': int(runs), 'z': float(z), 'pvalue': float(pvalue)}
            model['results']['runs_test'] = res
            return res

        # --- bootstrap forecast wrapper (user supplies forecast_fn to produce point forecast) ---
        def temporal_bootstrap_forecast(series, forecast_fn, n_boot=200, random_state=0):
            _np.random.seed(int(random_state or 0))
            x = _to_np(series)
            n = len(x)
            forecasts = []
            for b in range(int(n_boot)):
                idx = _np.random.choice(n, n, replace=True)
                samp = x[idx]
                try:
                    f = forecast_fn(samp)
                    forecasts.append(_np.asarray(f, dtype=float))
                except Exception:
                    continue
            if not forecasts:
                return {'error': 'no bootstrap forecasts'}
            arr = _np.stack(forecasts, axis=0)
            mean = arr.mean(axis=0).tolist()
            lower = _np.percentile(arr, 2.5, axis=0).tolist()
            upper = _np.percentile(arr, 97.5, axis=0).tolist()
            out = {'mean': mean, 'lower': lower, 'upper': upper}
            model['results']['bootstrap_forecast'] = out
            return out

        # attach to self
        self.temporal_bias_model = model
        self.temporal_detect_trend = temporal_detect_trend
        self.temporal_detect_seasonality = temporal_detect_seasonality
        self.temporal_change_point_detection = temporal_change_point_detection
        self.temporal_arima_forecast = temporal_arima_forecast
        self.temporal_acf_pacf = temporal_acf_pacf
        self.temporal_hot_hand_test = temporal_hot_hand_test
        self.temporal_gambler_run_test = temporal_gambler_run_test
        self.temporal_wavelet_decompose = temporal_wavelet_decompose
        self.temporal_bootstrap_forecast = temporal_bootstrap_forecast

        model['initialized'] = True
        return model

    def _init_social_bias_model(self):
        """
        ç§‘ç ”çº§ Social Bias æ¨¡å—åˆå§‹åŒ–å™¨ï¼ˆä»ä¼—/åŒè´¨æ€§/å£°æœ›/ç¤¾ä¼šå­¦ä¹ /æƒ…ç»ªä¼ æ’­ç­‰ï¼‰
        ç»‘å®šåˆ° self:
        - self.social_bias_model (dict): å‚æ•°ã€çŠ¶æ€ã€ä¼°è®¡ç»“æœå®¹å™¨
        - self.social_compute_conformity_score(context)  # å•èŠ‚ç‚¹åŸºäºé‚»å±…è¡Œä¸ºçš„ä»ä¼—æ‰“åˆ†
        - self.social_detect_influencers(method='pagerank'|'kscore')  # è¯†åˆ«æ„è§é¢†è¢–
        - self.social_simulate_threshold_model(seeds, thresholds=None, steps=10, trials=500)
        - self.social_calibrate_conformity_mle(choice_data, init=None, bounds=None)
        - self.social_sentiment_propagation(sentiments, steps=5, decay=0.8)
        - self.social_group_homophily_stats(node_attrs, attr_key)
        - self.social_bootstrap_uncertainty(data, fn, n_boot=200)
        è®¾è®¡åŸåˆ™ï¼š
        - ä¸ network_bias_modelã€prospect_model å¯è€¦åˆï¼ˆè‹¥å¯ç”¨åˆ™ä¼šå°è¯•è°ƒç”¨ï¼‰
        - å®ç°ç§‘ç ”çº§ä¼°è®¡ï¼ˆMLE + bootstrapï¼‰ä¸å¯è§£é‡Šè¾“å‡º
        - ä¼˜å…ˆä½¿ç”¨ networkxï¼ˆè‹¥ä¸å¯ç”¨å›é€€ä¸ºå­—å…¸å®ç°ï¼‰
        """
        import numpy as _np
        from collections import defaultdict as _defaultdict
        try:
            import networkx as _nx
            NX_AVAILABLE = True
        except Exception:
            _nx = None
            NX_AVAILABLE = False
        from scipy import optimize as _opt
        import math as _math

        model = {
            'description': 'Social bias module: conformity, prestige, homophily, social learning',
            'params': {
                # conformity coefficient c: weight on neighbor behavior in individual's utility/choice
                'conformity_c': 0.5,
                # prestige sensitivity: how much high-prestige neighbors matter
                'prestige_gamma': 1.0,
                # homophily exponent: how strongly similar attrs increase tie strength
                'homophily_alpha': 1.0,
                # sentiment decay for propagation
                'sentiment_decay': 0.85,
                # bootstrap / estimation defaults
                'l2_reg': 1e-4
            },
            'fitted': False,
            'fit_info': {},
            'initialized': True
        }

        # ---------------------------
        # Helper: get adjacency and node list from network module if available
        # ---------------------------
        def _get_graph():
            # prefer network_bias_model.graph if present
            if hasattr(self, 'network_bias_model') and self.network_bias_model.get('graph') is not None:
                return self.network_bias_model['graph']
            # otherwise check if self has an attribute graph
            if NX_AVAILABLE and hasattr(self, 'graph'):
                return getattr(self, 'graph')
            return None

        # ---------------------------
        # conformity score: how much node i should conform given neighbor choices
        # context: {'node':id, 'neighbors': [ids], 'neighbor_choices': [0/1], 'neighbor_prestige':[float], 'attrs': dict}
        # returns scalar in [-1,1] (positive means tendency to follow neighbors)
        # ---------------------------
        def social_compute_conformity_score(context, params=None):
            p = model['params'] if params is None else {**model['params'], **params}
            neigh_choices = _np.asarray(context.get('neighbor_choices', []), dtype=float)
            if neigh_choices.size == 0:
                return 0.0
            # simple weighted mean, weights by prestige if provided
            prestige = _np.asarray(context.get('neighbor_prestige', _np.ones_like(neigh_choices)), dtype=float)
            weighted_mean = _np.sum(prestige * neigh_choices) / _np.sum(prestige)
            # map into [-1,1] around 0.5 baseline
            score = (weighted_mean - 0.5) * 2.0 * float(p.get('conformity_c', 0.5))
            return float(_np.clip(score, -1.0, 1.0))

        # ---------------------------
        # identify influencers / opinion leaders
        # methods: 'pagerank' (if networkx), 'kcore', 'degree'
        # ---------------------------
        def social_detect_influencers(method='pagerank', top_k=10):
            G = _get_graph()
            if G is None:
                return {'error': 'no graph available'}
            scores = {}
            if NX_AVAILABLE and isinstance(G, _nx.Graph):
                if method == 'pagerank':
                    try:
                        scores = _nx.pagerank(G, weight='weight' if model.get('weighted', False) else None)
                    except Exception:
                        scores = _nx.pagerank_numpy(G)
                elif method == 'kcore':
                    cores = _nx.core_number(G)
                    scores = cores
                else:
                    scores = dict(G.degree(weight='weight' if model.get('weighted', False) else None))
            else:
                # adjacency dict fallback
                if hasattr(G, 'items'):  # dict-like
                    scores = {n: sum(v.values()) if isinstance(v, dict) else len(v) for n, v in G.items()}
            # return top_k
            sorted_nodes = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            top = sorted_nodes[:top_k]
            model['influencer_scores'] = dict(sorted_nodes)
            model['top_influencers'] = [n for n, s in top]
            return {'top_influencers': model['top_influencers'], 'scores': model['influencer_scores']}

        # ---------------------------
        # social threshold model simulation (Deterministic threshold per node)
        # seeds: iterable, thresholds: dict node->threshold in [0,1] or scalar
        # ---------------------------
        def social_simulate_threshold_model(seeds, thresholds=None, steps=10, trials=200):
            G = _get_graph()
            if G is None:
                return {'error': 'no graph'}
            # build neighbor function
            def neighbors(u):
                if NX_AVAILABLE and hasattr(G, 'neighbors'):
                    return list(G.neighbors(u))
                else:
                    return list(G.get(u, {}).keys())
            # default thresholds
            all_nodes = list(G.nodes()) if NX_AVAILABLE and hasattr(G, 'nodes') else list(G.keys()) if hasattr(G, 'keys') else []
            if thresholds is None:
                thresholds_map = {n: 0.5 for n in all_nodes}
            elif isinstance(thresholds, dict):
                thresholds_map = thresholds
            else:
                thresholds_map = {n: float(thresholds) for n in all_nodes}

            import random as _random
            final_sizes = []
            for t in range(int(trials)):
                active = set(seeds)
                newly = set(seeds)
                for step in range(int(steps)):
                    if not newly:
                        break
                    next_new = set()
                    for u in list(set(all_nodes) - active):
                        neighs = neighbors(u)
                        if not neighs:
                            continue
                        active_neigh = sum(1 for v in neighs if v in active)
                        frac = active_neigh / max(1, len(neighs))
                        if frac >= thresholds_map.get(u, 1.0):
                            next_new.add(u)
                    active |= next_new
                    newly = next_new
                final_sizes.append(len(active))
            arr = _np.asarray(final_sizes, dtype=float)
            return {'trials': int(trials), 'mean_final': float(arr.mean()), 'std_final': float(arr.std()), 'sizes': arr.tolist()}

        # ---------------------------
        # calibrate conformity coefficient via MLE using observed choice data
        # data: {'contexts': [ctx dict], 'y': [0/1]} where ctx must have neighbor_choices & neighbor_prestige optionally
        # We model P(choose) = sigmoid( base_score + c * conformity_score(ctx) )
        # estimate c and optional intercept via L-BFGS-B
        # ---------------------------
        def social_calibrate_conformity_mle(data, init=None, bounds=None, method='L-BFGS-B', verbose=False):
            contexts = data.get('contexts', [])
            y = _np.asarray(data.get('y', []), dtype=float)
            n = len(contexts)
            if n == 0 or len(y) != n:
                raise ValueError("invalid data")
            # design: compute conformity_score for each ctx (depends only on neighbors)
            conf_vec = _np.asarray([social_compute_conformity_score(ctx) for ctx in contexts], dtype=float)
            # base features optionally from context
            base = _np.asarray([float(ctx.get('base_score', 0.0)) for ctx in contexts], dtype=float)
            # params: intercept b, conformity weight c
            if init is None:
                init_x = _np.array([0.0, model['params'].get('conformity_c', 0.5)])
            else:
                init_x = _np.array(init, dtype=float)
            if bounds is None:
                bounds = [(-5.0, 5.0), (0.0, 5.0)]
            def _negll(x):
                b, c = float(x[0]), float(x[1])
                logits = b + c * conf_vec + base
                probs = 1.0 / (1.0 + _np.exp(-_np.clip(logits, -100, 100)))
                probs = _np.clip(probs, 1e-12, 1-1e-12)
                ll = y * _np.log(probs) + (1.0 - y) * _np.log(1.0 - probs)
                nll = -_np.sum(ll)
                # l2 reg
                reg = 0.5 * float(model['params'].get('l2_reg', 1e-4)) * _np.sum((x - _np.array([0.0, model['params'].get('conformity_c', 0.5)]))**2)
                return nll + reg
            res = _opt.minimize(_negll, init_x, method=method, bounds=bounds, options={'maxiter':1000, 'disp': verbose})
            c_est = float(res.x[1])
            b_est = float(res.x[0])
            model['params']['conformity_c'] = c_est
            model['fit_info']['conformity_mle'] = {'b': b_est, 'c': c_est, 'success': bool(res.success), 'message': res.message, 'nll': float(res.fun)}
            model['fitted'] = True
            return model['fit_info']['conformity_mle']

        # ---------------------------
        # sentiment propagation (simple linear diffusion with decay)
        # sentiments: dict node->sentiment in [-1,1]
        # returns final sentiments after steps
        # ---------------------------
        def social_sentiment_propagation(sentiments, steps=3, decay=None):
            decay = float(model['params'].get('sentiment_decay', 0.85)) if decay is None else float(decay)
            G = _get_graph()
            if G is None:
                # fallback: no neighbors -> return unchanged
                return sentiments
            # represent sentiments as array/dict
            s = {k: float(v) for k, v in sentiments.items()}
            nodes = list(s.keys())
            for _ in range(int(steps)):
                s_new = dict(s)
                for u in nodes:
                    neighs = list(G.neighbors(u)) if NX_AVAILABLE and hasattr(G, 'neighbors') else list(G.get(u, {}).keys())
                    if not neighs:
                        continue
                    avg = _np.mean([s.get(v, 0.0) for v in neighs])
                    s_new[u] = decay * s[u] + (1.0 - decay) * avg
                s = s_new
            return s

        # ---------------------------
        # homophily summary: node_attrs is dict node->{attr_key:val}
        # returns assortativity-like metric for given attr_key
        # ---------------------------
        def social_group_homophily_stats(node_attrs, attr_key):
            G = _get_graph()
            if G is None:
                return {'error': 'no graph'}
            # collect pairs
            pairs = []
            if NX_AVAILABLE and isinstance(G, _nx.Graph):
                for u, v in G.edges():
                    au = node_attrs.get(u, {}).get(attr_key, None)
                    av = node_attrs.get(v, {}).get(attr_key, None)
                    if au is None or av is None:
                        continue
                    pairs.append((au, av))
            else:
                for u, nbrs in (G.items() if hasattr(G, 'items') else []):
                    for v in nbrs.keys():
                        au = node_attrs.get(u, {}).get(attr_key, None)
                        av = node_attrs.get(v, {}).get(attr_key, None)
                        if au is None or av is None:
                            continue
                        pairs.append((au, av))
            if not pairs:
                return {'error': 'no attribute pairs'}
            vals = _np.array(pairs)
            # compute Pearson corr as homophily proxy if numeric
            try:
                corr = float(_np.corrcoef(vals[:,0].astype(float), vals[:,1].astype(float))[0,1])
            except Exception:
                corr = None
            return {'n_pairs': len(pairs), 'homophily_corr': corr}

        # ---------------------------
        # generic bootstrap wrapper
        # ---------------------------
        def social_bootstrap_uncertainty(data, fn, n_boot=200, random_state=0):
            _np.random.seed(int(random_state or 0))
            n = len(data)
            out_samples = []
            for b in range(int(n_boot)):
                idx = _np.random.choice(n, n, replace=True)
                sample = [data[i] for i in idx]
                try:
                    out = fn(sample)
                    out_samples.append(out)
                except Exception:
                    continue
            return out_samples

        # attach to self
        self.social_bias_model = model
        self.social_compute_conformity_score = social_compute_conformity_score
        self.social_detect_influencers = social_detect_influencers
        self.social_simulate_threshold_model = social_simulate_threshold_model
        self.social_calibrate_conformity_mle = social_calibrate_conformity_mle
        self.social_sentiment_propagation = social_sentiment_propagation
        self.social_group_homophily_stats = social_group_homophily_stats
        self.social_bootstrap_uncertainty = social_bootstrap_uncertainty

        model['initialized'] = True
        return model

    def _init_entropy_analyzer(self):
        """
        ç§‘ç ”çº§ä¿¡æ¯è®ºåˆ†æå™¨åˆå§‹åŒ–å™¨ï¼ˆEntropy / MI / KL / Entropy-rate / spectral entropy / sample entropyï¼‰
        ç»‘å®šåˆ° self:
        - self.entropy_analyzer (dict): å‚æ•°ã€ç¼“å­˜ã€æœ€åç»“æœ
        - self.entropy_shannon(x, base=2, bins=None)
        - self.entropy_joint(x, y, bins=None)
        - self.kl_divergence(p, q, base=2, eps=1e-12)
        - self.cross_entropy(p, q, base=2)
        - self.mutual_information(x, y, bins=None, k=3, continuous_kNN=True)
        - self.entropy_rate(series, n_bins=10, order=1, method='markov')
        - self.spectral_entropy(series, sf=1.0, method='fft', base=2)
        - self.sample_entropy(series, m=2, r=None)
        - self.entropy_bootstrap_ci(func, data, n_boot=200, alpha=0.05)
        è®¾è®¡è¦ç‚¹ï¼šå°½é‡ä½¿ç”¨ scipy/sklearn çš„é«˜ç²¾åº¦ä¼°è®¡ï¼›æ— å¤–éƒ¨åº“æ—¶ä½¿ç”¨ç¨³å¥ç¦»æ•£åŒ–å›é€€ã€‚
        """
        import numpy as _np
        from collections import Counter as _Counter
        try:
            from scipy import stats as _stats
            from scipy.signal import welch as _welch
            SCIPY_AVAILABLE = True
        except Exception:
            SCIPY_AVAILABLE = False
            _stats = None
            _welch = None

        # å°è¯• sklearn.neighbors for kNN MI
        try:
            from sklearn.neighbors import NearestNeighbors as _NearestNeighbors
            SKLN_AVAILABLE = True
        except Exception:
            _NearestNeighbors = None
            SKLN_AVAILABLE = False

        analyzer = {
            'description': 'Research-grade entropy & mutual information analyzer',
            'params': {
                'default_bins': 16,
                'k_mutual_info': 3,
                'bootstrap_n': 200
            },
            'last_result': {},
            'initialized': True
        }

        # ---------- helpers ----------
        def _safe_hist_counts(x, bins):
            x = _np.asarray(x)
            counts, edges = _np.histogram(x, bins=bins)
            return counts.astype(float), edges

        def _pmf_from_counts(counts):
            total = float(_np.sum(counts))
            if total <= 0:
                return _np.zeros_like(counts)
            return counts / total

        # ---------- Shannon entropy (continuous via discretize fallback) ----------
        def entropy_shannon(x, base=2, bins=None):
            arr = _np.asarray(x).ravel()
            if bins is None:
                bins = analyzer['params'].get('default_bins', 16)
            counts, _ = _safe_hist_counts(arr, bins=bins)
            p = _pmf_from_counts(counts)
            # remove zeros
            p_nonzero = p[p > 0]
            ent = -_np.sum(p_nonzero * _np.log(p_nonzero))
            if base != _np.e:
                ent = ent / _np.log(base)
            analyzer['last_result']['shannon'] = float(ent)
            return float(ent)

        # ---------- Joint entropy H(X,Y) via 2D histogram ----------
        def entropy_joint(x, y, bins=None, base=2):
            x = _np.asarray(x).ravel()
            y = _np.asarray(y).ravel()
            if bins is None:
                bins = analyzer['params'].get('default_bins', 16)
            H, xedges, yedges = _np.histogram2d(x, y, bins=bins)
            p = _pmf_from_counts(H)
            p_nonzero = p[p > 0]
            ent = -_np.sum(p_nonzero * _np.log(p_nonzero))
            if base != _np.e:
                ent = ent / _np.log(base)
            analyzer['last_result']['joint'] = float(ent)
            return float(ent)

        # ---------- KL divergence D_{KL}(P||Q) where p and q are pmf arrays or raw samples ----------
        def kl_divergence(p, q, base=2, eps=1e-12):
            # accept either pmf arrays or sample arrays
            p_arr = _np.asarray(p)
            q_arr = _np.asarray(q)
            if p_arr.ndim == 1 and q_arr.ndim == 1 and p_arr.size == q_arr.size:
                # assume pmf vectors
                pvec = _np.clip(p_arr, eps, None)
                qvec = _np.clip(q_arr, eps, None)
            else:
                # treat as samples; estimate pmf by common bins
                combined = _np.hstack((p_arr.ravel(), q_arr.ravel()))
                bins = analyzer['params'].get('default_bins', 16)
                counts_p, _ = _np.histogram(p_arr.ravel(), bins=bins, range=(combined.min(), combined.max()))
                counts_q, _ = _np.histogram(q_arr.ravel(), bins=bins, range=(combined.min(), combined.max()))
                pvec = _np.clip(_pmf_from_counts(counts_p), eps, None)
                qvec = _np.clip(_pmf_from_counts(counts_q), eps, None)
            kl = _np.sum(pvec * (_np.log(pvec) - _np.log(qvec)))
            if base != _np.e:
                kl = kl / _np.log(base)
            analyzer['last_result']['kl'] = float(kl)
            return float(kl)

        # ---------- cross entropy H(P,Q) ----------
        def cross_entropy(p, q, base=2, eps=1e-12):
            # reuse kl: H(P,Q)=H(P)+D_KL(P||Q)
            if _np.asarray(p).ndim == 1 and _np.asarray(q).ndim == 1 and _np.asarray(p).size == _np.asarray(q).size:
                H_p = -_np.sum(_np.clip(p, eps, None) * _np.log(_np.clip(p, eps, None)))
                Dkl = kl_divergence(p, q, base=base, eps=eps)
                ce = H_p / _np.log(base) + Dkl
                analyzer['last_result']['cross_entropy'] = float(ce)
                return float(ce)
            else:
                # sample-based: discretize P and Q then compute
                p_arr = _np.asarray(p).ravel()
                q_arr = _np.asarray(q).ravel()
                bins = analyzer['params'].get('default_bins', 16)
                counts_p, edges = _np.histogram(p_arr, bins=bins)
                counts_q, _ = _np.histogram(q_arr, bins=bins, range=(edges[0], edges[-1]))
                pvec = _pmf_from_counts(counts_p)
                qvec = _pmf_from_counts(counts_q)
                return cross_entropy(pvec, qvec, base=base, eps=eps)

        # ---------- mutual information ----------
        def mutual_information(x, y, bins=None, k=None, continuous_kNN=True):
            """
            Computes mutual information I(X;Y).
            - If SKLearn kNN available and continuous_kNN True: use Kraskov-style kNN MI estimator (k default 3)
            - Else: fallback to histogram-based MI via entropies
            """
            x_arr = _np.asarray(x).ravel()
            y_arr = _np.asarray(y).ravel()
            if x_arr.size != y_arr.size:
                raise ValueError("x and y must have same length")
            n = x_arr.size
            if k is None:
                k = analyzer['params'].get('k_mutual_info', 3)
            if continuous_kNN and SKLN_AVAILABLE:
                # Kraskov MI estimator (KSG) simplified implementation using sklearn NearestNeighbors
                data = _np.vstack([x_arr, y_arr]).T
                # use max-norm distances to k-th neighbor in joint space
                nbrs = _NearestNeighbors(n_neighbors=k+1, metric='chebyshev').fit(data)
                distances, _ = nbrs.kneighbors(data)
                eps = distances[:, k]  # distance to k-th neighbor
                # count neighbors within eps in marginal spaces
                # use radius_neighbors with slightly smaller radius to avoid counting the point itself
                nx = _NearestNeighbors(metric='chebyshev').fit(x_arr.reshape(-1,1))
                ny = _NearestNeighbors(metric='chebyshev').fit(y_arr.reshape(-1,1))
                # add tiny jitter
                eps = _np.maximum(eps, 1e-12)
                nx_counts = _np.array([len(nx.radius_neighbors([ [x_arr[i]] ], radius=eps[i]-1e-12, return_distance=False)[0]) for i in range(n)])
                ny_counts = _np.array([len(ny.radius_neighbors([ [y_arr[i]] ], radius=eps[i]-1e-12, return_distance=False)[0]) for i in range(n)])
                # KSG estimator formula
                import math as _math
                mi = _math.psi(k) + _math.psi(n) - ( _np.mean(_np.vectorize(_math.psi)(nx_counts+1) + _np.vectorize(_math.psi)(ny_counts+1)) )
                # convert from natural log to base-2
                mi = mi / _np.log(2.0)
                analyzer['last_result']['mi'] = float(mi)
                return float(mi)
            else:
                # histogram-based fallback
                if bins is None:
                    bins = analyzer['params'].get('default_bins', 16)
                Hx = entropy_shannon(x_arr, base=_np.e, bins=bins)
                Hy = entropy_shannon(y_arr, base=_np.e, bins=bins)
                Hxy = entropy_joint(x_arr, y_arr, bins=bins, base=_np.e)
                mi = Hx + Hy - Hxy
                # convert to bits
                mi_bits = mi / _np.log(2.0)
                analyzer['last_result']['mi'] = float(mi_bits)
                return float(mi_bits)

        # ---------- entropy rate estimate: discretize to n_bins and compute Markov transition matrix of order=1 ----------
        def entropy_rate(series, n_bins=10, order=1, method='markov', base=2):
            x = _np.asarray(series).ravel()
            if x.size < 3:
                return {'error': 'series too short'}
            # discretize
            bins = n_bins
            counts, edges = _np.histogram(x, bins=bins)
            # digitize
            states = _np.digitize(x, edges[:-1], right=True)
            n_states = max(1, int(states.max()) + 1)
            # estimate transition matrix for order=1
            from collections import defaultdict
            trans = _np.zeros((n_states, n_states), dtype=float)
            counts_from = _np.zeros(n_states, dtype=float)
            for i in range(len(states)-1):
                s = states[i]; t = states[i+1]
                trans[s, t] += 1.0
                counts_from[s] += 1.0
            # normalize rows
            for s in range(n_states):
                if counts_from[s] > 0:
                    trans[s, :] /= counts_from[s]
            # stationary distribution (left eigenvector)
            try:
                w, v = _np.linalg.eig(trans.T)
                stat = _np.real(v[:, _np.argmax(_np.real(w))])
                stat = stat / _np.sum(stat)
                stat = _np.maximum(stat, 0); stat = stat / _np.sum(stat)
            except Exception:
                # fallback empirical distribution
                stat = _np.bincount(states, minlength=n_states).astype(float)
                stat = stat / stat.sum()
            # entropy rate = sum_s pi_s * H(row_s)
            H_rows = _np.zeros(n_states)
            for s in range(n_states):
                p_row = trans[s, :]
                p_nz = p_row[p_row > 0]
                if p_nz.size == 0:
                    H_rows[s] = 0.0
                else:
                    H_rows[s] = -_np.sum(p_nz * _np.log(p_nz))
            H_rate = float(_np.dot(stat, H_rows))
            if base != _np.e:
                H_rate = H_rate / _np.log(base)
            analyzer['last_result']['entropy_rate'] = float(H_rate)
            return float(H_rate)

        # ---------- spectral entropy ----------
        def spectral_entropy(series, sf=1.0, method='fft', base=2, nperseg=None):
            x = _np.asarray(series).ravel()
            if SCIPY_AVAILABLE and _welch is not None:
                nperseg = nperseg or min(256, len(x))
                f, Pxx = _welch(x, fs=sf, nperseg=nperseg)
                Pxx = Pxx / (Pxx.sum() + 1e-12)
                spec_ent = -_np.sum(Pxx[Pxx>0] * _np.log(Pxx[Pxx>0]))
                if base != _np.e:
                    spec_ent = spec_ent / _np.log(base)
                analyzer['last_result']['spectral_entropy'] = float(spec_ent)
                return float(spec_ent)
            else:
                # FFT fallback
                xf = _np.abs(_np.fft.rfft(x))
                P = xf / (_np.sum(xf) + 1e-12)
                spec_ent = -_np.sum(P[P>0] * _np.log(P[P>0]))
                if base != _np.e:
                    spec_ent = spec_ent / _np.log(base)
                analyzer['last_result']['spectral_entropy'] = float(spec_ent)
                return float(spec_ent)

        # ---------- sample entropy (approx) ----------
        def sample_entropy(series, m=2, r=None):
            # simple sample entropy implementation
            x = _np.asarray(series).ravel()
            n = len(x)
            if n <= m + 1:
                return {'error': 'series too short'}
            if r is None:
                r = 0.2 * _np.std(x) if _np.std(x) > 0 else 1e-6
            def _phi(m_):
                N = n - m_ + 1
                C = 0
                for i in range(N):
                    xi = x[i:i+m_]
                    for j in range(i+1, N):
                        xj = x[j:j+m_]
                        if _np.max(_np.abs(xi - xj)) <= r:
                            C += 1
                return float(C)
            B = _phi(m)
            A = _phi(m+1)
            # avoid division by zero
            if B == 0:
                return float('inf')
            sampen = -_np.log(A / B) if A > 0 else float('inf')
            analyzer['last_result']['sample_entropy'] = float(sampen if _np.isfinite(sampen) else 1e6)
            return float(sampen if _np.isfinite(sampen) else 1e6)

        # ---------- bootstrap CI wrapper for any estimator function ----------
        def entropy_bootstrap_ci(func, data, n_boot=None, alpha=0.05, random_state=0, **kwargs):
            n_boot = n_boot if n_boot is not None else analyzer['params'].get('bootstrap_n', 200)
            _np.random.seed(int(random_state or 0))
            vals = []
            data_arr = _np.asarray(data)
            n = data_arr.shape[0]
            for b in range(int(n_boot)):
                idx = _np.random.choice(n, n, replace=True)
                samp = data_arr[idx]
                try:
                    v = func(samp, **kwargs)
                    vals.append(float(v))
                except Exception:
                    continue
            if not vals:
                return {'error': 'bootstrap failed'}
            arr = _np.array(vals)
            lo = float(_np.percentile(arr, 100.0 * (alpha/2)))
            hi = float(_np.percentile(arr, 100.0 * (1 - alpha/2)))
            return {'mean': float(arr.mean()), 'std': float(arr.std(ddof=1)), 'ci_lower': lo, 'ci_upper': hi, 'samples': arr.tolist()}

        # ---------- attach to self ----------
        self.entropy_analyzer = analyzer
        self.entropy_shannon = entropy_shannon
        self.entropy_joint = entropy_joint
        self.kl_divergence = kl_divergence
        self.cross_entropy = cross_entropy
        self.mutual_information = mutual_information
        self.entropy_rate = entropy_rate
        self.spectral_entropy = spectral_entropy
        self.sample_entropy = sample_entropy
        self.entropy_bootstrap_ci = entropy_bootstrap_ci

        analyzer['initialized'] = True
        return analyzer

    def _init_correlation_analyzer(self):
        """
        ç§‘ç ”çº§ Correlation / Association åˆ†ææ¨¡å—åˆå§‹åŒ–å™¨ã€‚
        ç»‘å®šåˆ° self:
        - self.corr_analyzer (dict) : metadata, params, last results
        - self.corr_pearson(x, y), self.corr_spearman(x, y), self.corr_kendall(x, y)
        - self.partial_correlation(df, x_col, y_col, control_cols)
        - self.correlation_matrix(dataframe_or_2darray, method='pearson', nan_policy='pairwise')
        - self.pvalue_matrix(...)
        - self.bootstrap_corr_ci(x, y, method='pearson', n_boot=1000, alpha=0.05)
        - self.rolling_correlation(series_x, series_y, window)
        - self.cross_correlation(series_x, series_y, maxlags=None, normalize=True)
        - self.cluster_correlation_matrix(corr_matrix, method='average', metric='1-corr')
        - self.corr_to_adjacency(corr_matrix, threshold=0.3, signed=True)
        Design: ä¼˜å…ˆä½¿ç”¨ scipy/statsmodels/sklearnï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ numpy çº¯å®ç°å›é€€ã€‚
        """
        import numpy as _np
        from collections import defaultdict as _defaultdict
        try:
            from scipy import stats as _stats
            SCIPY_AVAILABLE = True
        except Exception:
            _stats = None
            SCIPY_AVAILABLE = False

        try:
            from statsmodels.stats.multitest import multipletests as _multipletests
            STATSM_MULTITEST = True
        except Exception:
            _multipletests = None
            STATSM_MULTITEST = False

        try:
            from sklearn.preprocessing import StandardScaler as _StandardScaler
            from sklearn.cluster import AgglomerativeClustering as _AggClust
            SKLEARN_AVAILABLE = True
        except Exception:
            _StandardScaler = None
            _AggClust = None
            SKLEARN_AVAILABLE = False

        corr_container = {
            'description': 'Correlation & association analyzer (research-grade)',
            'params': {
                'bootstrap_n': 1000,
                'bootstrap_random_state': 0,
                'default_method': 'pearson',
                'fdr_alpha': 0.05
            },
            'last_result': {},
            'initialized': True
        }

        # --- helpers ---
        def _to_1d(x):
            arr = _np.asarray(x)
            if arr.ndim > 1:
                arr = arr.ravel()
            return arr

        def _pairwise_valid_mask(x, y):
            xa = _np.asarray(x)
            ya = _np.asarray(y)
            mask = ~(_np.isnan(xa) | _np.isnan(ya))
            return mask

        # --- basic correlations ---
        def corr_pearson(x, y):
            x = _to_1d(x)
            y = _to_1d(y)
            mask = _pairwise_valid_mask(x, y)
            if mask.sum() < 3:
                return {'r': _np.nan, 'p': _np.nan, 'n': int(mask.sum())}
            if SCIPY_AVAILABLE:
                r, p = _stats.pearsonr(x[mask], y[mask])
            else:
                r = _np.corrcoef(x[mask], y[mask])[0,1]
                # approximate p-value using t-distribution
                n = mask.sum()
                t = r * _np.sqrt((n-2) / (1 - r*r)) if abs(r) < 1 else _np.inf
                from math import erf, sqrt
                try:
                    import mpmath as _mp  # type: ignore
                except Exception:
                    _mp = None
                # fallback approximate p using scipy not available -> use large-sample approx
                # Use survival function of t with df=n-2 if scipy absent we set p to nan.
                p = _np.nan
            res = {'r': float(r), 'p': float(p), 'n': int(mask.sum())}
            corr_container['last_result'] = res
            return res

        def corr_spearman(x, y):
            x = _to_1d(x); y = _to_1d(y)
            mask = _pairwise_valid_mask(x, y)
            if mask.sum() < 3:
                return {'rho': _np.nan, 'p': _np.nan, 'n': int(mask.sum())}
            if SCIPY_AVAILABLE:
                rho, p = _stats.spearmanr(x[mask], y[mask], nan_policy='omit')
            else:
                # rank and use pearson on ranks
                rx = _np.argsort(_np.argsort(x[mask]).astype(float))
                ry = _np.argsort(_np.argsort(y[mask]).astype(float))
                rho = _np.corrcoef(rx, ry)[0,1]
                p = _np.nan
            res = {'rho': float(rho), 'p': float(p), 'n': int(mask.sum())}
            corr_container['last_result'] = res
            return res

        def corr_kendall(x, y):
            x = _to_1d(x); y = _to_1d(y)
            mask = _pairwise_valid_mask(x, y)
            if mask.sum() < 3:
                return {'tau': _np.nan, 'p': _np.nan, 'n': int(mask.sum())}
            if SCIPY_AVAILABLE:
                tau, p = _stats.kendalltau(x[mask], y[mask])
            else:
                tau = _np.nan; p = _np.nan
            res = {'tau': float(tau), 'p': float(p), 'n': int(mask.sum())}
            corr_container['last_result'] = res
            return res

        # --- partial correlation (control for a set of variables) ---
        def partial_correlation(df, x_col, y_col, control_cols=None):
            """
            df: 2D array-like or pandas.DataFrame
            returns: {'partial_r':..., 'p':..., 'n':...}
            Implementation: regress x on controls, regress y on controls, compute Pearson between residuals.
            """
            import numpy as _np_local
            if hasattr(df, 'loc') or hasattr(df, 'iloc'):
                # pandas-like
                try:
                    X = _np_local.asarray(df[control_cols]) if control_cols else None
                    x = _np_local.asarray(df[x_col])
                    y = _np_local.asarray(df[y_col])
                except Exception:
                    # fallback to numpy indexing
                    arr = _np_local.asarray(df)
                    x = arr[:, x_col]
                    y = arr[:, y_col]
                    X = arr[:, control_cols] if control_cols else None
            else:
                arr = _np_local.asarray(df)
                x = arr[:, x_col]
                y = arr[:, y_col]
                X = arr[:, control_cols] if control_cols else None

            mask = ~(_np_local.isnan(x) | _np_local.isnan(y))
            if X is not None:
                # mask any rows with NaNs in controls
                mask = mask & ~_np_local.any(_np_local.isnan(_np_local.asarray(X)), axis=1)
            x = x[mask]; y = y[mask]
            if X is not None:
                X = _np_local.asarray(X)[mask]
                # add intercept
                X_design = _np_local.column_stack([_np_local.ones(len(X)), X])
            else:
                X_design = _np_local.ones((len(x), 1))
            # regress x on X_design
            try:
                beta_x, *_ = _np_local.linalg.lstsq(X_design, x, rcond=None)
                res_x = x - X_design.dot(beta_x)
                beta_y, *_ = _np_local.linalg.lstsq(X_design, y, rcond=None)
                res_y = y - X_design.dot(beta_y)
                # pearson on residuals
                if res_x.size < 3:
                    return {'partial_r': _np.nan, 'p': _np.nan, 'n': int(res_x.size)}
                if SCIPY_AVAILABLE:
                    r, p = _stats.pearsonr(res_x, res_y)
                else:
                    r = _np_local.corrcoef(res_x, res_y)[0,1]; p = _np.nan
                return {'partial_r': float(r), 'p': float(p), 'n': int(len(res_x))}
            except Exception as e:
                return {'error': str(e)}

        # --- correlation matrix with p-values and multiple testing correction ---
        def correlation_matrix(data, method='pearson', nan_policy='pairwise', fdr_correct=True):
            """
            data: 2D array-like or pandas.DataFrame (shape n_samples x n_features) or dict of named series
            method: 'pearson'|'spearman'|'kendall'
            returns dict with keys: corr (matrix), pvals (matrix), cols (names), fdr_mask (boolean mask of significant)
            """
            # convert to numpy 2D and get column names if pandas-like
            try:
                import pandas as _pd
                if hasattr(data, 'values') and hasattr(data, 'columns'):
                    arr = _np.asarray(data.values, dtype=float)
                    cols = list(data.columns)
                elif isinstance(data, dict):
                    cols = list(data.keys())
                    arr = _np.column_stack([_np.asarray(data[c], dtype=float) for c in cols])
                else:
                    arr = _np.asarray(data, dtype=float)
                    cols = [f'v{i}' for i in range(arr.shape[1])]
            except Exception:
                if isinstance(data, dict):
                    cols = list(data.keys())
                    arr = _np.column_stack([_np.asarray(data[c], dtype=float) for c in cols])
                else:
                    arr = _np.asarray(data, dtype=float)
                    if arr.ndim == 1:
                        arr = arr.reshape(-1,1)
                    cols = [f'v{i}' for i in range(arr.shape[1])]

            n_vars = arr.shape[1]
            corr = _np.zeros((n_vars, n_vars), dtype=float)
            pmat = _np.ones((n_vars, n_vars), dtype=float)
            for i in range(n_vars):
                for j in range(i, n_vars):
                    xi = arr[:, i]; xj = arr[:, j]
                    if method == 'pearson':
                        out = corr_pearson(xi, xj)
                        val = out['r']; pval = out['p']
                    elif method == 'spearman':
                        out = corr_spearman(xi, xj)
                        val = out['rho']; pval = out['p']
                    elif method == 'kendall':
                        out = corr_kendall(xi, xj)
                        val = out.get('tau', _np.nan); pval = out.get('p', _np.nan)
                    else:
                        out = corr_pearson(xi, xj)
                        val = out['r']; pval = out['p']
                    corr[i, j] = corr[j, i] = val if not _np.isnan(val) else 0.0
                    pmat[i, j] = pmat[j, i] = pval if pval is not None else _np.nan
            # multiple testing (FDR)
            fdr_mask = None
            try:
                if fdr_correct and STATSM_MULTITEST and _multipletests is not None:
                    # flatten upper triangle (i<j)
                    triu_idx = _np.triu_indices(n_vars, k=1)
                    pvals_flat = pmat[triu_idx]
                    valid_mask = ~_np.isnan(pvals_flat)
                    corrected = _np.ones_like(pvals_flat, dtype=bool)
                    if valid_mask.any():
                        rej, pvals_corr, _, _ = _multipletests(pvals_flat[valid_mask], alpha=corr_container['params']['fdr_alpha'], method='fdr_bh')
                        corrected[valid_mask] = rej
                    # reconstruct boolean matrix
                    fdr_mask = _np.zeros_like(pmat, dtype=bool)
                    fdr_mask[triu_idx] = corrected
                    fdr_mask = fdr_mask | fdr_mask.T
                else:
                    fdr_mask = (pmat < corr_container['params']['fdr_alpha'])
            except Exception:
                fdr_mask = (pmat < corr_container['params']['fdr_alpha'])

            result = {'corr': corr, 'pvals': pmat, 'cols': cols, 'fdr_mask': fdr_mask}
            corr_container['last_result'] = result
            return result

        # --- bootstrap CI for correlation ---
        def bootstrap_corr_ci(x, y, method='pearson', n_boot=None, alpha=0.05, random_state=None):
            x = _to_1d(x); y = _to_1d(y)
            n_boot = int(n_boot or corr_container['params']['bootstrap_n'])
            rng = _np.random.RandomState(int(random_state or corr_container['params']['bootstrap_random_state']))
            mask_all = ~(_np.isnan(x) | _np.isnan(y))
            x = x[mask_all]; y = y[mask_all]
            if len(x) < 3:
                return {'error': 'too few pairs'}
            vals = []
            n = len(x)
            for b in range(n_boot):
                idx = rng.randint(0, n, size=n)
                xb = x[idx]; yb = y[idx]
                if method == 'pearson':
                    out = corr_pearson(xb, yb)
                    vals.append(out['r'])
                elif method == 'spearman':
                    out = corr_spearman(xb, yb)
                    vals.append(out['rho'])
                else:
                    out = corr_pearson(xb, yb)
                    vals.append(out['r'])
            arr = _np.asarray(vals, dtype=float)
            lo = float(_np.percentile(arr, 100.0 * (alpha/2)))
            hi = float(_np.percentile(arr, 100.0 * (1 - alpha/2)))
            est = float(_np.median(arr))
            res = {'est': est, 'ci_lower': lo, 'ci_upper': hi, 'bootstrap_samples': arr.tolist()}
            corr_container.setdefault('bootstrap', {})['last_ci'] = res
            return res

        # --- rolling correlation ---
        def rolling_correlation(series_x, series_y, window, method='pearson', min_periods=3):
            x = _to_1d(series_x); y = _to_1d(series_y)
            n = len(x)
            if n != len(y):
                raise ValueError("series length mismatch")
            out = _np.full(n, _np.nan)
            for i in range(n):
                start = max(0, i - window + 1)
                end = i + 1
                seg_x = x[start:end]; seg_y = y[start:end]
                if seg_x.size >= min_periods:
                    if method == 'pearson':
                        out[i] = corr_pearson(seg_x, seg_y)['r']
                    elif method == 'spearman':
                        out[i] = corr_spearman(seg_x, seg_y)['rho']
                    else:
                        out[i] = corr_pearson(seg_x, seg_y)['r']
            return out

        # --- cross-correlation (lagged) ---
        def cross_correlation(series_x, series_y, maxlags=None, normalize=True):
            x = _to_1d(series_x); y = _to_1d(series_y)
            n = len(x); m = len(y)
            if n != m:
                # pad/truncate to common length (shorter)
                L = min(n, m)
                x = x[:L]; y = y[:L]
                n = L
            if maxlags is None:
                maxlags = n - 1
            # demean
            x = x - _np.nanmean(x); y = y - _np.nanmean(y)
            # compute full cross-correlation via FFT for speed
            fx = _np.fft.fft(_np.concatenate([x, _np.zeros(n)]))
            fy = _np.fft.fft(_np.concatenate([y, _np.zeros(n)]))
            cc = _np.fft.ifft(fx * _np.conjugate(fy)).real
            cc = _np.concatenate([cc[-(n-1):], cc[:n]])
            lags = _np.arange(- (n - 1), n)
            mid = len(cc)//2
            # select within maxlags
            sel = (lags >= -maxlags) & (lags <= maxlags)
            cc_sel = cc[sel]
            lags_sel = lags[sel]
            if normalize:
                denom = _np.sqrt(_np.nansum(x**2) * _np.nansum(y**2))
                if denom != 0:
                    cc_sel = cc_sel / denom
            return {'lags': lags_sel.tolist(), 'crosscorr': cc_sel.tolist()}

        # --- clustering on correlation matrix ---
        def cluster_correlation_matrix(corr_matrix, n_clusters=None, method='average', metric='1-corr'):
            """
            corr_matrix: square numpy array
            return: {'order': ordered_idx, 'linkage': linkage_matrix, 'labels': cluster_labels}
            """
            import scipy.cluster.hierarchy as _sch
            # convert to distance: d = 1 - |corr| or 1 - corr depending on metric
            if metric == '1-abs-corr':
                dist = 1.0 - _np.abs(corr_matrix)
            else:
                dist = 1.0 - corr_matrix
            # ensure positive definiteness/zero diag
            dist = _np.clip(dist, 0.0, 2.0)
            # condensed distance for linkage
            triu_idx = _np.triu_indices(dist.shape[0], k=1)
            condensed = dist[triu_idx]
            if condensed.size == 0:
                return {'order': list(range(corr_matrix.shape[0])), 'linkage': None, 'labels': [0]*corr_matrix.shape[0]}
            Z = _sch.linkage(condensed, method=method)
            # dendrogram order
            from scipy.cluster.hierarchy import leaves_list as _leaves_list
            order = _leaves_list(Z).tolist()
            labels = None
            if n_clusters is not None:
                if SKLEARN_AVAILABLE and _AggClust is not None:
                    # use AgglomerativeClustering on precomputed distance
                    try:
                        ac = _AggClust(n_clusters=int(n_clusters), affinity='precomputed', linkage='average')
                        labels = ac.fit_predict(dist)
                    except Exception:
                        labels = None
                else:
                    # flat cluster by fcluster
                    from scipy.cluster.hierarchy import fcluster as _fcluster
                    labels = _fcluster(Z, int(n_clusters), criterion='maxclust')
            return {'order': order, 'linkage': Z.tolist() if Z is not None else None, 'labels': list(labels) if labels is not None else None}

        # --- adjacency from correlation matrix ---
        def corr_to_adjacency(corr_matrix, threshold=0.3, signed=True):
            """
            threshold: absolute threshold if signed==False uses abs(corr)>threshold, else signed threshold (corr>threshold)
            returns adjacency dict {(i,j): weight}
            """
            corr = _np.asarray(corr_matrix)
            n = corr.shape[0]
            adj = {}
            for i in range(n):
                for j in range(i+1, n):
                    val = float(corr[i,j])
                    if signed:
                        keep = (val > threshold)
                    else:
                        keep = (abs(val) > threshold)
                    if keep:
                        adj[(i,j)] = val
                        adj[(j,i)] = val
            corr_container['last_result'].setdefault('adjacency', {})  # ensure key exists
            corr_container['last_result']['adjacency'] = adj
            return adj

        # attach to self
        self.corr_analyzer = corr_container
        self.corr_pearson = corr_pearson
        self.corr_spearman = corr_spearman
        self.corr_kendall = corr_kendall
        self.partial_correlation = partial_correlation
        self.correlation_matrix = correlation_matrix
        self.bootstrap_corr_ci = bootstrap_corr_ci
        self.rolling_correlation = rolling_correlation
        self.cross_correlation = cross_correlation
        self.cluster_correlation_matrix = cluster_correlation_matrix
        self.corr_to_adjacency = corr_to_adjacency

        corr_container['initialized'] = True
        return corr_container

    def _init_clustering_analyzer(self):
        """
        ç§‘ç ”çº§èšç±»åˆ†ææ¨¡å—åˆå§‹åŒ–å™¨ï¼ˆKMeans/Hierarchical/DBSCAN + éªŒè¯ä¸ç¨³å®šæ€§ï¼‰
        ç»‘å®šåˆ° self:
        - self.clustering_analyzer (dict)
        - self.cluster_kmeans(data, n_clusters=..., n_init=10, random_state=None)
        - self.cluster_agglomerative(data, n_clusters=..., linkage='average')
        - self.cluster_dbscan(data, eps=0.5, min_samples=5)
        - self.cluster_pca_reduce(data, n_components=2)
        - self.cluster_validation_indices(data, labels)
        - self.cluster_profiles(data, labels)
        - self.cluster_stability_bootstrap(data, cluster_fn, n_boot=100, sample_frac=0.8)
        - self.cluster_consensus(ensemble_labels, n_clusters=None, method='coassoc')
        è¯´æ˜ï¼š
        - ä¼˜å…ˆä½¿ç”¨ sklearnï¼ˆå¦‚å¯ç”¨ï¼‰ï¼Œå¦åˆ™å›é€€ä¸ºçº¯ numpy å®ç°ï¼ˆKMeansï¼‰ã€‚
        - æ‰€æœ‰å‡½æ•°è¿”å›ç»“æ„åŒ–å­—å…¸ï¼Œä¾¿äºè‡ªåŠ¨åŒ–å¤„ç†ã€‚
        """
        import numpy as _np
        from collections import Counter as _Counter
        import math as _math

        # optional sklearn imports
        try:
            from sklearn.cluster import KMeans as _SKKMeans, AgglomerativeClustering as _SKAgg, DBSCAN as _SKDBSCAN
            from sklearn.decomposition import PCA as _SKPCA
            from sklearn.metrics import silhouette_score as _sk_silhouette, calinski_harabasz_score as _sk_ch, davies_bouldin_score as _sk_db
            SKLEARN_AVAILABLE = True
        except Exception:
            _SKKMeans = None; _SKAgg = None; _SKDBSCAN = None; _SKPCA = None
            _sk_silhouette = None; _sk_ch = None; _sk_db = None
            SKLEARN_AVAILABLE = False

        analyzer = {
            'description': 'Research-grade clustering analyzer (kmeans, agglomerative, dbscan, validation, stability, consensus)',
            'params': {'kmeans_n_init': 10, 'bootstrap_n': 100},
            'last_result': {},
            'initialized': True
        }

        # -------------------------
        # helper: ensure 2D numpy array
        # -------------------------
        def _to_2d(X):
            arr = _np.asarray(X, dtype=float)
            if arr.ndim == 1:
                return arr.reshape(-1, 1)
            return arr

        # -------------------------
        # fallback simple KMeans (Lloyd)
        # -------------------------
        def _kmeans_numpy(X, n_clusters=8, n_init=10, max_iter=300, tol=1e-4, random_state=None):
            X = _to_2d(X)
            n_samples, n_features = X.shape
            rng = _np.random.RandomState(int(random_state or 0))
            best_inertia = _np.inf
            best_labels = None
            best_centers = None
            for run in range(int(n_init)):
                # initialize centers by random sampling of points
                idx = rng.choice(n_samples, n_clusters, replace=False) if n_samples >= n_clusters else rng.randint(0, n_samples, size=n_clusters)
                centers = X[idx].astype(float)
                labels = _np.zeros(n_samples, dtype=int)
                for it in range(int(max_iter)):
                    # assign
                    dists = _np.sum((X[:, None, :] - centers[None, :, :])**2, axis=2)  # (n_samples, k)
                    new_labels = _np.argmin(dists, axis=1)
                    # update centers
                    new_centers = _np.zeros_like(centers)
                    for k in range(n_clusters):
                        pts = X[new_labels == k]
                        if len(pts) == 0:
                            # reinitialize empty cluster
                            new_centers[k] = X[rng.randint(0, n_samples)]
                        else:
                            new_centers[k] = pts.mean(axis=0)
                    # check convergence
                    center_shift = _np.sqrt(((new_centers - centers)**2).sum(axis=1)).max()
                    centers = new_centers
                    labels = new_labels
                    if center_shift <= tol:
                        break
                # inertia
                inertia = float(_np.sum((X - centers[labels])**2))
                if inertia < best_inertia:
                    best_inertia = inertia
                    best_labels = labels.copy()
                    best_centers = centers.copy()
            return {'labels': best_labels, 'centers': best_centers, 'inertia': float(best_inertia)}

        # -------------------------
        # KMeans wrapper
        # -------------------------
        def cluster_kmeans(data, n_clusters=8, n_init=None, random_state=None, return_model=False):
            X = _to_2d(data)
            n_init = int(n_init or analyzer['params'].get('kmeans_n_init', 10))
            if SKLEARN_AVAILABLE and _SKKMeans is not None:
                model = _SKKMeans(n_clusters=int(n_clusters), n_init=n_init, random_state=int(random_state or 0))
                labels = model.fit_predict(X)
                centers = model.cluster_centers_
                inertia = float(model.inertia_) if hasattr(model, 'inertia_') else float(_np.sum((X - centers[labels])**2))
                res = {'labels': labels.tolist(), 'centers': centers.tolist(), 'inertia': inertia, 'sk_model': model if return_model else None}
            else:
                out = _kmeans_numpy(X, n_clusters=int(n_clusters), n_init=n_init, random_state=random_state)
                res = {'labels': out['labels'].tolist(), 'centers': out['centers'].tolist(), 'inertia': out['inertia'], 'sk_model': None}
            analyzer['last_result']['kmeans'] = {'n_clusters': int(n_clusters), 'result': res}
            return res

        # -------------------------
        # Agglomerative wrapper
        # -------------------------
        def cluster_agglomerative(data, n_clusters=8, linkage='average'):
            X = _to_2d(data)
            if SKLEARN_AVAILABLE and _SKAgg is not None:
                model = _SKAgg(n_clusters=int(n_clusters), linkage=linkage)
                labels = model.fit_predict(X)
                res = {'labels': labels.tolist(), 'model': model}
            else:
                # fallback: use simple hierarchical via scipy if available, else kmeans fallback
                try:
                    import scipy.cluster.hierarchy as _sch
                    from scipy.spatial.distance import pdist, squareform
                    D = pdist(X, metric='euclidean')
                    Z = _sch.linkage(D, method=linkage)
                    from scipy.cluster.hierarchy import fcluster
                    labels = fcluster(Z, t=int(n_clusters), criterion='maxclust') - 1
                    res = {'labels': labels.tolist(), 'linkage': Z.tolist()}
                except Exception:
                    # fallback to kmeans
                    res = cluster_kmeans(X, n_clusters=int(n_clusters))
            analyzer['last_result']['agglomerative'] = {'n_clusters': int(n_clusters), 'result': res}
            return res

        # -------------------------
        # DBSCAN wrapper
        # -------------------------
        def cluster_dbscan(data, eps=0.5, min_samples=5):
            X = _to_2d(data)
            if SKLEARN_AVAILABLE and _SKDBSCAN is not None:
                model = _SKDBSCAN(eps=float(eps), min_samples=int(min_samples))
                labels = model.fit_predict(X)
                res = {'labels': labels.tolist(), 'core_sample_indices': getattr(model, 'core_sample_indices_', None)}
            else:
                # fallback simple density clustering: label all points as noise (-1)
                n = X.shape[0]
                res = {'labels': [-1]*n}
            analyzer['last_result']['dbscan'] = {'eps': float(eps), 'min_samples': int(min_samples), 'result': res}
            return res

        # -------------------------
        # PCA reduction wrapper
        # -------------------------
        def cluster_pca_reduce(data, n_components=2):
            X = _to_2d(data)
            if SKLEARN_AVAILABLE and _SKPCA is not None:
                pca = _SKPCA(n_components=int(n_components))
                proj = pca.fit_transform(X)
                res = {'proj': proj.tolist(), 'explained_variance_ratio': pca.explained_variance_ratio_.tolist(), 'pca': pca}
            else:
                # basic SVD
                Xc = X - X.mean(axis=0)
                U, S, Vt = _np.linalg.svd(Xc, full_matrices=False)
                proj = U[:, :int(n_components)] * S[:int(n_components)]
                total = float((S**2).sum())
                evr = [(S[i]**2)/total for i in range(min(len(S), int(n_components)))]
                res = {'proj': proj.tolist(), 'explained_variance_ratio': [float(x) for x in evr], 'pca': None}
            analyzer['last_result']['pca'] = res
            return res

        # -------------------------
        # cluster validation indices
        # -------------------------
        def cluster_validation_indices(data, labels):
            X = _to_2d(data)
            labels = _np.asarray(labels, dtype=int)
            unique_labels = _np.unique(labels[labels >= 0])
            n_clusters = len(unique_labels)
            out = {'n_clusters': int(n_clusters)}
            if SKLEARN_AVAILABLE and _sk_silhouette is not None and n_clusters >= 2:
                try:
                    out['silhouette'] = float(_sk_silhouette(X, labels))
                except Exception:
                    out['silhouette'] = None
            else:
                out['silhouette'] = None
            if SKLEARN_AVAILABLE and _sk_ch is not None and n_clusters >= 2:
                try:
                    out['calinski_harabasz'] = float(_sk_ch(X, labels))
                except Exception:
                    out['calinski_harabasz'] = None
            else:
                out['calinski_harabasz'] = None
            if SKLEARN_AVAILABLE and _sk_db is not None and n_clusters >= 2:
                try:
                    out['davies_bouldin'] = float(_sk_db(X, labels))
                except Exception:
                    out['davies_bouldin'] = None
            else:
                out['davies_bouldin'] = None
            # inertia
            try:
                # compute within-cluster sum of squares
                wss = 0.0
                for k in unique_labels:
                    pts = X[labels == k]
                    if len(pts) == 0:
                        continue
                    cen = pts.mean(axis=0)
                    wss += float(((pts - cen)**2).sum())
                out['inertia'] = float(wss)
            except Exception:
                out['inertia'] = None
            analyzer['last_result']['validation'] = out
            return out

        # -------------------------
        # profiles: centroid, size, within-variance
        # -------------------------
        def cluster_profiles(data, labels):
            X = _to_2d(data)
            labels = _np.asarray(labels, dtype=int)
            nodes = {}
            for k in _np.unique(labels[labels >= 0]):
                pts = X[labels == k]
                if len(pts) == 0:
                    continue
                cen = pts.mean(axis=0)
                var = float(((pts - cen)**2).sum(axis=0).mean())
                nodes[int(k)] = {'size': int(len(pts)), 'centroid': cen.tolist(), 'within_var': float(var)}
            analyzer['last_result']['profiles'] = nodes
            return nodes

        # -------------------------
        # bootstrap-based stability: re-sample rows and re-cluster, compare ARI-like agreement (simple)
        # cluster_fn should accept (data, **kwargs) and return dict with 'labels'
        # -------------------------
        def cluster_stability_bootstrap(data, cluster_fn, n_boot=50, sample_frac=0.8, random_state=None, **cluster_kwargs):
            X = _to_2d(data)
            rng = _np.random.RandomState(int(random_state or 0))
            n = X.shape[0]
            n_sub = max(2, int(_math.floor(sample_frac * n)))
            from collections import defaultdict
            coassoc = _np.zeros((n, n), dtype=float)
            counts = _np.zeros((n, n), dtype=int)
            for b in range(int(n_boot)):
                idx = rng.choice(n, n_sub, replace=False)
                Xb = X[idx]
                res = cluster_fn(Xb, **cluster_kwargs)
                labels_b = _np.asarray(res.get('labels', res.get('labels', None)))
                if labels_b is None:
                    continue
                # map labels back to original indices via nearest neighbor matching by row equality (approx)
                # simplest: for each pair i,j in idx increment coassoc if same label
                for i_pos, i in enumerate(idx):
                    for j_pos, j in enumerate(idx):
                        counts[i, j] += 1
                        if labels_b[i_pos] == labels_b[j_pos]:
                            coassoc[i, j] += 1
            # normalize
            with _np.errstate(divide='ignore', invalid='ignore'):
                coassoc_norm = _np.divide(coassoc, _np.maximum(counts, 1))
            analyzer['last_result']['stability_coassoc'] = coassoc_norm.tolist()
            # compute mean pairwise agreement as stability score
            stability_score = float(_np.nanmean(coassoc_norm))
            return {'coassociation': coassoc_norm, 'stability_score': stability_score}

        # -------------------------
        # Consensus clustering via co-association matrix -> hierarchical clustering
        # ensemble_labels: list of label arrays (each length n_samples)
        # -------------------------
        def cluster_consensus(ensemble_labels, n_clusters=None, method='coassoc'):
            # ensemble_labels: list of iterables of labels
            if not ensemble_labels:
                return {'error': 'empty ensemble'}
            labels_arr = [_np.asarray(l) for l in ensemble_labels]
            n = labels_arr[0].shape[0]
            m = len(labels_arr)
            coassoc = _np.zeros((n, n), dtype=float)
            for l in labels_arr:
                for i in range(n):
                    for j in range(n):
                        if l[i] == l[j]:
                            coassoc[i, j] += 1
            coassoc = coassoc / float(m)
            # convert to distance
            dist = 1.0 - coassoc
            # use hierarchical clustering on dist matrix to get final labels
            try:
                import scipy.cluster.hierarchy as _sch
                from scipy.spatial.distance import squareform
                # condensed distance
                cond = squareform(dist, checks=False)
                Z = _sch.linkage(cond, method='average')
                from scipy.cluster.hierarchy import fcluster
                if n_clusters is None:
                    # heuristic: use elbow -> here pick 2
                    n_clusters = 2
                labels = fcluster(Z, t=int(n_clusters), criterion='maxclust') - 1
                return {'labels': labels.tolist(), 'coassociation': coassoc.tolist()}
            except Exception:
                # fallback: simple majority voting per pair via threshold
                # Convert coassoc -> graph of edges where coassoc>0.5 and take connected components
                thr = 0.5
                visited = set()
                comps = []
                adj = {i: [j for j in range(n) if coassoc[i, j] > thr and i != j] for i in range(n)}
                for i in range(n):
                    if i in visited:
                        continue
                    stack = [i]; comp = []
                    while stack:
                        u = stack.pop()
                        if u in visited:
                            continue
                        visited.add(u)
                        comp.append(u)
                        for v in adj.get(u, []):
                            if v not in visited:
                                stack.append(v)
                    comps.append(comp)
                labels = _np.full(n, -1, dtype=int)
                for cid, comp in enumerate(comps):
                    for v in comp:
                        labels[v] = cid
                return {'labels': labels.tolist(), 'coassociation': coassoc.tolist()}

        # -------------------------
        # attach to self
        # -------------------------
        self.clustering_analyzer = analyzer
        self.cluster_kmeans = cluster_kmeans
        self.cluster_agglomerative = cluster_agglomerative
        self.cluster_dbscan = cluster_dbscan
        self.cluster_pca_reduce = cluster_pca_reduce
        self.cluster_validation_indices = cluster_validation_indices
        self.cluster_profiles = cluster_profiles
        self.cluster_stability_bootstrap = cluster_stability_bootstrap
        self.cluster_consensus = cluster_consensus

        analyzer['initialized'] = True
        return analyzer

    def _init_anomaly_detector(self):
        """
        Research-grade Anomaly Detection module initializer.
        Binds:
        - self.anomaly_detector (dict): metadata, params, fitted models, last results
        - self.fit_detector(X, method='isolation_forest', params=None)
        - self.score_anomalies(X, method=None) -> array of anomaly scores (higher = more anomalous)
        - self.predict_anomalies(X, method=None, threshold='quantile', q=0.975) -> boolean mask
        - self.detect_rolling(X_series, window, method='zscore', thresh=3.0)
        - self.explain_anomaly_by_feature_influence(x_single, baseline='median') -> contributions
        - self.evaluate_detector(y_true, y_pred_mask) -> precision/recall/F1/roc-auc (if applicable)
        - self.fit_reconstruction_autoencoder(X, latent_dim=8, epochs=100, batch_size=64)
        - self.reconstruction_scores(X)
        Design:
        - Prefer sklearn; optional PyTorch/TensorFlow autoencoder if available.
        - Provide robust statistical fallbacks if ML packages unavailable.
        - Return dictionaries with interpretable fields.
        """
        import numpy as _np
        from collections import defaultdict as _defaultdict
        import math as _math

        # try optional libs
        try:
            from sklearn.ensemble import IsolationForest as _SKFIF
            from sklearn.svm import OneClassSVM as _SKOCSVM
            from sklearn.covariance import EllipticEnvelope as _SKEE
            from sklearn.neighbors import LocalOutlierFactor as _SKLOF
            from sklearn.preprocessing import StandardScaler as _SKScaler
            from sklearn.neural_network import MLPRegressor as _SKMLP
            SKLEARN_AVAILABLE = True
        except Exception:
            _SKFIF = _SKOCSVM = _SKEE = _SKLOF = _SKScaler = _SKMLP = None
            SKLEARN_AVAILABLE = False

        # try torch for autoencoder (optional)
        try:
            import torch as _torch
            import torch.nn as _nn
            TORCH_AVAILABLE = True
        except Exception:
            _torch = None; _nn = None; TORCH_AVAILABLE = False

        detector = {
            'description': 'Anomaly detection suite (statistical + ML + reconstruction)',
            'params': {
                'standardize': True,
                'default_method': 'isolation_forest',
                'outlier_quantile': 0.975,
                'robust_thresh_z': 3.0
            },
            'models': {},         # store fitted detectors keyed by method
            'reconstruction': {}, # autoencoder artifacts
            'last_result': {},
            'initialized': True
        }

        # -------------------- helpers --------------------
        def _to_2d(X):
            arr = _np.asarray(X, dtype=float)
            if arr.ndim == 1:
                arr = arr.reshape(-1, 1)
            return arr

        def _standardize_fit(X):
            if SKLEARN_AVAILABLE and _SKScaler is not None:
                scaler = _SKScaler()
                Xs = scaler.fit_transform(X)
                return Xs, scaler
            else:
                mu = _np.nanmean(X, axis=0)
                sigma = _np.nanstd(X, axis=0)
                sigma[sigma == 0] = 1.0
                Xs = (X - mu) / sigma
                return Xs, {'mu': mu, 'sigma': sigma}

        def _standardize_transform(X, scaler):
            if SKLEARN_AVAILABLE and _SKScaler is not None and hasattr(scaler, 'transform'):
                return scaler.transform(X)
            else:
                mu = scaler['mu']; sigma = scaler['sigma']
                return (X - mu) / sigma

        # -------------------- primary fit function --------------------
        def fit_detector(X, method=None, params=None, standardize=None, random_state=None):
            """
            X: 2D array-like (n_samples x n_features)
            method: 'isolation_forest'|'oneclass_svm'|'elliptic_envelope'|'lof'|'zscore'|'mad'|'mahalanobis'|'reconstruction'
            params: dict passed to underlying estimator
            Returns: dict with fitted model metadata
            """
            X = _to_2d(X)
            method = method or detector['params']['default_method']
            params = params or {}
            standardize_flag = detector['params'].get('standardize', True) if standardize is None else bool(standardize)

            model_meta = {'method': method, 'n_samples': X.shape[0], 'n_features': X.shape[1]}
            # standardize if requested (store scaler)
            if standardize_flag:
                Xs, scaler = _standardize_fit(X)
                model_meta['scaler'] = scaler
            else:
                Xs = X
                model_meta['scaler'] = None

            # choose algorithm
            if method == 'isolation_forest' and SKLEARN_AVAILABLE and _SKFIF is not None:
                clf = _SKFIF(n_estimators=params.get('n_estimators', 200),
                            max_samples=params.get('max_samples', 'auto'),
                            contamination=params.get('contamination', 'auto'),
                            random_state=int(random_state or 0))
                clf.fit(Xs)
                detector['models'][method] = clf
                model_meta['estimator'] = 'sklearn.IsolationForest'
            elif method == 'oneclass_svm' and SKLEARN_AVAILABLE and _SKOCSVM is not None:
                clf = _SKOCSVM(nu=params.get('nu', 0.05), kernel=params.get('kernel', 'rbf'), gamma=params.get('gamma', 'scale'))
                clf.fit(Xs)
                detector['models'][method] = clf
                model_meta['estimator'] = 'sklearn.OneClassSVM'
            elif method == 'elliptic_envelope' and SKLEARN_AVAILABLE and _SKEE is not None:
                clf = _SKEE(contamination=params.get('contamination', 0.05), support_fraction=params.get('support_fraction', None))
                clf.fit(Xs)
                detector['models'][method] = clf
                model_meta['estimator'] = 'sklearn.EllipticEnvelope'
            elif method == 'lof' and SKLEARN_AVAILABLE and _SKLOF is not None:
                # LOF is unsupervised and has no predict for new data unless used with novelty=False; use fit_predict on train
                clf = _SKLOF(n_neighbors=params.get('n_neighbors', 20), contamination=params.get('contamination', 0.05), novelty=params.get('novelty', True))
                clf.fit(Xs)
                detector['models'][method] = clf
                model_meta['estimator'] = 'sklearn.LocalOutlierFactor'
            elif method == 'reconstruction':
                # fit autoencoder (sklearn MLP fallback or PyTorch if available)
                res = fit_reconstruction_autoencoder(X, latent_dim=params.get('latent_dim', 8),
                                                    epochs=params.get('epochs', 100),
                                                    batch_size=params.get('batch_size', 64),
                                                    random_state=random_state)
                detector['reconstruction'] = res
                model_meta['estimator'] = 'reconstruction_autoencoder'
                detector['models'][method] = 'reconstruction_autoencoder'
            elif method in ('zscore', 'mad', 'mahalanobis'):
                # statistical detectors - nothing to fit beyond storing central tendency/scale/cov
                if method == 'zscore':
                    mu = _np.nanmean(X, axis=0)
                    sigma = _np.nanstd(X, axis=0)
                    sigma[sigma == 0] = 1.0
                    model_meta['mu'] = mu; model_meta['sigma'] = sigma
                elif method == 'mad':
                    med = _np.nanmedian(X, axis=0)
                    mad = _np.median(_np.abs(X - med), axis=0)
                    mad[mad == 0] = 1.0
                    model_meta['median'] = med; model_meta['mad'] = mad
                else:  # mahalanobis
                    cov = _np.cov(X, rowvar=False)
                    # regularize
                    cov += _np.eye(cov.shape[0]) * 1e-6
                    try:
                        cov_inv = _np.linalg.inv(cov)
                    except Exception:
                        cov_inv = _np.linalg.pinv(cov)
                    mean = _np.nanmean(X, axis=0)
                    model_meta['mean'] = mean; model_meta['cov_inv'] = cov_inv
                detector['models'][method] = None
            else:
                # fallback: store empirical median/sd to allow zscore detection
                mu = _np.nanmean(X, axis=0)
                sigma = _np.nanstd(X, axis=0)
                sigma[sigma == 0] = 1.0
                model_meta['mu'] = mu; model_meta['sigma'] = sigma
                detector['models'][method] = None
                model_meta['estimator'] = 'naive_statistical'

            detector['last_result']['fit'] = model_meta
            return model_meta

        # -------------------- scoring & prediction --------------------
        def score_anomalies(X, method=None, transform=True):
            """
            Return anomaly score array (higher = more anomalous).
            For scikit detectors: use decision_function (negated so higher = more anomalous), or -score_samples.
            For statistical detectors: return abs(zscore) or MAD scaled, or Mahalanobis distance.
            """
            X = _to_2d(X)
            method = method or detector['params']['default_method']
            model_meta = detector['last_result'].get('fit', {}) if detector.get('last_result') else {}
            scaler = model_meta.get('scaler', None)
            if transform and scaler is not None:
                if SKLEARN_AVAILABLE and isinstance(scaler, _SKScaler):
                    Xs = scaler.transform(X)
                else:
                    # dict-based scaler
                    Xs = _standardize_transform(X, scaler)
            else:
                Xs = X

            # sklearn-based detectors
            if method in detector['models'] and detector['models'].get(method) is not None and SKLEARN_AVAILABLE:
                clf = detector['models'][method]
                # special-case LOF: if novelty True, we can call decision_function
                if hasattr(clf, 'decision_function'):
                    try:
                        df = clf.decision_function(Xs)
                        # decision_function: higher = more normal for many sklearn models, so invert to make higher=anomalous
                        scores = -1.0 * _np.asarray(df).astype(float)
                        return scores
                    except Exception:
                        pass
                if hasattr(clf, 'score_samples'):
                    try:
                        ss = clf.score_samples(Xs)
                        scores = -1.0 * _np.asarray(ss).astype(float)
                        return scores
                    except Exception:
                        pass
                # some estimators (LOF without novelty) may require fit_predict; fallback:
                try:
                    pred = clf.fit_predict(Xs)
                    # LOF: -1 means outlier, 1 inlier; map to scores
                    scores = _np.where(pred == -1, 1.0, 0.0)
                    return scores.astype(float)
                except Exception:
                    pass

            # reconstruction-based
            if method == 'reconstruction' and detector.get('reconstruction'):
                return reconstruction_scores(X)

            # statistical detectors
            if method == 'zscore':
                mu = model_meta.get('mu'); sigma = model_meta.get('sigma')
                if mu is None:
                    mu = _np.nanmean(X, axis=0); sigma = _np.nanstd(X, axis=0)
                z = _np.abs((X - mu) / (sigma + 1e-12))
                # combine per-feature to one score: max or norm; use max for sensitivity
                sc = _np.max(z, axis=1)
                return sc
            if method == 'mad':
                med = model_meta.get('median'); mad = model_meta.get('mad')
                if med is None:
                    med = _np.nanmedian(X, axis=0); mad = _np.median(_np.abs(X - med), axis=0)
                mads = _np.abs(X - med) / (mad + 1e-12)
                sc = _np.max(mads, axis=1)
                return sc
            if method == 'mahalanobis':
                mean = model_meta.get('mean'); cov_inv = model_meta.get('cov_inv')
                if mean is None or cov_inv is None:
                    mean = _np.nanmean(X, axis=0)
                    cov = _np.cov(X, rowvar=False) + _np.eye(X.shape[1]) * 1e-6
                    try:
                        cov_inv = _np.linalg.inv(cov)
                    except Exception:
                        cov_inv = _np.linalg.pinv(cov)
                diff = X - mean
                # md^2
                md2 = _np.sum((diff @ cov_inv) * diff, axis=1)
                return _np.sqrt(_np.maximum(md2, 0.0))

            # fallback: use zscore by default
            mu = _np.nanmean(X, axis=0); sigma = _np.nanstd(X, axis=0)
            sigma[sigma == 0] = 1.0
            sc = _np.max(_np.abs((X - mu) / sigma), axis=1)
            return sc

        def predict_anomalies(X, method=None, threshold='quantile', q=None, fixed_threshold=None, invert=False):
            """
            Return boolean mask: True = anomaly
            threshold: 'quantile' or 'fixed'
            q: quantile for top anomalies (e.g., 0.975)
            fixed_threshold: specify numeric threshold on score
            invert: if True, treat score small => anomaly
            """
            X = _to_2d(X)
            scores = score_anomalies(X, method=method)
            if q is None:
                q = detector['params'].get('outlier_quantile', 0.975)
            if threshold == 'quantile':
                thr = _np.nanquantile(scores, q)
            elif threshold == 'fixed' and fixed_threshold is not None:
                thr = float(fixed_threshold)
            else:
                thr = detector['params'].get('robust_thresh_z', 3.0)
            if invert:
                mask = scores <= thr
            else:
                mask = scores >= thr
            return mask.astype(bool)

        # -------------------- rolling / time series helpers --------------------
        def detect_rolling(X_series, window=50, method='zscore', thresh=None, step=1):
            """
            X_series: 2D array-like (time x features) or 1D (time)
            sliding window anomaly detection: compute model on window and score the trailing point
            returns dict with 'times', 'scores', 'flags'
            """
            arr = _to_2d(X_series)
            n = arr.shape[0]
            if thresh is None:
                thresh = detector['params'].get('robust_thresh_z', 3.0)
            scores = _np.full(n, _np.nan)
            flags = _np.zeros(n, dtype=bool)
            for i in range(window, n, step):
                window_X = arr[i-window:i]
                model_meta = fit_detector(window_X, method=method)
                s = score_anomalies(arr[i:i+1], method=method)
                scores[i] = float(s[0])
                flags[i] = bool(s[0] >= thresh)
            return {'scores': scores.tolist(), 'flags': flags.tolist(), 'window': int(window), 'method': method}

        # -------------------- reconstruction autoencoder --------------------
        def fit_reconstruction_autoencoder(X, latent_dim=8, epochs=50, batch_size=64, random_state=None):
            """
            Fit a simple reconstruction model.
            If PyTorch available -> use small torch AE (fast). Else if sklearn MLPRegressor available -> fit symmetric MLP.
            Returns dict with 'model' (callable or trained object), 'reconstruction_train_error' etc.
            """
            X = _to_2d(X)
            n, d = X.shape
            rng = _np.random.RandomState(int(random_state or 0))

            # sklearn MLP autoencoder fallback
            if SKLEARN_AVAILABLE and _SKMLP is not None and not TORCH_AVAILABLE:
                # symmetric hidden layer sizes
                hidden = tuple(max(1, int(latent_dim)))
                # Build two-stage regressor: encoder + decoder fused into single MLP that maps X->X (autoencoder)
                mlp = _SKMLP(hidden_layer_sizes=(max(8, latent_dim),), activation='relu', max_iter=epochs, random_state=int(random_state or 0))
                mlp.fit(X, X)
                # compute train reconstruction error (MSE per sample)
                recon = mlp.predict(X)
                mse = _np.mean(_np.sum((X - recon)**2, axis=1))
                res = {'framework': 'sklearn_mlp', 'model': mlp, 'train_mse': float(mse)}
                detector['reconstruction'] = res
                return res

            if TORCH_AVAILABLE:
                # lightweight torch autoencoder
                import torch.utils.data as _data_utils
                X_t = _torch.tensor(X.astype(_np.float32))
                dataset = _data_utils.TensorDataset(X_t)
                loader = _data_utils.DataLoader(dataset, batch_size=int(batch_size), shuffle=True)
                class AutoEncoder(_nn.Module):
                    def __init__(self, d_in, d_lat):
                        super(AutoEncoder, self).__init__()
                        self.enc = _nn.Sequential(_nn.Linear(d_in, max(d_lat, 4)), _nn.ReLU(), _nn.Linear(max(d_lat,4), d_lat))
                        self.dec = _nn.Sequential(_nn.Linear(d_lat, max(d_lat, 4)), _nn.ReLU(), _nn.Linear(max(d_lat,4), d_in))
                    def forward(self, x):
                        z = self.enc(x)
                        return self.dec(z)
                ae = AutoEncoder(d, latent_dim)
                opt = _torch.optim.Adam(ae.parameters(), lr=1e-3)
                loss_fn = _nn.MSELoss()
                ae.train()
                for ep in range(int(epochs)):
                    for (batch,) in loader:
                        opt.zero_grad()
                        out = ae(batch)
                        loss = loss_fn(out, batch)
                        loss.backward()
                        opt.step()
                ae.eval()
                with _torch.no_grad():
                    recon = ae(_torch.tensor(X.astype(_np.float32))).numpy()
                mse = _np.mean(_np.sum((X - recon)**2, axis=1))
                res = {'framework': 'torch_ae', 'model': ae, 'train_mse': float(mse)}
                detector['reconstruction'] = res
                return res

            # last resort: PCA reconstruction (linear)
            try:
                # compute low-rank SVD reconstruction using latent_dim components
                U, S, Vt = _np.linalg.svd(X - X.mean(axis=0), full_matrices=False)
                k = min(int(latent_dim), X.shape[1])
                recon = (U[:, :k] * S[:k]) @ Vt[:k, :] + X.mean(axis=0)
                mse = _np.mean(_np.sum((X - recon)**2, axis=1))
                res = {'framework': 'svd_linear', 'k': k, 'train_mse': float(mse)}
                detector['reconstruction'] = res
                return res
            except Exception as e:
                return {'error': f'reconstruction_fit_failed: {e}'}

        def reconstruction_scores(X):
            """
            Compute reconstruction error for X using fitted reconstruction model; return per-sample MSE
            """
            X = _to_2d(X)
            recon_meta = detector.get('reconstruction', {})
            if recon_meta is None or not recon_meta:
                return _np.full(X.shape[0], _np.nan)
            framework = recon_meta.get('framework')
            if framework == 'sklearn_mlp':
                mlp = recon_meta['model']
                recon = mlp.predict(X)
                mse = _np.sum((X - recon)**2, axis=1)
                return mse
            if framework == 'torch_ae' and TORCH_AVAILABLE:
                ae = recon_meta['model']
                import torch as _torch_local
                ae.eval()
                with _torch_local.no_grad():
                    recon = ae(_torch_local.tensor(X.astype(_np.float32))).numpy()
                mse = _np.sum((X - recon)**2, axis=1)
                return mse
            if framework == 'svd_linear':
                # reconstruct using stored SVD approach (we stored no explicit matrices, so recompute quick SVD)
                try:
                    U, S, Vt = _np.linalg.svd(X - X.mean(axis=0), full_matrices=False)
                    k = recon_meta.get('k', min(X.shape[1], 1))
                    recon = (U[:, :k] * S[:k]) @ Vt[:k, :] + X.mean(axis=0)
                    mse = _np.sum((X - recon)**2, axis=1)
                    return mse
                except Exception:
                    return _np.full(X.shape[0], _np.nan)
            return _np.full(X.shape[0], _np.nan)

        # -------------------- explain anomaly --------------------
        def explain_anomaly_by_feature_influence(x_single, baseline=None, top_k=5):
            """
            For a single observation x_single, return per-feature contribution to anomaly score.
            baseline: reference (median or mean) to compare; default median of training data if available.
            Method: difference * feature-wise normalized scale.
            """
            x = _np.asarray(x_single).ravel()
            # try to use reconstruction error per-feature if available
            recon_meta = detector.get('reconstruction', {})
            if recon_meta:
                # if recon model exists, compute reconstruction and per-feature squared error
                if recon_meta.get('framework') == 'sklearn_mlp':
                    mlp = recon_meta['model']
                    recon = mlp.predict(x.reshape(1, -1))[0]
                    per_feat = (x - recon)**2
                    idx = _np.argsort(per_feat)[::-1][:top_k]
                    return {'per_feature_error': per_feat.tolist(), 'top_idx': idx.tolist()}
            # else fallback to difference from baseline divided by scale
            # baseline: median stored in last fit if exists
            last_fit = detector.get('last_result', {}).get('fit', {})
            scaler = last_fit.get('scaler')
            if baseline is None:
                if scaler and not (SKLEARN_AVAILABLE and isinstance(scaler, _SKScaler)):
                    base = scaler.get('mu') if isinstance(scaler, dict) else None
                else:
                    base = None
            if baseline is None:
                # fallback median
                base = _np.zeros_like(x)
            diff = _np.abs(x - base)
            idx = _np.argsort(diff)[::-1][:top_k]
            return {'diff': diff.tolist(), 'top_idx': idx.tolist()}

        # -------------------- evaluation --------------------
        def evaluate_detector(y_true, y_pred_mask):
            """
            y_true: binary array-like (1 anomaly / 0 normal)
            y_pred_mask: boolean mask from predict_anomalies
            returns dict with precision, recall, f1, support
            """
            y_true = _np.asarray(y_true).astype(int)
            y_pred = _np.asarray(y_pred_mask).astype(int)
            tp = int(_np.sum((y_true == 1) & (y_pred == 1)))
            fp = int(_np.sum((y_true == 0) & (y_pred == 1)))
            fn = int(_np.sum((y_true == 1) & (y_pred == 0)))
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            return {'precision': precision, 'recall': recall, 'f1': f1, 'tp': tp, 'fp': fp, 'fn': fn}

        # -------------------- attach to self --------------------
        self.anomaly_detector = detector
        self.fit_detector = fit_detector
        self.score_anomalies = score_anomalies
        self.predict_anomalies = predict_anomalies
        self.detect_rolling = detect_rolling
        self.fit_reconstruction_autoencoder = fit_reconstruction_autoencoder
        self.reconstruction_scores = reconstruction_scores
        self.explain_anomaly_by_feature_influence = explain_anomaly_by_feature_influence
        self.evaluate_detector = evaluate_detector

        detector['initialized'] = True
        return detector
    
    def _init_trend_analyzer(self):
        """
        Research-grade trend analysis module initializer.

        Binds to self:
        - self.trend_analyzer (dict)
        - self.trend_fit_linear(series, add_intercept=True)
        - self.trend_fit_theilsen(series)
        - self.trend_mann_kendall(series)
        - self.trend_decompose_stl(series, period=None)
        - self.trend_hp_filter(series, lamb=1600)
        - self.trend_change_point_detection(series, method='ruptures', pen=None, n_bkps=3)
        - self.trend_bootstrap_slope_ci(series, method='theil-sen', n_boot=500, block_length=None, alpha=0.05)
        - self.trend_forecast_arima(series, order=None, seasonal_order=None, steps=10)
        - self.trend_summary(series, period=None, do_decompose=True, do_change_points=True)
        Design notes:
        - Prefer statsmodels / scipy / ruptures when available; provide robust fallbacks.
        - Use block bootstrap for time series (non-overlapping blocks) for slope CI.
        """
        import numpy as _np
        import math as _math
        from collections import defaultdict as _defaultdict

        # optional libs
        try:
            from scipy import stats as _scipy_stats
            from scipy.stats import theilslopes as _scipy_theilslopes
            SCIPY_AVAILABLE = True
        except Exception:
            _scipy_stats = None
            _scipy_theilslopes = None
            SCIPY_AVAILABLE = False

        try:
            import statsmodels.api as _sm
            from statsmodels.tsa.seasonal import STL as _STL
            from statsmodels.tsa.filters.hp_filter import hpfilter as _hpfilter
            STATSMODELS_AVAILABLE = True
        except Exception:
            _sm = None
            _STL = None
            _hpfilter = None
            STATSMODELS_AVAILABLE = False

        try:
            import ruptures as _ruptures
            RUPTURES_AVAILABLE = True
        except Exception:
            _ruptures = None
            RUPTURES_AVAILABLE = False

        trend = {
            'description': 'Research-grade trend analyzer (linear, robust, MK, STL, HP, change points, bootstrap CI, ARIMA forecast)',
            'params': {
                'bootstrap_n': 500,
                'bootstrap_block_length': None,
                'mann_kendall_alpha': 0.05,
                'theil_sen_consider_nan': True,
                'hp_lambda': 1600  # quarterly series default; user can override
            },
            'results': {},
            'initialized': True
        }

        def _to_1d(series):
            a = _np.asarray(series, dtype=float)
            if a.ndim > 1:
                a = a.ravel()
            return a

        # -----------------------
        # Linear OLS trend
        # -----------------------
        def trend_fit_linear(series, add_intercept=True):
            """
            Fit y = a + b*t  (or y = b*t if add_intercept=False)
            Returns dict: {'slope':..., 'intercept':..., 't_slope':..., 'p_slope':..., 'r2':..., 'n':...}
            """
            y = _to_1d(series)
            n = y.size
            t = _np.arange(n).astype(float)
            # remove NaNs pairwise
            mask = ~_np.isnan(y)
            if mask.sum() < 3:
                return {'error': 'not enough valid points'}
            yv = y[mask]; tv = t[mask]
            if add_intercept:
                X = _np.column_stack([_np.ones_like(tv), tv])
            else:
                X = tv.reshape(-1,1)
            try:
                # OLS via normal equations (stable enough for small dims)
                beta, *_ = _np.linalg.lstsq(X, yv, rcond=None)
                if add_intercept:
                    intercept, slope = float(beta[0]), float(beta[1])
                else:
                    intercept = 0.0
                    slope = float(beta[0])
                # residuals, sse, se
                y_pred = (X @ beta).ravel()
                resid = yv - y_pred
                sse = float((resid**2).sum())
                dof = max(1, len(yv) - X.shape[1])
                s2 = sse / dof
                # covariance
                XtX_inv = _np.linalg.pinv(X.T @ X)
                se_beta = _np.sqrt(_np.diag(XtX_inv) * s2)
                # slope t-test
                slope_se = se_beta[-1]
                t_stat = slope / (slope_se if slope_se > 0 else _np.nan)
                p_val = None
                if SCIPY_AVAILABLE:
                    # two-sided p
                    p_val = float(2.0 * _scipy_stats.t.sf(abs(t_stat), df=dof))
                else:
                    p_val = _np.nan
                # R2
                ss_tot = float(((yv - yv.mean())**2).sum())
                r2 = 1.0 - sse/ss_tot if ss_tot > 0 else 0.0
                res = {
                    'slope': float(slope),
                    'intercept': float(intercept),
                    'slope_se': float(slope_se),
                    't_slope': float(t_stat) if not _np.isnan(t_stat) else None,
                    'p_slope': p_val,
                    'r2': float(r2),
                    'n': int(len(yv))
                }
                trend['results']['linear'] = res
                return res
            except Exception as e:
                return {'error': f'linear_fit_failed: {e}'}

        # -----------------------
        # Theil-Sen robust slope
        # -----------------------
        def trend_fit_theilsen(series):
            """
            Robust slope estimator: Theil-Sen. If scipy available, call scipy.stats.theilslopes.
            Returns {'slope':..., 'intercept':..., 'lower_slope':..., 'upper_slope':...}
            """
            y = _to_1d(series)
            mask = ~_np.isnan(y)
            if mask.sum() < 3:
                return {'error': 'not enough valid points'}
            yv = y[mask]
            tv = _np.arange(len(y))[mask].astype(float)
            try:
                if SCIPY_AVAILABLE and _scipy_theilslopes is not None:
                    slope, intercept, lower, upper = _scipy_theilslopes(yv, tv, 0.95)
                    res = {'slope': float(slope), 'intercept': float(intercept), 'slope_ci_lower': float(lower), 'slope_ci_upper': float(upper)}
                else:
                    # fallback median of slopes
                    n = len(tv)
                    slopes = []
                    for i in range(n-1):
                        dx = tv[i+1:] - tv[i]
                        dy = yv[i+1:] - yv[i]
                        valid = dx != 0
                        slopes.extend((dy[valid]/dx[valid]).tolist())
                    if not slopes:
                        return {'error': 'no slopes computed'}
                    slope = float(_np.median(slopes))
                    intercept = float(_np.median(yv - slope * tv))
                    res = {'slope': slope, 'intercept': intercept, 'slope_ci_lower': None, 'slope_ci_upper': None}
                trend['results']['theil_sen'] = res
                return res
            except Exception as e:
                return {'error': f'theil_sen_failed: {e}'}

        # -----------------------
        # Mann-Kendall trend test (nonparametric)
        # -----------------------
        def trend_mann_kendall(series):
            """
            Implements Mann-Kendall test with normal approximation for large n.
            Returns {'S':..., 'varS':..., 'Z':..., 'p_two_sided':..., 'trend': 'increasing'/'decreasing'/'no trend'}
            """
            y = _to_1d(series)
            y = y[~_np.isnan(y)]
            n = len(y)
            if n < 6:
                return {'error': 'series too short for reliable MK test'}
            S = 0
            ties = {}
            for i in range(n-1):
                diff = y[i+1:] - y[i]
                pos = _np.sum(diff > 0)
                neg = _np.sum(diff < 0)
                S += (pos - neg)
                # collect ties counts for var correction
                for val in diff:
                    if val == 0:
                        ties[val] = ties.get(val, 0) + 1
            # variance of S with tie correction
            # tie handling simplified: assume few exact ties; use standard var formula ignoring ties if none
            # compute varS
            varS = (n*(n-1)*(2*n+5)) / 18.0
            # ties correction not fully implemented here; for many ties user should use exact method via scipy or specialized package
            if S > 0:
                Z = (S - 1) / _math.sqrt(varS)
            elif S < 0:
                Z = (S + 1) / _math.sqrt(varS)
            else:
                Z = 0.0
            # two-sided p-value
            try:
                if SCIPY_AVAILABLE:
                    p_two = float(2.0 * _scipy_stats.norm.sf(abs(Z)))
                else:
                    # normal approx using math.erfc
                    p_two = float(_math.erfc(abs(Z) / _math.sqrt(2)))
            except Exception:
                p_two = _np.nan
            trend_dir = 'no trend'
            if p_two < trend['params']['mann_kendall_alpha']:
                trend_dir = 'increasing' if Z > 0 else 'decreasing'
            res = {'S': int(S), 'varS': float(varS), 'Z': float(Z), 'p_two_sided': p_two, 'trend': trend_dir, 'n': n}
            trend['results']['mann_kendall'] = res
            return res

        # -----------------------
        # STL decomposition
        # -----------------------
        def trend_decompose_stl(series, period=None, robust=True):
            """
            STL decomposition using statsmodels if available; fallback to simple seasonal mean subtraction if not.
            Returns dict with 'trend', 'seasonal', 'resid' (lists) and summary stats.
            """
            y = _to_1d(series)
            n = len(y)
            if period is None:
                # heuristic: try to detect seasonality by autocorrelation peak
                if n < 8:
                    period = None
                else:
                    acf = _np.correlate(y - _np.nanmean(y), y - _np.nanmean(y), mode='full')[n-1:]
                    # ignore lag 0
                    if len(acf) > 2:
                        peak = int(_np.argmax(acf[1:]) + 1)
                        period = max(1, peak)
                    else:
                        period = None
            if STATSMODELS_AVAILABLE and _STL is not None and period is not None and period >= 2:
                try:
                    # statsmodels expects no NaNs
                    mask = ~_np.isnan(y)
                    if mask.sum() < 3:
                        return {'error': 'too few valid points'}
                    stl = _STL(y[mask], period=int(period), robust=bool(robust))
                    res = stl.fit()
                    # expand back to original length with NaNs where appropriate
                    trend_seg = _np.full(n, _np.nan); seasonal = _np.full(n, _np.nan); resid = _np.full(n, _np.nan)
                    trend_seg[mask] = res.trend
                    seasonal[mask] = res.seasonal
                    resid[mask] = res.resid
                    out = {'trend': trend_seg.tolist(), 'seasonal': seasonal.tolist(), 'resid': resid.tolist(), 'period': int(period)}
                except Exception as e:
                    out = {'error': f'stl_failed: {e}'}
            else:
                # fallback: simple seasonal-mean decomposition if period found
                if period is None or period < 2 or n < period * 2:
                    # no seasonality detected -> trend = moving average
                    window = max(3, int(min(51, n//10)))
                    trend_seg = _np.convolve(_np.nan_to_num(y, nan=_np.nanmean(y)), _np.ones(window)/window, mode='same')
                    seasonal = _np.zeros(n)
                    resid = y - trend_seg
                    out = {'trend': trend_seg.tolist(), 'seasonal': seasonal.tolist(), 'resid': resid.tolist(), 'period': None}
                else:
                    # seasonal mean per phase
                    seasonal = _np.zeros(n)
                    for i in range(n):
                        idxs = list(range(i % period, n, period))
                        seasonal[i] = _np.nanmean(y[idxs])
                    trend_seg = y - seasonal
                    resid = _np.nan_to_num(y) - trend_seg - seasonal
                    out = {'trend': trend_seg.tolist(), 'seasonal': seasonal.tolist(), 'resid': resid.tolist(), 'period': int(period)}
            trend['results']['decompose'] = out
            return out

        # -----------------------
        # Hodrick-Prescott filter
        # -----------------------
        def trend_hp_filter(series, lamb=None):
            y = _to_1d(series)
            if lamb is None:
                lamb = trend['params'].get('hp_lambda', 1600)
            if STATSMODELS_AVAILABLE and _hpfilter is not None:
                try:
                    cycle, trend_comp = _hpfilter(y, lamb=lamb)
                    out = {'trend': trend_comp.tolist(), 'cycle': cycle.tolist(), 'lambda': float(lamb)}
                except Exception as e:
                    out = {'error': f'hpfilter_failed: {e}'}
            else:
                # fallback: use simple low-pass via moving average with window approx sqrt(n)
                n = len(y)
                window = max(3, int(_np.sqrt(n)))
                trend_comp = _np.convolve(_np.nan_to_num(y, nan=_np.nanmean(y)), _np.ones(window)/window, mode='same')
                cycle = y - trend_comp
                out = {'trend': trend_comp.tolist(), 'cycle': cycle.tolist(), 'lambda': None}
            trend['results']['hp_filter'] = out
            return out

        # -----------------------
        # change point / piecewise trend detection
        # -----------------------
        def trend_change_point_detection(series, method='ruptures', pen=None, n_bkps=3):
            """
            Detect change points in mean/trend. Prefer ruptures PELT with 'rbf' or 'linear' model.
            Returns {'change_points': [...], 'method': ...}
            """
            y = _to_1d(series)
            n = len(y)
            if n < 10:
                return {'error': 'series too short'}
            if method == 'ruptures' and RUPTURES_AVAILABLE:
                try:
                    # use "linear" model for piecewise linear trend detection if available
                    algo = _ruptures.Pelt(model="linear").fit(y)
                    pen = pen if pen is not None else 3 * _np.std(y)  # heuristic
                    bkps = algo.predict(pen=pen)  # returns breakpoints
                    trend['results']['change_points'] = bkps
                    return {'method': 'ruptures_pelt_linear', 'change_points': bkps}
                except Exception:
                    # fallback to rbf
                    try:
                        algo = _ruptures.Pelt(model="rbf").fit(y)
                        pen = pen if pen is not None else 3 * _np.std(y)
                        bkps = algo.predict(pen=pen)
                        trend['results']['change_points'] = bkps
                        return {'method': 'ruptures_pelt_rbf', 'change_points': bkps}
                    except Exception as e:
                        return {'error': f'ruptures_failed: {e}'}
            else:
                # simple heuristic: detect large jumps in rolling slope
                window = max(3, int(min(50, n//10)))
                slopes = []
                for i in range(n - window):
                    xseg = _np.arange(window)
                    yseg = y[i:i+window]
                    if _np.isnan(yseg).any():
                        slopes.append(0.0)
                        continue
                    coef = _np.polyfit(xseg, yseg, 1)
                    slopes.append(coef[0])
                slopes = _np.array(slopes)
                ds = _np.abs(_np.diff(slopes))
                thresh = _np.mean(ds) + 3.0 * _np.std(ds)
                cps = [int(i + window) for i, v in enumerate(ds) if v > thresh]
                trend['results']['change_points'] = cps
                return {'method': 'cusum_heuristic', 'change_points': cps, 'threshold': float(thresh)}

        # -----------------------
        # block bootstrap for slope CI
        # -----------------------
        def trend_bootstrap_slope_ci(series, method='theil-sen', n_boot=None, block_length=None, alpha=0.05, random_state=0):
            """
            Block bootstrap to estimate CI of slope. Non-overlapping blocked bootstrap.
            method: 'theil-sen'|'linear'
            Returns {mean_slope, ci_lower, ci_upper, samples}
            """
            y = _to_1d(series)
            n = len(y)
            if n < 6:
                return {'error': 'series too short for bootstrap'}
            n_boot = int(n_boot or trend['params'].get('bootstrap_n', 500))
            if block_length is None:
                block_length = trend['params'].get('bootstrap_block_length', max(1, int(_np.round(_np.sqrt(n)))))
            rng = _np.random.RandomState(int(random_state or 0))
            slopes = []
            # precompute blocks
            n_blocks = int(_np.ceil(n / block_length))
            blocks = [list(range(i*block_length, min(n, (i+1)*block_length))) for i in range(n_blocks)]
            for b in range(n_boot):
                # sample blocks with replacement
                chosen = rng.randint(0, n_blocks, size=n_blocks)
                idx = []
                for c in chosen:
                    idx.extend(blocks[c])
                idx = idx[:n]  # truncate to length n
                samp = y[idx]
                if method == 'theil-sen':
                    out = trend_fit_theilsen(samp)
                    if 'slope' in out:
                        slopes.append(out['slope'])
                else:
                    out = trend_fit_linear(samp)
                    if 'slope' in out:
                        slopes.append(out['slope'])
            if not slopes:
                return {'error': 'bootstrap failed'}
            arr = _np.asarray(slopes, dtype=float)
            lo = float(_np.percentile(arr, 100.0 * (alpha/2)))
            hi = float(_np.percentile(arr, 100.0 * (1 - alpha/2)))
            res = {'mean_slope': float(arr.mean()), 'ci_lower': lo, 'ci_upper': hi, 'samples': arr.tolist()}
            trend['results'].setdefault('bootstrap', {})['slope_ci'] = res
            return res

        # -----------------------
        # ARIMA forecast (optional)
        # -----------------------
        def trend_forecast_arima(series, order=None, seasonal_order=None, steps=10):
            y = _to_1d(series)
            if not STATSMODELS_AVAILABLE or _sm is None:
                return {'error': 'statsmodels not available for ARIMA forecasting'}
            try:
                # simple default order if not provided
                if order is None:
                    order = (1,0,0)
                model = _sm.tsa.ARIMA(y, order=order, seasonal_order=seasonal_order)
                fitted = model.fit(method_kwargs={'warn_convergence': False})
                pred = fitted.get_forecast(steps=steps)
                mean = pred.predicted_mean.tolist()
                ci = pred.conf_int(alpha=0.05).tolist()
                out = {'mean': mean, 'conf_int': ci, 'aic': float(getattr(fitted, 'aic', _np.nan))}
                trend['results']['forecast_arima'] = out
                return out
            except Exception as e:
                return {'error': f'arima_failed: {e}'}

        # -----------------------
        # summary helper
        # -----------------------
        def trend_summary(series, period=None, do_decompose=True, do_change_points=True):
            y = _to_1d(series)
            n = len(y)
            res = {'n': n}
            res['linear'] = trend_fit_linear(y)
            res['theil_sen'] = trend_fit_theilsen(y)
            res['mann_kendall'] = trend_mann_kendall(y)
            if do_decompose:
                res['decompose'] = trend_decompose_stl(y, period=period)
                res['hp'] = trend_hp_filter(y, lamb=trend['params'].get('hp_lambda'))
            if do_change_points:
                res['change_points'] = trend_change_point_detection(y, n_bkps=3)
            # bootstrap slope CI (theil-sen)
            try:
                res['slope_ci_bootstrap'] = trend_bootstrap_slope_ci(y, method='theil-sen', n_boot=trend['params'].get('bootstrap_n', 500))
            except Exception:
                res['slope_ci_bootstrap'] = None
            trend['results']['summary'] = res
            return res

        # attach to self
        self.trend_analyzer = trend
        self.trend_fit_linear = trend_fit_linear
        self.trend_fit_theilsen = trend_fit_theilsen
        self.trend_mann_kendall = trend_mann_kendall
        self.trend_decompose_stl = trend_decompose_stl
        self.trend_hp_filter = trend_hp_filter
        self.trend_change_point_detection = trend_change_point_detection
        self.trend_bootstrap_slope_ci = trend_bootstrap_slope_ci
        self.trend_forecast_arima = trend_forecast_arima
        self.trend_summary = trend_summary

        trend['initialized'] = True
        return trend

    def _init_causal_analyzer(self):
        """
        Research-grade causal analysis module initializer.

        Provides:
        - self.causal_analyzer (dict) metadata & params
        - Discovery:
            * self.pc_discovery(data, alpha=0.05, variable_names=None)  # constraint-based (simplified PC)
            * self.ges_discovery(data, max_iter=100)                    # greedy score-based (BIC)
        - Time-series causality:
            * self.granger_test(data_matrix, maxlag=5, addconst=True)
        - Effect estimation:
            * self.estimate_propensity_score(X_covariates, treatment, model='logistic')
            * self.estimate_ate_ipw(y, treatment, X_covariates, stabilised=True)
            * self.estimate_ate_regression(y, treatment, X_covariates)
            * self.estimate_ate_matching(y, treatment, X_covariates, n_neighbors=1)
        - Instrumental variable:
            * self.iv_two_stage_least_squares(y, treatment, instrument, X_covariates=None)
        - Utilities:
            * self.bootstrap_ci(fn, args=(), n_boot=500, alpha=0.05)
            * self.estimate_adjustment_set(supplied_graph, treatment, outcome)  # heuristic
        Implementation notes:
        - Prefer statsmodels / sklearn / scipy when available.
        - Constraint-based PC is a simplified practical implementation: uses pairwise conditional independence (up to small conditioning sets) using partial correlations (Gaussian assumption).
        - GES is simplified greedy edge-add/remove with BIC score from linear regression (continuous Y).
        - Designed to be robust: clear errors, structured outputs, and reasonable defaults.
        """
        import numpy as _np
        import math as _math
        from itertools import combinations, permutations, chain
        try:
            from scipy import stats as _scipy_stats
            SCIPY_AVAILABLE = True
        except Exception:
            _scipy_stats = None
            SCIPY_AVAILABLE = False

        try:
            from sklearn.linear_model import LogisticRegression as _LogisticRegression
            from sklearn.neighbors import NearestNeighbors as _NearestNeighbors
            from sklearn.preprocessing import StandardScaler as _StandardScaler
            SKLEARN_AVAILABLE = True
        except Exception:
            _LogisticRegression = None
            _NearestNeighbors = None
            _StandardScaler = None
            SKLEARN_AVAILABLE = False

        try:
            import statsmodels.api as _sm
            from statsmodels.tsa.stattools import grangercausalitytests as _grangertest
            STATSMODELS_AVAILABLE = True
        except Exception:
            _sm = None
            _grangertest = None
            STATSMODELS_AVAILABLE = False

        try:
            import networkx as _nx
            NX_AVAILABLE = True
        except Exception:
            _nx = None
            NX_AVAILABLE = False

        causal = {
            'description': 'Causal discovery & effect estimation module',
            'params': {
                'pc_max_cond_set': 2,        # max size of conditioning sets in simplified PC
                'pc_alpha': 0.05,
                'ges_max_iter': 200,
                'bootstrap_n': 500
            },
            'last_result': {},
            'initialized': True
        }

        # ---------- helpers ----------
        def _to_2d_matrix(data):
            """
            Accept 2D array-like or pandas DataFrame; return numpy 2D (n_samples x n_vars) and var names.
            """
            try:
                import pandas as _pd
                if hasattr(data, 'values') and hasattr(data, 'columns'):
                    arr = _np.asarray(data.values, dtype=float)
                    cols = list(data.columns)
                    return arr, cols
            except Exception:
                pass
            arr = _np.asarray(data, dtype=float)
            if arr.ndim == 1:
                arr = arr.reshape(-1, 1)
            cols = [f'v{i}' for i in range(arr.shape[1])]
            return arr, cols

        def _pairwise_partial_corr(x, y, cond_set, data_matrix, var_index_map):
            """
            Compute partial correlation r_{xy|Z} using regression residuals (works under Gaussian assumption).
            Returns (r, pval) using t-approx if scipy available.
            """
            # Build arrays
            X = data_matrix[:, var_index_map[x]]
            Y = data_matrix[:, var_index_map[y]]
            if not cond_set:
                # simple Pearson
                mask = ~(_np.isnan(X) | _np.isnan(Y))
                if mask.sum() < 3:
                    return _np.nan, _np.nan
                r = _np.corrcoef(X[mask], Y[mask])[0,1]
                p = _np.nan
                if SCIPY_AVAILABLE:
                    try:
                        r, p = _scipy_stats.pearsonr(X[mask], Y[mask])
                    except Exception:
                        p = _np.nan
                return float(r), float(p if p is not None else _np.nan)

            # regress X on Z, Y on Z, compute corr of residuals
            Z_idx = [var_index_map[v] for v in cond_set]
            Z = data_matrix[:, Z_idx]
            # drop rows with NaN
            mask = ~(_np.isnan(X) | _np.isnan(Y) | _np.isnan(Z).any(axis=1))
            if mask.sum() < (len(Z_idx) + 3):
                return _np.nan, _np.nan
            Xv = X[mask]; Yv = Y[mask]; Zv = Z[mask]
            # add intercept
            Zv_design = _np.column_stack([_np.ones(Zv.shape[0]), Zv])
            try:
                bx, *_ = _np.linalg.lstsq(Zv_design, Xv, rcond=None)
                by, *_ = _np.linalg.lstsq(Zv_design, Yv, rcond=None)
                rx = Xv - Zv_design.dot(bx)
                ry = Yv - Zv_design.dot(by)
                r = _np.corrcoef(rx, ry)[0,1]
                p = _np.nan
                if SCIPY_AVAILABLE and not _np.isnan(r):
                    n = rx.size
                    if abs(r) < 1.0 and n > 3:
                        t = r * _np.sqrt((n - 2 - len(Z_idx)) / (1 - r*r))
                        # degrees of freedom approx n - 2 - |Z|
                        try:
                            p = float(2.0 * _scipy_stats.t.sf(abs(t), df=max(1, n - 2 - len(Z_idx))))
                        except Exception:
                            p = _np.nan
                return float(r), float(p if p is not None else _np.nan)
            except Exception:
                return _np.nan, _np.nan

        # ---------- PC algorithm (simplified, constraint-based) ----------
        def pc_discovery(data, alpha=None, variable_names=None, max_cond_set=None):
            """
            Simplified PC-style discovery:
            - Start with complete undirected graph
            - Remove edge X-Y if find conditional independence X _||_ Y | S for some S up to size max_cond_set.
            Returns adjacency dict or networkx Graph (if available).
            Note: This is a practical, Gaussian-assumption partial-correlation based implementation for research exploration.
            """
            data_mat, cols = _to_2d_matrix(data)
            if variable_names is not None:
                cols = list(variable_names)
                # assume columns in same order
            p = len(cols)
            var_index = {cols[i]: i for i in range(p)}
            alpha = alpha if alpha is not None else causal['params']['pc_alpha']
            max_cond = max_cond_set if max_cond_set is not None else causal['params']['pc_max_cond_set']

            # initial complete undirected graph (set of edges)
            edges = set()
            for i in range(p):
                for j in range(i+1, p):
                    edges.add((cols[i], cols[j]))

            sep_sets = { (a,b): None for (a,b) in edges }

            # iterate conditioning set sizes
            for s in range(0, max_cond+1):
                removed = []
                for (a,b) in list(edges):
                    # neighbors excluding b/a
                    neighs = [v for (x,v) in [(x,y) if x==a else (y,x) for (x,y) in edges if x==a or y==a] 
                            for _ in (0, )]  # trick, but simpler to get neighbors below
                    # simpler neighbor getter:
                    neighs = []
                    for (u,v) in edges:
                        if u == a and v != b:
                            neighs.append(v)
                        elif v == a and u != b:
                            neighs.append(u)
                    # possible conditioning sets from neighbors of a (excluding b) union neighbors of b (excluding a)
                    pool = list(set(neighs))
                    # if not enough to form s-size sets, try global other vars
                    if len(pool) < s:
                        pool = [c for c in cols if c not in (a,b)]
                    found_indep = False
                    for cond_set in combinations(pool, s):
                        r, pval = _pairwise_partial_corr(a, b, cond_set, data_mat, var_index)
                        if _np.isnan(r):
                            continue
                        # decide independence based on p-value if available, else on absolute smallness of r
                        if (not _np.isnan(pval) and pval > alpha) or ( _np.isnan(pval) and abs(r) < 0.1 ):
                            # consider independent: remove edge
                            if (a,b) in edges:
                                edges.discard((a,b))
                            if (b,a) in edges:
                                edges.discard((b,a))
                            sep_sets[(a,b)] = cond_set
                            sep_sets[(b,a)] = cond_set
                            found_indep = True
                            break
                    if found_indep:
                        removed.append((a,b))
                # stop early if no removals at this level
                if not removed:
                    # continue to next s
                    continue

            # return as adjacency matrix or networkx Graph
            if NX_AVAILABLE:
                G = _nx.Graph()
                G.add_nodes_from(cols)
                for (u,v) in edges:
                    G.add_edge(u, v)
                causal['last_result']['pc_edges'] = list(edges)
                causal['last_result']['pc_graph'] = G
                return {'edges': list(edges), 'graph': G, 'separating_sets': sep_sets}
            else:
                causal['last_result']['pc_edges'] = list(edges)
                return {'edges': list(edges), 'separating_sets': sep_sets}

        # ---------- GES-like greedy search (score-based) ----------
        def _bic_score_linear(data_mat, var_idx, parents):
            """
            BIC score for linear Gaussian: regress var on parents, compute log-likelihood approx and BIC.
            Lower is better for BIC.
            """
            y = data_mat[:, var_idx]
            if not parents:
                # null model
                mask = ~_np.isnan(y)
                n = mask.sum()
                rss = float(((y[mask] - _np.nanmean(y[mask]))**2).sum())
                # gaussian log-lik approx
                if n <= 1:
                    return _np.inf
                sigma2 = rss / n
                ll = -0.5 * n * ( _np.log(2*_math.pi*sigma2) + 1 )
                k = 1
            else:
                X = _np.column_stack([_np.ones(data_mat.shape[0])] + [data_mat[:, i] for i in parents])
                mask = ~_np.isnan(X).any(axis=1) & ~_np.isnan(y)
                if mask.sum() < (len(parents) + 2):
                    return _np.inf
                Xv = X[mask]; yv = y[mask]
                beta, *_ = _np.linalg.lstsq(Xv, yv, rcond=None)
                resid = yv - Xv.dot(beta)
                n = yv.size
                rss = float((resid**2).sum())
                sigma2 = rss / n
                ll = -0.5 * n * (_np.log(2*_math.pi*sigma2) + 1)
                k = Xv.shape[1]
            bic = -2.0 * ll + k * _np.log(n)
            return bic

        def ges_discovery(data, max_iter=None):
            """
            Simplified GES-style greedy search:
            - Start from empty DAG (no parents).
            - Attempt single-edge additions that reduce global BIC (sum over variables).
            - After no add helps, try deletions that reduce BIC.
            Returns adjacency list of directed edges.
            Note: This is a lightweight research tool â€” not a full implementation of GES.
            """
            data_mat, cols = _to_2d_matrix(data)
            p = len(cols)
            var_index = {cols[i]: i for i in range(p)}
            max_iter = int(max_iter or causal['params']['ges_max_iter'])
            # parents: dict var -> set(parent_indices)
            parents = {i: set() for i in range(p)}
            def global_bic(parents_dict):
                s = 0.0
                for i in range(p):
                    s += _bic_score_linear(data_mat, i, sorted(list(parents_dict[i])))
                return s

            curr_bic = global_bic(parents)
            improved = True
            it = 0
            while improved and it < max_iter:
                it += 1
                improved = False
                best_move = None
                best_bic = curr_bic
                # try addition moves
                for i, j in permutations(range(p), 2):
                    if j in parents[i]:
                        continue
                    # avoid cycles naive check: adding edge i->j where j already ancestor of i would make cycle; simple check:
                    # build tentative parents
                    temp = {k: set(v) for k,v in parents.items()}
                    temp[j].add(i)
                    # simple cycle detection via DFS
                    def _has_cycle(par):
                        visited = [0]*p
                        def dfs(u):
                            visited[u] = 1
                            for v in par[u]:
                                if visited[v] == 1:
                                    return True
                                if visited[v] == 0 and dfs(v):
                                    return True
                            visited[u] = 2
                            return False
                        for node in range(p):
                            if visited[node] == 0 and dfs(node):
                                return True
                        return False
                    if _has_cycle(temp):
                        continue
                    bic_val = global_bic(temp)
                    if bic_val + 1e-6 < best_bic:
                        best_bic = bic_val
                        best_move = ('add', i, j, temp)
                if best_move:
                    _, a, b, new_parents = best_move
                    parents = new_parents
                    curr_bic = best_bic
                    improved = True
                    continue
                # try deletion moves
                for j in range(p):
                    for a in list(parents[j]):
                        temp = {k: set(v) for k,v in parents.items()}
                        temp[j].discard(a)
                        bic_val = global_bic(temp)
                        if bic_val + 1e-6 < best_bic:
                            best_bic = bic_val
                            best_move = ('del', a, j, temp)
                if best_move:
                    _, a, b, new_parents = best_move
                    parents = new_parents
                    curr_bic = best_bic
                    improved = True
                    continue
            # construct edges
            edges = []
            for j, ps in parents.items():
                for i in ps:
                    edges.append((cols[i], cols[j]))
            causal['last_result']['ges_edges'] = edges
            return {'edges': edges, 'parents': {cols[i]: [cols[p] for p in parents[i]] for i in range(p)}}

        # ---------- Granger causality (time series) ----------
        def granger_test(data_matrix, maxlag=5, addconst=True):
            """
            data_matrix: 2D array (n_obs x n_vars) where columns are variables (ordered).
            Returns dict of pairwise granger test statistics and p-values up to maxlag.
            Requires statsmodels for best results.
            """
            X, cols = _to_2d_matrix(data_matrix)
            n, p = X.shape
            res = {}
            if STATSMODELS_AVAILABLE and _grangertest is not None:
                for i in range(p):
                    for j in range(p):
                        if i == j:
                            continue
                        series = _np.column_stack([X[:, j], X[:, i]])  # test whether j causes i
                        try:
                            out = _grangertest(series, maxlag=maxlag, verbose=False)
                            # statsmodels returns dict keyed by lag
                            res[(cols[j], cols[i])] = out
                        except Exception as e:
                            res[(cols[j], cols[i])] = {'error': str(e)}
            else:
                # fallback: fit VAR few lags via linear regressions and compare RSS (approx)
                for i in range(p):
                    for j in range(p):
                        if i == j:
                            continue
                        res[(cols[j], cols[i])] = {'error': 'statsmodels not available for granger tests'}
            causal['last_result']['granger'] = res
            return res

        # ---------- Propensity score & IPW ----------
        def estimate_propensity_score(X_covariates, treatment, model='logistic'):
            """
            Estimate P(T=1 | X) using logistic regression (sklearn) or simple glm (statsmodels).
            Returns propensity scores array and fitted model object (if applicable).
            """
            X = _np.asarray(X_covariates, dtype=float)
            t = _np.asarray(treatment, dtype=float).ravel()
            if X.ndim == 1:
                X = X.reshape(-1, 1)
            n = X.shape[0]
            if SKLEARN_AVAILABLE and _LogisticRegression is not None:
                try:
                    lr = _LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)
                    lr.fit(X, t)
                    ps = lr.predict_proba(X)[:,1]
                    return {'propensity': ps, 'model': lr}
                except Exception:
                    pass
            # fallback: simple logit via statsmodels if available
            if STATSMODELS_AVAILABLE and _sm is not None:
                try:
                    Xc = _sm.add_constant(X)
                    mod = _sm.Logit(t, Xc)
                    res = mod.fit(disp=0)
                    ps = res.predict(Xc)
                    return {'propensity': ps, 'model': res}
                except Exception:
                    pass
            # last resort: fit linear probability
            Xc = _np.column_stack([_np.ones(n), X])
            try:
                beta, *_ = _np.linalg.lstsq(Xc, t, rcond=None)
                ps = _np.clip(Xc.dot(beta), 1e-6, 1-1e-6)
                return {'propensity': ps, 'model': {'beta': beta}}
            except Exception:
                return {'error': 'propensity_estimation_failed'}

        def estimate_ate_ipw(y, treatment, X_covariates, stabilised=True):
            """
            Estimate ATE via inverse-propensity weighting.
            Returns dict with 'ate', 'var', 'weights' and optionally bootstrap CI.
            """
            y = _np.asarray(y, dtype=float).ravel()
            t = _np.asarray(treatment, dtype=float).ravel()
            ps_res = estimate_propensity_score(X_covariates, t)
            if 'propensity' not in ps_res:
                return {'error': 'propensity not available'}
            pscore = _np.asarray(ps_res['propensity'], dtype=float)
            eps = 1e-6
            pscore = _np.clip(pscore, eps, 1-eps)
            if stabilised:
                # stabilized weights
                p_t = pscore * t + (1-pscore)*(1-t)
                w = (t / pscore) / _np.mean(t / pscore) * (t.mean() if t.mean() is not None else 1.0)
                # simpler: wi = t/ps + (1-t)/(1-ps)
                w = t/pscore + (1-t)/(1-pscore)
            else:
                w = t/pscore + (1-t)/(1-pscore)
            ate = (_np.sum(w * t * y) / _np.sum(w * t)) - (_np.sum(w * (1-t) * y) / _np.sum(w * (1-t)))
            # variance approx via influence functions ignored -> provide bootstrap CI
            return {'ate': float(ate), 'weights': w.tolist(), 'propensity_model': ps_res.get('model')}

        # ---------- Regression adjustment ----------
        def estimate_ate_regression(y, treatment, X_covariates):
            """
            Estimate ATE via regression of y on treatment + covariates; return coefficient on treatment.
            """
            y = _np.asarray(y, dtype=float).ravel()
            t = _np.asarray(treatment, dtype=float).ravel()
            X = _np.asarray(X_covariates, dtype=float)
            if X.ndim == 1:
                X = X.reshape(-1,1)
            Xc = _np.column_stack([_np.ones(X.shape[0]), t, X])
            mask = ~_np.isnan(Xc).any(axis=1) & ~_np.isnan(y)
            if mask.sum() < Xc.shape[1] + 1:
                return {'error': 'insufficient data'}
            beta, *_ = _np.linalg.lstsq(Xc[mask], y[mask], rcond=None)
            ate = float(beta[1])
            return {'ate': ate, 'coefficients': beta.tolist()}

        # ---------- Matching (nearest neighbor) ----------
        def estimate_ate_matching(y, treatment, X_covariates, n_neighbors=1):
            """
            Nearest-neighbor matching on covariates (propensity or raw X). Simple implementation: match treated to controls.
            """
            y = _np.asarray(y, dtype=float).ravel()
            t = _np.asarray(treatment, dtype=int).ravel()
            X = _np.asarray(X_covariates, dtype=float)
            if X.ndim == 1:
                X = X.reshape(-1,1)
            treated_idx = _np.where(t == 1)[0]
            control_idx = _np.where(t == 0)[0]
            if len(treated_idx) == 0 or len(control_idx) == 0:
                return {'error': 'no treated or control units'}
            # scale covariates
            try:
                scaler = _StandardScaler() if SKLEARN_AVAILABLE and _StandardScaler is not None else None
                if scaler is not None:
                    Xs = scaler.fit_transform(X)
                else:
                    Xs = (X - _np.nanmean(X, axis=0)) / ( _np.nanstd(X, axis=0) + 1e-9)
            except Exception:
                Xs = X
            # use sklearn NearestNeighbors if available
            if SKLEARN_AVAILABLE and _NearestNeighbors is not None:
                nn = _NearestNeighbors(n_neighbors=n_neighbors)
                nn.fit(Xs[control_idx])
                dists, neigh = nn.kneighbors(Xs[treated_idx], return_distance=True)
                # compute matched control outcomes average
                matched_controls = control_idx[neigh]
                y_treated = y[treated_idx]
                y_controls = _np.mean(y[matched_controls], axis=1)
                ate = float(_np.mean(y_treated - y_controls))
                return {'ate': ate, 'matched_indices': matched_controls.tolist()}
            else:
                # naive O(n^2) matching
                matched_controls = []
                diffs = []
                for i in treated_idx:
                    dists = _np.sum((Xs[control_idx] - Xs[i])**2, axis=1)
                    nn_idx = control_idx[_np.argsort(dists)[:n_neighbors]]
                    matched_controls.append(nn_idx.tolist())
                    diffs.append(y[i] - _np.mean(y[nn_idx]))
                ate = float(_np.mean(diffs))
                return {'ate': ate, 'matched_indices': matched_controls}

        # ---------- Instrumental variable: 2SLS ----------
        def iv_two_stage_least_squares(y, treatment, instrument, X_covariates=None):
            """
            Two-stage least squares for single instrument.
            Returns {'ate_iv':..., 'first_stage':..., 'second_stage':...}
            """
            y = _np.asarray(y, dtype=float).ravel()
            t = _np.asarray(treatment, dtype=float).ravel()
            z = _np.asarray(instrument, dtype=float).ravel()
            if X_covariates is None:
                X = _np.ones((len(y), 1))
            else:
                X = _np.asarray(X_covariates, dtype=float)
                if X.ndim == 1:
                    X = X.reshape(-1,1)
                X = _np.column_stack([_np.ones(len(y)), X])
            # first stage: regress t on z and X
            Z_design = _np.column_stack([X, z])
            mask = ~_np.isnan(Z_design).any(axis=1) & ~_np.isnan(t)
            if mask.sum() < Z_design.shape[1] + 1:
                return {'error': 'insufficient data for IV'}
            beta1, *_ = _np.linalg.lstsq(Z_design[mask], t[mask], rcond=None)
            t_hat = Z_design.dot(beta1)
            # second stage: regress y on t_hat and X
            T_design = _np.column_stack([X, t_hat])
            beta2, *_ = _np.linalg.lstsq(T_design[mask], y[mask], rcond=None)
            ate_iv = float(beta2[-1])
            return {'ate_iv': ate_iv, 'first_stage_beta': beta1.tolist(), 'second_stage_beta': beta2.tolist()}

        # ---------- utility: simple adjustment set heuristic ----------
        def estimate_adjustment_set(graph, treatment, outcome):
            """
            Given a directed acyclic graph (networkx DiGraph or adjacency dict), provide a heuristic minimal adjustment set:
            - If graph is dict: keys -> list of neighbors (interpreted as directed edges if tuple keys)
            - Strategy (very simple): use parents of treatment and outcome, remove descendants, etc.
            This is a heuristic helper; for rigorous adjustment use domain knowledge.
            """
            # try networkx
            if NX_AVAILABLE and hasattr(graph, 'nodes'):
                G = graph
                # parents of outcome except descendants of treatment
                try:
                    parents_out = list(G.predecessors(outcome))
                except Exception:
                    parents_out = [u for u in G.nodes() if G.has_edge(u, outcome)]
                # remove treatment and its descendants
                try:
                    desc = set(_nx.descendants(G, treatment))
                except Exception:
                    desc = set()
                adj = [v for v in parents_out if v != treatment and v not in desc]
                return {'adjustment_set': adj}
            else:
                # fallback: if graph is adjacency dict keyed by node->parents list
                if isinstance(graph, dict):
                    parents_out = graph.get(outcome, [])
                    desc = set()
                    # compute descendants of treatment via BFS
                    stack = [treatment]
                    while stack:
                        cur = stack.pop()
                        for k,v in graph.items():
                            if cur in v and k not in desc:
                                desc.add(k); stack.append(k)
                    adj = [v for v in parents_out if v != treatment and v not in desc]
                    return {'adjustment_set': adj}
                return {'error': 'graph format not supported'}

        # ---------- bootstrap CI helper ----------
        def bootstrap_ci(fn, args=(), n_boot=None, alpha=0.05, random_state=0):
            n_boot = int(n_boot or causal['params'].get('bootstrap_n', 500))
            rng = _np.random.RandomState(int(random_state or 0))
            vals = []
            # assume first arg is data-like with n samples
            data0 = args[0] if args else None
            if data0 is None:
                return {'error': 'no data for bootstrap'}
            data0 = _np.asarray(data0)
            n = data0.shape[0]
            for b in range(n_boot):
                idx = rng.randint(0, n, size=n)
                new_args = []
                for a in args:
                    a_np = _np.asarray(a)
                    if a_np.shape[0] == n:
                        new_args.append(a_np[idx])
                    else:
                        new_args.append(a_np)
                try:
                    v = fn(*new_args)
                    # if dict with 'ate' or numeric, extract numeric
                    if isinstance(v, dict):
                        if 'ate' in v:
                            vals.append(float(v['ate']))
                        elif 'ace' in v:
                            vals.append(float(v['ace']))
                        else:
                            # try to convert to float if possible
                            try:
                                vals.append(float(v))
                            except Exception:
                                continue
                    else:
                        vals.append(float(v))
                except Exception:
                    continue
            if not vals:
                return {'error': 'bootstrap failed'}
            arr = _np.asarray(vals)
            lo = float(_np.percentile(arr, 100.0 * (alpha/2)))
            hi = float(_np.percentile(arr, 100.0 * (1 - alpha/2)))
            return {'mean': float(arr.mean()), 'ci_lower': lo, 'ci_upper': hi, 'samples': arr.tolist()}

        # attach to self
        self.causal_analyzer = causal
        self.pc_discovery = pc_discovery
        self.ges_discovery = ges_discovery
        self.granger_test = granger_test
        self.estimate_propensity_score = estimate_propensity_score
        self.estimate_ate_ipw = estimate_ate_ipw
        self.estimate_ate_regression = estimate_ate_regression
        self.estimate_ate_matching = estimate_ate_matching
        self.iv_two_stage_least_squares = iv_two_stage_least_squares
        self.estimate_adjustment_set = estimate_adjustment_set
        self.bootstrap_ci = bootstrap_ci

        causal['initialized'] = True
        return causal

    def _init_momentum_contrarian_model(self):
        """
        Research-grade Momentum-Contrarian strategy module initializer.

        Provides on self:
        - self.momentum_contrarian (dict) : metadata, params, last_result
        - self.mc_generate_signals(prices, mode='cross_sectional', lookback=20, z_window=60, vol_window=20, decay=0.5, topk=None)
            -> DataFrame of signals in [-1,1] (positive = long, negative = short)
        - self.mc_position_sizing(signals, prices, risk_target=0.01, vol_window=20, max_leverage=3.0)
            -> positions (weights)
        - self.mc_backtest(prices, positions, fees=0.0005, slippage=0.0005, freq='D')
            -> performance dict with timeseries & metrics
        - self.mc_tune_hyperparameters(prices, hyper_grid, scoring='sharpe', n_jobs=1)
            -> best params & results
        - self.mc_evaluate_performance(returns_ts)
        - self.mc_save_model(path), self.mc_load_model(path)
        Design principles:
        - Supports time-series and cross-sectional contrarian signals (z-score mean-reversion)
        - Volatility-adjusted signals and position sizing using target volatility
        - Transaction costs and slippage modeled
        - Grid search hyperparameter tuning (can be extended to Bayesian)
        - Deterministic numpy/pandas core for reproducibility
        """
        import numpy as _np
        import math as _math
        from collections import defaultdict as _defaultdict
        try:
            import pandas as _pd
        except Exception:
            _pd = None
        try:
            from sklearn.preprocessing import StandardScaler as _StandardScaler
            SKLEARN_AVAILABLE = True
        except Exception:
            _StandardScaler = None
            SKLEARN_AVAILABLE = False
        import json as _json
        import os as _os
        import pickle as _pickle
        import time as _time

        mc = {
            'description': 'Momentum-Contrarian strategy module (vol adj, cross-sectional & time-series contrarian)',
            'params': {
                'default_mode': 'cross_sectional',    # 'cross_sectional' or 'time_series'
                'lookback': 20,                       # for returns / rank momentum / mean reversion window
                'z_window': 60,                       # for z-score of cumulative returns
                'vol_window': 20,                     # for realized vol estimation
                'decay': 0.5,                         # exponential decay for weighted returns (0..1)
                'topk': None,                         # top-k long/short in cross-section (None => use continuous weights)
                'risk_target': 0.01,                  # target daily vol of portfolio (e.g., 1%)
                'fee': 0.0005,                        # proportional fee
                'slippage': 0.0005,                   # proportional slippage
                'max_leverage': 3.0,
                'min_trade_size': 1e-6,
                'leverage_mode': 'vol_target'         # 'vol_target' or 'equal_weight'
            },
            'last_result': {},
            'initialized': True
        }

        # -------- helpers --------
        def _ensure_df(prices):
            """Return pandas DataFrame if pandas available, else numpy 2D array + index/cols placeholders."""
            if _pd is not None:
                if isinstance(prices, _pd.DataFrame):
                    return prices.copy()
                else:
                    try:
                        df = _pd.DataFrame(prices)
                        return df
                    except Exception:
                        raise ValueError("prices must be convertible to pandas.DataFrame when pandas is available")
            else:
                # fallback: require numpy array
                arr = _np.asarray(prices, dtype=float)
                if arr.ndim == 1:
                    arr = arr.reshape(-1, 1)
                return arr

        def _returns(prices):
            """Simple log returns if DataFrame, else numpy pct change."""
            if _pd is not None and isinstance(prices, _pd.DataFrame):
                return _np.log(prices).diff().fillna(0)
            else:
                p = _np.asarray(prices, dtype=float)
                r = _np.zeros_like(p)
                r[1:] = _np.log(p[1:] / p[:-1])
                return r

        def _ewma(series, span):
            """EWMA fallback using numpy; span ~ alpha relation: alpha = 2/(span+1)"""
            alpha = 2.0 / (float(span) + 1.0)
            out = _np.zeros_like(series, dtype=float)
            out[0] = series[0]
            for t in range(1, len(series)):
                out[t] = alpha * series[t] + (1 - alpha) * out[t-1]
            return out

        def _rolling_std(series, window):
            if _pd is not None and isinstance(series, _pd.Series):
                return series.rolling(window=window, min_periods=1).std()
            else:
                a = _np.asarray(series, dtype=float)
                out = _np.full(a.shape, _np.nan)
                for i in range(a.shape[0]):
                    start = max(0, i - window + 1)
                    out[i] = _np.std(a[start:i+1], ddof=1) if i - start + 1 > 1 else 0.0
                return out

        # -------- signal generation --------
        def mc_generate_signals(prices, mode=None, lookback=None, z_window=None, vol_window=None, decay=None, topk=None):
            """
            Generate contrarian (mean-reversion) signals.
            - mode: 'cross_sectional' or 'time_series'
            - for cross_sectional: compute lookback returns, zscore across assets and use negative z (contrarian)
            - for time_series: compute zscore of past cumulative returns for each asset (time-series mean-reversion)
            Returns DataFrame of signals in [-1,1] (if pandas available) or numpy array.
            """
            mode = mode or mc['params']['default_mode']
            lookback = int(lookback or mc['params']['lookback'])
            z_window = int(z_window or mc['params']['z_window'])
            vol_window = int(vol_window or mc['params']['vol_window'])
            decay = float(decay if decay is not None else mc['params']['decay'])
            topk = topk if topk is not None else mc['params']['topk']

            df = _ensure_df(prices)
            # compute returns (log)
            if _pd is not None and isinstance(df, _pd.DataFrame):
                logp = _np.log(df)
                # lookback returns: log price diff over lookback
                ret_lb = logp.diff(periods=lookback)
                # rolling zscore of recent cumulative return for time_series mode
                if mode == 'time_series':
                    signals = _pd.DataFrame(index=df.index, columns=df.columns, data=0.0)
                    for col in df.columns:
                        series = ret_lb[col].rolling(window=z_window, min_periods=1).sum()
                        mu = series.rolling(window=z_window, min_periods=1).mean()
                        sigma = series.rolling(window=z_window, min_periods=1).std().replace(0, _np.nan)
                        z = (series - mu) / sigma
                        sig = -z  # contrarian => negative of zscore
                        # normalize by volatility
                        realized_vol = df[col].pct_change().rolling(vol_window, min_periods=1).std().replace(0, 1)
                        sig = sig.div(realized_vol)
                        signals[col] = sig
                else:  # cross_sectional
                    # at each time t, compute zscore across assets of lookback returns
                    signals = ret_lb.copy() * 0.0
                    for t_idx in range(lookback, len(df)):
                        slice_ret = ret_lb.iloc[t_idx]
                        valid = slice_ret.dropna()
                        if len(valid) == 0:
                            continue
                        mu = valid.mean()
                        sigma = valid.std(ddof=1) if valid.std(ddof=1) > 0 else 1.0
                        # zscore across assets
                        z = (slice_ret - mu) / sigma
                        # contrarian sign = -z
                        raw_sig = -z
                        # optional top-k filter
                        if topk is not None:
                            longs = raw_sig.nlargest(topk)
                            shorts = raw_sig.nsmallest(topk)
                            s = _pd.Series(0.0, index=slice_ret.index)
                            s.loc[longs.index] = longs
                            s.loc[shorts.index] = shorts
                        else:
                            s = raw_sig
                        # volatility scaling (cross-sectional)
                        vol = df.pct_change().iloc[t_idx - 1].abs().replace(0, 1.0)
                        s = s.div(vol)
                        signals.iloc[t_idx] = s
                    # fill earlier rows with zeros
                    signals = signals.fillna(0.0)
                # exponential decay smoothing across time (apply EWMA)
                signals_smoothed = signals.copy()
                if decay is not None and 0 < decay < 1:
                    alpha = 1 - decay
                    signals_smoothed = signals.ewm(alpha=alpha, adjust=False).mean()
                mc['last_result']['signals'] = signals_smoothed
                return signals_smoothed.fillna(0.0)
            else:
                # numpy fallback: assume 2D array (time x assets)
                P = _np.asarray(df, dtype=float)
                T, N = P.shape
                logP = _np.log(P)
                ret_lb = _np.zeros_like(P)
                ret_lb[lookback:] = logP[lookback:] - logP[:-lookback]
                S = _np.zeros_like(P)
                if mode == 'time_series':
                    for j in range(N):
                        # rolling sum series for z
                        cumsum = _np.convolve(ret_lb[:, j], _np.ones(z_window), mode='same')
                        # compute rolling mean/std
                        # naive: use global mean/std for fallback
                        mu = _np.nanmean(cumsum)
                        sigma = _np.nanstd(cumsum) if _np.nanstd(cumsum) > 0 else 1.0
                        z = (cumsum - mu) / sigma
                        S[:, j] = -z
                    # vol scaling
                    vol = _np.zeros_like(P)
                    for j in range(N):
                        vol[:, j] = _rolling_std(_np.diff(P[:, j], prepend=P[0, j]), vol_window)
                    S = S / (vol + 1e-9)
                else:
                    for t in range(lookback, T):
                        slice_ret = ret_lb[t, :]
                        valid_mask = ~_np.isnan(slice_ret)
                        if valid_mask.sum() == 0:
                            continue
                        mu = _np.nanmean(slice_ret[valid_mask])
                        sigma = _np.nanstd(slice_ret[valid_mask]) if _np.nanstd(slice_ret[valid_mask]) > 0 else 1.0
                        z = (slice_ret - mu) / sigma
                        raw_sig = -z
                        if topk is not None:
                            # keep only topk by absolute value
                            idx_sort = _np.argsort(_np.abs(raw_sig))[::-1]
                            mask_keep = _np.zeros_like(raw_sig, dtype=bool)
                            mask_keep[idx_sort[:topk]] = True
                            s = _np.zeros_like(raw_sig)
                            s[mask_keep] = raw_sig[mask_keep]
                        else:
                            s = raw_sig
                        # vol scaling: use abs pct change previous day
                        vol = _np.abs(P[t-1, :] / (P[t-2, :] + 1e-12) - 1.0) if t >= 2 else _np.ones_like(s)
                        s = s / (vol + 1e-9)
                        S[t, :] = s
                # exponential decay smoothing
                if decay is not None and 0 < decay < 1:
                    alpha = 1 - decay
                    for j in range(S.shape[1]):
                        S[:, j] = _ewma(S[:, j], span=int(1.0/alpha) if alpha > 0 else 1)
                mc['last_result']['signals'] = S
                return S

        # -------- position sizing --------
        def mc_position_sizing(signals, prices, risk_target=None, vol_window=None, max_leverage=None, min_trade_size=None):
            """
            Convert signals to portfolio weights.
            - risk_target: target portfolio vol (daily)
            - vol_window: lookback for realized vol per asset
            - returns DataFrame of weights aligned with signals
            """
            risk_target = float(risk_target if risk_target is not None else mc['params']['risk_target'])
            vol_window = int(vol_window or mc['params']['vol_window'])
            max_leverage = float(max_leverage if max_leverage is not None else mc['params']['max_leverage'])
            min_trade_size = float(min_trade_size if min_trade_size is not None else mc['params']['min_trade_size'])

            S = signals
            P = _ensure_df(prices)
            # unify types
            if _pd is not None and isinstance(S, _pd.DataFrame):
                # compute per-asset realized vol
                ret = P.pct_change().fillna(0.0)
                vol = ret.rolling(window=vol_window, min_periods=1).std().replace(0, _np.nan)
                # normalize signals by vol
                weights = S.div(vol).fillna(0.0)
                # at each time step rescale to target risk
                wts = weights.copy()
                for t in range(len(wts)):
                    row = wts.iloc[t].fillna(0.0)
                    # raw exposures
                    raw = row
                    if raw.abs().sum() < 1e-12:
                        wts.iloc[t] = raw
                        continue
                    # compute portfolio vol if weights raw scaled to 1 unit notional
                    # approximate using cross-sectional vol and ignore covariances (conservative)
                    approx_vol = _np.sqrt((raw.values**2 * (vol.iloc[t].fillna(0.0).values**2)).sum())
                    if approx_vol == 0 or _np.isnan(approx_vol):
                        scale = 0.0
                    else:
                        scale = risk_target / approx_vol
                    scale = min(scale, max_leverage)
                    w = raw * scale
                    # threshold tiny positions
                    w = w.where(w.abs() >= min_trade_size, 0.0)
                    wts.iloc[t] = w
                mc['last_result']['positions'] = wts
                return wts.fillna(0.0)
            else:
                # numpy fallback
                S_arr = _np.asarray(S, dtype=float)
                P_arr = _np.asarray(P, dtype=float)
                T, N = S_arr.shape
                # compute vol matrix
                vol = _np.zeros((T, N))
                pct = _np.zeros_like(P_arr)
                pct[1:] = P_arr[1:] / (P_arr[:-1] + 1e-12) - 1.0
                for t in range(T):
                    start = max(0, t - vol_window + 1)
                    vol[t] = _np.std(pct[start:t+1], axis=0, ddof=1)
                wts = _np.zeros_like(S_arr)
                for t in range(T):
                    raw = S_arr[t, :]
                    denom = _np.sqrt(_np.nansum((raw**2) * (vol[t]**2)))
                    if denom == 0 or _np.isnan(denom):
                        scale = 0.0
                    else:
                        scale = risk_target / denom
                    scale = min(scale, max_leverage)
                    w = raw * scale
                    w[_np.abs(w) < min_trade_size] = 0.0
                    wts[t, :] = w
                mc['last_result']['positions'] = wts
                return wts

        # -------- backtesting engine --------
        def mc_backtest(prices, positions, fees=None, slippage=None, freq='D'):
            """
            Backtest simulated strategy:
            - prices: DataFrame (time x assets) or numpy array
            - positions: DataFrame of weights (aligns with prices) or numpy array of same shape
            Returns:
            - performance dict: returns_ts (series), nav_ts, metrics (sharpe, cagr, maxdd, turnover, annualized vol)
            """
            fees = float(fees if fees is not None else mc['params']['fee'])
            slippage = float(slippage if slippage is not None else mc['params']['slippage'])
            P = _ensure_df(prices)
            W = positions
            # convert to returns
            if _pd is not None and isinstance(P, _pd.DataFrame):
                ret = P.pct_change().fillna(0.0)
                if isinstance(W, _pd.DataFrame):
                    W_aligned = W.reindex(index=P.index, columns=P.columns).fillna(0.0)
                    # daily portfolio return = sum(weights * asset returns) - costs
                    port_ret = (W_aligned.shift(1) * ret).sum(axis=1)  # assume weights set at t-1 use returns at t
                    # turnover costs: sum abs(weight_change) * fees
                    wchg = (W_aligned - W_aligned.shift(1)).abs().sum(axis=1).fillna(0.0)
                    cost = wchg * fees + wchg * slippage
                    net_ret = port_ret - cost
                    nav = (1 + net_ret).cumprod()
                    metrics = mc_evaluate_performance(net_ret, freq=freq)
                    mc['last_result']['backtest'] = {'returns': net_ret, 'nav': nav, 'metrics': metrics}
                    return {'returns': net_ret, 'nav': nav, 'metrics': metrics}
                else:
                    # positions numpy fallback
                    W_arr = _np.asarray(W, dtype=float)
                    ret_arr = ret.values
                    # assume same shape
                    port_ret_arr = _np.sum(_np.vstack([_np.roll(W_arr[:, j], 1) * ret_arr[:, j] for j in range(ret_arr.shape[1])]).T, axis=1)
                    wchg = _np.sum(_np.abs(W_arr - _np.vstack([_np.zeros(W_arr.shape[1]), W_arr[:-1]])), axis=1)
                    cost = wchg * (fees + slippage)
                    net = port_ret_arr - cost
                    # metrics
                    metrics = mc_evaluate_performance(net, freq=freq)
                    nav = _np.cumprod(1 + net)
                    mc['last_result']['backtest'] = {'returns': net, 'nav': nav, 'metrics': metrics}
                    return {'returns': net, 'nav': nav, 'metrics': metrics}
            else:
                # numpy-only path
                P_arr = _np.asarray(P, dtype=float)
                R = _np.zeros_like(P_arr)
                R[1:] = P_arr[1:] / (P_arr[:-1] + 1e-12) - 1.0
                W_arr = _np.asarray(W, dtype=float)
                # compute portfolio returns: previous day's weights * today's returns
                port_ret = _np.sum(_np.roll(W_arr, 1, axis=0) * R, axis=1)
                wchg = _np.sum(_np.abs(W_arr - _np.roll(W_arr, 1, axis=0)), axis=1)
                cost = wchg * (fees + slippage)
                net_ret = port_ret - cost
                nav = _np.cumprod(1 + net_ret)
                metrics = mc_evaluate_performance(net_ret, freq=freq)
                mc['last_result']['backtest'] = {'returns': net_ret, 'nav': nav, 'metrics': metrics}
                return {'returns': net_ret, 'nav': nav, 'metrics': metrics}

        # -------- performance evaluation --------
        def mc_evaluate_performance(returns_ts, freq='D'):
            """
            Evaluate returns time series. returns_ts: pandas Series or numpy array of periodic returns (not log).
            Metrics: annualized return, annualized vol, sharpe (0 rf), max drawdown, CAGR
            """
            if _pd is not None and hasattr(returns_ts, 'values'):
                r = returns_ts.fillna(0.0).values
            else:
                r = _np.asarray(returns_ts, dtype=float)
            # infer periods per year
            if freq == 'D':
                per_year = 252
            elif freq == 'W':
                per_year = 52
            elif freq == 'M':
                per_year = 12
            else:
                per_year = 252
            # simple metrics
            avg = _np.nanmean(r)
            ann_ret = (1 + avg) ** per_year - 1 if avg > -1 else _np.nan
            ann_vol = _np.nanstd(r, ddof=1) * _np.sqrt(per_year)
            sharpe = (avg * per_year) / ann_vol if ann_vol > 0 else _np.nan
            # cagr
            try:
                nav = _np.cumprod(1 + r)
                cagr = (nav[-1]) ** (1.0 / (len(r) / per_year)) - 1 if len(r) > 0 else _np.nan
                peak = _np.maximum.accumulate(nav)
                dd = (nav - peak) / peak
                maxdd = float(_np.min(dd))
            except Exception:
                cagr = _np.nan; maxdd = _np.nan
            return {'annual_return': ann_ret, 'annual_vol': ann_vol, 'sharpe': sharpe, 'cagr': cagr, 'max_drawdown': maxdd}

        # -------- hyperparameter tuning (grid search) --------
        def mc_tune_hyperparameters(prices, hyper_grid, scoring='sharpe', n_jobs=1, freq='D'):
            """
            hyper_grid: dict of param_name -> list of values (e.g., {'lookback':[10,20], 'z_window':[30,60], 'decay':[0.3,0.6]})
            returns best_params and all results list
            """
            # build grid
            import itertools as _it
            keys = list(hyper_grid.keys())
            combos = list(_it.product(*[hyper_grid[k] for k in keys]))
            results = []
            P = _ensure_df(prices)
            for combo in combos:
                params = dict(zip(keys, combo))
                sig = mc_generate_signals(P, mode=params.get('mode', None),
                                        lookback=params.get('lookback', None),
                                        z_window=params.get('z_window', None),
                                        vol_window=params.get('vol_window', None),
                                        decay=params.get('decay', None),
                                        topk=params.get('topk', None))
                pos = mc_position_sizing(sig, P, risk_target=params.get('risk_target', None),
                                        vol_window=params.get('vol_window', None),
                                        max_leverage=params.get('max_leverage', None))
                bt = mc_backtest(P, pos, fees=mc['params']['fee'], slippage=mc['params']['slippage'], freq=freq)
                metric = bt['metrics'].get(scoring) if isinstance(bt['metrics'], dict) and scoring in bt['metrics'] else bt['metrics'].get('sharpe')
                results.append({'params': params, 'metrics': bt['metrics']})
            # choose best by sharpe or scoring
            best = sorted(results, key=lambda x: x['metrics'].get(scoring) if x['metrics'].get(scoring) is not None else -_np.inf, reverse=True)[0]
            mc['last_result']['tuning'] = {'all': results, 'best': best}
            return {'all': results, 'best': best}

        # -------- persistence --------
        def mc_save_model(path):
            meta = {'module': 'momentum_contrarian', 'params': mc['params'], 'timestamp': _time.time()}
            _os.makedirs(_os.path.dirname(path), exist_ok=True) if _os.path.dirname(path) else None
            with open(path, 'wb') as fh:
                _pickle.dump({'meta': meta, 'last_result': mc['last_result']}, fh)
            return {'status': 'saved', 'path': path}

        def mc_load_model(path):
            with open(path, 'rb') as fh:
                data = _pickle.load(fh)
            mc['last_result'] = data.get('last_result', {})
            return {'status': 'loaded', 'path': path, 'meta': data.get('meta', None)}

        # attach functions to self
        self.momentum_contrarian = mc
        self.mc_generate_signals = mc_generate_signals
        self.mc_position_sizing = mc_position_sizing
        self.mc_backtest = mc_backtest
        self.mc_evaluate_performance = mc_evaluate_performance
        self.mc_tune_hyperparameters = mc_tune_hyperparameters
        self.mc_save_model = mc_save_model
        self.mc_load_model = mc_load_model

        mc['initialized'] = True
        return mc

    def _init_momentum_contrarian_model(self):
        """
        Research-grade Momentum-Contrarian strategy module initializer.

        Exposes on self:
        - self.momentum_contrarian (dict) : metadata, params, last_result
        - self.mc_generate_signals(prices, mode='cross_sectional', lookback=20, z_window=60, vol_window=20, decay=0.5, topk=None)
        - self.mc_position_sizing(signals, prices, risk_target=0.01, vol_window=20, max_leverage=3.0)
        - self.mc_backtest(prices, positions, fees=0.0005, slippage=0.0005, freq='D')
        - self.mc_tune_hyperparameters(prices, hyper_grid, scoring='sharpe', n_jobs=1)
        - self.mc_evaluate_performance(returns_ts, freq='D')
        - self.mc_save_model(path), self.mc_load_model(path)
        Design notes:
        - ä¼˜å…ˆä½¿ç”¨ pandas / sklearnï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ numpy å›é€€å®ç°
        - æ”¯æŒæ¨ªæˆªé¢ä¸æ—¶åºåè½¬ä¿¡å·ã€æ³¢åŠ¨ç‡è°ƒèŠ‚ã€å¤´å¯¸ç¼©æ”¾ä¸åŸºæœ¬å›æµ‹
        - è¯¥å®ç°é€‚åˆç ”ç©¶ä¸å›æµ‹ï¼›è‹¥ç”¨äºå®ç›˜éœ€è¡¥å……è®¢å•ç®¡ç†ã€å®æ—¶é£æ§ç­‰
        """
        import numpy as _np
        import math as _math
        from collections import defaultdict as _defaultdict
        try:
            import pandas as _pd
        except Exception:
            _pd = None
        try:
            from sklearn.preprocessing import StandardScaler as _StandardScaler
            from sklearn.neighbors import NearestNeighbors as _NearestNeighbors
            SKLEARN_AVAILABLE = True
        except Exception:
            _StandardScaler = None
            _NearestNeighbors = None
            SKLEARN_AVAILABLE = False
        import pickle as _pickle
        import os as _os
        import time as _time

        mc = {
            'description': 'Momentum-Contrarian module (vol adj, cross-sectional & time-series contrarian)',
            'params': {
                'default_mode': 'cross_sectional',
                'lookback': 20,
                'z_window': 60,
                'vol_window': 20,
                'decay': 0.5,
                'topk': None,
                'risk_target': 0.01,    # target daily vol
                'fee': 0.0005,
                'slippage': 0.0005,
                'max_leverage': 3.0,
                'min_trade_size': 1e-6
            },
            'last_result': {},
            'initialized': True
        }

        # ---------- helpers ----------
        def _ensure_df(prices):
            if _pd is not None:
                if isinstance(prices, _pd.DataFrame):
                    return prices.copy()
                try:
                    return _pd.DataFrame(prices)
                except Exception:
                    raise ValueError("prices must be convertible to pandas.DataFrame when pandas is available")
            else:
                arr = _np.asarray(prices, dtype=float)
                if arr.ndim == 1:
                    arr = arr.reshape(-1, 1)
                return arr

        def _ewma_np(series, span):
            # simple EWMA fallback
            alpha = 2.0 / (span + 1.0) if span > 0 else 1.0
            out = _np.empty_like(series, dtype=float)
            out[0] = series[0]
            for i in range(1, len(series)):
                out[i] = alpha * series[i] + (1 - alpha) * out[i-1]
            return out

        def _rolling_std_np(x, window):
            x = _np.asarray(x, dtype=float)
            n = len(x)
            out = _np.zeros(n)
            for i in range(n):
                start = max(0, i - window + 1)
                seg = x[start:i+1]
                out[i] = _np.std(seg, ddof=1) if seg.size > 1 else 0.0
            return out

        # ---------- signal generation ----------
        def mc_generate_signals(prices, mode=None, lookback=None, z_window=None, vol_window=None, decay=None, topk=None):
            mode = mode or mc['params']['default_mode']
            lookback = int(lookback or mc['params']['lookback'])
            z_window = int(z_window or mc['params']['z_window'])
            vol_window = int(vol_window or mc['params']['vol_window'])
            decay = float(decay if decay is not None else mc['params']['decay'])
            topk = topk if topk is not None else mc['params']['topk']

            df = _ensure_df(prices)
            # pandas path
            if _pd is not None and isinstance(df, _pd.DataFrame):
                logp = _np.log(df)
                ret_lb = logp.diff(periods=lookback)
                if mode == 'time_series':
                    signals = _pd.DataFrame(index=df.index, columns=df.columns, data=0.0)
                    for col in df.columns:
                        s = ret_lb[col].rolling(window=z_window, min_periods=1).sum()
                        mu = s.rolling(window=z_window, min_periods=1).mean()
                        sigma = s.rolling(window=z_window, min_periods=1).std().replace(0, _np.nan)
                        z = (s - mu) / sigma
                        sig = -z
                        realized_vol = df[col].pct_change().rolling(vol_window, min_periods=1).std().replace(0, 1.0)
                        signals[col] = (sig / realized_vol).fillna(0.0)
                else:  # cross_sectional
                    signals = _pd.DataFrame(0.0, index=df.index, columns=df.columns)
                    for t in range(lookback, len(df)):
                        slice_ret = ret_lb.iloc[t].dropna()
                        if slice_ret.empty:
                            continue
                        mu = slice_ret.mean()
                        sigma = slice_ret.std(ddof=1) if slice_ret.std(ddof=1) > 0 else 1.0
                        z = (slice_ret - mu) / sigma
                        raw = -z  # contrarian
                        if topk is not None:
                            longs = raw.nlargest(topk)
                            shorts = raw.nsmallest(topk)
                            s = _pd.Series(0.0, index=df.columns)
                            s.loc[longs.index] = longs
                            s.loc[shorts.index] = shorts
                        else:
                            s = raw.reindex(df.columns).fillna(0.0)
                        vol = df.pct_change().iloc[t-1].abs().replace(0, 1.0)
                        signals.iloc[t] = (s / vol).fillna(0.0)
                # smoothing
                if decay is not None and 0 < decay < 1:
                    alpha = 1 - decay
                    signals = signals.ewm(alpha=alpha, adjust=False).mean().fillna(0.0)
                mc['last_result']['signals'] = signals
                return signals.fillna(0.0)
            # numpy fallback
            P = _np.asarray(df, dtype=float)
            T, N = P.shape
            logP = _np.log(P)
            ret_lb = _np.zeros_like(P)
            if lookback < T:
                ret_lb[lookback:] = logP[lookback:] - logP[:-lookback]
            S = _np.zeros_like(P)
            if mode == 'time_series':
                for j in range(N):
                    csum = _np.convolve(ret_lb[:, j], _np.ones(z_window), mode='same')
                    mu = _np.nanmean(csum)
                    sigma = _np.nanstd(csum) if _np.nanstd(csum) > 0 else 1.0
                    z = (csum - mu) / sigma
                    S[:, j] = -z
                # vol scaling
                vol = _np.zeros_like(P)
                for j in range(N):
                    pct = _np.zeros(T)
                    pct[1:] = P[1:, j] / (P[:-1, j] + 1e-12) - 1.0
                    vol[:, j] = _rolling_std_np(pct, vol_window)
                S = S / (vol + 1e-9)
            else:
                for t in range(lookback, T):
                    slice_ret = ret_lb[t, :]
                    valid = ~_np.isnan(slice_ret)
                    if not valid.any():
                        continue
                    mu = _np.nanmean(slice_ret[valid])
                    sigma = _np.nanstd(slice_ret[valid]) if _np.nanstd(slice_ret[valid]) > 0 else 1.0
                    z = (slice_ret - mu) / sigma
                    raw = -z
                    if topk is not None:
                        idx = _np.argsort(_np.abs(raw))[::-1][:topk]
                        s = _np.zeros_like(raw)
                        s[idx] = raw[idx]
                    else:
                        s = raw
                    vol = _np.abs(P[t-1, :] / (P[t-2, :] + 1e-12) - 1.0) if t >= 2 else _np.ones_like(s)
                    S[t, :] = s / (vol + 1e-9)
            # smoothing
            if decay is not None and 0 < decay < 1:
                span = max(1, int(1.0 / (1 - decay)))
                for j in range(S.shape[1]):
                    S[:, j] = _ewma_np(S[:, j], span=span)
            mc['last_result']['signals'] = S
            return S

        # ---------- position sizing ----------
        def mc_position_sizing(signals, prices, risk_target=None, vol_window=None, max_leverage=None, min_trade_size=None):
            risk_target = float(risk_target if risk_target is not None else mc['params']['risk_target'])
            vol_window = int(vol_window if vol_window is not None else mc['params']['vol_window'])
            max_leverage = float(max_leverage if max_leverage is not None else mc['params']['max_leverage'])
            min_trade_size = float(min_trade_size if min_trade_size is not None else mc['params']['min_trade_size'])

            P = _ensure_df(prices)
            if _pd is not None and isinstance(P, _pd.DataFrame) and isinstance(signals, _pd.DataFrame):
                ret = P.pct_change().fillna(0.0)
                vol = ret.rolling(window=vol_window, min_periods=1).std().replace(0, _np.nan)
                weights = signals.div(vol).fillna(0.0)
                wts = weights.copy()
                for t in range(len(wts)):
                    raw = wts.iloc[t].fillna(0.0)
                    if raw.abs().sum() < 1e-12:
                        wts.iloc[t] = raw
                        continue
                    approx_vol = _math.sqrt(_np.sum((raw.values**2) * (vol.iloc[t].fillna(0.0).values**2)))
                    scale = 0.0 if approx_vol == 0 or _np.isnan(approx_vol) else min(risk_target / approx_vol, max_leverage)
                    w = raw * scale
                    w = w.where(w.abs() >= min_trade_size, 0.0)
                    wts.iloc[t] = w
                mc['last_result']['positions'] = wts
                return wts.fillna(0.0)
            else:
                S = _np.asarray(signals, dtype=float)
                P_arr = _np.asarray(P, dtype=float)
                T, N = S.shape
                pct = _np.zeros_like(P_arr)
                pct[1:] = P_arr[1:] / (P_arr[:-1] + 1e-12) - 1.0
                vol = _np.zeros_like(P_arr)
                for j in range(N):
                    vol[:, j] = _rolling_std_np(pct[:, j], vol_window)
                wts = _np.zeros_like(S)
                for t in range(T):
                    raw = S[t, :]
                    denom = _math.sqrt(_np.nansum((raw**2) * (vol[t]**2)))
                    scale = 0.0 if denom == 0 or _np.isnan(denom) else min(risk_target / denom, max_leverage)
                    w = raw * scale
                    w[_np.abs(w) < min_trade_size] = 0.0
                    wts[t, :] = w
                mc['last_result']['positions'] = wts
                return wts

        # ---------- backtest ----------
        def mc_backtest(prices, positions, fees=None, slippage=None, freq='D'):
            fees = float(fees if fees is not None else mc['params']['fee'])
            slippage = float(slippage if slippage is not None else mc['params']['slippage'])
            P = _ensure_df(prices)
            # pandas path
            if _pd is not None and isinstance(P, _pd.DataFrame) and isinstance(positions, _pd.DataFrame):
                ret = P.pct_change().fillna(0.0)
                W = positions.reindex(index=P.index, columns=P.columns).fillna(0.0)
                port_ret = (W.shift(1) * ret).sum(axis=1)
                wchg = (W - W.shift(1)).abs().sum(axis=1).fillna(0.0)
                cost = wchg * (fees + slippage)
                net = port_ret - cost
                nav = (1 + net).cumprod()
                metrics = mc_evaluate_performance(net, freq=freq)
                mc['last_result']['backtest'] = {'returns': net, 'nav': nav, 'metrics': metrics}
                return {'returns': net, 'nav': nav, 'metrics': metrics}
            # numpy fallback
            P_arr = _np.asarray(P, dtype=float)
            R = _np.zeros_like(P_arr)
            R[1:] = P_arr[1:] / (P_arr[:-1] + 1e-12) - 1.0
            W = _np.asarray(positions, dtype=float)
            port_ret = _np.sum(_np.roll(W, 1, axis=0) * R, axis=1)
            wchg = _np.sum(_np.abs(W - _np.roll(W, 1, axis=0)), axis=1)
            cost = wchg * (fees + slippage)
            net = port_ret - cost
            nav = _np.cumprod(1 + net)
            metrics = mc_evaluate_performance(net, freq=freq)
            mc['last_result']['backtest'] = {'returns': net, 'nav': nav, 'metrics': metrics}
            return {'returns': net, 'nav': nav, 'metrics': metrics}

        # ---------- performance ----------
        def mc_evaluate_performance(returns_ts, freq='D'):
            if _pd is not None and hasattr(returns_ts, 'values'):
                r = returns_ts.fillna(0.0).values
            else:
                r = _np.asarray(returns_ts, dtype=float)
            per_year = 252 if freq.upper() == 'D' else (52 if freq.upper() == 'W' else 12)
            avg = _np.nanmean(r)
            ann_ret = (1 + avg) ** per_year - 1 if avg > -1 else _np.nan
            ann_vol = _np.nanstd(r, ddof=1) * _np.sqrt(per_year)
            sharpe = (avg * per_year) / ann_vol if ann_vol > 0 else _np.nan
            try:
                nav = _np.cumprod(1 + r)
                cagr = nav[-1] ** (1.0 / (len(r) / per_year)) - 1 if len(r) > 0 else _np.nan
                peak = _np.maximum.accumulate(nav)
                dd = (nav - peak) / peak
                maxdd = float(_np.min(dd))
            except Exception:
                cagr = _np.nan; maxdd = _np.nan
            return {'annual_return': ann_ret, 'annual_vol': ann_vol, 'sharpe': sharpe, 'cagr': cagr, 'max_drawdown': maxdd}

        # ---------- hyperparameter tuning ----------
        def mc_tune_hyperparameters(prices, hyper_grid, scoring='sharpe', n_jobs=1, freq='D'):
            import itertools as _it
            P = _ensure_df(prices)
            keys = list(hyper_grid.keys())
            combos = list(_it.product(*[hyper_grid[k] for k in keys]))
            results = []
            for combo in combos:
                params = dict(zip(keys, combo))
                sig = mc_generate_signals(P,
                                        mode=params.get('mode', None),
                                        lookback=params.get('lookback', None),
                                        z_window=params.get('z_window', None),
                                        vol_window=params.get('vol_window', None),
                                        decay=params.get('decay', None),
                                        topk=params.get('topk', None))
                pos = mc_position_sizing(sig, P, risk_target=params.get('risk_target', None),
                                        vol_window=params.get('vol_window', None),
                                        max_leverage=params.get('max_leverage', None))
                bt = mc_backtest(P, pos, fees=mc['params']['fee'], slippage=mc['params']['slippage'], freq=freq)
                metric = bt['metrics'].get(scoring) if isinstance(bt['metrics'], dict) else bt['metrics']
                results.append({'params': params, 'metrics': bt['metrics']})
            best = sorted(results, key=lambda x: x['metrics'].get(scoring) if x['metrics'].get(scoring) is not None else -_np.inf, reverse=True)[0]
            mc['last_result']['tuning'] = {'all': results, 'best': best}
            return {'all': results, 'best': best}

        # ---------- persistence ----------
        def mc_save_model(path):
            meta = {'module': 'momentum_contrarian', 'params': mc['params'], 'timestamp': _time.time()}
            d = _os.path.dirname(path)
            if d:
                _os.makedirs(d, exist_ok=True)
            with open(path, 'wb') as fh:
                _pickle.dump({'meta': meta, 'last_result': mc['last_result']}, fh)
            return {'status': 'saved', 'path': path}

        def mc_load_model(path):
            with open(path, 'rb') as fh:
                data = _pickle.load(fh)
            mc['last_result'] = data.get('last_result', {})
            return {'status': 'loaded', 'path': path, 'meta': data.get('meta', None)}

        # attach to self
        self.momentum_contrarian = mc
        self.mc_generate_signals = mc_generate_signals
        self.mc_position_sizing = mc_position_sizing
        self.mc_backtest = mc_backtest
        self.mc_evaluate_performance = mc_evaluate_performance
        self.mc_tune_hyperparameters = mc_tune_hyperparameters
        self.mc_save_model = mc_save_model
        self.mc_load_model = mc_load_model

        mc['initialized'] = True
        return mc

    def analyze_cognitive_biases(self, data_sequence: List[Dict], 
                               decision_context: Dict = None) -> Dict:
        """
        åˆ†æè®¤çŸ¥åå·® - å¤šç»´åº¦ç»¼åˆåˆ†æ
        
        Args:
            data_sequence: å†å²æ•°æ®åºåˆ—
            decision_context: å†³ç­–ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: è®¤çŸ¥åå·®åˆ†æç»“æœ
        """
        try:
            print(f"ğŸ§  å¼€å§‹è®¤çŸ¥åå·®åˆ†æ...")
            
            if len(data_sequence) < 5:
                return self._generate_insufficient_data_result()
            
            # === 1. æ ¸å¿ƒè®¤çŸ¥åå·®æ£€æµ‹ ===
            
            # é”šå®šåå·®åˆ†æ
            anchoring_analysis = self._analyze_anchoring_bias_comprehensive(data_sequence)
            
            # å¯å¾—æ€§åå·®åˆ†æ
            availability_analysis = self._analyze_availability_bias_comprehensive(data_sequence)
            
            # ç¡®è®¤åå·®åˆ†æ
            confirmation_analysis = self._analyze_confirmation_bias_comprehensive(data_sequence)
            
            # ä»£è¡¨æ€§åå·®åˆ†æ
            representativeness_analysis = self._analyze_representativeness_bias_comprehensive(
                data_sequence
            )
            
            # è¿‘å› åå·®åˆ†æ
            recency_analysis = self._analyze_recency_bias_comprehensive(data_sequence)
            
            # æŸå¤±åŒæ¶åˆ†æ
            loss_aversion_analysis = self._analyze_loss_aversion_comprehensive(
                data_sequence, decision_context
            )
            
            # è¿‡åº¦è‡ªä¿¡åˆ†æ
            overconfidence_analysis = self._analyze_overconfidence_bias_comprehensive(
                data_sequence
            )
            
            # æ¡†æ¶æ•ˆåº”åˆ†æ
            framing_analysis = self._analyze_framing_effect_comprehensive(
                data_sequence, decision_context
            )
            
            # === 2. é«˜çº§åå·®æ¨¡å‹åˆ†æ ===
            
            # å‰æ™¯ç†è®ºåˆ†æ
            prospect_theory_analysis = self._apply_prospect_theory_analysis(
                data_sequence, decision_context
            )
            
            # åŒç³»ç»Ÿç†è®ºåˆ†æ
            dual_process_analysis = self._apply_dual_process_analysis(data_sequence)
            
            # è´å¶æ–¯åå·®åˆ†æ
            bayesian_bias_analysis = self._apply_bayesian_bias_analysis(data_sequence)
            
            # === 3. åå·®äº¤äº’æ•ˆåº”åˆ†æ ===
            bias_interactions = self._analyze_bias_interaction_effects({
                'anchoring': anchoring_analysis,
                'availability': availability_analysis,
                'confirmation': confirmation_analysis,
                'representativeness': representativeness_analysis,
                'recency': recency_analysis,
                'loss_aversion': loss_aversion_analysis,
                'overconfidence': overconfidence_analysis,
                'framing': framing_analysis
            })
            
            # === 4. èµ”ç‡ç‰¹å®šåå·®åˆ†æ ===
            odds_specific_biases = self._analyze_odds_specific_biases(
                data_sequence, decision_context
            )
            
            # === 5. æ—¶é—´åŠ¨æ€åå·®åˆ†æ ===
            temporal_bias_dynamics = self._analyze_temporal_bias_dynamics(data_sequence)
            
            # === 6. ç¤¾ä¼šè®¤çŸ¥åå·®åˆ†æ ===
            social_cognitive_biases = self._analyze_social_cognitive_biases(data_sequence)
            
            # === 7. å…ƒè®¤çŸ¥åå·®åˆ†æ ===
            metacognitive_biases = self._analyze_metacognitive_biases(data_sequence)
            
            # === 8. åå·®å¼ºåº¦ç»¼åˆè¯„ä¼° ===
            comprehensive_bias_assessment = self._assess_comprehensive_bias_strength({
                'anchoring': anchoring_analysis,
                'availability': availability_analysis,
                'confirmation': confirmation_analysis,
                'representativeness': representativeness_analysis,
                'recency': recency_analysis,
                'loss_aversion': loss_aversion_analysis,
                'overconfidence': overconfidence_analysis,
                'framing': framing_analysis,
                'prospect_theory': prospect_theory_analysis,
                'dual_process': dual_process_analysis,
                'bayesian': bayesian_bias_analysis
            })
            
            # === 9. åå·®å½±å“è¯„ä¼° ===
            bias_impact_assessment = self._assess_bias_impact_on_decisions(
                comprehensive_bias_assessment, decision_context
            )
            
            # === 10. åå·®ç¼“è§£ç­–ç•¥ ===
            bias_mitigation_strategies = self._generate_bias_mitigation_strategies(
                comprehensive_bias_assessment, bias_interactions
            )
            
            # === æ›´æ–°åå·®å†å² ===
            bias_record = {
                'timestamp': self._get_timestamp(),
                'data_length': len(data_sequence),
                'bias_scores': comprehensive_bias_assessment,
                'dominant_biases': self._identify_dominant_biases(comprehensive_bias_assessment),
                'bias_stability': self._assess_bias_stability(comprehensive_bias_assessment),
                'detection_confidence': self._calculate_bias_detection_confidence(
                    comprehensive_bias_assessment
                )
            }
            
            self.bias_strength_history.append(bias_record)
            
            # === æ„å»ºå®Œæ•´åˆ†æç»“æœ ===
            comprehensive_bias_analysis = {
                'timestamp': self._get_timestamp(),
                
                # æ ¸å¿ƒåå·®åˆ†æ
                'anchoring_analysis': anchoring_analysis,
                'availability_analysis': availability_analysis,
                'confirmation_analysis': confirmation_analysis,
                'representativeness_analysis': representativeness_analysis,
                'recency_analysis': recency_analysis,
                'loss_aversion_analysis': loss_aversion_analysis,
                'overconfidence_analysis': overconfidence_analysis,
                'framing_analysis': framing_analysis,
                
                # é«˜çº§æ¨¡å‹åˆ†æ
                'prospect_theory_analysis': prospect_theory_analysis,
                'dual_process_analysis': dual_process_analysis,
                'bayesian_bias_analysis': bayesian_bias_analysis,
                
                # å¤åˆåˆ†æ
                'bias_interactions': bias_interactions,
                'odds_specific_biases': odds_specific_biases,
                'temporal_bias_dynamics': temporal_bias_dynamics,
                'social_cognitive_biases': social_cognitive_biases,
                'metacognitive_biases': metacognitive_biases,
                
                # ç»¼åˆè¯„ä¼°
                'comprehensive_bias_assessment': comprehensive_bias_assessment,
                'bias_impact_assessment': bias_impact_assessment,
                'dominant_biases': bias_record['dominant_biases'],
                'overall_bias_intensity': self._calculate_overall_bias_intensity(
                    comprehensive_bias_assessment
                ),
                'bias_risk_level': self._assess_bias_risk_level(comprehensive_bias_assessment),
                
                # åº”ç”¨æŒ‡å¯¼
                'bias_mitigation_strategies': bias_mitigation_strategies,
                'decision_quality_impact': self._assess_decision_quality_impact(
                    comprehensive_bias_assessment
                ),
                'rationality_index': self._calculate_rationality_index(
                    comprehensive_bias_assessment
                ),
                
                # è´¨é‡ä¿è¯
                'detection_confidence': bias_record['detection_confidence'],
                'analysis_reliability': self._assess_analysis_reliability(bias_record),
                'model_consensus': self._calculate_model_consensus({
                    'anchoring': anchoring_analysis,
                    'availability': availability_analysis,
                    'confirmation': confirmation_analysis
                }),
                
                # å…ƒæ•°æ®
                'analysis_method': 'multi_model_cognitive_bias_analysis',
                'models_used': list(self.bias_detection_models.keys()),
                'analysis_depth': len(data_sequence),
                'context_considered': decision_context is not None
            }
            
            # === è‡ªé€‚åº”å­¦ä¹ æ›´æ–° ===
            self._update_bias_learning_system(comprehensive_bias_analysis)
            
            print(f"âœ… è®¤çŸ¥åå·®åˆ†æå®Œæˆ - æ€»ä½“å¼ºåº¦: {comprehensive_bias_analysis['overall_bias_intensity']:.3f}")
            
            return comprehensive_bias_analysis
            
        except Exception as e:
            print(f"âŒ è®¤çŸ¥åå·®åˆ†æå¤±è´¥: {e}")
            return self._generate_error_result(str(e))
    
    def _analyze_anchoring_bias_comprehensive(self, data_sequence: List[Dict]) -> Dict:
        """ç»¼åˆé”šå®šåå·®åˆ†æ"""
        try:
            import numpy as np
            
            anchoring_effects = {}
            anchor_persistence = {}
            recency_anchoring = {}
            
            # åˆ†ææ¯ä¸ªå°¾æ•°ä½œä¸ºé”šç‚¹çš„æ•ˆåº”
            for anchor_tail in range(10):
                anchor_instances = []
                subsequent_influences = []
                
                # å¯»æ‰¾é”šç‚¹å‡ºç°ä½ç½®
                for i, period in enumerate(data_sequence):
                    if anchor_tail in period.get('tails', []):
                        anchor_instances.append(i)
                        
                        # åˆ†æåç»­å½±å“
                        for j in range(1, min(6, len(data_sequence) - i)):
                            if i + j < len(data_sequence):
                                future_tails = data_sequence[i + j].get('tails', [])
                                if anchor_tail in future_tails:
                                    # è€ƒè™‘èµ”ç‡æƒé‡
                                    odds_weight = 2.0 if anchor_tail == 0 else 1.8
                                    influence_strength = (odds_weight / 2.0) * (1.0 / j)  # è·ç¦»è¡°å‡
                                    subsequent_influences.append(influence_strength)
                
                # è®¡ç®—é”šå®šæ•ˆåº”å¼ºåº¦
                if anchor_instances:
                    anchoring_strength = len(subsequent_influences) / len(anchor_instances)
                    anchoring_effects[anchor_tail] = anchoring_strength
                    
                    # è®¡ç®—æŒç»­æ€§
                    if subsequent_influences:
                        persistence = np.mean(subsequent_influences)
                        anchor_persistence[anchor_tail] = persistence
                    else:
                        anchor_persistence[anchor_tail] = 0.0
                else:
                    anchoring_effects[anchor_tail] = 0.0
                    anchor_persistence[anchor_tail] = 0.0
            
            # è¿‘æœŸé”šå®šæ•ˆåº”åˆ†æ
            recent_periods = min(5, len(data_sequence))
            for i in range(recent_periods):
                period_tails = data_sequence[i].get('tails', [])
                recency_weight = 1.0 / (i + 1)  # æ—¶é—´è¡°å‡æƒé‡
                
                for tail in period_tails:
                    if tail in recency_anchoring:
                        recency_anchoring[tail] += recency_weight
                    else:
                        recency_anchoring[tail] = recency_weight
            
            # èµ”ç‡æ•æ„Ÿæ€§é”šå®šåˆ†æ
            zero_tail_anchoring = anchoring_effects.get(0, 0) * self.odds_bias_parameters['zero_tail_anchoring_strength']
            other_tails_anchoring = np.mean([anchoring_effects.get(i, 0) for i in range(1, 10)])
            
            # åºåˆ—ä½ç½®é”šå®šæ•ˆåº”
            positional_anchoring = self._analyze_positional_anchoring_effects(data_sequence)
            
            # é”šå®šåå·®çš„æ—¶é—´è¡°å‡æ¨¡å‹
            temporal_decay_model = self._model_anchoring_temporal_decay(
                anchoring_effects, anchor_persistence
            )
            
            # ç»¼åˆé”šå®šåå·®å¼ºåº¦
            overall_anchoring_strength = (
                np.mean(list(anchoring_effects.values())) * 0.4 +
                np.mean(list(anchor_persistence.values())) * 0.3 +
                np.mean(list(recency_anchoring.values())) if recency_anchoring else 0 * 0.2 +
                positional_anchoring * 0.1
            )
            
            return {
                'anchoring_strength': float(overall_anchoring_strength),
                'anchoring_effects': anchoring_effects,
                'anchor_persistence': anchor_persistence,
                'recency_anchoring': recency_anchoring,
                'zero_tail_anchoring': float(zero_tail_anchoring),
                'other_tails_anchoring': float(other_tails_anchoring),
                'odds_anchoring_differential': float(abs(zero_tail_anchoring - other_tails_anchoring)),
                'positional_anchoring': float(positional_anchoring),
                'temporal_decay_model': temporal_decay_model,
                'confidence': min(1.0, len(data_sequence) / 20.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'anchoring_strength': 0.0}
    
    def _apply_prospect_theory_analysis(self, data_sequence: List[Dict], 
                                      decision_context: Dict = None) -> Dict:
        """åº”ç”¨å‰æ™¯ç†è®ºè¿›è¡Œåå·®åˆ†æ"""
        try:
            import numpy as np
            import math
            
            # å‰æ™¯ç†è®ºå‚æ•°
            alpha = 0.88  # ä»·å€¼å‡½æ•°å‡¹å‡¸æ€§å‚æ•°
            beta = 0.88   # æŸå¤±åŸŸå‡¹å‡¸æ€§å‚æ•°
            lambda_param = 2.25  # æŸå¤±åŒæ¶ç³»æ•°
            
            # æ„å»ºä»·å€¼å‡½æ•°
            def value_function(x, reference_point=0):
                if x >= reference_point:
                    return (x - reference_point) ** alpha
                else:
                    return -lambda_param * ((reference_point - x) ** beta)
            
            # æ¦‚ç‡æƒé‡å‡½æ•°
            def probability_weighting(p, gamma=0.61):
                return (p ** gamma) / ((p ** gamma + (1 - p) ** gamma) ** (1/gamma))
            
            # åˆ†ææ¯æœŸçš„å‰æ™¯ä»·å€¼
            prospect_values = []
            reference_points = []
            
            for i, period in enumerate(data_sequence):
                tails = period.get('tails', [])
                
                # è®¡ç®—å‚è€ƒç‚¹ï¼ˆåŸºäºå†å²å¹³å‡æ”¶ç›Šï¼‰
                if i > 0:
                    historical_outcomes = self._extract_historical_outcomes(data_sequence[:i])
                    reference_point = np.mean(historical_outcomes) if historical_outcomes else 0
                else:
                    reference_point = 0
                
                reference_points.append(reference_point)
                
                # è®¡ç®—æœŸæœ›å‰æ™¯ä»·å€¼
                period_prospect_value = 0
                for tail in tails:
                    # è·å–å°¾æ•°å¯¹åº”çš„èµ”ç‡å’Œæ¦‚ç‡
                    odds = 2.0 if tail == 0 else 1.8
                    probability = 1.0 / 10.0  # ç®€åŒ–å‡è®¾ç­‰æ¦‚ç‡
                    
                    # è®¡ç®—æ½œåœ¨æ”¶ç›Š/æŸå¤±
                    potential_gain = odds - 1  # å‡€æ”¶ç›Š
                    potential_loss = -1  # æŸå¤±æœ¬é‡‘
                    
                    # åº”ç”¨ä»·å€¼å‡½æ•°
                    gain_value = value_function(potential_gain, reference_point)
                    loss_value = value_function(potential_loss, reference_point)
                    
                    # åº”ç”¨æ¦‚ç‡æƒé‡
                    weighted_gain_prob = probability_weighting(probability)
                    weighted_loss_prob = probability_weighting(1 - probability)
                    
                    # è®¡ç®—å‰æ™¯ä»·å€¼
                    tail_prospect_value = (
                        weighted_gain_prob * gain_value +
                        weighted_loss_prob * loss_value
                    )
                    
                    period_prospect_value += tail_prospect_value
                
                prospect_values.append(period_prospect_value)
            
            # å‰æ™¯ç†è®ºåå·®æŒ‡æ ‡
            
            # 1. æŸå¤±åŒæ¶å¼ºåº¦
            loss_aversion_strength = self._measure_loss_aversion_strength(
                prospect_values, reference_points
            )
            
            # 2. æ¦‚ç‡æ‰­æ›²ç¨‹åº¦
            probability_distortion = self._measure_probability_distortion(data_sequence)
            
            # 3. å‚è€ƒç‚¹ä¾èµ–æ€§
            reference_point_dependence = self._measure_reference_point_dependence(
                prospect_values, reference_points
            )
            
            # 4. é£é™©å¯»æ±‚vsé£é™©åŒæ¶æ¨¡å¼
            risk_attitude_pattern = self._analyze_risk_attitude_pattern(prospect_values)
            
            # 5. æ¡†æ¶æ•ˆåº”å¼ºåº¦
            framing_effect_strength = self._measure_framing_effect_strength(
                data_sequence, prospect_values
            )
            
            # 6. æ—¶é—´ä¸ä¸€è‡´åå¥½
            temporal_inconsistency = self._measure_temporal_inconsistency(prospect_values)
            
            # ç»¼åˆå‰æ™¯ç†è®ºåå·®å¾—åˆ†
            prospect_theory_bias_score = (
                loss_aversion_strength * 0.3 +
                probability_distortion * 0.25 +
                reference_point_dependence * 0.2 +
                framing_effect_strength * 0.15 +
                temporal_inconsistency * 0.1
            )
            
            return {
                'prospect_theory_bias_score': float(prospect_theory_bias_score),
                'prospect_values': prospect_values,
                'reference_points': reference_points,
                'loss_aversion_strength': float(loss_aversion_strength),
                'probability_distortion': float(probability_distortion),
                'reference_point_dependence': float(reference_point_dependence),
                'risk_attitude_pattern': risk_attitude_pattern,
                'framing_effect_strength': float(framing_effect_strength),
                'temporal_inconsistency': float(temporal_inconsistency),
                'model_parameters': {
                    'alpha': alpha,
                    'beta': beta,
                    'lambda': lambda_param
                },
                'confidence': min(1.0, len(data_sequence) / 25.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'prospect_theory_bias_score': 0.0}

class EmotionDynamicsTracker:
    """æƒ…ç»ªåŠ¨åŠ›å­¦è¿½è¸ªå™¨"""
    def __init__(self):
        self.emotion_states = []
        self.tracking_window = 10
    
    def track_emotions(self, data):
        """è¿½è¸ªæƒ…ç»ªå˜åŒ–"""
        try:
            return {
                'current_emotion': 'optimistic',
                'emotion_intensity': 0.65,
                'trend': 'increasing'
            }
        except Exception as e:
            return {'error': str(e)}

class ContrarianStrategyGenerator:
    """
    ç§‘ç ”çº§åå‘ç­–ç•¥ç”Ÿæˆå™¨ - åŸºäºåšå¼ˆè®ºå’Œåå‘æŠ•èµ„ç†è®º
    
    ç†è®ºåŸºç¡€ï¼š
    - åå‘æŠ•èµ„ç†è®ºï¼ˆContrarian Investment Theoryï¼‰
    - è¡Œä¸ºé‡‘èå­¦åå‘ç­–ç•¥
    - åšå¼ˆè®ºä¸­çš„åå‘å‡è¡¡
    - å¸‚åœºå¼‚è±¡åˆ©ç”¨ç†è®º
    - ç¾¤ä½“æ™ºæ…§vsç¾¤ä½“æ„šè ¢ç†è®º
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç§‘ç ”çº§åå‘ç­–ç•¥ç”Ÿæˆå™¨"""
        print("ğŸ”„ å¯åŠ¨ç§‘ç ”çº§åå‘ç­–ç•¥ç”Ÿæˆå™¨...")
        
        # ç­–ç•¥æˆåŠŸå†å²å’Œæ¨¡å¼
        self.strategy_success_history = deque(maxlen=1000)
        self.strategy_patterns = {
            'timing_patterns': {},          # æ—¶æœºæ¨¡å¼
            'intensity_patterns': {},       # å¼ºåº¦æ¨¡å¼
            'duration_patterns': {},        # æŒç»­æ—¶é—´æ¨¡å¼
            'market_condition_patterns': {},# å¸‚åœºæ¡ä»¶æ¨¡å¼
            'crowd_behavior_patterns': {},  # ç¾¤ä½“è¡Œä¸ºæ¨¡å¼
            'volatility_patterns': {},      # æ³¢åŠ¨æ€§æ¨¡å¼
            'sentiment_patterns': {},       # æƒ…ç»ªæ¨¡å¼
            'contrarian_cycles': {}         # åå‘å‘¨æœŸ
        }
        
        # å¤šå±‚æ¬¡åå‘ç­–ç•¥æ¨¡å‹
        self.strategy_models = {
            'momentum_contrarian_model': self._init_momentum_contrarian_model(),
            'sentiment_contrarian_model': self._init_sentiment_contrarian_model(),
            'volatility_contrarian_model': self._init_volatility_contrarian_model(),
            'crowd_contrarian_model': self._init_crowd_contrarian_model(),
            'value_contrarian_model': self._init_value_contrarian_model(),
            'technical_contrarian_model': self._init_technical_contrarian_model(),
            'behavioral_contrarian_model': self._init_behavioral_contrarian_model(),
            'game_theory_contrarian_model': self._init_game_theory_contrarian_model()
        }
        
        # åå‘ç­–ç•¥æ€§èƒ½æŒ‡æ ‡
        self.strategy_metrics = {
            'contrarian_success_rate': 0.0,      # åå‘æˆåŠŸç‡
            'contrarian_precision': 0.0,         # åå‘ç²¾ç¡®åº¦
            'contrarian_recall': 0.0,            # åå‘å¬å›ç‡
            'contrarian_f1_score': 0.0,          # åå‘F1åˆ†æ•°
            'contrarian_sharpe_ratio': 0.0,      # åå‘å¤æ™®æ¯”ç‡
            'contrarian_max_drawdown': 0.0,      # åå‘æœ€å¤§å›æ’¤
            'contrarian_win_loss_ratio': 0.0,    # åå‘èƒœè´Ÿæ¯”
            'contrarian_profit_factor': 0.0,     # åå‘åˆ©æ¶¦å› å­
            'contrarian_consistency': 0.0,       # åå‘ä¸€è‡´æ€§
            'contrarian_adaptability': 0.0       # åå‘é€‚åº”æ€§
        }
        
        # åŠ¨æ€ç­–ç•¥å‚æ•°
        self.strategy_parameters = {
            'contrarian_sensitivity': 0.75,      # åå‘æ•æ„Ÿåº¦
            'timing_precision': 0.8,             # æ—¶æœºç²¾ç¡®åº¦
            'intensity_threshold': 0.7,          # å¼ºåº¦é˜ˆå€¼
            'duration_factor': 0.6,              # æŒç»­å› å­
            'risk_tolerance': 0.5,               # é£é™©å®¹å¿åº¦
            'adaptation_speed': 0.1,             # é€‚åº”é€Ÿåº¦
            'confidence_threshold': 0.65,        # ç½®ä¿¡åº¦é˜ˆå€¼
            'diversification_factor': 0.3        # åˆ†æ•£åŒ–å› å­
        }
        
        # èµ”ç‡ä¼˜åŒ–ç­–ç•¥å‚æ•°
        self.odds_strategy_parameters = {
            'zero_tail_contrarian_multiplier': 1.4,  # 0å°¾åå‘å€æ•°
            'high_odds_contrarian_bonus': 0.2,       # é«˜èµ”ç‡åå‘å¥–åŠ±
            'odds_differential_exploitation': 0.15,  # èµ”ç‡å·®å¼‚åˆ©ç”¨
            'risk_adjusted_contrarian_factor': 1.2   # é£é™©è°ƒæ•´åå‘å› å­
        }
        
        # ç­–ç•¥å­¦ä¹ å’Œä¼˜åŒ–ç³»ç»Ÿ
        self.learning_system = {
            'strategy_performance_tracking': [],
            'parameter_optimization_history': [],
            'adaptive_learning_rate': 0.08,
            'strategy_evolution_tracking': [],
            'ensemble_weight_optimization': {},
            'cross_validation_results': [],
            'hyperparameter_tuning_results': {},
            'meta_learning_insights': []
        }
        
        # é«˜çº§åˆ†æå’Œä¼˜åŒ–å·¥å…·
        self.optimization_tools = {
            'genetic_algorithm': self._init_genetic_algorithm(),
            'simulated_annealing': self._init_simulated_annealing(),
            'particle_swarm_optimization': self._init_particle_swarm(),
            'bayesian_optimization': self._init_bayesian_optimization(),
            'reinforcement_learning': self._init_reinforcement_learning(),
            'neural_evolution': self._init_neural_evolution()
        }
        
        print("âœ… ç§‘ç ”çº§åå‘ç­–ç•¥ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_contrarian_strategy(self, market_data: List[Dict], 
                                   crowd_behavior: Dict, 
                                   emotion_state: Dict,
                                   candidates: List[int] = None) -> Dict:
        """
        ç”Ÿæˆåå‘ç­–ç•¥ - å¤šæ¨¡å‹é›†æˆä¼˜åŒ–
        
        Args:
            market_data: å¸‚åœºæ•°æ®åºåˆ—
            crowd_behavior: ç¾¤ä½“è¡Œä¸ºåˆ†æç»“æœ
            emotion_state: æƒ…ç»ªçŠ¶æ€åˆ†æç»“æœ
            candidates: å€™é€‰å°¾æ•°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: åå‘ç­–ç•¥ç”Ÿæˆç»“æœ
        """
        try:
            print(f"ğŸ”„ å¼€å§‹åå‘ç­–ç•¥ç”Ÿæˆ...")
            
            if len(market_data) < 5:
                return self._generate_insufficient_data_result()
            
            # === 1. å¸‚åœºæ¡ä»¶åˆ†æ ===
            
            # å¸‚åœºçŠ¶æ€è¯†åˆ«
            market_state_analysis = self._analyze_market_state(market_data)
            
            # è¶‹åŠ¿å¼ºåº¦åˆ†æ
            trend_strength_analysis = self._analyze_trend_strength(market_data)
            
            # æ³¢åŠ¨æ€§åˆ†æ
            volatility_analysis = self._analyze_market_volatility(market_data)
            
            # æµåŠ¨æ€§åˆ†æ
            liquidity_analysis = self._analyze_market_liquidity(market_data)
            
            # === 2. ç¾¤ä½“è¡Œä¸ºåå‘æœºä¼šè¯†åˆ« ===
            
            # ç¾Šç¾¤è¡Œä¸ºåå‘æœºä¼š
            herd_contrarian_opportunities = self._identify_herd_contrarian_opportunities(
                crowd_behavior, market_data
            )
            
            # ç¾¤ä½“æƒ…ç»ªåå‘æœºä¼š
            emotion_contrarian_opportunities = self._identify_emotion_contrarian_opportunities(
                emotion_state, market_data
            )
            
            # ç¤¾ä¼šè¯æ˜åå‘æœºä¼š
            social_proof_contrarian = self._identify_social_proof_contrarian_opportunities(
                crowd_behavior, emotion_state
            )
            
            # === 3. å¤šæ¨¡å‹åå‘ç­–ç•¥ç”Ÿæˆ ===
            
            # åŠ¨é‡åå‘ç­–ç•¥
            momentum_contrarian_strategy = self._generate_momentum_contrarian_strategy(
                trend_strength_analysis, volatility_analysis
            )
            
            # æƒ…ç»ªåå‘ç­–ç•¥
            sentiment_contrarian_strategy = self._generate_sentiment_contrarian_strategy(
                emotion_state, crowd_behavior
            )
            
            # æ³¢åŠ¨æ€§åå‘ç­–ç•¥
            volatility_contrarian_strategy = self._generate_volatility_contrarian_strategy(
                volatility_analysis, market_state_analysis
            )
            
            # ä»·å€¼åå‘ç­–ç•¥
            value_contrarian_strategy = self._generate_value_contrarian_strategy(
                market_data, candidates
            )
            
            # æŠ€æœ¯é¢åå‘ç­–ç•¥
            technical_contrarian_strategy = self._generate_technical_contrarian_strategy(
                market_data, trend_strength_analysis
            )
            
            # è¡Œä¸ºåå‘ç­–ç•¥
            behavioral_contrarian_strategy = self._generate_behavioral_contrarian_strategy(
                crowd_behavior, emotion_state
            )
            
            # åšå¼ˆè®ºåå‘ç­–ç•¥
            game_theory_contrarian_strategy = self._generate_game_theory_contrarian_strategy(
                market_data, crowd_behavior
            )
            
            # === 4. èµ”ç‡ä¼˜åŒ–åå‘ç­–ç•¥ ===
            odds_optimized_contrarian = self._generate_odds_optimized_contrarian_strategy(
                candidates, market_data, crowd_behavior
            )
            
            # === 5. ç­–ç•¥é›†æˆå’Œä¼˜åŒ– ===
            
            # ç­–ç•¥æƒé‡ä¼˜åŒ–
            strategy_weights = self._optimize_strategy_weights({
                'momentum': momentum_contrarian_strategy,
                'sentiment': sentiment_contrarian_strategy,
                'volatility': volatility_contrarian_strategy,
                'value': value_contrarian_strategy,
                'technical': technical_contrarian_strategy,
                'behavioral': behavioral_contrarian_strategy,
                'game_theory': game_theory_contrarian_strategy,
                'odds_optimized': odds_optimized_contrarian
            })
            
            # é›†æˆç­–ç•¥ç”Ÿæˆ
            ensemble_contrarian_strategy = self._generate_ensemble_contrarian_strategy(
                {
                    'momentum': momentum_contrarian_strategy,
                    'sentiment': sentiment_contrarian_strategy,
                    'volatility': volatility_contrarian_strategy,
                    'value': value_contrarian_strategy,
                    'technical': technical_contrarian_strategy,
                    'behavioral': behavioral_contrarian_strategy,
                    'game_theory': game_theory_contrarian_strategy,
                    'odds_optimized': odds_optimized_contrarian
                },
                strategy_weights
            )
            
            # === 6. ç­–ç•¥æ—¶æœºä¼˜åŒ– ===
            
            # æœ€ä¼˜è¿›å…¥æ—¶æœº
            optimal_entry_timing = self._optimize_contrarian_entry_timing(
                ensemble_contrarian_strategy, market_state_analysis
            )
            
            # æŒç»­æ—¶é—´é¢„æµ‹
            strategy_duration_prediction = self._predict_contrarian_strategy_duration(
                ensemble_contrarian_strategy, volatility_analysis
            )
            
            # é€€å‡ºç­–ç•¥è®¾è®¡
            exit_strategy_design = self._design_contrarian_exit_strategy(
                ensemble_contrarian_strategy, trend_strength_analysis
            )
            
            # === 7. é£é™©ç®¡ç†å’Œä¼˜åŒ– ===
            
            # é£é™©è¯„ä¼°
            strategy_risk_assessment = self._assess_contrarian_strategy_risk(
                ensemble_contrarian_strategy, market_data
            )
            
            # èµ„é‡‘ç®¡ç†
            position_sizing_strategy = self._optimize_contrarian_position_sizing(
                ensemble_contrarian_strategy, strategy_risk_assessment
            )
            
            # å¯¹å†²ç­–ç•¥
            hedging_strategy = self._design_contrarian_hedging_strategy(
                ensemble_contrarian_strategy, volatility_analysis
            )
            
            # === 8. ç­–ç•¥éªŒè¯å’Œå›æµ‹ ===
            
            # å†å²å›æµ‹
            backtest_results = self._backtest_contrarian_strategy(
                ensemble_contrarian_strategy, market_data
            )
            
            # æ•æ„Ÿæ€§åˆ†æ
            sensitivity_analysis = self._perform_contrarian_sensitivity_analysis(
                ensemble_contrarian_strategy, strategy_parameters
            )
            
            # é²æ£’æ€§æµ‹è¯•
            robustness_testing = self._test_contrarian_strategy_robustness(
                ensemble_contrarian_strategy, market_data
            )
            
            # === 9. æ€§èƒ½é¢„æœŸå’Œç½®ä¿¡åº¦ ===
            
            # é¢„æœŸæ”¶ç›Šåˆ†æ
            expected_returns = self._calculate_contrarian_expected_returns(
                ensemble_contrarian_strategy, backtest_results
            )
            
            # ç½®ä¿¡åº¦è¯„ä¼°
            strategy_confidence = self._assess_contrarian_strategy_confidence(
                ensemble_contrarian_strategy, backtest_results, robustness_testing
            )
            
            # æˆåŠŸæ¦‚ç‡ä¼°è®¡
            success_probability = self._estimate_contrarian_success_probability(
                ensemble_contrarian_strategy, market_state_analysis
            )
            
            # === æ›´æ–°ç­–ç•¥å†å² ===
            strategy_record = {
                'timestamp': self._get_timestamp(),
                'strategy_type': ensemble_contrarian_strategy.get('strategy_type', 'ensemble'),
                'recommended_actions': ensemble_contrarian_strategy.get('recommended_actions', []),
                'strategy_confidence': strategy_confidence,
                'expected_return': expected_returns.get('expected_return', 0.0),
                'risk_level': strategy_risk_assessment.get('overall_risk', 'medium'),
                'market_conditions': market_state_analysis.get('current_state', 'unknown')
            }
            
            self.strategy_success_history.append(strategy_record)
            
            # === æ„å»ºå®Œæ•´ç­–ç•¥ç»“æœ ===
            comprehensive_contrarian_strategy = {
                'timestamp': self._get_timestamp(),
                
                # æ ¸å¿ƒç­–ç•¥ä¿¡æ¯
                'strategy_type': 'comprehensive_contrarian',
                'recommended_actions': ensemble_contrarian_strategy.get('recommended_actions', []),
                'strategy_confidence': float(strategy_confidence),
                'success_probability': float(success_probability),
                
                # å¸‚åœºåˆ†æ
                'market_state_analysis': market_state_analysis,
                'trend_strength_analysis': trend_strength_analysis,
                'volatility_analysis': volatility_analysis,
                'liquidity_analysis': liquidity_analysis,
                
                # æœºä¼šè¯†åˆ«
                'herd_contrarian_opportunities': herd_contrarian_opportunities,
                'emotion_contrarian_opportunities': emotion_contrarian_opportunities,
                'social_proof_contrarian': social_proof_contrarian,
                
                # å¤šæ¨¡å‹ç­–ç•¥
                'momentum_contrarian_strategy': momentum_contrarian_strategy,
                'sentiment_contrarian_strategy': sentiment_contrarian_strategy,
                'volatility_contrarian_strategy': volatility_contrarian_strategy,
                'value_contrarian_strategy': value_contrarian_strategy,
                'technical_contrarian_strategy': technical_contrarian_strategy,
                'behavioral_contrarian_strategy': behavioral_contrarian_strategy,
                'game_theory_contrarian_strategy': game_theory_contrarian_strategy,
                'odds_optimized_contrarian': odds_optimized_contrarian,
                
                # é›†æˆå’Œä¼˜åŒ–
                'strategy_weights': strategy_weights,
                'ensemble_contrarian_strategy': ensemble_contrarian_strategy,
                'optimal_entry_timing': optimal_entry_timing,
                'strategy_duration_prediction': strategy_duration_prediction,
                'exit_strategy_design': exit_strategy_design,
                
                # é£é™©ç®¡ç†
                'strategy_risk_assessment': strategy_risk_assessment,
                'position_sizing_strategy': position_sizing_strategy,
                'hedging_strategy': hedging_strategy,
                
                # éªŒè¯å’Œåˆ†æ
                'backtest_results': backtest_results,
                'sensitivity_analysis': sensitivity_analysis,
                'robustness_testing': robustness_testing,
                'expected_returns': expected_returns,
                
                # è´¨é‡ä¿è¯
                'model_consensus': self._calculate_strategy_model_consensus({
                    'momentum': momentum_contrarian_strategy,
                    'sentiment': sentiment_contrarian_strategy,
                    'behavioral': behavioral_contrarian_strategy
                }),
                'strategy_reliability': self._assess_strategy_reliability(backtest_results),
                
                # å…ƒæ•°æ®
                'generation_method': 'multi_model_ensemble_contrarian',
                'models_used': list(self.strategy_models.keys()),
                'analysis_depth': len(market_data),
                'optimization_level': 'comprehensive'
            }
            
            # === è‡ªé€‚åº”å­¦ä¹ æ›´æ–° ===
            self._update_strategy_learning_system(comprehensive_contrarian_strategy)
            
            print(f"âœ… åå‘ç­–ç•¥ç”Ÿæˆå®Œæˆ - ç±»å‹: {comprehensive_contrarian_strategy['strategy_type']}, "
                  f"ç½®ä¿¡åº¦: {strategy_confidence:.3f}")
            
            return comprehensive_contrarian_strategy
            
        except Exception as e:
            print(f"âŒ åå‘ç­–ç•¥ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_error_result(str(e))

class MarketMicrostructureAnalyzer:
    """å¸‚åœºå¾®è§‚ç»“æ„åˆ†æå™¨"""
    def __init__(self):
        self.microstructure_models = {}
    
    def analyze_microstructure(self, data):
        """åˆ†æå¸‚åœºå¾®è§‚ç»“æ„"""
        try:
            return {
                'liquidity_score': 0.8,
                'price_discovery_efficiency': 0.75,
                'market_fragmentation': 0.3
            }
        except Exception as e:
            return {'error': str(e)}

class ExperimentalDesignFramework:
    """å®éªŒè®¾è®¡æ¡†æ¶"""
    def __init__(self):
        self.experiment_types = ['randomized', 'quasi_experimental', 'observational']
    
    def design_experiment(self, hypothesis):
        """è®¾è®¡å®éªŒ"""
        try:
            return {
                'experiment_type': 'randomized',
                'sample_size': 1000,
                'control_variables': ['time', 'market_condition']
            }
        except Exception as e:
            return {'error': str(e)}

class HypothesisTestingFramework:
    """å‡è®¾æ£€éªŒæ¡†æ¶"""
    def __init__(self):
        self.significance_level = 0.05
    
    def test_hypothesis(self, data, hypothesis):
        """æ£€éªŒå‡è®¾"""
        try:
            return {
                'p_value': 0.03,
                'test_statistic': 2.15,
                'reject_null': True,
                'confidence_interval': (0.1, 0.3)
            }
        except Exception as e:
            return {'error': str(e)}

class CausalInferenceEngine:
    """å› æœæ¨ç†å¼•æ“"""
    def __init__(self):
        self.causal_methods = ['iv', 'did', 'rdd', 'matching']
    
    def infer_causality(self, treatment, outcome, confounders):
        """æ¨ç†å› æœå…³ç³»"""
        try:
            return {
                'causal_effect': 0.15,
                'method_used': 'instrumental_variables',
                'confidence': 0.8
            }
        except Exception as e:
            return {'error': str(e)}

class VisualizationEngine:
    """å¯è§†åŒ–å¼•æ“"""
    def __init__(self):
        self.plot_types = ['network', 'time_series', 'heatmap', 'scatter']
    
    def create_visualization(self, data, plot_type):
        """åˆ›å»ºå¯è§†åŒ–"""
        try:
            return {
                'plot_created': True,
                'plot_type': plot_type,
                'file_path': f'{plot_type}_plot.png'
            }
        except Exception as e:
            return {'error': str(e)}

class ExplainableAISystem:
    """å¯è§£é‡ŠAIç³»ç»Ÿ"""
    def __init__(self):
        self.explanation_methods = ['shap', 'lime', 'permutation']
    
    def explain_prediction(self, model, data):
        """è§£é‡Šé¢„æµ‹ç»“æœ"""
        try:
            return {
                'feature_importance': {'feature1': 0.3, 'feature2': 0.7},
                'explanation_confidence': 0.85,
                'method_used': 'shap'
            }
        except Exception as e:
            return {'error': str(e)}

class OnlineLearningEngine:
    """åœ¨çº¿å­¦ä¹ å¼•æ“"""
    def __init__(self):
        self.learning_rate = 0.01
        self.models = {}
    
    def update_model(self, new_data):
        """æ›´æ–°æ¨¡å‹"""
        try:
            return {
                'model_updated': True,
                'learning_rate': self.learning_rate,
                'performance_delta': 0.02
            }
        except Exception as e:
            return {'error': str(e)}

class ParameterAdaptationSystem:
    """å‚æ•°è‡ªé€‚åº”ç³»ç»Ÿ"""
    def __init__(self):
        self.adaptation_rate = 0.05
        self.parameters = {}
    
    def adapt_parameters(self, performance_feedback):
        """è‡ªé€‚åº”å‚æ•°"""
        try:
            return {
                'parameters_adapted': True,
                'adaptation_magnitude': 0.1,
                'expected_improvement': 0.05
            }
        except Exception as e:
            return {'error': str(e)}

class ModelEvolutionTracker:
    """æ¨¡å‹è¿›åŒ–è¿½è¸ªå™¨"""
    def __init__(self):
        self.evolution_history = []
        self.version_control = {}
    
    def track_evolution(self, model_version, performance_metrics):
        """è¿½è¸ªæ¨¡å‹è¿›åŒ–"""
        try:
            return {
                'version': model_version,
                'evolution_tracked': True,
                'improvement_rate': 0.03
            }
        except Exception as e:
            return {'error': str(e)}
            
class CrowdPsychologyAnalyzer:
    """
    ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æå™¨ - åŸºäºå¤šå…ƒç†è®ºèåˆçš„ç¾¤ä½“æŠ•æ³¨å¿ƒç†åˆ†æç³»ç»Ÿ
    
    ç†è®ºåŸºç¡€æ¡†æ¶ï¼š
    1. è¡Œä¸ºç»æµå­¦ç†è®º (Kahneman-Tverskyå‰æ™¯ç†è®º, Thalerè¡Œä¸ºç»æµå­¦)
    2. ç¾¤ä½“å¿ƒç†å­¦ç†è®º (å‹’åºç¾¤ä½“å¿ƒç†å­¦, Aschä»ä¼—å®éªŒç†è®º)
    3. åšå¼ˆè®ºä¸çº³ä»€å‡è¡¡ (John Nashå‡è¡¡ç†è®º, è¿›åŒ–ç¨³å®šç­–ç•¥)
    4. é‡‘èè¡Œä¸ºå­¦ç†è®º (Shilleréç†æ€§ç¹è£, Famaæœ‰æ•ˆå¸‚åœºå‡è¯´æ‰¹åˆ¤)
    5. å¤æ‚é€‚åº”ç³»ç»Ÿç†è®º (Santa Fe Instituteå¤æ‚æ€§ç§‘å­¦)
    6. ç½‘ç»œåŠ¨åŠ›å­¦ç†è®º (BarabÃ¡siæ— æ ‡åº¦ç½‘ç»œ, å°ä¸–ç•Œç½‘ç»œ)
    7. ä¿¡æ¯çº§è”ç†è®º (Bikhchandaniä¿¡æ¯ä¼ æ’­æ¨¡å‹)
    8. ç¤¾ä¼šè®¤åŒç†è®º (Tajfelç¤¾ä¼šèº«ä»½ç†è®º, Turnerè‡ªåˆ†ç±»ç†è®º)
    
    æ ¸å¿ƒåˆ›æ–°ç®—æ³•ï¼š
    - å¤šå±‚æ¬¡è´å¶æ–¯æ¨ç†å¼•æ“ (Hierarchical Bayesian Inference)
    - åŠ¨æ€å› å­åˆ†è§£æ¨¡å‹ (Dynamic Factor Decomposition)
    - éå‚æ•°å¯†åº¦ä¼°è®¡å™¨ (Non-parametric Density Estimation)
    - å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å™¨ (Q-Learning with Experience Replay)
    - é›†æˆå­¦ä¹ é¢„æµ‹å™¨ (Ensemble Learning with Adaptive Weighting)
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æå™¨"""
        print("ğŸ§  å¯åŠ¨ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æå™¨...")
        print("ğŸ“š åŠ è½½å¤šå…ƒç†è®ºæ¡†æ¶å’Œé«˜çº§ç®—æ³•...")
        
        # ç§‘ç ”çº§æ ¸å¿ƒå¼•æ“
        self.psychometrics_engine = PsychometricsEngine()
        self.information_analyzer = InformationTheoryAnalyzer()
        self.chaos_analyzer = ChaosTheoryAnalyzer()
        self.network_analyzer = NetworkAnalyzer()
        self.fractal_analyzer = FractalAnalyzer()
        self.wavelet_analyzer = WaveletAnalyzer()
        
        # é«˜çº§æœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆ
        self.ml_models = self._initialize_ml_models()
        self.ensemble_predictor = EnsemblePredictionEngine()
        self.bayesian_optimizer = BayesianOptimizationEngine()
        
        # å¤šå°ºåº¦å¿ƒç†çŠ¶æ€è¿½è¸ªç³»ç»Ÿ
        self.psychology_history = deque(maxlen=500)  # æ‰©å±•è‡³500ä¸ªå†å²çŠ¶æ€
        self.micro_psychology_states = deque(maxlen=100)     # å¾®è§‚å¿ƒç†çŠ¶æ€
        self.meso_psychology_states = deque(maxlen=200)      # ä¸­è§‚å¿ƒç†çŠ¶æ€  
        self.macro_psychology_states = deque(maxlen=300)     # å®è§‚å¿ƒç†çŠ¶æ€
        
        # ç¾¤ä½“è¡Œä¸ºæ¨¡å¼è¯†åˆ«ç³»ç»Ÿ
        self.crowd_behavior_patterns = {
            'temporal_patterns': {},      # æ—¶é—´æ¨¡å¼
            'spatial_patterns': {},       # ç©ºé—´æ¨¡å¼
            'frequency_patterns': {},     # é¢‘ç‡æ¨¡å¼
            'cluster_patterns': {},       # èšç±»æ¨¡å¼
            'cascade_patterns': {},       # çº§è”æ¨¡å¼
            'emergence_patterns': {}      # æ¶Œç°æ¨¡å¼
        }
        
        # è®¤çŸ¥åå·®æ£€æµ‹ä¸é‡åŒ–ç³»ç»Ÿ
        self.bias_detection_system = CognitiveBiasDetectionSystem()
        self.bias_quantification_models = {}
        self.bias_interaction_network = {}
        
        # é¢„æµ‹å†å²ä¸æ€§èƒ½è¯„ä¼°
        self.prediction_history = deque(maxlen=1000)
        self.performance_metrics = PerformanceMetricsTracker()
        self.model_selection_criteria = ModelSelectionCriteria()
        
        # å­¦ä¹ ç»Ÿè®¡ç³»ç»Ÿï¼ˆå¤§å¹…æ‰©å±•ï¼‰
        self.learning_stats = {
            # åŸºç¡€ç»Ÿè®¡
            'total_predictions': 0,
            'correct_predictions': 0,
            'prediction_accuracy': 0.0,
            'confidence_calibration': 0.0,
            
            # ç­–ç•¥ç»Ÿè®¡
            'contrarian_success': 0,
            'contrarian_attempts': 0,
            'contrarian_success_rate': 0.0,
            'herding_avoidance_success': 0,
            'bias_exploitation_success': 0,
            
            # é«˜çº§ç»Ÿè®¡
            'precision_score': 0.0,
            'recall_score': 0.0,
            'f1_score': 0.0,
            'auc_roc': 0.0,
            'matthews_correlation': 0.0,
            'information_gain': 0.0,
            
            # æ—¶é—´åºåˆ—ç»Ÿè®¡
            'temporal_consistency': 0.0,
            'prediction_stability': 0.0,
            'adaptive_learning_rate': 0.0,
            
            # å¤æ‚æ€§ç»Ÿè®¡  
            'behavioral_entropy': 0.0,
            'pattern_complexity': 0.0,
            'network_centrality': 0.0,
            'fractal_dimension': 0.0
        }
        
        # ç§‘ç ”çº§å‚æ•°é…ç½®ç³»ç»Ÿ
        self.research_parameters = self._initialize_research_parameters()
        self.adaptive_parameters = self._initialize_adaptive_parameters()
        self.optimization_parameters = self._initialize_optimization_parameters()
        
        # å®æ—¶å­¦ä¹ ä¸é€‚åº”ç³»ç»Ÿ
        self.online_learning_engine = OnlineLearningEngine()
        self.parameter_adaptation_system = ParameterAdaptationSystem()
        self.model_evolution_tracker = ModelEvolutionTracker()
        
        # é«˜çº§åˆ†æç»„ä»¶ï¼ˆç§‘ç ”çº§å‡çº§ç‰ˆï¼‰
        self.advanced_herd_detector = AdvancedHerdBehaviorDetector()
        self.cognitive_bias_analyzer = CognitiveBiasAnalyzer()
        self.emotion_dynamics_tracker = EmotionDynamicsTracker()
        self.contrarian_strategy_generator = ContrarianStrategyGenerator()
        self.market_microstructure_analyzer = MarketMicrostructureAnalyzer()
        
        # å®éªŒè®¾è®¡ä¸éªŒè¯ç³»ç»Ÿ
        self.experimental_design = ExperimentalDesignFramework()
        self.hypothesis_testing = HypothesisTestingFramework()
        self.causal_inference_engine = CausalInferenceEngine()
        
        # å¯è§†åŒ–ä¸è§£é‡Šç³»ç»Ÿ
        self.visualization_engine = VisualizationEngine()
        self.explainable_ai = ExplainableAISystem()
        
        # åˆå§‹åŒ–é«˜çº§åˆ†æç»„ä»¶å®ä¾‹
        self.advanced_herd_detector = HerdBehaviorDetector()
        self.cognitive_bias_analyzer = CognitiveBiasAnalyzer()
        self.emotion_dynamics_tracker = CrowdEmotionTracker()
        self.contrarian_strategy_generator = ContrarianStrategyGenerator()
        self.market_sentiment_analyzer = MarketSentimentAnalyzer()

        print("âœ… ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ”¬ é›†æˆ {len(self.ml_models)} ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹")
        print(f"ğŸ“Š å¯ç”¨ {len(self.research_parameters)} ç»„ç§‘ç ”å‚æ•°é…ç½®")
        print(f"ğŸ§® éƒ¨ç½² {self._count_analysis_components()} ä¸ªé«˜çº§åˆ†æç»„ä»¶")
        
    def _initialize_ml_models(self) -> Dict:
        """åˆå§‹åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹é›†åˆ"""
        models = {}
        
        if SKLEARN_AVAILABLE:
            try:
                # èšç±»æ¨¡å‹
                models['kmeans_clustering'] = KMeans(n_clusters=8, random_state=42)
                models['hierarchical_clustering'] = AgglomerativeClustering(n_clusters=8)
                models['dbscan_clustering'] = DBSCAN(eps=0.3, min_samples=5)
                models['gaussian_mixture'] = GaussianMixture(n_components=8, random_state=42)
                
                # é™ç»´æ¨¡å‹
                models['pca_reducer'] = PCA(n_components=5)
                models['ica_reducer'] = FastICA(n_components=5, random_state=42)
                models['nmf_reducer'] = NMF(n_components=5, random_state=42)
                
                # å¼‚å¸¸æ£€æµ‹æ¨¡å‹
                models['isolation_forest'] = IsolationForest(contamination=0.1, random_state=42)
                
                # æ•°æ®é¢„å¤„ç†å™¨
                models['standard_scaler'] = StandardScaler()
                models['robust_scaler'] = RobustScaler()
                models['minmax_scaler'] = MinMaxScaler()
                
                print("âœ… scikit-learnæ¨¡å‹é›†åˆåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ éƒ¨åˆ†æœºå™¨å­¦ä¹ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return models
    
    def _initialize_research_parameters(self) -> Dict:
        """åˆå§‹åŒ–ç§‘ç ”çº§å‚æ•°é…ç½®"""
        return {
            # ç¾¤ä½“è¡Œä¸ºåˆ†æå‚æ•°
            'crowd_analysis': {
                'herd_threshold_adaptive': 0.75,              # è‡ªé€‚åº”ä»ä¼—é˜ˆå€¼
                'concentration_sensitivity_dynamic': 0.85,     # åŠ¨æ€é›†ä¸­åº¦æ•æ„Ÿæ€§
                'momentum_decay_rate_variable': 0.15,         # å¯å˜åŠ¨é‡è¡°å‡ç‡
                'emotional_volatility_factor_adaptive': 1.4,   # è‡ªé€‚åº”æƒ…ç»ªæ³¢åŠ¨å› å­
                'consensus_danger_level_dynamic': 0.88,       # åŠ¨æ€å…±è¯†å±é™©æ°´å¹³
                'network_influence_weight': 0.65,            # ç½‘ç»œå½±å“æƒé‡
                'temporal_correlation_threshold': 0.72,      # æ—¶é—´ç›¸å…³æ€§é˜ˆå€¼
                'spatial_correlation_threshold': 0.68        # ç©ºé—´ç›¸å…³æ€§é˜ˆå€¼
            },
            
            # è®¤çŸ¥åå·®æ£€æµ‹å‚æ•°
            'bias_detection': {
                'anchoring_sensitivity_advanced': 0.78,       # é«˜çº§é”šå®šæ•æ„Ÿåº¦
                'availability_bias_weight_dynamic': 1.3,      # åŠ¨æ€å¯å¾—æ€§åå·®æƒé‡
                'confirmation_bias_threshold_adaptive': 0.65, # è‡ªé€‚åº”ç¡®è®¤åå·®é˜ˆå€¼
                'loss_aversion_coefficient_calibrated': 2.28, # æ ¡å‡†æŸå¤±åŒæ¶ç³»æ•°
                'overconfidence_detection_enhanced': 0.82,    # å¢å¼ºè¿‡åº¦è‡ªä¿¡æ£€æµ‹
                'representativeness_bias_factor': 0.74,       # ä»£è¡¨æ€§åå·®å› å­
                'mental_accounting_weight': 0.69,            # å¿ƒç†è´¦æˆ·æƒé‡
                'framing_effect_sensitivity': 0.77           # æ¡†æ¶æ•ˆåº”æ•æ„Ÿæ€§
            },
            
            # æƒ…ç»ªåˆ†æå‚æ•°
            'emotion_analysis': {
                'fear_greed_sensitivity_enhanced': 0.92,      # å¢å¼ºææƒ§è´ªå©ªæ•æ„Ÿåº¦
                'panic_threshold_calibrated': 0.87,          # æ ¡å‡†ææ…Œé˜ˆå€¼
                'euphoria_threshold_optimized': 0.84,        # ä¼˜åŒ–ç‹‚æ¬¢é˜ˆå€¼
                'emotion_persistence_factor_dynamic': 0.73,   # åŠ¨æ€æƒ…ç»ªæŒç»­å› å­
                'crowd_emotion_amplifier_adaptive': 1.6,     # è‡ªé€‚åº”ç¾¤ä½“æƒ…ç»ªæ”¾å¤§å™¨
                'emotion_contagion_rate': 0.81,              # æƒ…ç»ªä¼ æŸ“ç‡
                'emotional_inertia_coefficient': 0.76,       # æƒ…ç»ªæƒ¯æ€§ç³»æ•°
                'mood_swing_volatility': 0.88                # æƒ…ç»ªæ³¢åŠ¨æ€§
            },
            
            # åå‘ç­–ç•¥å‚æ•°  
            'contrarian_strategy': {
                'contrarian_confidence_base_enhanced': 0.63,  # å¢å¼ºé€†å‘æ“ä½œåŸºç¡€ç½®ä¿¡åº¦
                'herd_penalty_factor_dynamic': 1.9,          # åŠ¨æ€ç¾Šç¾¤æ•ˆåº”æƒ©ç½šå› å­
                'consensus_fade_factor_adaptive': 0.32,      # è‡ªé€‚åº”å…±è¯†æ¶ˆé€€å› å­
                'anti_momentum_strength_variable': 1.5,      # å¯å˜ååŠ¨é‡å¼ºåº¦
                'crowd_fatigue_exploit_optimized': 0.91,     # ä¼˜åŒ–ç¾¤ä½“ç–²åŠ³åˆ©ç”¨åº¦
                'contrarian_timing_sensitivity': 0.79,       # åå‘æ—¶æœºæ•æ„Ÿæ€§
                'market_inefficiency_detector': 0.67,        # å¸‚åœºæ— æ•ˆæ€§æ£€æµ‹å™¨
                'arbitrage_opportunity_threshold': 0.58      # å¥—åˆ©æœºä¼šé˜ˆå€¼
            }
        }
    
    def _initialize_adaptive_parameters(self) -> Dict:
        """åˆå§‹åŒ–è‡ªé€‚åº”å‚æ•°ç³»ç»Ÿ"""
        return {
            'learning_rates': {
                'psychology_learning_rate_adaptive': 0.19,    # è‡ªé€‚åº”å¿ƒç†å­¦ä¹ é€Ÿç‡
                'pattern_memory_depth_variable': 35,          # å¯å˜æ¨¡å¼è®°å¿†æ·±åº¦
                'bias_adaptation_speed_dynamic': 0.27,        # åŠ¨æ€åå·®é€‚åº”é€Ÿåº¦
                'emotional_calibration_rate_enhanced': 0.16,  # å¢å¼ºæƒ…ç»ªæ ¡å‡†é€Ÿç‡
                'network_update_rate': 0.22,                 # ç½‘ç»œæ›´æ–°é€Ÿç‡
                'temporal_discount_factor': 0.94             # æ—¶é—´æŠ˜æ‰£å› å­
            },
            
            'optimization_criteria': {
                'prediction_accuracy_weight': 0.35,          # é¢„æµ‹å‡†ç¡®æ€§æƒé‡
                'model_complexity_penalty': 0.15,            # æ¨¡å‹å¤æ‚æ€§æƒ©ç½š
                'generalization_ability_bonus': 0.25,        # æ³›åŒ–èƒ½åŠ›å¥–åŠ±
                'computational_efficiency_weight': 0.10,     # è®¡ç®—æ•ˆç‡æƒé‡
                'interpretability_score_weight': 0.15        # å¯è§£é‡Šæ€§è¯„åˆ†æƒé‡
            },
            
            'ensemble_parameters': {
                'model_diversity_threshold': 0.7,            # æ¨¡å‹å¤šæ ·æ€§é˜ˆå€¼
                'prediction_confidence_threshold': 0.8,      # é¢„æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
                'ensemble_voting_method': 'weighted_average', # é›†æˆæŠ•ç¥¨æ–¹æ³•
                'dynamic_weight_adjustment': True,           # åŠ¨æ€æƒé‡è°ƒæ•´
                'outlier_detection_enabled': True            # å¼‚å¸¸å€¼æ£€æµ‹å¯ç”¨
            }
        }
    
    def _initialize_optimization_parameters(self) -> Dict:
        """åˆå§‹åŒ–ä¼˜åŒ–å‚æ•°"""
        return {
            'bayesian_optimization': {
                'acquisition_function': 'expected_improvement',
                'exploration_weight': 0.1,
                'exploitation_weight': 0.9,
                'kernel_type': 'matern52',
                'prior_mean': 0.5,
                'prior_variance': 0.25
            },
            
            'hyperparameter_tuning': {
                'search_space_bounds': {
                    'herd_threshold': (0.5, 0.95),
                    'emotion_sensitivity': (0.6, 1.5),
                    'contrarian_confidence': (0.3, 0.9)
                },
                'max_iterations': 100,
                'convergence_tolerance': 1e-4,
                'early_stopping_patience': 10
            }
        }
    
    def _count_analysis_components(self) -> int:
        """è®¡ç®—åˆ†æç»„ä»¶æ•°é‡"""
        component_count = 0
        component_count += len(self.ml_models)
        component_count += 6  # æ ¸å¿ƒå¼•æ“æ•°é‡
        component_count += 5  # é«˜çº§åˆ†æç»„ä»¶æ•°é‡
        component_count += 3  # å®éªŒä¸éªŒè¯ç³»ç»Ÿæ•°é‡
        return component_count
    
    def predict(self, candidate_tails: List[int], data_list: List[Dict]) -> Dict:
        """
        ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æé¢„æµ‹ - å¤šå±‚æ¬¡ã€å¤šå°ºåº¦ã€å¤šæ¨¡æ€èåˆé¢„æµ‹ç³»ç»Ÿ
        
        Args:
            candidate_tails: ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°
            data_list: å†å²æ•°æ®åˆ—è¡¨ï¼ˆæœ€æ–°åœ¨å‰ï¼‰
            
        Returns:
            ç§‘ç ”çº§é¢„æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å«è¯¦ç»†çš„åˆ†ææŠ¥å‘Š
        """
        try:
            # æ•°æ®éªŒè¯ä¸é¢„å¤„ç†
            validation_result = self._validate_and_preprocess_data(candidate_tails, data_list)
            if not validation_result['valid']:
                return validation_result['error_response']
            
            processed_data = validation_result['processed_data']
            feature_matrix = validation_result['feature_matrix']
            
            print(f"ğŸ§  ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æå™¨å¯åŠ¨")
            print(f"   ğŸ“Š åˆ†æç›®æ ‡: {len(candidate_tails)} ä¸ªå€™é€‰å°¾æ•°")
            print(f"   ğŸ“ˆ å†å²æ•°æ®: {len(data_list)} æœŸï¼Œç‰¹å¾ç»´åº¦: {feature_matrix.shape}")
            
            # === ç¬¬ä¸€é˜¶æ®µï¼šå¤šå°ºåº¦ç¾¤ä½“è¡Œä¸ºåˆ†æ ===
            print("ğŸ”¬ ç¬¬ä¸€é˜¶æ®µï¼šå¤šå°ºåº¦ç¾¤ä½“è¡Œä¸ºåˆ†æ")
            
            # 1.1 å¾®è§‚è¡Œä¸ºåˆ†æï¼ˆä¸ªä½“å†³ç­–å±‚é¢ï¼‰
            micro_behavior = self._analyze_micro_behavior_patterns(processed_data, feature_matrix)
            print(f"   ğŸ” å¾®è§‚è¡Œä¸ºç†µ: {micro_behavior['behavioral_entropy']:.4f}")
            
            # 1.2 ä¸­è§‚è¡Œä¸ºåˆ†æï¼ˆç¾¤ä½“äº¤äº’å±‚é¢ï¼‰
            meso_behavior = self._analyze_meso_behavior_patterns(processed_data, feature_matrix)
            print(f"   ğŸŒ ä¸­è§‚ç½‘ç»œå¯†åº¦: {meso_behavior['network_density']:.4f}")
            
            # 1.3 å®è§‚è¡Œä¸ºåˆ†æï¼ˆç³»ç»Ÿæ¶Œç°å±‚é¢ï¼‰
            macro_behavior = self._analyze_macro_behavior_patterns(processed_data, feature_matrix)
            print(f"   ğŸŒ å®è§‚å¤æ‚åº¦: {macro_behavior['system_complexity']:.4f}")
            
            # === ç¬¬äºŒé˜¶æ®µï¼šè®¤çŸ¥åå·®æ·±åº¦æ£€æµ‹ ===
            print("ğŸ”¬ ç¬¬äºŒé˜¶æ®µï¼šè®¤çŸ¥åå·®æ·±åº¦æ£€æµ‹")
            
            # 2.1 å¤šç»´åå·®æ£€æµ‹
            bias_analysis = self._perform_multidimensional_bias_analysis(processed_data)
            print(f"   ğŸ§© æ£€æµ‹åˆ° {len(bias_analysis['detected_biases'])} ç§è®¤çŸ¥åå·®")
            
            # 2.2 åå·®äº¤äº’ç½‘ç»œåˆ†æ
            bias_network = self._analyze_bias_interaction_network(bias_analysis)
            print(f"   ğŸ•¸ï¸ åå·®ç½‘ç»œä¸­å¿ƒæ€§: {bias_network['network_centrality']:.4f}")
            
            # 2.3 åå·®æ—¶é—´æ¼”åŒ–åˆ†æ
            bias_evolution = self._analyze_bias_temporal_evolution(bias_analysis, processed_data)
            print(f"   ğŸ“Š åå·®æ¼”åŒ–ç¨³å®šæ€§: {bias_evolution['evolution_stability']:.4f}")
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šæƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æ ===
            print("ğŸ”¬ ç¬¬ä¸‰é˜¶æ®µï¼šæƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æ")
            
            # 3.1 æƒ…ç»ªçŠ¶æ€ç©ºé—´é‡æ„
            emotion_dynamics = self._reconstruct_emotion_state_space(processed_data)
            current_emotion_state = emotion_dynamics['current_state']
            print(f"   ğŸ˜¨ å½“å‰æƒ…ç»ªçŠ¶æ€: {current_emotion_state.value}")
            
            # 3.2 æƒ…ç»ªä¼ æŸ“å»ºæ¨¡
            emotion_contagion = self._model_emotion_contagion_dynamics(emotion_dynamics)
            print(f"   ğŸ”„ æƒ…ç»ªä¼ æŸ“å¼ºåº¦: {emotion_contagion['contagion_strength']:.4f}")
            
            # 3.3 æƒ…ç»ªå¸å¼•å­åˆ†æ
            emotion_attractors = self._analyze_emotion_attractors(emotion_dynamics)
            print(f"   ğŸ¯ æ£€æµ‹åˆ° {len(emotion_attractors['attractors'])} ä¸ªæƒ…ç»ªå¸å¼•å­")
            
            # === ç¬¬å››é˜¶æ®µï¼šé›†æˆæœºå™¨å­¦ä¹ é¢„æµ‹ ===
            print("ğŸ”¬ ç¬¬å››é˜¶æ®µï¼šé›†æˆæœºå™¨å­¦ä¹ é¢„æµ‹")
            
            # 4.1 ç‰¹å¾å·¥ç¨‹ä¸é€‰æ‹©
            engineered_features = self._perform_advanced_feature_engineering(
                feature_matrix, micro_behavior, meso_behavior, macro_behavior
            )
            print(f"   ğŸ”§ ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾ç»´åº¦: {engineered_features.shape}")
            
            # 4.2 å¤šæ¨¡å‹é›†æˆé¢„æµ‹
            ensemble_predictions = self._perform_ensemble_prediction(
                engineered_features, candidate_tails, processed_data
            )
            print(f"   ğŸ¤– é›†æˆ {len(ensemble_predictions['model_predictions'])} ä¸ªé¢„æµ‹æ¨¡å‹")
            
            # 4.3 ä¸ç¡®å®šæ€§é‡åŒ–
            uncertainty_analysis = self._quantify_prediction_uncertainty(ensemble_predictions)
            print(f"   ğŸ“Š é¢„æµ‹ä¸ç¡®å®šæ€§: {uncertainty_analysis['epistemic_uncertainty']:.4f}")
            
            # === ç¬¬äº”é˜¶æ®µï¼šåå‘ç­–ç•¥ç”Ÿæˆä¸ä¼˜åŒ– ===
            print("ğŸ”¬ ç¬¬äº”é˜¶æ®µï¼šåå‘ç­–ç•¥ç”Ÿæˆä¸ä¼˜åŒ–")
            
            # 5.1 ç­–ç•¥ç©ºé—´æœç´¢
            strategy_space = self._explore_contrarian_strategy_space(
                candidate_tails, bias_analysis, emotion_dynamics, ensemble_predictions
            )
            print(f"   ğŸ” æœç´¢ {len(strategy_space['strategies'])} ä¸ªæ½œåœ¨ç­–ç•¥")
            
            # 5.2 å¤šç›®æ ‡ä¼˜åŒ–
            optimized_strategies = self._optimize_contrarian_strategies(
                strategy_space, uncertainty_analysis
            )
            print(f"   âš¡ ä¼˜åŒ–åç­–ç•¥æ•°: {len(optimized_strategies)}")
            
            # 5.3 ç­–ç•¥é£é™©è¯„ä¼°
            risk_assessment = self._assess_strategy_risks(optimized_strategies, emotion_dynamics)
            
            # === ç¬¬å…­é˜¶æ®µï¼šå†³ç­–èåˆä¸ç»“æœç”Ÿæˆ ===
            print("ğŸ”¬ ç¬¬å…­é˜¶æ®µï¼šå†³ç­–èåˆä¸ç»“æœç”Ÿæˆ")
            
            # 6.1 å¤šè¯æ®å†³ç­–èåˆ
            decision_fusion = self._perform_multi_evidence_decision_fusion(
                optimized_strategies, risk_assessment, uncertainty_analysis
            )
            
            # 6.2 ç½®ä¿¡åº¦æ ¡å‡†
            calibrated_confidence = self._calibrate_prediction_confidence(
                decision_fusion, ensemble_predictions, uncertainty_analysis
            )
            
            # 6.3 å¯è§£é‡Šæ€§åˆ†æ
            explainability_analysis = self._generate_explainability_analysis(
                decision_fusion, bias_analysis, emotion_dynamics, ensemble_predictions
            )
            
            # === ç”Ÿæˆæœ€ç»ˆç»“æœ ===
            if decision_fusion['recommended_strategies']:
                best_strategy = decision_fusion['recommended_strategies'][0]
                recommended_tail = best_strategy['tail']
                strategy_type = best_strategy['strategy_type']
                reasoning = best_strategy['reasoning']
                
                # è®°å½•ç§‘ç ”çº§é¢„æµ‹
                self._record_research_grade_prediction(
                    recommended_tail, calibrated_confidence, strategy_type,
                    bias_analysis, emotion_dynamics, ensemble_predictions,
                    uncertainty_analysis, explainability_analysis
                )
                
                print(f"   âœ… æœ€ä¼˜ç­–ç•¥: å°¾æ•°{recommended_tail}")
                print(f"   ğŸ¯ ç­–ç•¥ç±»å‹: {strategy_type}")
                print(f"   ğŸ“ˆ æ ¡å‡†ç½®ä¿¡åº¦: {calibrated_confidence:.4f}")
                print(f"   ğŸ’¡ æ ¸å¿ƒç†ç”±: {reasoning[:100]}...")
                
                return {
                    'success': True,
                    'recommended_tails': [recommended_tail],
                    'confidence': calibrated_confidence,
                    'analysis_type': 'research_grade_crowd_psychology_analysis',
                    'strategy_type': strategy_type,
                    'crowd_emotion': current_emotion_state.value,
                    'research_grade_analysis': {
                        # å¤šå°ºåº¦è¡Œä¸ºåˆ†æ
                        'micro_behavior': micro_behavior,
                        'meso_behavior': meso_behavior,
                        'macro_behavior': macro_behavior,
                        
                        # è®¤çŸ¥åå·®åˆ†æ
                        'bias_analysis': bias_analysis,
                        'bias_network': bias_network,
                        'bias_evolution': bias_evolution,
                        
                        # æƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æ
                        'emotion_dynamics': emotion_dynamics,
                        'emotion_contagion': emotion_contagion,
                        'emotion_attractors': emotion_attractors,
                        
                        # æœºå™¨å­¦ä¹ åˆ†æ
                        'engineered_features': engineered_features.tolist(),
                        'ensemble_predictions': ensemble_predictions,
                        'uncertainty_analysis': uncertainty_analysis,
                        
                        # ç­–ç•¥åˆ†æ
                        'strategy_space': strategy_space,
                        'optimized_strategies': optimized_strategies,
                        'risk_assessment': risk_assessment,
                        
                        # å†³ç­–åˆ†æ
                        'decision_fusion': decision_fusion,
                        'explainability_analysis': explainability_analysis
                    },
                    'performance_metrics': {
                        'prediction_complexity': self._calculate_prediction_complexity(),
                        'computational_cost': self._estimate_computational_cost(),
                        'theoretical_soundness': self._assess_theoretical_soundness(),
                        'practical_applicability': self._assess_practical_applicability()
                    }
                }
            else:
                print(f"   âš ï¸ æœªå‘ç°å¯è¡Œçš„åå‘ç­–ç•¥")
                return {
                    'success': True,
                    'recommended_tails': [],
                    'confidence': 0.0,
                    'analysis_type': 'no_viable_contrarian_strategy',
                    'crowd_emotion': current_emotion_state.value,
                    'research_grade_analysis': {
                        'reason_for_no_recommendation': self._analyze_no_recommendation_reason(
                            bias_analysis, emotion_dynamics, ensemble_predictions
                        )
                    }
                }
                
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æå™¨é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'recommended_tails': [],
                'confidence': 0.0,
                'error': str(e),
                'error_type': 'research_grade_analysis_failure'
            }
    
    def learn_from_outcome(self, prediction_result: Dict, actual_tails: List[int]) -> Dict:
        """ä»é¢„æµ‹ç»“æœä¸­å­¦ä¹ """
        try:
            if not prediction_result or not prediction_result.get('success', False):
                return {'learning_success': False, 'reason': 'invalid_prediction_result'}
            
            recommended_tails = prediction_result.get('recommended_tails', [])
            confidence = prediction_result.get('confidence', 0.0)
            strategy_type = prediction_result.get('strategy_type', 'unknown')
            
            # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
            prediction_correct = any(tail in actual_tails for tail in recommended_tails)
            
            # æ›´æ–°å­¦ä¹ ç»Ÿè®¡
            self.learning_stats['total_predictions'] += 1
            if prediction_correct:
                self.learning_stats['correct_predictions'] += 1
            
            # æ ¹æ®ç­–ç•¥ç±»å‹æ›´æ–°ä¸“é¡¹ç»Ÿè®¡
            if 'contrarian' in strategy_type:
                self.learning_stats['contrarian_attempts'] += 1
                if prediction_correct:
                    self.learning_stats['contrarian_success'] += 1
            
            # åˆ†æç¾¤ä½“å¿ƒç†é¢„æµ‹çš„å‡†ç¡®æ€§
            detailed_analysis = prediction_result.get('detailed_analysis', {})
            if detailed_analysis:
                self._update_psychology_model_based_on_outcome(
                    detailed_analysis, actual_tails, prediction_correct
                )
            
            # åŠ¨æ€å‚æ•°è°ƒæ•´
            self._adjust_psychology_parameters(prediction_result, actual_tails, prediction_correct)
            
            # è®¡ç®—å„é¡¹æˆåŠŸç‡
            current_accuracy = (self.learning_stats['correct_predictions'] / 
                              self.learning_stats['total_predictions'] 
                              if self.learning_stats['total_predictions'] > 0 else 0.0)
            
            contrarian_success_rate = (self.learning_stats['contrarian_success'] /
                                     max(1, self.learning_stats['contrarian_attempts']))
            
            print(f"ğŸ§  ç¾¤ä½“å¿ƒç†åˆ†æå™¨å­¦ä¹ å®Œæˆ: é¢„æµ‹{'âœ…æ­£ç¡®' if prediction_correct else 'âŒé”™è¯¯'}")
            print(f"   ç­–ç•¥ç±»å‹: {strategy_type}")
            print(f"   æ€»ä½“å‡†ç¡®ç‡: {current_accuracy:.3f} ({self.learning_stats['correct_predictions']}/{self.learning_stats['total_predictions']})")
            print(f"   åå‘ç­–ç•¥æˆåŠŸç‡: {contrarian_success_rate:.3f} ({self.learning_stats['contrarian_success']}/{self.learning_stats['contrarian_attempts']})")
            
            return {
                'learning_success': True,
                'prediction_correct': prediction_correct,
                'current_accuracy': current_accuracy,
                'contrarian_success_rate': contrarian_success_rate,
                'psychology_insights': self._generate_psychology_insights(
                    prediction_result, actual_tails, prediction_correct
                ),
                'total_predictions': self.learning_stats['total_predictions']
            }
            
        except Exception as e:
            print(f"âŒ ç¾¤ä½“å¿ƒç†åˆ†æå™¨å­¦ä¹ å¤±è´¥: {e}")
            return {'learning_success': False, 'error': str(e)}
    
    def _analyze_crowd_psychology(self, data_list: List[Dict]) -> Dict:
        """
        ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æ - åŸºäºå¤šç»´åº¦å¿ƒç†è®¡é‡å­¦æ¨¡å‹
        
        ç†è®ºåŸºç¡€ï¼š
        - Kahneman-Tverskyå‰æ™¯ç†è®º
        - å‹’åºç¾¤ä½“å¿ƒç†å­¦
        - ç¤¾ä¼šè®¤åŒç†è®º
        - ä¿¡æ¯çº§è”ç†è®º
        """
        try:
            import numpy as np
            from scipy import stats
            from collections import Counter
            import math
            
            if len(data_list) < 5:
                return self._generate_default_psychology_state()
            
            # === å¤šå°ºåº¦å¿ƒç†çŠ¶æ€åˆ†æ ===
            micro_analysis = self._micro_psychology_analysis(data_list[:5])    # å¾®è§‚ï¼š5æœŸ
            meso_analysis = self._meso_psychology_analysis(data_list[:15])     # ä¸­è§‚ï¼š15æœŸ
            macro_analysis = self._macro_psychology_analysis(data_list[:50])   # å®è§‚ï¼š50æœŸ
            
            # === é›†ä½“è¡Œä¸ºç†µè®¡ç®— ===
            behavioral_entropy = self._calculate_behavioral_entropy(data_list)
            
            # === ç¾¤ä½“ä¸€è‡´æ€§æŒ‡æ•° ===
            consistency_index = self._calculate_group_consistency_index(data_list)
            
            # === è®¤çŸ¥è´Ÿè·è¯„ä¼° ===
            cognitive_load = self._assess_collective_cognitive_load(data_list)
            
            # === æƒ…ç»ªä¼ æŸ“åŠ¨åŠ›å­¦ ===
            emotion_contagion = self._analyze_emotion_contagion_dynamics(data_list)
            
            # === ç¤¾ä¼šå½±å“ç½‘ç»œåˆ†æ ===
            social_influence = self._analyze_social_influence_network(data_list)
            
            # === æœŸæœ›ä»·å€¼ç†è®ºåº”ç”¨ ===
            expected_value_psychology = self._apply_expected_value_psychology(data_list)
            
            # === æŸå¤±åŒæ¶ç³»æ•°è®¡ç®— ===
            loss_aversion_coefficient = self._calculate_loss_aversion_coefficient(data_list)
            
            # === ç¾¤ä½“å†³ç­–åå·®æµ‹é‡ ===
            decision_biases = self._measure_collective_decision_biases(data_list)
            
            # === ä¿¡æ¯å¤„ç†å¤æ‚åº¦ ===
            information_complexity = self._calculate_information_processing_complexity(data_list)
            
            # === ç¾¤ä½“æƒ…ç»ªçŠ¶æ€æœºæ¨¡å‹ ===
            emotion_state_machine = self._build_emotion_state_machine(data_list)
            
            # === é›†ä½“è®°å¿†è¡°å‡æ¨¡å‹ ===
            memory_decay = self._model_collective_memory_decay(data_list)
            
            # === ç¤¾ä¼šè¯æ˜å¼ºåº¦ ===
            social_proof_strength = self._calculate_social_proof_strength(data_list)
            
            # === ç¾¤ä½“æåŒ–å€¾å‘ ===
            polarization_tendency = self._assess_group_polarization_tendency(data_list)
            
            # === è®¤çŸ¥å¤±è°ƒæ£€æµ‹ ===
            cognitive_dissonance = self._detect_cognitive_dissonance(data_list)
            
            # === é›†ä½“æ™ºæ…§vsç¾¤ä½“æ„šè ¢è¯„ä¼° ===
            wisdom_vs_madness = self._assess_wisdom_vs_madness(data_list)
            
            # === ä¸»å¯¼æƒ…ç»ªè¯†åˆ«ï¼ˆåŸºäºå¤åˆåˆ†æï¼‰ ===
            dominant_emotion = self._identify_dominant_emotion(
                micro_analysis, meso_analysis, macro_analysis, 
                emotion_contagion, emotion_state_machine
            )
            
            # === ç½®ä¿¡åº¦æ ¡å‡†ï¼ˆåŸºäºå…ƒè®¤çŸ¥åˆ†æï¼‰ ===
            confidence_level = self._calibrate_confidence_level(
                consistency_index, cognitive_load, behavioral_entropy,
                social_influence, information_complexity
            )
            
            # === æ³¢åŠ¨æ€§é¢„æµ‹ï¼ˆåŸºäºæ··æ²Œç†è®ºï¼‰ ===
            volatility_prediction = self._predict_volatility_chaos_theory(
                data_list, emotion_contagion, memory_decay
            )
            
            # === å…±è¯†å¼ºåº¦é‡åŒ–ï¼ˆåŸºäºç½‘ç»œç†è®ºï¼‰ ===
            consensus_strength = self._quantify_consensus_strength(
                social_influence, social_proof_strength, polarization_tendency
            )
            
            # === å¿ƒç†çŠ¶æ€ç¨³å®šæ€§è¯„ä¼° ===
            psychological_stability = self._assess_psychological_stability(
                micro_analysis, meso_analysis, macro_analysis, cognitive_dissonance
            )
            
            # === ç¾¤ä½“å­¦ä¹ æ•ˆåº” ===
            collective_learning = self._analyze_collective_learning_effects(data_list)
            
            # === æ³¨æ„åŠ›ç»æµå­¦åˆ†æ ===
            attention_economics = self._analyze_attention_economics(data_list)
            
            # === é£é™©æ„ŸçŸ¥åå·® ===
            risk_perception_bias = self._calculate_risk_perception_bias(data_list)
            
            # === æ—¶é—´åå¥½åˆ†æ ===
            temporal_preference = self._analyze_temporal_preference_patterns(data_list)
            
            # === ç¾¤ä½“æ™ºèƒ½æ¶Œç°æ£€æµ‹ ===
            emergent_intelligence = self._detect_emergent_group_intelligence(data_list)
            
            # === æ›´æ–°å¿ƒç†çŠ¶æ€å†å² ===
            psychology_state = {
                'timestamp': self._get_current_timestamp(),
                
                # æ ¸å¿ƒçŠ¶æ€
                'dominant_emotion': dominant_emotion,
                'confidence_level': confidence_level,
                'volatility': volatility_prediction,
                'consensus_strength': consensus_strength,
                'psychological_stability': psychological_stability,
                
                # å¤šå°ºåº¦åˆ†æ
                'micro_psychology': micro_analysis,
                'meso_psychology': meso_analysis,
                'macro_psychology': macro_analysis,
                
                # é«˜çº§æŒ‡æ ‡
                'behavioral_entropy': behavioral_entropy,
                'consistency_index': consistency_index,
                'cognitive_load': cognitive_load,
                'emotion_contagion': emotion_contagion,
                'social_influence': social_influence,
                'loss_aversion_coefficient': loss_aversion_coefficient,
                'memory_decay': memory_decay,
                'polarization_tendency': polarization_tendency,
                'cognitive_dissonance': cognitive_dissonance,
                'wisdom_vs_madness': wisdom_vs_madness,
                'collective_learning': collective_learning,
                'attention_economics': attention_economics,
                'risk_perception_bias': risk_perception_bias,
                'temporal_preference': temporal_preference,
                'emergent_intelligence': emergent_intelligence,
                
                # å†³ç­–æ”¯æŒ
                'expected_value_psychology': expected_value_psychology,
                'decision_biases': decision_biases,
                'information_complexity': information_complexity,
                'social_proof_strength': social_proof_strength,
                
                # å…ƒæ•°æ®
                'analysis_depth': len(data_list),
                'confidence_interval': (confidence_level - 0.1, confidence_level + 0.1),
                'reliability_score': self._calculate_analysis_reliability(
                    behavioral_entropy, consistency_index, cognitive_load
                )
            }
            
            # å­˜å‚¨åˆ°ä¸åŒå°ºåº¦çš„å†å²è®°å½•
            self.micro_psychology_states.append({
                'state': micro_analysis,
                'timestamp': psychology_state['timestamp']
            })
            
            self.meso_psychology_states.append({
                'state': meso_analysis,
                'timestamp': psychology_state['timestamp']
            })
            
            self.macro_psychology_states.append({
                'state': macro_analysis,
                'timestamp': psychology_state['timestamp']
            })
            
            return psychology_state
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§ç¾¤ä½“å¿ƒç†åˆ†æå¤±è´¥: {e}")
            return self._generate_default_psychology_state()

    def _micro_psychology_analysis(self, data_list: List[Dict]) -> Dict:
        """å¾®è§‚å¿ƒç†åˆ†æ - èšç„¦ä¸ªä½“å†³ç­–ç¬é—´"""
        try:
            import numpy as np
            
            if len(data_list) < 2:
                return {'state': 'insufficient_data', 'confidence': 0.0}
            
            # å³æ—¶ååº”æ¨¡å¼åˆ†æ
            immediate_reactions = []
            for i in range(min(3, len(data_list) - 1)):
                curr_tails = set(data_list[i].get('tails', []))
                prev_tails = set(data_list[i + 1].get('tails', []))
                
                # è®¡ç®—ååº”å¼ºåº¦
                reaction_intensity = len(curr_tails.symmetric_difference(prev_tails)) / 10.0
                immediate_reactions.append(reaction_intensity)
            
            # å†²åŠ¨å†³ç­–æŒ‡æ•°
            impulsive_index = np.std(immediate_reactions) if len(immediate_reactions) > 1 else 0
            
            # çŸ­æœŸè®°å¿†æ•ˆåº”
            short_term_memory = self._calculate_short_term_memory_effect(data_list[:3])
            
            # ç¬æ—¶æ³¨æ„åŠ›åˆ†é…
            attention_allocation = self._calculate_instantaneous_attention(data_list[:2])
            
            return {
                'immediate_reactions': immediate_reactions,
                'impulsive_index': float(impulsive_index),
                'short_term_memory': short_term_memory,
                'attention_allocation': attention_allocation,
                'micro_volatility': np.mean(immediate_reactions) if immediate_reactions else 0,
                'confidence': min(0.9, len(immediate_reactions) / 3.0)
            }
            
        except Exception as e:
            return {'state': 'analysis_failed', 'error': str(e), 'confidence': 0.0}

    def _meso_psychology_analysis(self, data_list: List[Dict]) -> Dict:
        """ä¸­è§‚å¿ƒç†åˆ†æ - èšç„¦ç¾¤ä½“åŠ¨åŠ›å­¦"""
        try:
            import numpy as np
            from scipy import stats
            
            if len(data_list) < 5:
                return {'state': 'insufficient_data', 'confidence': 0.0}
            
            # ç¾¤ä½“åŠ¨åŠ›å­¦åˆ†æ
            group_dynamics = self._analyze_group_dynamics_patterns(data_list)
            
            # ç¤¾ä¼šå­¦ä¹ æ•ˆåº”
            social_learning = self._measure_social_learning_effects(data_list)
            
            # ä»ä¼—å‹åŠ›å¼ºåº¦
            conformity_pressure = self._calculate_conformity_pressure(data_list)
            
            # ç¾¤ä½“æ€ç»´å€¾å‘
            groupthink_tendency = self._assess_groupthink_tendency(data_list)
            
            # ä¿¡æ¯çº§è”å¼ºåº¦
            information_cascade = self._measure_information_cascade_strength(data_list)
            
            # ç¤¾ä¼šå½±å“ä¼ æ’­
            influence_propagation = self._analyze_influence_propagation(data_list)
            
            # ç¾¤ä½“æåŒ–åº¦é‡
            group_polarization = self._measure_group_polarization(data_list)
            
            return {
                'group_dynamics': group_dynamics,
                'social_learning': social_learning,
                'conformity_pressure': conformity_pressure,
                'groupthink_tendency': groupthink_tendency,
                'information_cascade': information_cascade,
                'influence_propagation': influence_propagation,
                'group_polarization': group_polarization,
                'meso_coherence': self._calculate_meso_coherence(
                    conformity_pressure, groupthink_tendency, information_cascade
                ),
                'confidence': min(0.9, len(data_list) / 15.0)
            }
            
        except Exception as e:
            return {'state': 'analysis_failed', 'error': str(e), 'confidence': 0.0}

    def _macro_psychology_analysis(self, data_list: List[Dict]) -> Dict:
        """å®è§‚å¿ƒç†åˆ†æ - èšç„¦é•¿æœŸè¶‹åŠ¿å’Œæ–‡åŒ–æ¨¡å¼"""
        try:
            import numpy as np
            
            if len(data_list) < 10:
                return {'state': 'insufficient_data', 'confidence': 0.0}
            
            # é•¿æœŸè¡Œä¸ºæ¨¡å¼è¯†åˆ«
            long_term_patterns = self._identify_long_term_behavioral_patterns(data_list)
            
            # æ–‡åŒ–è®°å¿†åˆ†æ
            cultural_memory = self._analyze_cultural_memory_patterns(data_list)
            
            # é›†ä½“å­¦ä¹ æ›²çº¿
            learning_curve = self._model_collective_learning_curve(data_list)
            
            # é€‚åº”æ€§å¤æ‚ç³»ç»Ÿç‰¹å¾
            complex_system_features = self._analyze_complex_system_features(data_list)
            
            # é•¿æœŸç¨³å®šæ€§è¯„ä¼°
            long_term_stability = self._assess_long_term_stability(data_list)
            
            # æ¼”åŒ–åšå¼ˆåŠ¨åŠ›å­¦
            evolutionary_dynamics = self._analyze_evolutionary_game_dynamics(data_list)
            
            # é›†ä½“æ™ºæ…§æ¶Œç°
            collective_wisdom = self._detect_collective_wisdom_emergence(data_list)
            
            return {
                'long_term_patterns': long_term_patterns,
                'cultural_memory': cultural_memory,
                'learning_curve': learning_curve,
                'complex_system_features': complex_system_features,
                'long_term_stability': long_term_stability,
                'evolutionary_dynamics': evolutionary_dynamics,
                'collective_wisdom': collective_wisdom,
                'macro_trend': self._extract_macro_trend(long_term_patterns, cultural_memory),
                'confidence': min(0.95, len(data_list) / 50.0)
            }
            
        except Exception as e:
            return {'state': 'analysis_failed', 'error': str(e), 'confidence': 0.0}
    
    def _detect_herd_behavior_patterns(self, data_list: List[Dict]) -> Dict:
        """
        ç§‘ç ”çº§ç¾Šç¾¤è¡Œä¸ºæ¨¡å¼æ£€æµ‹ - åŸºäºå¤æ‚ç½‘ç»œç†è®ºå’Œè¡Œä¸ºç»æµå­¦
        
        ç†è®ºåŸºç¡€ï¼š
        - Aschä»ä¼—å®éªŒç†è®º
        - ä¿¡æ¯çº§è”æ¨¡å‹
        - ç½‘ç»œä¼ æ’­åŠ¨åŠ›å­¦
        - ä¸´ç•Œç›¸å˜ç†è®º
        """
        try:
            import numpy as np
            from scipy import stats, signal
            from collections import defaultdict
            import math
            
            if len(data_list) < 3:
                return self._generate_default_herd_state()
            
            # === å¤šç»´åº¦ç¾Šç¾¤è¡Œä¸ºæ£€æµ‹ ===
            
            # 1. æ—¶é—´åºåˆ—ç›¸ä¼¼æ€§åˆ†æ
            temporal_similarity = self._analyze_temporal_similarity_patterns(data_list)
            
            # 2. ç©ºé—´èšé›†æ€§åˆ†æ
            spatial_clustering = self._analyze_spatial_clustering_behavior(data_list)
            
            # 3. é¢‘ç‡åŸŸç¾Šç¾¤è¡Œä¸ºæ£€æµ‹
            frequency_domain_herd = self._detect_frequency_domain_herding(data_list)
            
            # 4. ç½‘ç»œä¼ æ’­åŠ¨åŠ›å­¦åˆ†æ
            network_propagation = self._analyze_network_propagation_dynamics(data_list)
            
            # 5. ä¸´ç•Œç›¸å˜æ£€æµ‹
            phase_transition = self._detect_critical_phase_transitions(data_list)
            
            # 6. ä¿¡æ¯çº§è”å¼ºåº¦æµ‹é‡
            cascade_intensity = self._measure_information_cascade_intensity(data_list)
            
            # 7. æ¨¡ä»¿è¡Œä¸ºé‡åŒ–
            imitation_behavior = self._quantify_imitation_behavior(data_list)
            
            # 8. ç¤¾ä¼šä¼ æŸ“æ¨¡å‹
            social_contagion = self._model_social_contagion_dynamics(data_list)
            
            # 9. ç¾¤ä½“åŒæ­¥æ€§åˆ†æ
            synchronization = self._analyze_group_synchronization(data_list)
            
            # 10. ç¾Šç¾¤è¡Œä¸ºå¼ºåº¦æ—¶å˜åˆ†æ
            time_varying_intensity = self._analyze_time_varying_herd_intensity(data_list)
            
            # === é«˜çº§ç¾Šç¾¤è¡Œä¸ºæŒ‡æ ‡è®¡ç®— ===
            
            # èµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°ï¼ˆé›†ä¸­åº¦ï¼‰
            hhi_index = self._calculate_herfindahl_hirschman_index(data_list)
            
            # ç¾Šç¾¤è¡Œä¸ºæŒç»­æ€§æµ‹é‡
            persistence_measure = self._calculate_herd_persistence(data_list)
            
            # åé¦ˆç¯å¼ºåº¦
            feedback_loop_strength = self._measure_feedback_loop_strength(data_list)
            
            # ç¾¤ä½“è®°å¿†æ•ˆåº”
            collective_memory_effect = self._analyze_collective_memory_in_herding(data_list)
            
            # ç¾Šç¾¤è¡Œä¸ºçš„ç©ºé—´æ‰©æ•£
            spatial_diffusion = self._model_spatial_diffusion_of_herding(data_list)
            
            # === ç¾Šç¾¤è¡Œä¸ºç±»å‹è¯†åˆ« ===
            herd_type = self._identify_herd_behavior_type(
                temporal_similarity, spatial_clustering, cascade_intensity,
                imitation_behavior, synchronization
            )
            
            # === ç¾Šç¾¤å¼ºåº¦ç»¼åˆè¯„ä¼° ===
            herd_intensity = self._calculate_comprehensive_herd_intensity(
                temporal_similarity, cascade_intensity, imitation_behavior,
                synchronization, hhi_index, persistence_measure
            )
            
            # === ç¾Šç¾¤è¡Œä¸ºé¢„æµ‹æ¨¡å‹ ===
            herd_prediction = self._predict_future_herd_behavior(
                data_list, time_varying_intensity, feedback_loop_strength
            )
            
            # === ç¾Šç¾¤è¡Œä¸ºç¨³å®šæ€§åˆ†æ ===
            stability_analysis = self._analyze_herd_stability(
                persistence_measure, feedback_loop_strength, phase_transition
            )
            
            # === åç¾Šç¾¤è¡Œä¸ºæœºä¼šè¯†åˆ« ===
            contrarian_opportunities = self._identify_contrarian_opportunities(
                herd_intensity, persistence_measure, phase_transition
            )
            
            # === ç¾Šç¾¤è¡Œä¸ºé£é™©è¯„ä¼° ===
            risk_assessment = self._assess_herd_behavior_risks(
                herd_intensity, stability_analysis, collective_memory_effect
            )
            
            # === ç¾¤ä½“æ™ºæ…§vsç¾¤ä½“æ„šè ¢åˆ¤æ–­ ===
            wisdom_assessment = self._assess_crowd_wisdom_vs_madness(
                herd_type, herd_intensity, collective_memory_effect
            )
            
            # æ„å»ºå®Œæ•´çš„ç¾Šç¾¤è¡Œä¸ºåˆ†æç»“æœ
            herd_analysis = {
                'timestamp': self._get_current_timestamp(),
                
                # æ ¸å¿ƒæ£€æµ‹ç»“æœ
                'herd_detected': herd_intensity > self.research_parameters['crowd_analysis']['herd_threshold_adaptive'],
                'herd_intensity': float(herd_intensity),
                'herd_type': herd_type,
                
                # è¯¦ç»†åˆ†æç»„ä»¶
                'temporal_similarity': temporal_similarity,
                'spatial_clustering': spatial_clustering,
                'frequency_domain_herd': frequency_domain_herd,
                'network_propagation': network_propagation,
                'phase_transition': phase_transition,
                'cascade_intensity': cascade_intensity,
                'imitation_behavior': imitation_behavior,
                'social_contagion': social_contagion,
                'synchronization': synchronization,
                'time_varying_intensity': time_varying_intensity,
                
                # é«˜çº§æŒ‡æ ‡
                'hhi_index': hhi_index,
                'persistence_measure': persistence_measure,
                'feedback_loop_strength': feedback_loop_strength,
                'collective_memory_effect': collective_memory_effect,
                'spatial_diffusion': spatial_diffusion,
                
                # é¢„æµ‹å’Œè¯„ä¼°
                'herd_prediction': herd_prediction,
                'stability_analysis': stability_analysis,
                'contrarian_opportunities': contrarian_opportunities,
                'risk_assessment': risk_assessment,
                'wisdom_assessment': wisdom_assessment,
                
                # å…ƒæ•°æ®
                'analysis_confidence': min(0.95, len(data_list) / 30.0),
                'detection_sensitivity': self.research_parameters['crowd_analysis']['herd_threshold_adaptive'],
                'reliability_score': self._calculate_herd_detection_reliability(
                    temporal_similarity, cascade_intensity, synchronization
                )
            }
            
            return herd_analysis
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§ç¾Šç¾¤è¡Œä¸ºæ£€æµ‹å¤±è´¥: {e}")
            return self._generate_default_herd_state()

    def _analyze_temporal_similarity_patterns(self, data_list: List[Dict]) -> Dict:
        """æ—¶é—´åºåˆ—ç›¸ä¼¼æ€§æ¨¡å¼åˆ†æ"""
        try:
            import numpy as np
            from scipy.spatial.distance import cosine, euclidean
            
            similarities = []
            autocorrelations = []
            cross_correlations = []
            
            for i in range(min(10, len(data_list) - 1)):
                curr_vector = self._tail_set_to_vector(data_list[i].get('tails', []))
                next_vector = self._tail_set_to_vector(data_list[i + 1].get('tails', []))
                
                # ä½™å¼¦ç›¸ä¼¼åº¦
                cosine_sim = 1 - cosine(curr_vector, next_vector)
                similarities.append(cosine_sim if not np.isnan(cosine_sim) else 0)
                
                # è‡ªç›¸å…³åˆ†æ
                autocorr = np.corrcoef(curr_vector, next_vector)[0, 1]
                autocorrelations.append(autocorr if not np.isnan(autocorr) else 0)
            
            # äº¤å‰ç›¸å…³åˆ†æï¼ˆæ»åæ•ˆåº”ï¼‰
            if len(data_list) >= 5:
                for lag in range(1, min(4, len(data_list) - 1)):
                    if lag < len(data_list):
                        curr_vector = self._tail_set_to_vector(data_list[0].get('tails', []))
                        lag_vector = self._tail_set_to_vector(data_list[lag].get('tails', []))
                        cross_corr = np.corrcoef(curr_vector, lag_vector)[0, 1]
                        cross_correlations.append(cross_corr if not np.isnan(cross_corr) else 0)
            
            return {
                'similarity_scores': similarities,
                'mean_similarity': float(np.mean(similarities)) if similarities else 0,
                'similarity_stability': float(1 - np.std(similarities)) if len(similarities) > 1 else 0,
                'autocorrelations': autocorrelations,
                'cross_correlations': cross_correlations,
                'temporal_persistence': float(np.mean(autocorrelations)) if autocorrelations else 0
            }
            
        except Exception as e:
            return {'error': str(e), 'mean_similarity': 0}

    def _analyze_spatial_clustering_behavior(self, data_list: List[Dict]) -> Dict:
        """ç©ºé—´èšé›†æ€§è¡Œä¸ºåˆ†æ"""
        try:
            import numpy as np
            
            # å°¾æ•°ç©ºé—´åˆ†å¸ƒåˆ†æ
            tail_positions = defaultdict(list)
            for period_idx, period in enumerate(data_list[:20]):
                for tail in period.get('tails', []):
                    tail_positions[tail].append(period_idx)
            
            # è®¡ç®—ç©ºé—´èšé›†æŒ‡æ•°
            clustering_indices = {}
            spatial_entropy = 0
            
            for tail, positions in tail_positions.items():
                if len(positions) > 1:
                    # è®¡ç®—ä½ç½®é—´è·çš„æ–¹å·®ï¼ˆèšé›†åº¦æŒ‡æ ‡ï¼‰
                    position_diffs = np.diff(sorted(positions))
                    clustering_indices[tail] = float(1 / (1 + np.var(position_diffs)))
                else:
                    clustering_indices[tail] = 0
            
            # æ•´ä½“ç©ºé—´ç†µ
            total_positions = sum(len(positions) for positions in tail_positions.values())
            if total_positions > 0:
                for positions in tail_positions.values():
                    if len(positions) > 0:
                        prob = len(positions) / total_positions
                        spatial_entropy -= prob * np.log2(prob)
            
            return {
                'clustering_indices': clustering_indices,
                'mean_clustering': float(np.mean(list(clustering_indices.values()))) if clustering_indices else 0,
                'spatial_entropy': float(spatial_entropy),
                'dispersion_measure': float(1 - spatial_entropy / np.log2(10)) if spatial_entropy > 0 else 0
            }
            
        except Exception as e:
            return {'error': str(e), 'mean_clustering': 0}

    def _analyze_anchoring_bias_advanced(self, data_list: List[Dict]) -> Dict:
        """é«˜çº§é”šå®šåå·®åˆ†æ"""
        try:
            import numpy as np
            
            anchoring_effects = []
            anchor_persistence = []
            
            # åˆ†ææ¯ä¸ªå°¾æ•°ä½œä¸ºé”šç‚¹çš„æ•ˆåº”
            for anchor_tail in range(10):
                anchor_periods = []
                subsequent_influences = []
                
                for i, period in enumerate(data_list):
                    if anchor_tail in period.get('tails', []):
                        anchor_periods.append(i)
                        
                        # åˆ†æåç»­3æœŸçš„å½±å“
                        for j in range(1, min(4, len(data_list) - i)):
                            if i + j < len(data_list):
                                future_tails = data_list[i + j].get('tails', [])
                                if anchor_tail in future_tails:
                                    subsequent_influences.append(j)
                
                if anchor_periods and subsequent_influences:
                    # è®¡ç®—é”šå®šå¼ºåº¦
                    anchor_strength = len(subsequent_influences) / len(anchor_periods)
                    anchoring_effects.append(anchor_strength)
                    
                    # è®¡ç®—é”šå®šæŒç»­æ€§
                    avg_persistence = np.mean(subsequent_influences) if subsequent_influences else 0
                    anchor_persistence.append(avg_persistence)
            
            # é”šå®šåå·®çš„èµ”ç‡æ•æ„Ÿæ€§åˆ†æ
            # 0å°¾ï¼ˆ2å€èµ”ç‡ï¼‰vs 1-9å°¾ï¼ˆ1.8å€èµ”ç‡ï¼‰çš„é”šå®šæ•ˆåº”å·®å¼‚
            zero_tail_anchoring = self._analyze_tail_specific_anchoring(data_list, 0, 2.0)
            other_tails_anchoring = np.mean([
                self._analyze_tail_specific_anchoring(data_list, tail, 1.8) 
                for tail in range(1, 10)
            ])
            
            odds_sensitivity = abs(zero_tail_anchoring - other_tails_anchoring)
            
            return {
                'anchoring_effects': anchoring_effects,
                'mean_anchoring_strength': float(np.mean(anchoring_effects)) if anchoring_effects else 0,
                'anchor_persistence': anchor_persistence,
                'mean_persistence': float(np.mean(anchor_persistence)) if anchor_persistence else 0,
                'zero_tail_anchoring': zero_tail_anchoring,
                'other_tails_anchoring': other_tails_anchoring,
                'odds_sensitivity': odds_sensitivity,
                'anchoring_bias_score': self._calculate_anchoring_bias_score(
                    anchoring_effects, anchor_persistence, odds_sensitivity
                )
            }
            
        except Exception as e:
            return {'error': str(e), 'anchoring_bias_score': 0}

    def _analyze_tail_specific_anchoring(self, data_list: List[Dict], target_tail: int, odds: float) -> float:
        """åˆ†æç‰¹å®šå°¾æ•°çš„é”šå®šæ•ˆåº”"""
        try:
            anchor_instances = 0
            subsequent_appearances = 0
            
            for i, period in enumerate(data_list[:-3]):  # ä¿ç•™å3æœŸç”¨äºåˆ†æ
                if target_tail in period.get('tails', []):
                    anchor_instances += 1
                    
                    # æ£€æŸ¥åç»­3æœŸçš„å‡ºç°
                    for j in range(1, 4):
                        if i + j < len(data_list):
                            if target_tail in data_list[i + j].get('tails', []):
                                # è€ƒè™‘èµ”ç‡æƒé‡ï¼šé«˜èµ”ç‡å¯èƒ½å¢å¼ºé”šå®šæ•ˆåº”
                                weight = odds / 2.0  # æ ‡å‡†åŒ–åˆ°1.0åŸºå‡†
                                subsequent_appearances += weight
                                break
            
            return subsequent_appearances / anchor_instances if anchor_instances > 0 else 0
            
        except:
            return 0.0
    
    def _analyze_emotion_dynamics(self, data_list: List[Dict]) -> Dict:
        """
        ç§‘ç ”çº§æƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æ - åŸºäºæƒ…æ„Ÿè®¡ç®—å’ŒåŠ¨åŠ›ç³»ç»Ÿç†è®º
        
        ç†è®ºåŸºç¡€ï¼š
        - æƒ…æ„Ÿè½®æ¨¡å‹ï¼ˆPlutchik's Wheel of Emotionsï¼‰
        - æƒ…ç»ªä¼ æŸ“ç†è®ºï¼ˆEmotional Contagionï¼‰
        - æƒ…ç»ªè°ƒèŠ‚ç†è®ºï¼ˆEmotion Regulation Theoryï¼‰
        - å¤æ‚ç³»ç»Ÿä¸­çš„æƒ…ç»ªåŠ¨åŠ›å­¦
        """
        try:
            import numpy as np
            from scipy import signal, stats
            from collections import deque
            import math
            
            if len(data_list) < 5:
                return self._generate_default_emotion_state()
            
            # === 1. å¤šç»´æƒ…ç»ªçŠ¶æ€è¯†åˆ« ===
            emotion_states = self._identify_multidimensional_emotion_states(data_list)
            
            # === 2. æƒ…ç»ªå¼ºåº¦æ—¶é—´åºåˆ—åˆ†æ ===
            emotion_intensity_series = self._extract_emotion_intensity_time_series(data_list)
            
            # === 3. æƒ…ç»ªä¼ æŸ“åŠ¨åŠ›å­¦æ¨¡å‹ ===
            contagion_dynamics = self._model_emotion_contagion_dynamics(data_list)
            
            # === 4. æƒ…ç»ªç›¸å˜æ£€æµ‹ ===
            emotion_phase_transitions = self._detect_emotion_phase_transitions(data_list)
            
            # === 5. æƒ…ç»ªè®°å¿†æ•ˆåº”åˆ†æ ===
            emotion_memory_effects = self._analyze_emotion_memory_effects(data_list)
            
            # === 6. æƒ…ç»ªæ³¢åŠ¨æ€§åˆ†æ ===
            emotion_volatility = self._analyze_emotion_volatility_patterns(data_list)
            
            # === 7. æƒ…ç»ªåè°ƒæ€§åˆ†æ ===
            emotion_coherence = self._analyze_emotion_coherence_patterns(data_list)
            
            # === 8. æƒ…ç»ªåé¦ˆç¯è·¯åˆ†æ ===
            feedback_loops = self._analyze_emotion_feedback_loops(data_list)
            
            # === 9. æƒ…ç»ªé€‚åº”æ€§åˆ†æ ===
            emotion_adaptation = self._analyze_emotion_adaptation_mechanisms(data_list)
            
            # === 10. æƒ…ç»ªå†³ç­–å½±å“åˆ†æ ===
            decision_influence = self._analyze_emotion_decision_influence(data_list)
            
            # === 11. æƒ…ç»ªç½‘ç»œåˆ†æ ===
            emotion_network = self._analyze_emotion_network_structure(data_list)
            
            # === 12. æƒ…ç»ªé¢„æµ‹æ¨¡å‹ ===
            emotion_prediction = self._build_emotion_prediction_model(data_list)
            
            # === æƒ…ç»ªä¸èµ”ç‡å…³ç³»åˆ†æ ===
            odds_emotion_relationship = self._analyze_odds_emotion_relationship(data_list)
            
            # === å½“å‰ä¸»å¯¼æƒ…ç»ªè¯†åˆ« ===
            current_dominant_emotion = self._identify_current_dominant_emotion(
                emotion_states, emotion_intensity_series, contagion_dynamics
            )
            
            # === æƒ…ç»ªç¨³å®šæ€§è¯„ä¼° ===
            emotion_stability = self._assess_emotion_stability(
                emotion_volatility, emotion_phase_transitions, emotion_coherence
            )
            
            # === æƒ…ç»ªè¶‹åŠ¿é¢„æµ‹ ===
            emotion_trend = self._predict_emotion_trend(
                emotion_intensity_series, emotion_prediction, feedback_loops
            )
            
            # === æƒ…ç»ªè°ƒèŠ‚èƒ½åŠ›è¯„ä¼° ===
            regulation_capacity = self._assess_emotion_regulation_capacity(
                emotion_adaptation, emotion_volatility, emotion_memory_effects
            )
            
            # === é›†ä½“æƒ…ç»ªæ™ºèƒ½è¯„ä¼° ===
            collective_emotional_intelligence = self._assess_collective_emotional_intelligence(
                emotion_coherence, emotion_adaptation, regulation_capacity
            )
            
            # === æƒ…ç»ªé©±åŠ¨çš„å†³ç­–è´¨é‡ ===
            emotion_driven_decision_quality = self._assess_emotion_driven_decision_quality(
                decision_influence, current_dominant_emotion, emotion_stability
            )
            
            # === æ„å»ºå®Œæ•´æƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æç»“æœ ===
            emotion_dynamics_analysis = {
                'timestamp': self._get_current_timestamp(),
                
                # æ ¸å¿ƒæƒ…ç»ªçŠ¶æ€
                'current_emotion': current_dominant_emotion,
                'emotion_intensity': self._calculate_current_emotion_intensity(emotion_intensity_series),
                'emotion_stability': emotion_stability,
                'emotion_trend': emotion_trend,
                
                # è¯¦ç»†åˆ†æç»„ä»¶
                'emotion_states': emotion_states,
                'emotion_intensity_series': emotion_intensity_series,
                'contagion_dynamics': contagion_dynamics,
                'emotion_phase_transitions': emotion_phase_transitions,
                'emotion_memory_effects': emotion_memory_effects,
                'emotion_volatility': emotion_volatility,
                'emotion_coherence': emotion_coherence,
                'feedback_loops': feedback_loops,
                'emotion_adaptation': emotion_adaptation,
                'decision_influence': decision_influence,
                'emotion_network': emotion_network,
                'emotion_prediction': emotion_prediction,
                
                # é«˜çº§æŒ‡æ ‡
                'regulation_capacity': regulation_capacity,
                'collective_emotional_intelligence': collective_emotional_intelligence,
                'emotion_driven_decision_quality': emotion_driven_decision_quality,
                'odds_emotion_relationship': odds_emotion_relationship,
                
                # æƒ…ç»ªé£é™©è¯„ä¼°
                'emotion_risk_factors': self._identify_emotion_risk_factors(
                    current_dominant_emotion, emotion_volatility, emotion_phase_transitions
                ),
                'emotional_vulnerability': self._assess_emotional_vulnerability(
                    emotion_stability, regulation_capacity, feedback_loops
                ),
                
                # å…ƒæ•°æ®
                'analysis_confidence': min(0.95, len(data_list) / 25.0),
                'emotion_detection_accuracy': self._estimate_emotion_detection_accuracy(
                    emotion_coherence, emotion_network
                ),
                'reliability_score': self._calculate_emotion_analysis_reliability(
                    emotion_stability, emotion_coherence, regulation_capacity
                )
            }
            
            return emotion_dynamics_analysis
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§æƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æå¤±è´¥: {e}")
            return self._generate_default_emotion_state()

    def _identify_multidimensional_emotion_states(self, data_list: List[Dict]) -> Dict:
        """å¤šç»´æƒ…ç»ªçŠ¶æ€è¯†åˆ«"""
        try:
            import numpy as np
            from collections import Counter
            emotion_dimensions = {
                'valence': [],      # æƒ…æ„Ÿæ•ˆä»·ï¼ˆæ­£é¢-è´Ÿé¢ï¼‰
                'arousal': [],      # æƒ…æ„Ÿå”¤é†’åº¦ï¼ˆæ¿€æ´»-å¹³é™ï¼‰
                'dominance': [],    # æƒ…æ„Ÿæ”¯é…æ€§ï¼ˆæ§åˆ¶-è¢«æ§åˆ¶ï¼‰
                'certainty': [],    # ç¡®å®šæ€§ï¼ˆç¡®å®š-ä¸ç¡®å®šï¼‰
                'intensity': []     # å¼ºåº¦ï¼ˆå¼º-å¼±ï¼‰
            }
            
            for i, period in enumerate(data_list[:20]):
                tails = period.get('tails', [])
                
                # æ•ˆä»·åˆ†æï¼šåŸºäºå°¾æ•°åˆ†å¸ƒçš„å¯¹ç§°æ€§
                if tails:
                    tail_mean = np.mean(tails)
                    symmetry = 1 - abs(tail_mean - 4.5) / 4.5
                    valence = (symmetry - 0.5) * 2  # æ ‡å‡†åŒ–åˆ°[-1, 1]
                else:
                    valence = 0
                
                # å”¤é†’åº¦åˆ†æï¼šåŸºäºå°¾æ•°å˜åŒ–å¹…åº¦
                if i > 0:
                    prev_tails = set(data_list[i-1].get('tails', []))
                    curr_tails = set(tails)
                    change_magnitude = len(curr_tails.symmetric_difference(prev_tails)) / 10.0
                    arousal = min(1.0, change_magnitude * 2)
                else:
                    arousal = 0.5
                
                # æ”¯é…æ€§åˆ†æï¼šåŸºäºé€‰æ‹©çš„é›†ä¸­åº¦
                if tails:
                    tail_counts = Counter(tails)
                    max_count = max(tail_counts.values())
                    concentration = max_count / len(tails)
                    dominance = concentration
                else:
                    dominance = 0.5
                
                # ç¡®å®šæ€§åˆ†æï¼šåŸºäºå°¾æ•°æ•°é‡çš„ä¸€è‡´æ€§
                certainty = 1 - abs(len(tails) - 5) / 5.0 if tails else 0
                
                # å¼ºåº¦åˆ†æï¼šç»¼åˆå¤šä¸ªå› ç´ 
                intensity = (arousal + dominance + certainty) / 3.0
                
                emotion_dimensions['valence'].append(valence)
                emotion_dimensions['arousal'].append(arousal)
                emotion_dimensions['dominance'].append(dominance)
                emotion_dimensions['certainty'].append(certainty)
                emotion_dimensions['intensity'].append(intensity)
            
            # æƒ…ç»ªç±»åˆ«æ˜ å°„
            emotion_categories = self._map_dimensions_to_emotions(emotion_dimensions)
            
            return {
                'dimensions': emotion_dimensions,
                'emotion_categories': emotion_categories,
                'current_emotion_vector': [
                    emotion_dimensions['valence'][-1] if emotion_dimensions['valence'] else 0,
                    emotion_dimensions['arousal'][-1] if emotion_dimensions['arousal'] else 0,
                    emotion_dimensions['dominance'][-1] if emotion_dimensions['dominance'] else 0,
                    emotion_dimensions['certainty'][-1] if emotion_dimensions['certainty'] else 0,
                    emotion_dimensions['intensity'][-1] if emotion_dimensions['intensity'] else 0
                ]
            }
            
        except Exception as e:
            return {'error': str(e), 'dimensions': {}}

    def _map_dimensions_to_emotions(self, dimensions: Dict) -> Dict:
        """å°†æƒ…ç»ªç»´åº¦æ˜ å°„åˆ°å…·ä½“æƒ…ç»ªç±»åˆ«"""
        try:
            import numpy as np
            
            if not dimensions['valence'] or not dimensions['arousal']:
                return {'current_emotion': 'neutral', 'confidence': 0.5}
            
            # è·å–æœ€æ–°çš„æƒ…ç»ªç»´åº¦å€¼
            valence = dimensions['valence'][-1]
            arousal = dimensions['arousal'][-1]
            dominance = dimensions['dominance'][-1]
            certainty = dimensions['certainty'][-1]
            
            # åŸºäºç»´åº¦ç»„åˆè¯†åˆ«æƒ…ç»ª
            if valence > 0.3 and arousal > 0.6:
                if dominance > 0.6:
                    emotion = 'excitement'  # å…´å¥‹
                else:
                    emotion = 'joy'         # å¿«ä¹
            elif valence < -0.3 and arousal > 0.6:
                if dominance < 0.4:
                    emotion = 'fear'        # ææƒ§
                else:
                    emotion = 'anger'       # æ„¤æ€’
            elif valence > 0.2 and arousal < 0.4:
                emotion = 'calm'            # å¹³é™
            elif valence < -0.2 and arousal < 0.4:
                emotion = 'sadness'         # æ‚²ä¼¤
            elif certainty < 0.3:
                emotion = 'confusion'       # å›°æƒ‘
            elif arousal > 0.7:
                emotion = 'anxiety'         # ç„¦è™‘
            else:
                emotion = 'neutral'         # ä¸­æ€§
            
            # è®¡ç®—æƒ…ç»ªè¯†åˆ«çš„ç½®ä¿¡åº¦
            confidence = min(0.95, abs(valence) + arousal + dominance) / 3.0
            
            return {
                'current_emotion': emotion,
                'confidence': confidence,
                'emotion_coordinates': {
                    'valence': valence,
                    'arousal': arousal,
                    'dominance': dominance,
                    'certainty': certainty
                }
            }
            
        except Exception as e:
            return {'current_emotion': 'neutral', 'confidence': 0.5, 'error': str(e)}
    
    def _generate_contrarian_signals(self, candidate_tails: List[int], psychology_state: Dict, 
                                herd_behavior: Dict) -> Dict:
        """
        ç§‘ç ”çº§åå‘ä¿¡å·ç”Ÿæˆ - åŸºäºåšå¼ˆè®ºå’Œåå‘å¿ƒç†å­¦
        
        ç†è®ºåŸºç¡€ï¼š
        - åå‘æŠ•èµ„ç†è®ºï¼ˆContrarian Investment Theoryï¼‰
        - ç¾¤ä½“æåŒ–ç†è®ºï¼ˆGroup Polarization Theoryï¼‰
        - å¸‚åœºå¼‚è±¡ç†è®ºï¼ˆMarket Anomaly Theoryï¼‰
        - è¡Œä¸ºé‡‘èåå‘ç­–ç•¥
        """
        try:
            import numpy as np
            from scipy import stats, optimize
            from collections import defaultdict
            import math
            
            if not candidate_tails:
                return self._generate_default_contrarian_signals()
            
            # === 1. å¤šå±‚æ¬¡åå‘ä¿¡å·æ£€æµ‹ ===
            
            # ç¾¤ä½“è¡Œä¸ºåå‘ä¿¡å·
            herd_contrarian_signals = self._generate_herd_contrarian_signals(
                candidate_tails, herd_behavior, psychology_state
            )
            
            # æƒ…ç»ªåå‘ä¿¡å·
            emotion_contrarian_signals = self._generate_emotion_contrarian_signals(
                candidate_tails, psychology_state
            )
            
            # è®¤çŸ¥åå·®åå‘ä¿¡å·
            bias_contrarian_signals = self._generate_bias_contrarian_signals(
                candidate_tails, psychology_state
            )
            
            # å¸‚åœºæ•ˆç‡åå‘ä¿¡å·
            efficiency_contrarian_signals = self._generate_efficiency_contrarian_signals(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # ä¿¡æ¯çº§è”åå‘ä¿¡å·
            cascade_contrarian_signals = self._generate_cascade_contrarian_signals(
                candidate_tails, herd_behavior
            )
            
            # === 2. èµ”ç‡åŠ æƒåå‘åˆ†æ ===
            odds_weighted_contrarian = self._generate_odds_weighted_contrarian_signals(
                candidate_tails, psychology_state
            )
            
            # === 3. æ—¶é—´åŠ¨æ€åå‘ä¿¡å· ===
            temporal_contrarian_signals = self._generate_temporal_contrarian_signals(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # === 4. åšå¼ˆè®ºåå‘ç­–ç•¥ ===
            game_theory_contrarian = self._generate_game_theory_contrarian_signals(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # === 5. å¤æ‚ç½‘ç»œåå‘ä¿¡å· ===
            network_contrarian_signals = self._generate_network_contrarian_signals(
                candidate_tails, psychology_state
            )
            
            # === 6. ä¿¡æ¯è®ºåå‘ä¿¡å· ===
            information_theory_contrarian = self._generate_information_theory_contrarian_signals(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # === 7. æ··æ²Œç†è®ºåå‘ä¿¡å· ===
            chaos_theory_contrarian = self._generate_chaos_theory_contrarian_signals(
                candidate_tails, psychology_state
            )
            
            # === 8. é‡å­åšå¼ˆåå‘ä¿¡å· ===
            quantum_contrarian_signals = self._generate_quantum_contrarian_signals(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # === ä¿¡å·å¼ºåº¦ç»¼åˆè®¡ç®— ===
            comprehensive_signal_strength = self._calculate_comprehensive_contrarian_strength({
                'herd_contrarian': herd_contrarian_signals,
                'emotion_contrarian': emotion_contrarian_signals,
                'bias_contrarian': bias_contrarian_signals,
                'efficiency_contrarian': efficiency_contrarian_signals,
                'cascade_contrarian': cascade_contrarian_signals,
                'odds_weighted': odds_weighted_contrarian,
                'temporal_contrarian': temporal_contrarian_signals,
                'game_theory': game_theory_contrarian,
                'network_contrarian': network_contrarian_signals,
                'information_theory': information_theory_contrarian,
                'chaos_theory': chaos_theory_contrarian,
                'quantum_contrarian': quantum_contrarian_signals
            })
            
            # === åå‘ç›®æ ‡ä¼˜åŒ–é€‰æ‹© ===
            optimized_contrarian_targets = self._optimize_contrarian_target_selection(
                candidate_tails, comprehensive_signal_strength, psychology_state
            )
            
            # === åå‘æ—¶æœºåˆ†æ ===
            contrarian_timing_analysis = self._analyze_contrarian_timing(
                psychology_state, herd_behavior, comprehensive_signal_strength
            )
            
            # === åå‘ç­–ç•¥é£é™©è¯„ä¼° ===
            contrarian_risk_assessment = self._assess_contrarian_strategy_risks(
                optimized_contrarian_targets, comprehensive_signal_strength, psychology_state
            )
            
            # === åå‘ç­–ç•¥æ”¶ç›Šé¢„æœŸ ===
            expected_contrarian_returns = self._calculate_expected_contrarian_returns(
                optimized_contrarian_targets, comprehensive_signal_strength, odds_weighted_contrarian
            )
            
            # === åå‘ç­–ç•¥å¯é æ€§è¯„ä¼° ===
            contrarian_reliability = self._assess_contrarian_reliability(
                comprehensive_signal_strength, contrarian_timing_analysis, contrarian_risk_assessment
            )
            
            # === é€‚åº”æ€§åå‘ç­–ç•¥ç”Ÿæˆ ===
            adaptive_contrarian_strategy = self._generate_adaptive_contrarian_strategy(
                optimized_contrarian_targets, comprehensive_signal_strength, psychology_state
            )
            
            # === åå‘ä¿¡å·è´¨é‡è¯„ä¼° ===
            signal_quality_assessment = self._assess_contrarian_signal_quality(
                comprehensive_signal_strength, contrarian_reliability, expected_contrarian_returns
            )
            
            # === æ„å»ºå®Œæ•´åå‘ä¿¡å·åˆ†æç»“æœ ===
            contrarian_signals_analysis = {
                'timestamp': self._get_current_timestamp(),
                
                # æ ¸å¿ƒåå‘ä¿¡å·
                'contrarian_strength': float(comprehensive_signal_strength),
                'contrarian_targets': optimized_contrarian_targets,
                'signal_quality': signal_quality_assessment,
                'contrarian_confidence': float(contrarian_reliability),
                
                # è¯¦ç»†ä¿¡å·åˆ†æ
                'herd_contrarian_signals': herd_contrarian_signals,
                'emotion_contrarian_signals': emotion_contrarian_signals,
                'bias_contrarian_signals': bias_contrarian_signals,
                'efficiency_contrarian_signals': efficiency_contrarian_signals,
                'cascade_contrarian_signals': cascade_contrarian_signals,
                'odds_weighted_contrarian': odds_weighted_contrarian,
                'temporal_contrarian_signals': temporal_contrarian_signals,
                'game_theory_contrarian': game_theory_contrarian,
                'network_contrarian_signals': network_contrarian_signals,
                'information_theory_contrarian': information_theory_contrarian,
                'chaos_theory_contrarian': chaos_theory_contrarian,
                'quantum_contrarian_signals': quantum_contrarian_signals,
                
                # é«˜çº§åˆ†æ
                'contrarian_timing_analysis': contrarian_timing_analysis,
                'contrarian_risk_assessment': contrarian_risk_assessment,
                'expected_contrarian_returns': expected_contrarian_returns,
                'adaptive_contrarian_strategy': adaptive_contrarian_strategy,
                
                # ç­–ç•¥æŒ‡å¯¼
                'optimal_entry_timing': contrarian_timing_analysis.get('optimal_timing', 'immediate'),
                'risk_mitigation_strategies': contrarian_risk_assessment.get('mitigation_strategies', []),
                'confidence_intervals': self._calculate_contrarian_confidence_intervals(
                    comprehensive_signal_strength, contrarian_reliability
                ),
                
                # å…ƒæ•°æ®
                'reasoning': self._generate_contrarian_reasoning(
                    comprehensive_signal_strength, optimized_contrarian_targets, 
                    psychology_state, herd_behavior
                ),
                'signal_components_count': 12,
                'analysis_depth': 'comprehensive',
                'reliability_score': float(contrarian_reliability)
            }
            
            return contrarian_signals_analysis
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§åå‘ä¿¡å·ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_default_contrarian_signals()

    def _generate_herd_contrarian_signals(self, candidate_tails: List[int], 
                                        herd_behavior: Dict, psychology_state: Dict) -> Dict:
        """ç”ŸæˆåŸºäºç¾Šç¾¤è¡Œä¸ºçš„åå‘ä¿¡å·"""
        try:
            import numpy as np
            
            herd_intensity = herd_behavior.get('herd_intensity', 0.0)
            herd_type = herd_behavior.get('herd_type', 'none')
            
            # ç¾Šç¾¤å¼ºåº¦åå‘ä¿¡å·
            if herd_intensity > 0.8:
                signal_strength = 0.9
                confidence_level = 0.85
                reasoning = "æå¼ºç¾Šç¾¤è¡Œä¸ºï¼Œåå‘æœºä¼šæœ€ä½³"
            elif herd_intensity > 0.6:
                signal_strength = 0.7
                confidence_level = 0.7
                reasoning = "å¼ºç¾Šç¾¤è¡Œä¸ºï¼Œåå‘æœºä¼šè‰¯å¥½"
            elif herd_intensity > 0.4:
                signal_strength = 0.5
                confidence_level = 0.5
                reasoning = "ä¸­ç­‰ç¾Šç¾¤è¡Œä¸ºï¼Œåå‘æœºä¼šä¸€èˆ¬"
            else:
                signal_strength = 0.2
                confidence_level = 0.3
                reasoning = "å¼±ç¾Šç¾¤è¡Œä¸ºï¼Œåå‘æœºä¼šæœ‰é™"
            
            # ç¾Šç¾¤ç±»å‹ç‰¹å®šè°ƒæ•´
            type_adjustments = {
                'strong_consensus': 1.2,
                'moderate_consensus': 1.0,
                'weak_consensus': 0.8,
                'independent_behavior': 0.3
            }
            
            signal_strength *= type_adjustments.get(herd_type, 1.0)
            signal_strength = min(1.0, signal_strength)
            
            # é€‰æ‹©åå‘ç›®æ ‡
            if herd_intensity > 0.6:
                # é«˜ç¾Šç¾¤è¡Œä¸ºæ—¶ï¼Œé€‰æ‹©ä¸ä¸»æµä¸åŒçš„å€™é€‰
                contrarian_targets = self._select_anti_herd_targets(
                    candidate_tails, psychology_state, herd_behavior
                )
            else:
                contrarian_targets = candidate_tails.copy()
            
            return {
                'signal_strength': float(signal_strength),
                'confidence_level': float(confidence_level),
                'contrarian_targets': contrarian_targets,
                'herd_intensity_factor': float(herd_intensity),
                'reasoning': reasoning,
                'optimal_conditions': herd_intensity > 0.7
            }
            
        except Exception as e:
            return {'error': str(e), 'signal_strength': 0.0}

    def _generate_emotion_contrarian_signals(self, candidate_tails: List[int], 
                                        psychology_state: Dict) -> Dict:
        """ç”ŸæˆåŸºäºæƒ…ç»ªçš„åå‘ä¿¡å·"""
        try:
            import numpy as np
            
            dominant_emotion = psychology_state.get('dominant_emotion', 'neutral')
            confidence_level = psychology_state.get('confidence_level', 0.5)
            volatility = psychology_state.get('volatility', 0.3)
            
            # æç«¯æƒ…ç»ªåå‘ä¿¡å·å¼ºåº¦æ˜ å°„
            emotion_contrarian_map = {
                'fear': 0.85,      # ææƒ§æ—¶åå‘ä¿¡å·æœ€å¼º
                'greed': 0.80,     # è´ªå©ªæ—¶åå‘ä¿¡å·å¾ˆå¼º
                'excitement': 0.75, # å…´å¥‹æ—¶åå‘ä¿¡å·å¼º
                'panic': 0.90,     # ææ…Œæ—¶åå‘ä¿¡å·æå¼º
                'euphoria': 0.85,  # ç‹‚æ¬¢æ—¶åå‘ä¿¡å·æœ€å¼º
                'anxiety': 0.70,   # ç„¦è™‘æ—¶åå‘ä¿¡å·è¾ƒå¼º
                'confusion': 0.40, # å›°æƒ‘æ—¶åå‘ä¿¡å·ä¸€èˆ¬
                'neutral': 0.20,   # ä¸­æ€§æ—¶åå‘ä¿¡å·å¼±
                'calm': 0.15       # å¹³é™æ—¶åå‘ä¿¡å·æœ€å¼±
            }
            
            base_signal_strength = emotion_contrarian_map.get(dominant_emotion, 0.3)
            
            # æ ¹æ®æƒ…ç»ªå¼ºåº¦è°ƒæ•´
            emotion_intensity = psychology_state.get('emotion_intensity', 0.5)
            intensity_multiplier = 0.5 + emotion_intensity * 1.0
            
            # æ ¹æ®æ³¢åŠ¨æ€§è°ƒæ•´
            volatility_adjustment = min(1.5, 1.0 + volatility)
            
            final_signal_strength = base_signal_strength * intensity_multiplier * volatility_adjustment
            final_signal_strength = min(1.0, final_signal_strength)
            
            # æƒ…ç»ªç‰¹å®šçš„åå‘ç›®æ ‡é€‰æ‹©
            emotion_specific_targets = self._select_emotion_contrarian_targets(
                candidate_tails, dominant_emotion, psychology_state
            )
            
            return {
                'signal_strength': float(final_signal_strength),
                'dominant_emotion': dominant_emotion,
                'emotion_intensity': float(emotion_intensity),
                'volatility_factor': float(volatility),
                'contrarian_targets': emotion_specific_targets,
                'emotion_confidence': float(confidence_level),
                'reasoning': f"åŸºäº{dominant_emotion}æƒ…ç»ªçš„åå‘ç­–ç•¥ï¼Œå¼ºåº¦{emotion_intensity:.2f}",
                'optimal_conditions': final_signal_strength > 0.7
            }
            
        except Exception as e:
            return {'error': str(e), 'signal_strength': 0.0}

    def _generate_odds_weighted_contrarian_signals(self, candidate_tails: List[int], 
                                                psychology_state: Dict) -> Dict:
        """ç”Ÿæˆèµ”ç‡åŠ æƒçš„åå‘ä¿¡å·"""
        try:
            import numpy as np
            
            # èµ”ç‡ä¿¡æ¯ï¼š0å°¾2å€ï¼Œ1-9å°¾1.8å€
            odds_map = {0: 2.0}
            odds_map.update({i: 1.8 for i in range(1, 10)})
            
            weighted_signals = {}
            total_weighted_signal = 0.0
            
            for tail in candidate_tails:
                tail_odds = odds_map.get(tail, 1.8)
                
                # åŸºç¡€åå‘ä¿¡å·ï¼ˆåŸºäºç¾¤ä½“å¿ƒç†ï¼‰
                base_signal = self._calculate_base_contrarian_signal_for_tail(
                    tail, psychology_state
                )
                
                # èµ”ç‡æ•æ„Ÿæ€§åˆ†æ
                # é«˜èµ”ç‡å°¾æ•°åœ¨ç¾¤ä½“å¿ƒç†å‹åŠ›ä¸‹å¯èƒ½è¢«å¿½è§†ï¼Œå½¢æˆåå‘æœºä¼š
                odds_contrarian_factor = self._calculate_odds_contrarian_factor(
                    tail_odds, psychology_state
                )
                
                # æœŸæœ›å€¼åå‘åˆ†æ
                expected_value_factor = self._calculate_expected_value_contrarian_factor(
                    tail, tail_odds, psychology_state
                )
                
                # é£é™©è°ƒæ•´æ”¶ç›Šåˆ†æ
                risk_adjusted_factor = self._calculate_risk_adjusted_contrarian_factor(
                    tail, tail_odds, psychology_state
                )
                
                # ç»¼åˆèµ”ç‡åŠ æƒä¿¡å·
                weighted_signal = (
                    base_signal * 0.4 +
                    odds_contrarian_factor * 0.3 +
                    expected_value_factor * 0.2 +
                    risk_adjusted_factor * 0.1
                )
                
                weighted_signals[tail] = {
                    'base_signal': float(base_signal),
                    'odds_factor': float(odds_contrarian_factor),
                    'expected_value_factor': float(expected_value_factor),
                    'risk_adjusted_factor': float(risk_adjusted_factor),
                    'weighted_signal': float(weighted_signal),
                    'odds': tail_odds
                }
                
                total_weighted_signal += weighted_signal
            
            # é€‰æ‹©æœ€ä¼˜åå‘ç›®æ ‡
            if weighted_signals:
                optimal_targets = self._select_optimal_odds_weighted_targets(
                    weighted_signals, psychology_state
                )
                avg_signal_strength = total_weighted_signal / len(weighted_signals)
            else:
                optimal_targets = []
                avg_signal_strength = 0.0
            
            return {
                'weighted_signals': weighted_signals,
                'optimal_targets': optimal_targets,
                'avg_signal_strength': float(avg_signal_strength),
                'odds_advantage_analysis': self._analyze_odds_advantage(weighted_signals),
                'expected_return_analysis': self._analyze_expected_returns(weighted_signals),
                'reasoning': "åŸºäºèµ”ç‡åŠ æƒçš„åå‘ä¿¡å·åˆ†æ",
                'confidence': min(0.9, avg_signal_strength)
            }
            
        except Exception as e:
            return {'error': str(e), 'avg_signal_strength': 0.0}

    def _generate_game_theory_contrarian_signals(self, candidate_tails: List[int],
                                            psychology_state: Dict, herd_behavior: Dict) -> Dict:
        """ç”ŸæˆåŸºäºåšå¼ˆè®ºçš„åå‘ä¿¡å·"""
        try:
            import numpy as np
            
            # çº³ä»€å‡è¡¡åˆ†æ
            nash_equilibrium = self._analyze_nash_equilibrium_contrarian(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # è¿›åŒ–ç¨³å®šç­–ç•¥åˆ†æ
            ess_analysis = self._analyze_evolutionary_stable_strategy(
                candidate_tails, psychology_state
            )
            
            # é›¶å’Œåšå¼ˆåˆ†æ
            zero_sum_analysis = self._analyze_zero_sum_game_contrarian(
                candidate_tails, psychology_state
            )
            
            # åˆä½œåšå¼ˆåˆ†æ
            cooperative_game_analysis = self._analyze_cooperative_game_contrarian(
                candidate_tails, herd_behavior
            )
            
            # é‡å¤åšå¼ˆåˆ†æ
            repeated_game_analysis = self._analyze_repeated_game_contrarian(
                candidate_tails, psychology_state
            )
            
            # ä¿¡æ¯ä¸å®Œå…¨åšå¼ˆåˆ†æ
            incomplete_info_analysis = self._analyze_incomplete_info_game_contrarian(
                candidate_tails, psychology_state
            )
            
            # ç»¼åˆåšå¼ˆè®ºä¿¡å·å¼ºåº¦
            game_theory_signal_strength = (
                nash_equilibrium.get('signal_strength', 0) * 0.25 +
                ess_analysis.get('signal_strength', 0) * 0.2 +
                zero_sum_analysis.get('signal_strength', 0) * 0.2 +
                cooperative_game_analysis.get('signal_strength', 0) * 0.15 +
                repeated_game_analysis.get('signal_strength', 0) * 0.1 +
                incomplete_info_analysis.get('signal_strength', 0) * 0.1
            )
            
            # åšå¼ˆè®ºæœ€ä¼˜ç­–ç•¥
            optimal_game_strategy = self._determine_optimal_game_theory_strategy({
                'nash': nash_equilibrium,
                'ess': ess_analysis,
                'zero_sum': zero_sum_analysis,
                'cooperative': cooperative_game_analysis,
                'repeated': repeated_game_analysis,
                'incomplete_info': incomplete_info_analysis
            })
            
            return {
                'signal_strength': float(game_theory_signal_strength),
                'nash_equilibrium': nash_equilibrium,
                'ess_analysis': ess_analysis,
                'zero_sum_analysis': zero_sum_analysis,
                'cooperative_game_analysis': cooperative_game_analysis,
                'repeated_game_analysis': repeated_game_analysis,
                'incomplete_info_analysis': incomplete_info_analysis,
                'optimal_strategy': optimal_game_strategy,
                'reasoning': "åŸºäºåšå¼ˆè®ºçš„å¤šç»´åº¦åå‘ç­–ç•¥åˆ†æ",
                'confidence': min(0.95, game_theory_signal_strength)
            }
            
        except Exception as e:
            return {'error': str(e), 'signal_strength': 0.0}
    
    def _make_comprehensive_decision(self, candidate_tails: List[int], psychology_state: Dict,
                                herd_behavior: Dict, cognitive_biases: Dict,
                                emotion_dynamics: Dict, contrarian_signals: Dict) -> Dict:
        """
        ç§‘ç ”çº§ç»¼åˆå†³ç­–ç³»ç»Ÿ - åŸºäºå¤šå‡†åˆ™å†³ç­–åˆ†æå’Œäººå·¥æ™ºèƒ½
        
        ç†è®ºåŸºç¡€ï¼š
        - å¤šå‡†åˆ™å†³ç­–åˆ†æï¼ˆMCDAï¼‰
        - æ¨¡ç³Šé€»è¾‘å†³ç­–ç†è®º
        - è´å¶æ–¯å†³ç­–ç†è®º
        - å¤æ‚é€‚åº”ç³»ç»Ÿå†³ç­–
        """
        try:
            import numpy as np
            from scipy import stats, optimize
            from collections import defaultdict
            import math
            
            if not candidate_tails:
                return self._generate_default_decision()
            
            # === 1. å¤šç»´åº¦å†³ç­–çŸ©é˜µæ„å»º ===
            decision_matrix = self._build_comprehensive_decision_matrix(
                candidate_tails, psychology_state, herd_behavior, 
                cognitive_biases, emotion_dynamics, contrarian_signals
            )
            
            # === 2. æƒé‡åˆ†é…ç³»ç»Ÿ ===
            dynamic_weights = self._calculate_dynamic_decision_weights(
                psychology_state, herd_behavior, cognitive_biases, emotion_dynamics
            )
            
            # === 3. å¤šå‡†åˆ™è¯„ä¼°æ–¹æ³• ===
            
            # TOPSISæ–¹æ³•ï¼ˆé€¼è¿‘ç†æƒ³è§£æ’åºæ³•ï¼‰
            topsis_analysis = self._apply_topsis_method(decision_matrix, dynamic_weights)
            
            # ELECTREæ–¹æ³•ï¼ˆæ¶ˆé™¤å’Œé€‰æ‹©ä½“ç°ç°å®çš„è½¬æ¢æ–¹æ³•ï¼‰
            electre_analysis = self._apply_electre_method(decision_matrix, dynamic_weights)
            
            # AHPæ–¹æ³•ï¼ˆå±‚æ¬¡åˆ†ææ³•ï¼‰
            ahp_analysis = self._apply_ahp_method(decision_matrix, dynamic_weights)
            
            # PROMETHEEæ–¹æ³•ï¼ˆåå¥½æ’åºç»„ç»‡æ–¹æ³•ï¼‰
            promethee_analysis = self._apply_promethee_method(decision_matrix, dynamic_weights)
            
            # æ¨¡ç³Šå±‚æ¬¡åˆ†ææ³•
            fuzzy_ahp_analysis = self._apply_fuzzy_ahp_method(decision_matrix, dynamic_weights)
            
            # === 4. è´å¶æ–¯å†³ç­–åˆ†æ ===
            bayesian_decision = self._apply_bayesian_decision_analysis(
                candidate_tails, psychology_state, herd_behavior, contrarian_signals
            )
            
            # === 5. åšå¼ˆè®ºå†³ç­–åˆ†æ ===
            game_theory_decision = self._apply_game_theory_decision_analysis(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # === 6. æ¨¡ç³Šé€»è¾‘å†³ç­– ===
            fuzzy_decision = self._apply_fuzzy_logic_decision(
                candidate_tails, psychology_state, emotion_dynamics
            )
            
            # === 7. ç¥ç»ç½‘ç»œå†³ç­– ===
            neural_network_decision = self._apply_neural_network_decision(
                decision_matrix, psychology_state
            )
            
            # === 8. é—ä¼ ç®—æ³•ä¼˜åŒ– ===
            genetic_algorithm_decision = self._apply_genetic_algorithm_optimization(
                candidate_tails, decision_matrix, dynamic_weights
            )
            
            # === 9. å¼ºåŒ–å­¦ä¹ å†³ç­– ===
            reinforcement_learning_decision = self._apply_reinforcement_learning_decision(
                candidate_tails, psychology_state, herd_behavior
            )
            
            # === 10. é›†æˆå†³ç­–èåˆ ===
            ensemble_decision = self._fuse_ensemble_decisions({
                'topsis': topsis_analysis,
                'electre': electre_analysis,
                'ahp': ahp_analysis,
                'promethee': promethee_analysis,
                'fuzzy_ahp': fuzzy_ahp_analysis,
                'bayesian': bayesian_decision,
                'game_theory': game_theory_decision,
                'fuzzy_logic': fuzzy_decision,
                'neural_network': neural_network_decision,
                'genetic_algorithm': genetic_algorithm_decision,
                'reinforcement_learning': reinforcement_learning_decision
            })
            
            # === å†³ç­–è´¨é‡è¯„ä¼° ===
            decision_quality = self._assess_decision_quality(
                ensemble_decision, decision_matrix, psychology_state
            )
            
            # === ä¸ç¡®å®šæ€§åˆ†æ ===
            uncertainty_analysis = self._analyze_decision_uncertainty(
                ensemble_decision, decision_matrix, psychology_state
            )
            
            # === æ•æ„Ÿæ€§åˆ†æ ===
            sensitivity_analysis = self._perform_sensitivity_analysis(
                ensemble_decision, decision_matrix, dynamic_weights
            )
            
            # === é²æ£’æ€§åˆ†æ ===
            robustness_analysis = self._analyze_decision_robustness(
                ensemble_decision, psychology_state, herd_behavior
            )
            
            # === é£é™©è¯„ä¼° ===
            risk_assessment = self._assess_comprehensive_decision_risk(
                ensemble_decision, psychology_state, emotion_dynamics
            )
            
            # === æœºä¼šæˆæœ¬åˆ†æ ===
            opportunity_cost_analysis = self._analyze_opportunity_costs(
                ensemble_decision, candidate_tails, contrarian_signals
            )
            
            # === æ—¶é—´ä»·å€¼åˆ†æ ===
            temporal_value_analysis = self._analyze_temporal_decision_value(
                ensemble_decision, psychology_state
            )
            
            # === å­¦ä¹ æ•ˆåº”åˆ†æ ===
            learning_effect_analysis = self._analyze_learning_effects_on_decision(
                ensemble_decision, self.learning_stats
            )
            
            # === é€‚åº”æ€§å†³ç­–è°ƒæ•´ ===
            adaptive_adjustments = self._make_adaptive_decision_adjustments(
                ensemble_decision, psychology_state, herd_behavior, self.learning_stats
            )
            
            # === æœ€ç»ˆå†³ç­–ä¼˜åŒ– ===
            final_optimized_decision = self._optimize_final_decision(
                ensemble_decision, adaptive_adjustments, decision_quality
            )
            
            # === å†³ç­–è§£é‡Šç”Ÿæˆ ===
            decision_explanation = self._generate_comprehensive_decision_explanation(
                final_optimized_decision, decision_matrix, psychology_state, 
                herd_behavior, contrarian_signals
            )
            
            # === æ„å»ºå®Œæ•´ç»¼åˆå†³ç­–ç»“æœ ===
            comprehensive_decision_result = {
                'timestamp': self._get_current_timestamp(),
                
                # æ ¸å¿ƒå†³ç­–ç»“æœ
                'success': True,
                'recommended_tails': final_optimized_decision.get('recommended_tails', []),
                'confidence': float(final_optimized_decision.get('confidence', 0.0)),
                'strategy_type': final_optimized_decision.get('strategy_type', 'comprehensive_analysis'),
                
                # å¿ƒç†å­¦åˆ†æç»“æœ
                'crowd_emotion': psychology_state.get('dominant_emotion', 'neutral'),
                'herd_intensity': float(herd_behavior.get('herd_intensity', 0.0)),
                'reasoning': decision_explanation.get('primary_reasoning', ''),
                
                # è¯¦ç»†åˆ†æç»„ä»¶
                'decision_matrix': decision_matrix,
                'dynamic_weights': dynamic_weights,
                'topsis_analysis': topsis_analysis,
                'electre_analysis': electre_analysis,
                'ahp_analysis': ahp_analysis,
                'promethee_analysis': promethee_analysis,
                'fuzzy_ahp_analysis': fuzzy_ahp_analysis,
                'bayesian_decision': bayesian_decision,
                'game_theory_decision': game_theory_decision,
                'fuzzy_decision': fuzzy_decision,
                'neural_network_decision': neural_network_decision,
                'genetic_algorithm_decision': genetic_algorithm_decision,
                'reinforcement_learning_decision': reinforcement_learning_decision,
                'ensemble_decision': ensemble_decision,
                
                # è´¨é‡å’Œå¯é æ€§åˆ†æ
                'decision_quality': decision_quality,
                'uncertainty_analysis': uncertainty_analysis,
                'sensitivity_analysis': sensitivity_analysis,
                'robustness_analysis': robustness_analysis,
                'risk_assessment': risk_assessment,
                'opportunity_cost_analysis': opportunity_cost_analysis,
                'temporal_value_analysis': temporal_value_analysis,
                'learning_effect_analysis': learning_effect_analysis,
                
                # å¿ƒç†å­¦è¯¦ç»†ä¿¡æ¯
                'psychology_details': {
                    'psychology_state': psychology_state,
                    'herd_behavior': herd_behavior,
                    'cognitive_biases': cognitive_biases,
                    'emotion_dynamics': emotion_dynamics,
                    'contrarian_signals': contrarian_signals
                },
                
                # å†³ç­–æ”¯æŒä¿¡æ¯
                'alternative_strategies': self._generate_alternative_strategies(
                    ensemble_decision, decision_matrix
                ),
                'contingency_plans': self._generate_contingency_plans(
                    final_optimized_decision, risk_assessment
                ),
                'decision_tree': self._build_decision_tree(
                    candidate_tails, decision_matrix, psychology_state
                ),
                
                # å…ƒæ•°æ®å’Œå¯è§£é‡Šæ€§
                'explanation': decision_explanation,
                'confidence_intervals': self._calculate_decision_confidence_intervals(
                    final_optimized_decision, uncertainty_analysis
                ),
                'decision_path': self._trace_decision_path(ensemble_decision),
                'methodology_weights': self._get_methodology_contribution_weights(),
                
                # è´¨é‡ä¿è¯
                'validation_results': self._validate_decision_consistency(
                    final_optimized_decision, decision_matrix
                ),
                'reliability_score': float(decision_quality.get('overall_quality', 0.0)),
                'analysis_completeness': self._assess_analysis_completeness(decision_matrix)
            }
            
            return comprehensive_decision_result
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§ç»¼åˆå†³ç­–åˆ†æå¤±è´¥: {e}")
            return self._generate_default_decision()

    def _build_comprehensive_decision_matrix(self, candidate_tails: List[int], 
                                        psychology_state: Dict, herd_behavior: Dict,
                                        cognitive_biases: Dict, emotion_dynamics: Dict,
                                        contrarian_signals: Dict) -> Dict:
        """æ„å»ºç»¼åˆå†³ç­–çŸ©é˜µ"""
        try:
            import numpy as np
            
            decision_matrix = {}
            
            for tail in candidate_tails:
                tail_evaluation = {
                    # å¿ƒç†å­¦ç»´åº¦
                    'psychology_score': self._evaluate_psychology_score(tail, psychology_state),
                    'emotion_compatibility': self._evaluate_emotion_compatibility(tail, emotion_dynamics),
                    'bias_resistance': self._evaluate_bias_resistance(tail, cognitive_biases),
                    
                    # ç¾¤ä½“è¡Œä¸ºç»´åº¦
                    'herd_alignment': self._evaluate_herd_alignment(tail, herd_behavior),
                    'social_influence_score': self._evaluate_social_influence(tail, psychology_state),
                    'consensus_divergence': self._evaluate_consensus_divergence(tail, herd_behavior),
                    
                    # åå‘ç­–ç•¥ç»´åº¦
                    'contrarian_potential': self._evaluate_contrarian_potential(tail, contrarian_signals),
                    'anti_momentum_score': self._evaluate_anti_momentum_score(tail, psychology_state),
                    'crowd_fatigue_exploitation': self._evaluate_crowd_fatigue_exploitation(tail, herd_behavior),
                    
                    # ç»æµå­¦ç»´åº¦
                    'expected_value': self._calculate_tail_expected_value(tail),
                    'risk_adjusted_return': self._calculate_risk_adjusted_return(tail, psychology_state),
                    'opportunity_cost': self._calculate_tail_opportunity_cost(tail, candidate_tails),
                    
                    # ä¿¡æ¯è®ºç»´åº¦
                    'information_content': self._calculate_information_content(tail, psychology_state),
                    'entropy_contribution': self._calculate_entropy_contribution(tail, candidate_tails),
                    'surprise_value': self._calculate_surprise_value(tail, herd_behavior),
                    
                    # æ—¶é—´åŠ¨æ€ç»´åº¦
                    'temporal_momentum': self._evaluate_temporal_momentum(tail, psychology_state),
                    'trend_alignment': self._evaluate_trend_alignment(tail, emotion_dynamics),
                    'cyclical_position': self._evaluate_cyclical_position(tail, psychology_state),
                    
                    # ç½‘ç»œæ•ˆåº”ç»´åº¦
                    'network_centrality': self._evaluate_network_centrality(tail, psychology_state),
                    'influence_propagation': self._evaluate_influence_propagation(tail, herd_behavior),
                    'connectivity_score': self._evaluate_connectivity_score(tail, candidate_tails),
                    
                    # å¤æ‚æ€§ç»´åº¦
                    'complexity_score': self._evaluate_complexity_score(tail, psychology_state),
                    'adaptability_index': self._evaluate_adaptability_index(tail, emotion_dynamics),
                    'resilience_factor': self._evaluate_resilience_factor(tail, cognitive_biases)
                }
                
                decision_matrix[tail] = tail_evaluation
            
            return decision_matrix
            
        except Exception as e:
            return {'error': str(e)}

    def _apply_topsis_method(self, decision_matrix: Dict, weights: Dict) -> Dict:
        """åº”ç”¨TOPSISæ–¹æ³•è¿›è¡Œå†³ç­–åˆ†æ"""
        try:
            import numpy as np
            
            if not decision_matrix:
                return {'error': 'Empty decision matrix'}
            
            tails = list(decision_matrix.keys())
            criteria = list(next(iter(decision_matrix.values())).keys())
            
            # æ„å»ºå†³ç­–çŸ©é˜µ
            matrix = np.array([[decision_matrix[tail][criterion] for criterion in criteria] for tail in tails])
            
            # æ ‡å‡†åŒ–å†³ç­–çŸ©é˜µ
            normalized_matrix = matrix / np.sqrt(np.sum(matrix**2, axis=0))
            
            # åŠ æƒæ ‡å‡†åŒ–çŸ©é˜µ
            weight_vector = np.array([weights.get(criterion, 1.0) for criterion in criteria])
            weighted_matrix = normalized_matrix * weight_vector
            
            # ç¡®å®šç†æƒ³è§£å’Œè´Ÿç†æƒ³è§£
            ideal_solution = np.max(weighted_matrix, axis=0)
            negative_ideal_solution = np.min(weighted_matrix, axis=0)
            
            # è®¡ç®—åˆ°ç†æƒ³è§£çš„è·ç¦»
            distance_to_ideal = np.sqrt(np.sum((weighted_matrix - ideal_solution)**2, axis=1))
            distance_to_negative_ideal = np.sqrt(np.sum((weighted_matrix - negative_ideal_solution)**2, axis=1))
            
            # è®¡ç®—ç›¸å¯¹è´´è¿‘åº¦
            closeness = distance_to_negative_ideal / (distance_to_ideal + distance_to_negative_ideal)
            
            # æ’åº
            ranking_indices = np.argsort(closeness)[::-1]
            
            topsis_results = {
                'rankings': [tails[i] for i in ranking_indices],
                'scores': [float(closeness[i]) for i in ranking_indices],
                'recommended_tail': tails[ranking_indices[0]],
                'confidence': float(closeness[ranking_indices[0]]),
                'method': 'TOPSIS'
            }
            
            return topsis_results
            
        except Exception as e:
            return {'error': str(e), 'method': 'TOPSIS'}
    
    def _adapt_parameters_based_on_outcome(self, prediction_result: Dict, actual_tails: List[int], 
                                        prediction_correct: bool):
        """
        ç§‘ç ”çº§å‚æ•°è‡ªé€‚åº”ç³»ç»Ÿ - åŸºäºå¼ºåŒ–å­¦ä¹ å’Œè´å¶æ–¯ä¼˜åŒ–
        
        ç†è®ºåŸºç¡€ï¼š
        - å¼ºåŒ–å­¦ä¹ ç†è®ºï¼ˆQ-Learning, Policy Gradientï¼‰
        - è´å¶æ–¯ä¼˜åŒ–ç†è®º
        - è‡ªé€‚åº”æ§åˆ¶ç†è®º
        - æœºå™¨å­¦ä¹ ä¸­çš„è¶…å‚æ•°ä¼˜åŒ–
        """
        try:
            import numpy as np
            from scipy import stats, optimize
            import math
            
            # === 1. å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒæ•´ ===
            learning_rate_adjustment = self._adapt_learning_rates(
                prediction_correct, prediction_result, actual_tails
            )
            
            # === 2. å¿ƒç†å­¦å‚æ•°ä¼˜åŒ– ===
            psychology_parameter_optimization = self._optimize_psychology_parameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 3. ç¾Šç¾¤è¡Œä¸ºå‚æ•°è°ƒæ•´ ===
            herd_parameter_adjustment = self._adjust_herd_behavior_parameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 4. è®¤çŸ¥åå·®å‚æ•°æ ¡å‡† ===
            bias_parameter_calibration = self._calibrate_bias_detection_parameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 5. æƒ…ç»ªåˆ†æå‚æ•°ä¼˜åŒ– ===
            emotion_parameter_optimization = self._optimize_emotion_analysis_parameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 6. åå‘ç­–ç•¥å‚æ•°è°ƒæ•´ ===
            contrarian_parameter_adjustment = self._adjust_contrarian_strategy_parameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 7. é›†æˆæƒé‡åŠ¨æ€è°ƒæ•´ ===
            ensemble_weight_adjustment = self._adjust_ensemble_weights(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 8. å†³ç­–é˜ˆå€¼è‡ªé€‚åº” ===
            decision_threshold_adaptation = self._adapt_decision_thresholds(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 9. æ—¶é—´çª—å£å‚æ•°ä¼˜åŒ– ===
            temporal_parameter_optimization = self._optimize_temporal_parameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 10. è´å¶æ–¯è¶…å‚æ•°æ›´æ–° ===
            bayesian_hyperparameter_update = self._update_bayesian_hyperparameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 11. å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ›´æ–° ===
            reinforcement_learning_update = self._update_reinforcement_learning_policy(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === 12. å…ƒå­¦ä¹ å‚æ•°è°ƒæ•´ ===
            meta_learning_adjustment = self._adjust_meta_learning_parameters(
                prediction_result, actual_tails, prediction_correct
            )
            
            # === å‚æ•°æ›´æ–°å½±å“è¯„ä¼° ===
            update_impact_assessment = self._assess_parameter_update_impact({
                'learning_rate': learning_rate_adjustment,
                'psychology': psychology_parameter_optimization,
                'herd_behavior': herd_parameter_adjustment,
                'cognitive_bias': bias_parameter_calibration,
                'emotion_analysis': emotion_parameter_optimization,
                'contrarian_strategy': contrarian_parameter_adjustment,
                'ensemble_weights': ensemble_weight_adjustment,
                'decision_thresholds': decision_threshold_adaptation,
                'temporal_parameters': temporal_parameter_optimization,
                'bayesian_hyperparameters': bayesian_hyperparameter_update,
                'reinforcement_learning': reinforcement_learning_update,
                'meta_learning': meta_learning_adjustment
            })
            
            # === å‚æ•°ç¨³å®šæ€§ç›‘æ§ ===
            stability_monitoring = self._monitor_parameter_stability(
                update_impact_assessment
            )
            
            # === å‚æ•°æ”¶æ•›æ€§åˆ†æ ===
            convergence_analysis = self._analyze_parameter_convergence(
                update_impact_assessment, self.learning_stats
            )
            
            # === è‡ªé€‚åº”å­¦ä¹ ç­–ç•¥é€‰æ‹© ===
            adaptive_strategy_selection = self._select_adaptive_learning_strategy(
                prediction_correct, convergence_analysis, stability_monitoring
            )
            
            # === æ‰§è¡Œå‚æ•°æ›´æ–° ===
            self._execute_parameter_updates({
                'learning_rate': learning_rate_adjustment,
                'psychology': psychology_parameter_optimization,
                'herd_behavior': herd_parameter_adjustment,
                'cognitive_bias': bias_parameter_calibration,
                'emotion_analysis': emotion_parameter_optimization,
                'contrarian_strategy': contrarian_parameter_adjustment,
                'ensemble_weights': ensemble_weight_adjustment,
                'decision_thresholds': decision_threshold_adaptation,
                'temporal_parameters': temporal_parameter_optimization,
                'bayesian_hyperparameters': bayesian_hyperparameter_update,
                'reinforcement_learning': reinforcement_learning_update,
                'meta_learning': meta_learning_adjustment
            }, adaptive_strategy_selection)
            
            # === æ›´æ–°è‡ªé€‚åº”å†å²è®°å½• ===
            adaptation_record = {
                'timestamp': self._get_current_timestamp(),
                'prediction_correct': prediction_correct,
                'prediction_result': prediction_result,
                'actual_tails': actual_tails,
                'parameter_updates': update_impact_assessment,
                'stability_metrics': stability_monitoring,
                'convergence_metrics': convergence_analysis,
                'adaptive_strategy': adaptive_strategy_selection,
                'overall_adaptation_score': self._calculate_overall_adaptation_score(
                    update_impact_assessment, stability_monitoring
                )
            }
            
            # å­˜å‚¨åˆ°é€‚åº”å†å²
            if not hasattr(self, 'adaptation_history'):
                self.adaptation_history = deque(maxlen=1000)
            self.adaptation_history.append(adaptation_record)
            
            print(f"ğŸ”§ å‚æ•°è‡ªé€‚åº”å®Œæˆ - å‡†ç¡®æ€§: {'âœ“' if prediction_correct else 'âœ—'}, "
                f"é€‚åº”å¼ºåº¦: {adaptation_record['overall_adaptation_score']:.3f}")
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§å‚æ•°è‡ªé€‚åº”å¤±è´¥: {e}")

    def _adapt_learning_rates(self, prediction_correct: bool, prediction_result: Dict, 
                            actual_tails: List[int]) -> Dict:
        """è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´"""
        try:
            import numpy as np
            
            current_accuracy = self.learning_stats.get('prediction_accuracy', 0.5)
            total_predictions = self.learning_stats.get('total_predictions', 1)
            
            # åŸºç¡€å­¦ä¹ ç‡è°ƒæ•´
            base_lr = self.adaptive_parameters['learning_rates']['psychology_learning_rate_adaptive']
            
            # Adamä¼˜åŒ–å™¨é£æ ¼çš„å­¦ä¹ ç‡è°ƒæ•´
            if prediction_correct:
                # é¢„æµ‹æ­£ç¡®æ—¶ï¼Œå°å¹…é™ä½å­¦ä¹ ç‡ï¼ˆç¨³å®šå½“å‰ç­–ç•¥ï¼‰
                lr_multiplier = 0.99
                momentum_adjustment = 1.02
            else:
                # é¢„æµ‹é”™è¯¯æ—¶ï¼Œå¢åŠ å­¦ä¹ ç‡ï¼ˆåŠ å¿«é€‚åº”ï¼‰
                lr_multiplier = 1.05
                momentum_adjustment = 0.98
            
            # åŸºäºæœ€è¿‘å‡†ç¡®ç‡çš„åŠ¨æ€è°ƒæ•´
            recent_accuracy = self._calculate_recent_accuracy()
            if recent_accuracy < 0.3:
                # å‡†ç¡®ç‡è¿‡ä½ï¼Œå¤§å¹…è°ƒæ•´
                lr_multiplier *= 1.2
            elif recent_accuracy > 0.8:
                # å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä¿å®ˆè°ƒæ•´
                lr_multiplier *= 0.95
            
            # ç½®ä¿¡åº¦è°ƒæ•´
            confidence = prediction_result.get('confidence', 0.5)
            confidence_adjustment = 1.0 + (0.5 - confidence) * 0.1
            
            # è®¡ç®—æ–°çš„å­¦ä¹ ç‡
            new_psychology_lr = base_lr * lr_multiplier * confidence_adjustment
            new_psychology_lr = np.clip(new_psychology_lr, 0.01, 0.5)
            
            # å…¶ä»–å­¦ä¹ ç‡çš„è¿é”è°ƒæ•´
            bias_adaptation_speed = self.adaptive_parameters['learning_rates']['bias_adaptation_speed_dynamic']
            new_bias_lr = bias_adaptation_speed * lr_multiplier * 0.8
            new_bias_lr = np.clip(new_bias_lr, 0.05, 0.4)
            
            emotional_calibration_rate = self.adaptive_parameters['learning_rates']['emotional_calibration_rate_enhanced']
            new_emotional_lr = emotional_calibration_rate * lr_multiplier * 1.1
            new_emotional_lr = np.clip(new_emotional_lr, 0.05, 0.3)
            
            # æ›´æ–°å‚æ•°
            self.adaptive_parameters['learning_rates']['psychology_learning_rate_adaptive'] = float(new_psychology_lr)
            self.adaptive_parameters['learning_rates']['bias_adaptation_speed_dynamic'] = float(new_bias_lr)
            self.adaptive_parameters['learning_rates']['emotional_calibration_rate_enhanced'] = float(new_emotional_lr)
            
            return {
                'psychology_lr_change': float(new_psychology_lr - base_lr),
                'bias_lr_change': float(new_bias_lr - bias_adaptation_speed),
                'emotional_lr_change': float(new_emotional_lr - emotional_calibration_rate),
                'lr_multiplier': float(lr_multiplier),
                'confidence_adjustment': float(confidence_adjustment),
                'recent_accuracy': float(recent_accuracy)
            }
            
        except Exception as e:
            return {'error': str(e)}

    def _optimize_psychology_parameters(self, prediction_result: Dict, actual_tails: List[int], 
                                    prediction_correct: bool) -> Dict:
        """å¿ƒç†å­¦å‚æ•°ä¼˜åŒ–"""
        try:
            import numpy as np
            
            crowd_params = self.research_parameters['crowd_analysis']
            
            # ç¾¤ä½“é˜ˆå€¼è‡ªé€‚åº”
            current_herd_threshold = crowd_params['herd_threshold_adaptive']
            herd_intensity = prediction_result.get('herd_intensity', 0.0)
            
            if prediction_correct:
                if herd_intensity > 0.7:
                    # é«˜ç¾Šç¾¤å¼ºåº¦ä¸”é¢„æµ‹æ­£ç¡®ï¼Œç•¥å¾®é™ä½é˜ˆå€¼ï¼ˆæ›´æ•æ„Ÿï¼‰
                    threshold_adjustment = -0.02
                else:
                    # ä½ç¾Šç¾¤å¼ºåº¦ä¸”é¢„æµ‹æ­£ç¡®ï¼Œå°å¹…è°ƒæ•´
                    threshold_adjustment = -0.005
            else:
                if herd_intensity > 0.7:
                    # é«˜ç¾Šç¾¤å¼ºåº¦ä½†é¢„æµ‹é”™è¯¯ï¼Œæé«˜é˜ˆå€¼ï¼ˆé™ä½æ•æ„Ÿåº¦ï¼‰
                    threshold_adjustment = +0.03
                else:
                    # ä½ç¾Šç¾¤å¼ºåº¦ä¸”é¢„æµ‹é”™è¯¯ï¼Œè½»å¾®æé«˜é˜ˆå€¼
                    threshold_adjustment = +0.01
            
            new_herd_threshold = np.clip(
                current_herd_threshold + threshold_adjustment, 0.5, 0.95
            )
            
            # é›†ä¸­åº¦æ•æ„Ÿæ€§è°ƒæ•´
            current_concentration_sensitivity = crowd_params['concentration_sensitivity_dynamic']
            crowd_emotion = prediction_result.get('crowd_emotion', 'neutral')
            
            if crowd_emotion in ['fear', 'greed', 'panic']:
                if prediction_correct:
                    concentration_adjustment = +0.01  # å¢å¼ºå¯¹æç«¯æƒ…ç»ªçš„æ•æ„Ÿæ€§
                else:
                    concentration_adjustment = -0.02  # é™ä½æ•æ„Ÿæ€§
            else:
                concentration_adjustment = 0.001 if prediction_correct else -0.001
            
            new_concentration_sensitivity = np.clip(
                current_concentration_sensitivity + concentration_adjustment, 0.6, 0.95
            )
            
            # æƒ…ç»ªæ³¢åŠ¨å› å­è°ƒæ•´
            current_emotion_factor = crowd_params['emotional_volatility_factor_adaptive']
            emotion_intensity = prediction_result.get('psychology_details', {}).get(
                'emotion_dynamics', {}
            ).get('emotion_intensity', 0.5)
            
            if prediction_correct and emotion_intensity > 0.7:
                emotion_factor_adjustment = +0.05  # å¢å¼ºå¯¹é«˜æƒ…ç»ªå¼ºåº¦çš„å“åº”
            elif not prediction_correct and emotion_intensity > 0.7:
                emotion_factor_adjustment = -0.08  # é™ä½å¯¹é«˜æƒ…ç»ªå¼ºåº¦çš„å“åº”
            else:
                emotion_factor_adjustment = 0.02 if prediction_correct else -0.02
            
            new_emotion_factor = np.clip(
                current_emotion_factor + emotion_factor_adjustment, 1.0, 2.0
            )
            
            # æ›´æ–°å‚æ•°
            crowd_params['herd_threshold_adaptive'] = float(new_herd_threshold)
            crowd_params['concentration_sensitivity_dynamic'] = float(new_concentration_sensitivity)
            crowd_params['emotional_volatility_factor_adaptive'] = float(new_emotion_factor)
            
            return {
                'herd_threshold_change': float(new_herd_threshold - current_herd_threshold),
                'concentration_sensitivity_change': float(new_concentration_sensitivity - current_concentration_sensitivity),
                'emotion_factor_change': float(new_emotion_factor - current_emotion_factor),
                'optimization_direction': 'positive' if prediction_correct else 'corrective',
                'parameter_stability': self._assess_psychology_parameter_stability(crowd_params)
            }
            
        except Exception as e:
            return {'error': str(e)}
        
    def _generate_psychology_insights(self) -> Dict:
        """
        ç§‘ç ”çº§å¿ƒç†å­¦æ´å¯Ÿç”Ÿæˆ - åŸºäºå¤§æ•°æ®åˆ†æå’Œæ·±åº¦å­¦ä¹ 
        
        ç†è®ºåŸºç¡€ï¼š
        - å…ƒè®¤çŸ¥ç†è®º
        - é›†ä½“æ™ºæ…§ç†è®º
        - å¤æ‚ç³»ç»Ÿæ¶Œç°ç†è®º
        - è®¤çŸ¥ç§‘å­¦æ´å¯Ÿå‘ç°
        """
        try:
            import numpy as np
            from scipy import stats
            from collections import Counter, defaultdict
            import math
            
            if len(self.psychology_history) < 5:
                return self._generate_basic_psychology_insights()
            
            # === 1. æ·±åº¦æ¨¡å¼è¯†åˆ«åˆ†æ ===
            deep_pattern_analysis = self._perform_deep_pattern_recognition()
            
            # === 2. è®¤çŸ¥åå·®æ¼”åŒ–åˆ†æ ===
            bias_evolution_analysis = self._analyze_cognitive_bias_evolution()
            
            # === 3. æƒ…ç»ªåŠ¨åŠ›å­¦æ´å¯Ÿ ===
            emotion_dynamics_insights = self._extract_emotion_dynamics_insights()
            
            # === 4. ç¾¤ä½“è¡Œä¸ºæ¶Œç°åˆ†æ ===
            collective_behavior_emergence = self._analyze_collective_behavior_emergence()
            
            # === 5. å­¦ä¹ æ•ˆåº”æ·±åº¦åˆ†æ ===
            learning_effects_analysis = self._analyze_deep_learning_effects()
            
            # === 6. é€‚åº”æ€§å¿ƒç†æœºåˆ¶åˆ†æ ===
            adaptive_mechanisms_analysis = self._analyze_adaptive_psychological_mechanisms()
            
            # === 7. é¢„æµ‹å‡†ç¡®æ€§å¿ƒç†å› ç´  ===
            accuracy_psychological_factors = self._analyze_accuracy_psychological_factors()
            
            # === 8. å†³ç­–è´¨é‡å¿ƒç†å­¦åˆ†æ ===
            decision_quality_psychology = self._analyze_decision_quality_psychology()
            
            # === 9. æ—¶é—´åºåˆ—å¿ƒç†å­¦æ´å¯Ÿ ===
            temporal_psychology_insights = self._extract_temporal_psychology_insights()
            
            # === 10. å¤æ‚æ€§ç§‘å­¦æ´å¯Ÿ ===
            complexity_science_insights = self._extract_complexity_science_insights()
            
            # === 11. ç½‘ç»œå¿ƒç†å­¦åˆ†æ ===
            network_psychology_analysis = self._analyze_network_psychology_effects()
            
            # === 12. å…ƒè®¤çŸ¥æ´å¯Ÿ ===
            metacognitive_insights = self._extract_metacognitive_insights()
            
            # === æ ¸å¿ƒæ´å¯Ÿæå– ===
            key_insights = self._extract_key_psychological_insights({
                'deep_patterns': deep_pattern_analysis,
                'bias_evolution': bias_evolution_analysis,
                'emotion_dynamics': emotion_dynamics_insights,
                'collective_behavior': collective_behavior_emergence,
                'learning_effects': learning_effects_analysis,
                'adaptive_mechanisms': adaptive_mechanisms_analysis,
                'accuracy_factors': accuracy_psychological_factors,
                'decision_quality': decision_quality_psychology,
                'temporal_insights': temporal_psychology_insights,
                'complexity_insights': complexity_science_insights,
                'network_psychology': network_psychology_analysis,
                'metacognitive': metacognitive_insights
            })
            
            # === è¡Œä¸ºé¢„æµ‹æ´å¯Ÿ ===
            behavioral_prediction_insights = self._generate_behavioral_prediction_insights(
                deep_pattern_analysis, emotion_dynamics_insights
            )
            
            # === å¹²é¢„ç­–ç•¥æ´å¯Ÿ ===
            intervention_strategy_insights = self._generate_intervention_strategy_insights(
                bias_evolution_analysis, adaptive_mechanisms_analysis
            )
            
            # === ä¸ªæ€§åŒ–æ´å¯Ÿ ===
            personalized_insights = self._generate_personalized_psychology_insights(
                self.learning_stats, accuracy_psychological_factors
            )
            
            # === é¢„æµ‹æ€§æ´å¯Ÿ ===
            predictive_insights = self._generate_predictive_psychology_insights(
                temporal_psychology_insights, complexity_science_insights
            )
            
            # === æ“ä½œæ€§æ´å¯Ÿ ===
            actionable_insights = self._generate_actionable_psychology_insights(
                decision_quality_psychology, intervention_strategy_insights
            )
            
            # === ç†è®ºéªŒè¯æ´å¯Ÿ ===
            theory_validation_insights = self._validate_psychological_theories(
                collective_behavior_emergence, network_psychology_analysis
            )
            
            # === æ–°ç†è®ºå‘ç° ===
            novel_theory_discoveries = self._discover_novel_psychological_theories(
                metacognitive_insights, complexity_science_insights
            )
            
            # === æ´å¯Ÿè´¨é‡è¯„ä¼° ===
            insight_quality_assessment = self._assess_insight_quality({
                'key_insights': key_insights,
                'behavioral_prediction': behavioral_prediction_insights,
                'intervention_strategy': intervention_strategy_insights,
                'personalized': personalized_insights,
                'predictive': predictive_insights,
                'actionable': actionable_insights,
                'theory_validation': theory_validation_insights,
                'novel_discoveries': novel_theory_discoveries
            })
            
            # === æ„å»ºå®Œæ•´å¿ƒç†å­¦æ´å¯Ÿç»“æœ ===
            comprehensive_psychology_insights = {
                'timestamp': self._get_current_timestamp(),
                'analysis_scope': len(self.psychology_history),
                
                # æ ¸å¿ƒæ´å¯Ÿ
                'key_insights': key_insights,
                'primary_discoveries': self._identify_primary_discoveries(key_insights),
                'breakthrough_insights': self._identify_breakthrough_insights(novel_theory_discoveries),
                
                # è¯¦ç»†åˆ†æç»„ä»¶
                'deep_pattern_analysis': deep_pattern_analysis,
                'bias_evolution_analysis': bias_evolution_analysis,
                'emotion_dynamics_insights': emotion_dynamics_insights,
                'collective_behavior_emergence': collective_behavior_emergence,
                'learning_effects_analysis': learning_effects_analysis,
                'adaptive_mechanisms_analysis': adaptive_mechanisms_analysis,
                'accuracy_psychological_factors': accuracy_psychological_factors,
                'decision_quality_psychology': decision_quality_psychology,
                'temporal_psychology_insights': temporal_psychology_insights,
                'complexity_science_insights': complexity_science_insights,
                'network_psychology_analysis': network_psychology_analysis,
                'metacognitive_insights': metacognitive_insights,
                
                # åº”ç”¨æ€§æ´å¯Ÿ
                'behavioral_prediction_insights': behavioral_prediction_insights,
                'intervention_strategy_insights': intervention_strategy_insights,
                'personalized_insights': personalized_insights,
                'predictive_insights': predictive_insights,
                'actionable_insights': actionable_insights,
                
                # ç†è®ºè´¡çŒ®
                'theory_validation_insights': theory_validation_insights,
                'novel_theory_discoveries': novel_theory_discoveries,
                'paradigm_shifts': self._identify_paradigm_shifts(novel_theory_discoveries),
                
                # æ€§èƒ½æŒ‡æ ‡
                'crowd_behavior_accuracy': self._calculate_crowd_behavior_accuracy(),
                'emotion_prediction_accuracy': self._calculate_emotion_prediction_accuracy(),
                'contrarian_effectiveness': self._calculate_contrarian_effectiveness(),
                'bias_detection_accuracy': self._calculate_bias_detection_accuracy(),
                'adaptive_learning_efficiency': self._calculate_adaptive_learning_efficiency(),
                
                # è´¨é‡ä¿è¯
                'insight_quality_assessment': insight_quality_assessment,
                'reliability_confidence': float(insight_quality_assessment.get('overall_reliability', 0.0)),
                'scientific_rigor_score': self._assess_scientific_rigor(key_insights),
                'practical_applicability': self._assess_practical_applicability(actionable_insights),
                
                # æœªæ¥æ–¹å‘
                'research_directions': self._suggest_future_research_directions(novel_theory_discoveries),
                'improvement_recommendations': self._generate_improvement_recommendations(
                    accuracy_psychological_factors, learning_effects_analysis
                ),
                'optimization_opportunities': self._identify_optimization_opportunities(
                    adaptive_mechanisms_analysis, decision_quality_psychology
                )
            }
            
            return comprehensive_psychology_insights
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§å¿ƒç†å­¦æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_basic_psychology_insights()

    def _perform_deep_pattern_recognition(self) -> Dict:
        """æ·±åº¦æ¨¡å¼è¯†åˆ«åˆ†æ"""
        try:
            import numpy as np
            from scipy import signal, stats
            
            # æå–å†å²å¿ƒç†çŠ¶æ€åºåˆ—
            emotion_sequence = [state.get('crowd_emotion', 'neutral') for state in self.psychology_history]
            confidence_sequence = [state.get('confidence', 0.5) for state in self.psychology_history]
            herd_intensity_sequence = [state.get('herd_intensity', 0.0) for state in self.psychology_history]
            
            # æ¨¡å¼ç±»å‹åˆ†æ
            pattern_types = {
                'cyclical_patterns': self._detect_cyclical_patterns(emotion_sequence),
                'trend_patterns': self._detect_trend_patterns(confidence_sequence),
                'volatility_patterns': self._detect_volatility_patterns(herd_intensity_sequence),
                'transition_patterns': self._detect_state_transition_patterns(emotion_sequence),
                'correlation_patterns': self._detect_correlation_patterns(
                    confidence_sequence, herd_intensity_sequence
                ),
                'anomaly_patterns': self._detect_anomaly_patterns(self.psychology_history),
                'emergence_patterns': self._detect_emergence_patterns(self.psychology_history),
                'persistence_patterns': self._detect_persistence_patterns(emotion_sequence)
            }
            
            # æ¨¡å¼å¼ºåº¦è¯„ä¼°
            pattern_strengths = {}
            for pattern_type, pattern_data in pattern_types.items():
                pattern_strengths[pattern_type] = self._assess_pattern_strength(pattern_data)
            
            # æ¨¡å¼é¢„æµ‹èƒ½åŠ›
            pattern_predictive_power = {}
            for pattern_type, pattern_data in pattern_types.items():
                pattern_predictive_power[pattern_type] = self._assess_pattern_predictive_power(
                    pattern_data, self.psychology_history
                )
            
            return {
                'pattern_types': pattern_types,
                'pattern_strengths': pattern_strengths,
                'pattern_predictive_power': pattern_predictive_power,
                'dominant_patterns': self._identify_dominant_patterns(pattern_strengths),
                'pattern_stability': self._assess_pattern_stability(pattern_types),
                'pattern_complexity': self._calculate_pattern_complexity(pattern_types)
            }
            
        except Exception as e:
            return {'error': str(e), 'pattern_types': {}}

    def _extract_key_psychological_insights(self, analysis_components: Dict) -> List[str]:
        """æå–å…³é”®å¿ƒç†å­¦æ´å¯Ÿ"""
        try:
            insights = []
            
            # ä»æ·±åº¦æ¨¡å¼åˆ†æä¸­æå–æ´å¯Ÿ
            deep_patterns = analysis_components.get('deep_patterns', {})
            dominant_patterns = deep_patterns.get('dominant_patterns', [])
            
            if 'cyclical_patterns' in dominant_patterns:
                insights.append("å‘ç°æ˜¾è‘—çš„å‘¨æœŸæ€§å¿ƒç†æ¨¡å¼ï¼Œç¾¤ä½“æƒ…ç»ªå‘ˆç°å¯é¢„æµ‹çš„å¾ªç¯ç‰¹å¾")
            
            if 'volatility_patterns' in dominant_patterns:
                insights.append("ç¾¤ä½“è¡Œä¸ºè¡¨ç°å‡ºé«˜åº¦æ³¢åŠ¨æ€§ï¼Œåæ˜ äº†å¤æ‚çš„æƒ…ç»ªåŠ¨åŠ›å­¦")
            
            # ä»åå·®æ¼”åŒ–åˆ†æä¸­æå–æ´å¯Ÿ
            bias_evolution = analysis_components.get('bias_evolution', {})
            if bias_evolution.get('adaptation_rate', 0) > 0.7:
                insights.append("è®¤çŸ¥åå·®å±•ç°å‡ºå¼ºé€‚åº”æ€§ï¼Œç¾¤ä½“å­¦ä¹ æ•ˆåº”æ˜¾è‘—")
            
            # ä»æƒ…ç»ªåŠ¨åŠ›å­¦ä¸­æå–æ´å¯Ÿ
            emotion_dynamics = analysis_components.get('emotion_dynamics', {})
            emotion_stability = emotion_dynamics.get('stability_score', 0.5)
            
            if emotion_stability > 0.8:
                insights.append("ç¾¤ä½“æƒ…ç»ªè¡¨ç°å‡ºé«˜åº¦ç¨³å®šæ€§ï¼Œæƒ…ç»ªè°ƒèŠ‚æœºåˆ¶æœ‰æ•ˆ")
            elif emotion_stability < 0.3:
                insights.append("ç¾¤ä½“æƒ…ç»ªé«˜åº¦ä¸ç¨³å®šï¼Œå­˜åœ¨æƒ…ç»ªä¼ æŸ“å’Œæ”¾å¤§æ•ˆåº”")
            
            # ä»é›†ä½“è¡Œä¸ºæ¶Œç°ä¸­æå–æ´å¯Ÿ
            collective_behavior = analysis_components.get('collective_behavior', {})
            emergence_score = collective_behavior.get('emergence_score', 0.5)
            
            if emergence_score > 0.7:
                insights.append("è§‚å¯Ÿåˆ°æ˜¾è‘—çš„é›†ä½“æ™ºæ…§æ¶Œç°ç°è±¡ï¼Œç¾¤ä½“å†³ç­–ä¼˜äºä¸ªä½“å†³ç­–")
            
            # ä»å­¦ä¹ æ•ˆåº”åˆ†æä¸­æå–æ´å¯Ÿ
            learning_effects = analysis_components.get('learning_effects', {})
            learning_efficiency = learning_effects.get('efficiency_score', 0.5)
            
            if learning_efficiency > 0.8:
                insights.append("ç¾¤ä½“å­¦ä¹ æ•ˆç‡æé«˜ï¼Œå¿«é€Ÿé€‚åº”ç¯å¢ƒå˜åŒ–")
            elif learning_efficiency < 0.3:
                insights.append("ç¾¤ä½“å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼Œå¯èƒ½å­˜åœ¨å­¦ä¹ éšœç¢æˆ–è®¤çŸ¥åƒµåŒ–")
            
            # ä»å‡†ç¡®æ€§å¿ƒç†å› ç´ ä¸­æå–æ´å¯Ÿ
            accuracy_factors = analysis_components.get('accuracy_factors', {})
            confidence_accuracy_correlation = accuracy_factors.get('confidence_accuracy_correlation', 0.0)
            
            if confidence_accuracy_correlation > 0.6:
                insights.append("ç¾¤ä½“ç½®ä¿¡åº¦ä¸é¢„æµ‹å‡†ç¡®æ€§å‘ˆç°å¼ºæ­£ç›¸å…³ï¼Œå…ƒè®¤çŸ¥èƒ½åŠ›è‰¯å¥½")
            elif confidence_accuracy_correlation < -0.3:
                insights.append("è¿‡åº¦è‡ªä¿¡ç°è±¡æ˜¾è‘—ï¼Œç¾¤ä½“å­˜åœ¨ç³»ç»Ÿæ€§è®¤çŸ¥åå·®")
            
            # ä»å¤æ‚æ€§ç§‘å­¦ä¸­æå–æ´å¯Ÿ
            complexity_insights = analysis_components.get('complexity_insights', {})
            system_complexity = complexity_insights.get('overall_complexity', 0.5)
            
            if system_complexity > 0.8:
                insights.append("ç¾¤ä½“å¿ƒç†ç³»ç»Ÿè¡¨ç°å‡ºé«˜åº¦å¤æ‚æ€§ï¼Œå­˜åœ¨éçº¿æ€§åŠ¨åŠ›å­¦ç‰¹å¾")
            
            # ä»ç½‘ç»œå¿ƒç†å­¦ä¸­æå–æ´å¯Ÿ
            network_psychology = analysis_components.get('network_psychology', {})
            network_effect_strength = network_psychology.get('effect_strength', 0.5)
            
            if network_effect_strength > 0.7:
                insights.append("ç½‘ç»œæ•ˆåº”å¯¹ç¾¤ä½“å¿ƒç†å½±å“å·¨å¤§ï¼Œç¤¾ä¼šä¼ æŸ“ç°è±¡æ˜æ˜¾")
            
            # ç¡®ä¿è‡³å°‘æœ‰åŸºæœ¬æ´å¯Ÿ
            if not insights:
                insights.append("ç¾¤ä½“å¿ƒç†çŠ¶æ€è¡¨ç°å‡ºåŠ¨æ€æ¼”åŒ–ç‰¹å¾ï¼ŒæŒç»­ç›‘æ§å’Œåˆ†æå…·æœ‰é‡è¦ä»·å€¼")
            
            return insights
            
        except Exception as e:
            return ["å¿ƒç†å­¦æ´å¯Ÿæå–è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ"]
        
    def _get_recent_parameter_adjustments(self) -> Dict:
        """
        ç§‘ç ”çº§å‚æ•°è°ƒæ•´è¿½è¸ªç³»ç»Ÿ - åŸºäºæ—¶é—´åºåˆ—åˆ†æå’Œå˜åŒ–æ£€æµ‹
        
        ç†è®ºåŸºç¡€ï¼š
        - æ—¶é—´åºåˆ—å˜åŒ–ç‚¹æ£€æµ‹
        - å‚æ•°æ¼”åŒ–è¿½è¸ªç†è®º
        - è‡ªé€‚åº”ç³»ç»Ÿç›‘æ§
        - å‚æ•°ç©ºé—´æ‹“æ‰‘åˆ†æ
        """
        try:
            import numpy as np
            from scipy import stats
            from collections import defaultdict
            import math
            
            # === 1. å‚æ•°å˜åŒ–å†å²åˆ†æ ===
            parameter_change_history = self._analyze_parameter_change_history()
            
            # === 2. æœ€è¿‘è°ƒæ•´ç»Ÿè®¡ ===
            recent_adjustment_statistics = self._calculate_recent_adjustment_statistics()
            
            # === 3. è°ƒæ•´é¢‘ç‡åˆ†æ ===
            adjustment_frequency_analysis = self._analyze_adjustment_frequency_patterns()
            
            # === 4. è°ƒæ•´å¹…åº¦åˆ†æ ===
            adjustment_magnitude_analysis = self._analyze_adjustment_magnitude_patterns()
            
            # === 5. å‚æ•°ç¨³å®šæ€§è¯„ä¼° ===
            parameter_stability_assessment = self._assess_parameter_stability()
            
            # === 6. æ”¶æ•›æ€§åˆ†æ ===
            convergence_analysis = self._analyze_parameter_convergence()
            
            # === 7. å‚æ•°ç›¸å…³æ€§åˆ†æ ===
            parameter_correlation_analysis = self._analyze_parameter_correlations()
            
            # === 8. è°ƒæ•´æœ‰æ•ˆæ€§è¯„ä¼° ===
            adjustment_effectiveness = self._assess_adjustment_effectiveness()
            
            # === 9. å‚æ•°ç©ºé—´æ¢ç´¢åˆ†æ ===
            parameter_space_exploration = self._analyze_parameter_space_exploration()
            
            # === 10. è‡ªé€‚åº”è¡Œä¸ºæ¨¡å¼ ===
            adaptive_behavior_patterns = self._identify_adaptive_behavior_patterns()
            
            # === å½“å‰å‚æ•°çŠ¶æ€ ===
            current_parameters = {
                'crowd_analysis': dict(self.research_parameters['crowd_analysis']),
                'bias_detection': dict(self.research_parameters['bias_detection']),
                'emotion_analysis': dict(self.research_parameters['emotion_analysis']),
                'contrarian_strategy': dict(self.research_parameters['contrarian_strategy']),
                'learning_rates': dict(self.adaptive_parameters['learning_rates']),
                'optimization_criteria': dict(self.adaptive_parameters['optimization_criteria']),
                'ensemble_parameters': dict(self.adaptive_parameters['ensemble_parameters'])
            }
            
            # === åŸºçº¿å‚æ•°å¯¹æ¯” ===
            baseline_comparison = self._compare_with_baseline_parameters(current_parameters)
            
            # === è°ƒæ•´å»ºè®®ç”Ÿæˆ ===
            adjustment_recommendations = self._generate_adjustment_recommendations(
                parameter_stability_assessment, convergence_analysis, adjustment_effectiveness
            )
            
            # === å‚æ•°ä¼˜åŒ–æœºä¼šè¯†åˆ« ===
            optimization_opportunities = self._identify_parameter_optimization_opportunities(
                parameter_space_exploration, adaptive_behavior_patterns
            )
            
            # === é£é™©è¯„ä¼° ===
            parameter_risk_assessment = self._assess_parameter_adjustment_risks(
                adjustment_magnitude_analysis, parameter_stability_assessment
            )
            
            # === å­¦ä¹ é€Ÿåº¦åˆ†æ ===
            learning_velocity_analysis = self._analyze_parameter_learning_velocity()
            
            # === é€‚åº”æ€§æ™ºèƒ½è¯„ä¼° ===
            adaptive_intelligence_score = self._calculate_adaptive_intelligence_score(
                adjustment_effectiveness, convergence_analysis, learning_velocity_analysis
            )
            
            # === æ„å»ºå®Œæ•´å‚æ•°è°ƒæ•´æŠ¥å‘Š ===
            comprehensive_adjustment_report = {
                'timestamp': self._get_current_timestamp(),
                'reporting_period': self._calculate_reporting_period(),
                
                # æ ¸å¿ƒè°ƒæ•´ä¿¡æ¯
                'current_parameters': current_parameters,
                'recent_adjustments': self._extract_recent_key_adjustments(),
                'adjustment_summary': self._generate_adjustment_summary(),
                
                # è¯¦ç»†åˆ†æ
                'parameter_change_history': parameter_change_history,
                'recent_adjustment_statistics': recent_adjustment_statistics,
                'adjustment_frequency_analysis': adjustment_frequency_analysis,
                'adjustment_magnitude_analysis': adjustment_magnitude_analysis,
                'parameter_stability_assessment': parameter_stability_assessment,
                'convergence_analysis': convergence_analysis,
                'parameter_correlation_analysis': parameter_correlation_analysis,
                'adjustment_effectiveness': adjustment_effectiveness,
                'parameter_space_exploration': parameter_space_exploration,
                'adaptive_behavior_patterns': adaptive_behavior_patterns,
                
                # å¯¹æ¯”å’Œè¯„ä¼°
                'baseline_comparison': baseline_comparison,
                'performance_impact': self._assess_performance_impact_of_adjustments(),
                'stability_vs_adaptation_balance': self._assess_stability_adaptation_balance(),
                
                # å‰ç»æ€§åˆ†æ
                'adjustment_recommendations': adjustment_recommendations,
                'optimization_opportunities': optimization_opportunities,
                'parameter_risk_assessment': parameter_risk_assessment,
                'learning_velocity_analysis': learning_velocity_analysis,
                'adaptive_intelligence_score': float(adaptive_intelligence_score),
                
                # ç³»ç»Ÿå¥åº·æŒ‡æ ‡
                'parameter_health_score': self._calculate_parameter_health_score(),
                'adaptation_efficiency': self._calculate_adaptation_efficiency(),
                'robustness_index': self._calculate_parameter_robustness_index(),
                
                # å…ƒå­¦ä¹ æŒ‡æ ‡
                'meta_learning_progress': self._assess_meta_learning_progress(),
                'transfer_learning_efficiency': self._assess_transfer_learning_efficiency(),
                'continual_learning_capability': self._assess_continual_learning_capability(),
                
                # è´¨é‡ä¿è¯
                'data_quality_assessment': self._assess_adjustment_data_quality(),
                'reliability_confidence': self._calculate_adjustment_reliability_confidence(),
                'scientific_validity': self._assess_adjustment_scientific_validity()
            }
            
            return comprehensive_adjustment_report
            
        except Exception as e:
            print(f"âŒ ç§‘ç ”çº§å‚æ•°è°ƒæ•´è¿½è¸ªå¤±è´¥: {e}")
            return self._generate_basic_parameter_report()

    def _analyze_parameter_change_history(self) -> Dict:
        """åˆ†æå‚æ•°å˜åŒ–å†å²"""
        try:
            import numpy as np
            
            if not hasattr(self, 'adaptation_history') or len(self.adaptation_history) < 3:
                return {'insufficient_data': True, 'history_length': 0}
            
            # æå–å‚æ•°å˜åŒ–æ—¶é—´åºåˆ—
            change_series = defaultdict(list)
            timestamps = []
            
            for record in self.adaptation_history:
                timestamps.append(record['timestamp'])
                parameter_updates = record.get('parameter_updates', {})
                
                for category, updates in parameter_updates.items():
                    if isinstance(updates, dict):
                        for param_name, change_info in updates.items():
                            if isinstance(change_info, dict) and 'change' in str(change_info):
                                # æå–å˜åŒ–é‡
                                change_key = [k for k in change_info.keys() if 'change' in k]
                                if change_key:
                                    change_value = change_info[change_key[0]]
                                    change_series[f"{category}_{param_name}"].append(float(change_value))
            
            # åˆ†æå˜åŒ–æ¨¡å¼
            change_patterns = {}
            for param_name, values in change_series.items():
                if len(values) >= 3:
                    change_patterns[param_name] = {
                        'mean_change': float(np.mean(values)),
                        'std_change': float(np.std(values)),
                        'trend': self._detect_parameter_trend(values),
                        'volatility': float(np.std(values) / (abs(np.mean(values)) + 1e-6)),
                        'change_frequency': len([v for v in values if abs(v) > 1e-6]),
                        'max_change': float(np.max(np.abs(values))),
                        'recent_direction': 'increasing' if values[-1] > 0 else 'decreasing' if values[-1] < 0 else 'stable'
                    }
            
            return {
                'change_series': dict(change_series),
                'change_patterns': change_patterns,
                'history_length': len(self.adaptation_history),
                'analysis_period': self._calculate_analysis_period(timestamps),
                'overall_change_intensity': self._calculate_overall_change_intensity(change_patterns)
            }
            
        except Exception as e:
            return {'error': str(e), 'history_length': 0}

    def _extract_recent_key_adjustments(self) -> Dict:
        """æå–æœ€è¿‘çš„å…³é”®è°ƒæ•´"""
        try:
            if not hasattr(self, 'adaptation_history') or len(self.adaptation_history) < 1:
                return {'no_recent_adjustments': True}
            
            # è·å–æœ€è¿‘çš„è°ƒæ•´è®°å½•
            recent_records = list(self.adaptation_history)[-5:]  # æœ€è¿‘5æ¬¡è°ƒæ•´
            
            key_adjustments = {
                'herd_threshold': None,
                'contrarian_confidence': None,
                'emotion_sensitivity': None,
                'learning_rates': {},
                'bias_parameters': {},
                'last_adjustment_time': None
            }
            
            for record in reversed(recent_records):  # ä»æœ€æ–°çš„å¼€å§‹
                timestamp = record.get('timestamp')
                parameter_updates = record.get('parameter_updates', {})
                
                # æå–å…³é”®å‚æ•°è°ƒæ•´
                if 'psychology' in parameter_updates:
                    psych_updates = parameter_updates['psychology']
                    if 'herd_threshold_change' in psych_updates and key_adjustments['herd_threshold'] is None:
                        key_adjustments['herd_threshold'] = {
                            'change': psych_updates['herd_threshold_change'],
                            'timestamp': timestamp,
                            'new_value': self.research_parameters['crowd_analysis']['herd_threshold_adaptive']
                        }
                
                if 'contrarian_strategy' in parameter_updates:
                    contrarian_updates = parameter_updates['contrarian_strategy']
                    if 'contrarian_confidence_change' in contrarian_updates and key_adjustments['contrarian_confidence'] is None:
                        key_adjustments['contrarian_confidence'] = {
                            'change': contrarian_updates['contrarian_confidence_change'],
                            'timestamp': timestamp,
                            'new_value': self.research_parameters['contrarian_strategy']['contrarian_confidence_base_enhanced']
                        }
                
                if 'learning_rate' in parameter_updates:
                    lr_updates = parameter_updates['learning_rate']
                    for lr_type, change_info in lr_updates.items():
                        if lr_type not in key_adjustments['learning_rates'] and 'change' in str(change_info):
                            key_adjustments['learning_rates'][lr_type] = {
                                'change': change_info,
                                'timestamp': timestamp
                            }
                
                if key_adjustments['last_adjustment_time'] is None:
                    key_adjustments['last_adjustment_time'] = timestamp
            
            return key_adjustments
            
        except Exception as e:
            return {'error': str(e)}

    def _calculate_parameter_health_score(self) -> float:
        """è®¡ç®—å‚æ•°å¥åº·è¯„åˆ†"""
        try:
            import numpy as np
            
            health_components = []
            
            # 1. å‚æ•°ç¨³å®šæ€§è¯„åˆ†
            stability_score = self._calculate_stability_score()
            health_components.append(stability_score * 0.3)
            
            # 2. æ”¶æ•›æ€§è¯„åˆ†
            convergence_score = self._calculate_convergence_score()
            health_components.append(convergence_score * 0.25)
            
            # 3. é€‚åº”æ€§è¯„åˆ†
            adaptability_score = self._calculate_adaptability_score()
            health_components.append(adaptability_score * 0.25)
            
            # 4. æ€§èƒ½æ”¹è¿›è¯„åˆ†
            performance_improvement_score = self._calculate_performance_improvement_score()
            health_components.append(performance_improvement_score * 0.2)
            
            overall_health_score = sum(health_components)
            return min(1.0, max(0.0, overall_health_score))
            
        except Exception as e:
            return 0.5
        
    def _get_current_timestamp(self) -> str:
        """
        ç§‘ç ”çº§æ—¶é—´æˆ³ç”Ÿæˆç³»ç»Ÿ - åŸºäºé«˜ç²¾åº¦æ—¶é—´æµ‹é‡å’Œå…ƒæ•°æ®å¢å¼º
        
        ç†è®ºåŸºç¡€ï¼š
        - æ—¶é—´åºåˆ—æ•°æ®ç§‘å­¦
        - åˆ†å¸ƒå¼ç³»ç»Ÿæ—¶é—´åŒæ­¥
        - å®éªŒæ•°æ®æ—¶é—´æ ‡è®°æ ‡å‡†
        - å¯é‡ç°ç ”ç©¶æ—¶é—´æˆ³è§„èŒƒ
        """
        try:
            import datetime
            import time
            import uuid
            import hashlib
            import platform
            import os
            
            # === 1. é«˜ç²¾åº¦æ—¶é—´è·å– ===
            
            # UTCæ—¶é—´ï¼ˆé¿å…æ—¶åŒºé—®é¢˜ï¼‰
            utc_now = datetime.datetime.utcnow()
            
            # æœ¬åœ°æ—¶é—´
            local_now = datetime.datetime.now()
            
            # é«˜ç²¾åº¦æ—¶é—´æˆ³
            high_precision_timestamp = time.time()
            
            # å•è°ƒæ—¶é—´ï¼ˆé€‚ç”¨äºæ€§èƒ½æµ‹é‡ï¼‰
            try:
                monotonic_time = time.monotonic()
            except AttributeError:
                monotonic_time = high_precision_timestamp
            
            # === 2. å¤šæ ¼å¼æ—¶é—´å­—ç¬¦ä¸²ç”Ÿæˆ ===
            
            # ISO 8601æ ‡å‡†æ ¼å¼ï¼ˆç§‘å­¦ç ”ç©¶æ ‡å‡†ï¼‰
            iso_format = utc_now.isoformat() + 'Z'
            
            # æ‰©å±•ISOæ ¼å¼ï¼ˆåŒ…å«å¾®ç§’ï¼‰
            extended_iso = utc_now.strftime('%Y-%m-%dT%H:%M:%S.%f') + 'Z'
            
            # äººç±»å¯è¯»æ ¼å¼
            human_readable = local_now.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
            
            # ç´§å‡‘æ ¼å¼ï¼ˆç”¨äºæ–‡ä»¶åç­‰ï¼‰
            compact_format = utc_now.strftime('%Y%m%d_%H%M%S_%f')[:-3]
            
            # ç§‘å­¦è®°æ•°æ³•æ—¶é—´æˆ³
            scientific_timestamp = f"{high_precision_timestamp:.6f}"
            
            # === 3. æ—¶é—´å…ƒæ•°æ®æ”¶é›† ===
            
            # æ—¶åŒºä¿¡æ¯
            timezone_info = {
                'local_timezone': str(local_now.astimezone().tzinfo),
                'utc_offset': int(local_now.astimezone().utcoffset().total_seconds()),
                'is_dst': bool(time.daylight and time.localtime().tm_isdst)
            }
            
            # ç³»ç»Ÿæ—¶é—´ä¿¡æ¯
            system_time_info = {
                'system_uptime': self._get_system_uptime(),
                'process_time': time.process_time(),
                'thread_time': getattr(time, 'thread_time', lambda: 0)(),
                'perf_counter': time.perf_counter()
            }
            
            # === 4. æ—¶é—´è´¨é‡è¯„ä¼° ===
            
            time_quality_metrics = {
                'precision_level': 'microsecond',
                'accuracy_confidence': self._assess_time_accuracy_confidence(),
                'synchronization_status': self._check_time_synchronization_status(),
                'drift_estimation': self._estimate_clock_drift()
            }
            
            # === 5. ä¼šè¯å’Œå®éªŒä¸Šä¸‹æ–‡ ===
            
            # ä¼šè¯IDï¼ˆåŸºäºå¯åŠ¨æ—¶é—´å’Œéšæœºæ•°ï¼‰
            if not hasattr(self, '_session_id'):
                session_seed = f"{high_precision_timestamp}_{uuid.uuid4().hex[:8]}"
                self._session_id = hashlib.md5(session_seed.encode()).hexdigest()[:16]
            
            # å®éªŒä¸Šä¸‹æ–‡
            experiment_context = {
                'session_id': self._session_id,
                'analysis_count': getattr(self, '_analysis_count', 0) + 1,
                'prediction_sequence': self.learning_stats.get('total_predictions', 0),
                'adaptation_cycle': len(getattr(self, 'adaptation_history', []))
            }
            
            # æ›´æ–°åˆ†æè®¡æ•°
            self._analysis_count = experiment_context['analysis_count']
            
            # === 6. ç¯å¢ƒå’Œç³»ç»Ÿä¿¡æ¯ ===
            
            environment_info = {
                'platform': platform.platform(),
                'python_version': platform.python_version(),
                'machine': platform.machine(),
                'processor': platform.processor(),
                'hostname': platform.node()
            }
            
            # === 7. æ—¶é—´æˆ³å”¯ä¸€æ€§ä¿è¯ ===
            
            # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
            uniqueness_components = [
                extended_iso,
                self._session_id,
                str(experiment_context['analysis_count']),
                str(os.getpid()),  # è¿›ç¨‹ID
                str(hash(str(monotonic_time)))[-8:]  # å•è°ƒæ—¶é—´å“ˆå¸Œ
            ]
            
            unique_identifier = hashlib.sha256('_'.join(uniqueness_components).encode()).hexdigest()[:32]
            
            # === 8. æ—¶é—´æˆ³éªŒè¯ ===
            
            validation_info = {
                'timestamp_valid': self._validate_timestamp_consistency(utc_now, local_now),
                'future_timestamp_check': utc_now < datetime.datetime.utcnow() + datetime.timedelta(seconds=1),
                'reasonable_time_check': datetime.datetime(2020, 1, 1) < utc_now < datetime.datetime(2030, 12, 31),
                'monotonic_consistency': monotonic_time > getattr(self, '_last_monotonic_time', 0)
            }
            
            # æ›´æ–°å•è°ƒæ—¶é—´è®°å½•
            self._last_monotonic_time = monotonic_time
            
            # === 9. ç ”ç©¶å¯é‡ç°æ€§æ”¯æŒ ===
            
            reproducibility_info = {
                'deterministic_seed': self._generate_deterministic_seed(high_precision_timestamp),
                'version_hash': self._calculate_system_version_hash(),
                'configuration_fingerprint': self._generate_configuration_fingerprint(),
                'data_lineage_id': self._generate_data_lineage_id()
            }
            
            # === 10. æ„å»ºå®Œæ•´æ—¶é—´æˆ³å¯¹è±¡ ===
            
            comprehensive_timestamp = {
                # ä¸»è¦æ—¶é—´æˆ³æ ¼å¼
                'iso_timestamp': iso_format,
                'extended_iso': extended_iso,
                'human_readable': human_readable,
                'compact_format': compact_format,
                'scientific_timestamp': scientific_timestamp,
                'unix_timestamp': high_precision_timestamp,
                'monotonic_time': monotonic_time,
                
                # æ—¶é—´ç»„ä»¶
                'year': utc_now.year,
                'month': utc_now.month,
                'day': utc_now.day,
                'hour': utc_now.hour,
                'minute': utc_now.minute,
                'second': utc_now.second,
                'microsecond': utc_now.microsecond,
                'weekday': utc_now.weekday(),
                'day_of_year': utc_now.timetuple().tm_yday,
                
                # æ—¶é—´å…ƒæ•°æ®
                'timezone_info': timezone_info,
                'system_time_info': system_time_info,
                'time_quality_metrics': time_quality_metrics,
                'experiment_context': experiment_context,
                'environment_info': environment_info,
                
                # å”¯ä¸€æ€§å’ŒéªŒè¯
                'unique_identifier': unique_identifier,
                'validation_info': validation_info,
                'reproducibility_info': reproducibility_info,
                
                # åˆ†æå’Œè°ƒè¯•ä¿¡æ¯
                'generation_method': 'comprehensive_scientific_timestamp',
                'precision_digits': 6,
                'reliability_score': self._calculate_timestamp_reliability_score(validation_info),
                'metadata_completeness': 1.0
            }
            
            # === æ ¹æ®è°ƒç”¨ä¸Šä¸‹æ–‡è¿”å›é€‚å½“æ ¼å¼ ===
            
            # æ£€æŸ¥è°ƒç”¨æ ˆï¼Œåˆ¤æ–­éœ€è¦çš„æ ¼å¼
            import inspect
            caller_info = inspect.stack()[1] if len(inspect.stack()) > 1 else None
            
            if caller_info:
                function_name = caller_info.function
                
                # æ ¹æ®è°ƒç”¨å‡½æ•°é€‰æ‹©åˆé€‚çš„æ ¼å¼
                if 'log' in function_name.lower() or 'record' in function_name.lower():
                    return extended_iso  # æ—¥å¿—è®°å½•ä½¿ç”¨æ‰©å±•ISOæ ¼å¼
                elif 'file' in function_name.lower() or 'save' in function_name.lower():
                    return compact_format  # æ–‡ä»¶æ“ä½œä½¿ç”¨ç´§å‡‘æ ¼å¼
                elif 'display' in function_name.lower() or 'show' in function_name.lower():
                    return human_readable  # æ˜¾ç¤ºä½¿ç”¨äººç±»å¯è¯»æ ¼å¼
                elif 'debug' in function_name.lower() or 'trace' in function_name.lower():
                    return str(comprehensive_timestamp)  # è°ƒè¯•ä½¿ç”¨å®Œæ•´å¯¹è±¡
            
            # é»˜è®¤è¿”å›ISOæ ¼å¼ï¼ˆæœ€é€šç”¨çš„ç§‘å­¦æ ‡å‡†ï¼‰
            return iso_format
            
        except Exception as e:
            # å¼‚å¸¸æƒ…å†µä¸‹çš„åå¤‡æ—¶é—´æˆ³
            try:
                import datetime
                fallback_timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'
                return fallback_timestamp
            except:
                # æœ€åçš„åå¤‡æ–¹æ¡ˆ
                import time
                return str(time.time())

    def _get_system_uptime(self) -> float:
        """è·å–ç³»ç»Ÿè¿è¡Œæ—¶é—´"""
        try:
            import platform
            import time
            
            if platform.system() == "Windows":
                import ctypes
                lib = ctypes.windll.kernel32
                t = lib.GetTickCount64()
                return t / 1000.0
            else:
                # Unix-likeç³»ç»Ÿ
                try:
                    with open('/proc/uptime', 'r') as f:
                        uptime_seconds = float(f.readline().split()[0])
                        return uptime_seconds
                except:
                    # å¦‚æœæ— æ³•è¯»å–ï¼Œä½¿ç”¨è¿›ç¨‹æ—¶é—´ä½œä¸ºä¼°è®¡
                    return time.time() - getattr(self, '_start_time', time.time())
        except:
            return 0.0

    def _assess_time_accuracy_confidence(self) -> float:
        """è¯„ä¼°æ—¶é—´å‡†ç¡®æ€§ç½®ä¿¡åº¦"""
        try:
            import time
            import datetime
            
            # å¤šæ¬¡æµ‹é‡æ—¶é—´ï¼Œè¯„ä¼°ä¸€è‡´æ€§
            measurements = []
            for _ in range(5):
                t1 = time.time()
                t2 = datetime.datetime.utcnow().timestamp()
                measurements.append(abs(t1 - t2))
                time.sleep(0.001)  # çŸ­æš‚æš‚åœ
            
            # è®¡ç®—æµ‹é‡ä¸€è‡´æ€§
            import numpy as np
            consistency = 1.0 - np.std(measurements)
            return max(0.0, min(1.0, consistency))
            
        except:
            return 0.8  # é»˜è®¤ç½®ä¿¡åº¦

    def _generate_deterministic_seed(self, timestamp: float) -> int:
        """ç”Ÿæˆç¡®å®šæ€§ç§å­ï¼ˆç”¨äºå¯é‡ç°ç ”ç©¶ï¼‰"""
        try:
            import hashlib
            
            # ç»“åˆæ—¶é—´æˆ³å’Œç³»ç»Ÿä¿¡æ¯ç”Ÿæˆç§å­
            seed_components = [
                str(int(timestamp * 1000000)),  # å¾®ç§’çº§æ—¶é—´æˆ³
                str(self.learning_stats.get('total_predictions', 0)),
                str(len(getattr(self, 'psychology_history', [])))
            ]
            
            seed_string = '_'.join(seed_components)
            seed_hash = hashlib.md5(seed_string.encode()).hexdigest()
            
            # è½¬æ¢ä¸º32ä½æ•´æ•°
            return int(seed_hash[:8], 16) & 0x7FFFFFFF
            
        except:
            return 42  # ç»å…¸é»˜è®¤ç§å­
        
    # ==================== å¿…è¦çš„è¾…åŠ©æ–¹æ³• ====================

    def _generate_default_psychology_state(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤å¿ƒç†çŠ¶æ€"""
        return {
            'dominant_emotion': 'neutral',
            'confidence_level': 0.5,
            'volatility': 0.3,
            'consensus_strength': 0.4,
            'psychological_stability': 0.5,
            'analysis_confidence': 0.3,
            'reliability_score': 0.4
        }

    def _generate_default_herd_state(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤ç¾Šç¾¤çŠ¶æ€"""
        return {
            'herd_detected': False,
            'herd_intensity': 0.0,
            'herd_type': 'none',
            'analysis_confidence': 0.3,
            'reliability_score': 0.4
        }

    def _generate_default_bias_analysis(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤åå·®åˆ†æ"""
        return {
            'anchoring_bias': 0.0,
            'availability_bias': 0.0,
            'confirmation_bias': 0.0,
            'recency_bias': 0.0,
            'overall_bias_score': 0.0,
            'analysis_confidence': 0.3
        }

    def _generate_default_emotion_state(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤æƒ…ç»ªçŠ¶æ€"""
        return {
            'current_emotion': 'neutral',
            'emotion_intensity': 0.5,
            'emotion_stability': 0.5,
            'emotion_trend': 'stable',
            'analysis_confidence': 0.3
        }

    def _generate_default_contrarian_signals(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤åå‘ä¿¡å·"""
        return {
            'contrarian_strength': 0.0,
            'contrarian_targets': [],
            'signal_quality': 'weak',
            'reasoning': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆåå‘ä¿¡å·'
        }

    def _generate_default_decision(self) -> Dict:
        """ç”Ÿæˆé»˜è®¤å†³ç­–"""
        return {
            'success': False,
            'recommended_tails': [],
            'confidence': 0.0,
            'strategy_type': 'fallback',
            'crowd_emotion': 'neutral',
            'herd_intensity': 0.0,
            'reasoning': 'å†³ç­–ç³»ç»Ÿå¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥'
        }

    def _generate_basic_psychology_insights(self) -> Dict:
        """ç”ŸæˆåŸºç¡€å¿ƒç†å­¦æ´å¯Ÿ"""
        return {
            'key_insights': ['æ•°æ®ç§¯ç´¯ä¸­ï¼Œæ´å¯Ÿç”Ÿæˆéœ€è¦æ›´å¤šå†å²æ•°æ®'],
            'crowd_behavior_accuracy': 0.5,
            'emotion_prediction_accuracy': 0.5,
            'contrarian_effectiveness': 0.5,
            'analysis_confidence': 0.3
        }

    def _generate_basic_parameter_report(self) -> Dict:
        """ç”ŸæˆåŸºç¡€å‚æ•°æŠ¥å‘Š"""
        return {
            'herd_threshold': self.research_parameters['crowd_analysis']['herd_threshold_adaptive'],
            'contrarian_confidence': self.research_parameters['contrarian_strategy']['contrarian_confidence_base_enhanced'],
            'last_adjustment': 'initialization',
            'adjustment_count': 0
        }

    def _tail_set_to_vector(self, tails: List[int]) -> np.ndarray:
        """å°†å°¾æ•°é›†åˆè½¬æ¢ä¸ºå‘é‡"""
        try:
            import numpy as np
            vector = np.zeros(10)
            for tail in tails:
                if 0 <= tail <= 9:
                    vector[tail] = 1
            return vector
        except:
            import numpy as np
            return np.zeros(10)

    def _calculate_recent_accuracy(self) -> float:
        """è®¡ç®—æœ€è¿‘å‡†ç¡®ç‡"""
        try:
            if len(self.psychology_history) < 5:
                return self.learning_stats.get('prediction_accuracy', 0.5)
            
            recent_predictions = list(self.psychology_history)[-5:]
            correct_count = sum(1 for p in recent_predictions if p.get('prediction_correct', False))
            return correct_count / len(recent_predictions)
        except:
            return 0.5

    
class EnsemblePredictionEngine:
    """é›†æˆé¢„æµ‹å¼•æ“ - å¤šæ¨¡å‹èåˆé¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.base_predictors = {}
        self.meta_predictor = None
        self.prediction_weights = {}
        self.performance_history = {}
        
    def add_predictor(self, name: str, predictor, weight: float = 1.0):
        """æ·»åŠ åŸºç¡€é¢„æµ‹å™¨"""
        self.base_predictors[name] = predictor
        self.prediction_weights[name] = weight
        self.performance_history[name] = []
    
    def ensemble_predict(self, features: np.ndarray, method: str = 'weighted_average') -> Dict:
        """é›†æˆé¢„æµ‹"""
        if not self.base_predictors:
            return {'prediction': 0.5, 'confidence': 0.0}
        
        predictions = {}
        confidences = {}
        
        # æ”¶é›†å„åŸºç¡€é¢„æµ‹å™¨çš„ç»“æœ
        for name, predictor in self.base_predictors.items():
            try:
                if hasattr(predictor, 'predict_proba'):
                    pred = predictor.predict_proba(features.reshape(1, -1))[0]
                    predictions[name] = pred[1] if len(pred) > 1 else pred[0]
                else:
                    predictions[name] = 0.5
                    
                confidences[name] = self.prediction_weights.get(name, 1.0)
            except Exception as e:
                predictions[name] = 0.5
                confidences[name] = 0.1
        
        # é›†æˆæ–¹æ³•
        if method == 'weighted_average':
            total_weight = sum(self.prediction_weights.values())
            if total_weight > 0:
                ensemble_pred = sum(pred * self.prediction_weights.get(name, 1.0) 
                                  for name, pred in predictions.items()) / total_weight
            else:
                ensemble_pred = 0.5
        elif method == 'median':
            ensemble_pred = np.median(list(predictions.values()))
        else:
            ensemble_pred = np.mean(list(predictions.values()))
        
        # è®¡ç®—é›†æˆç½®ä¿¡åº¦
        prediction_variance = np.var(list(predictions.values()))
        ensemble_confidence = max(0.1, min(0.95, 1.0 - prediction_variance))
        
        return {
            'prediction': ensemble_pred,
            'confidence': ensemble_confidence,
            'individual_predictions': predictions,
            'prediction_variance': prediction_variance
        }


class BayesianOptimizationEngine:
    """è´å¶æ–¯ä¼˜åŒ–å¼•æ“ - å‚æ•°è‡ªåŠ¨ä¼˜åŒ–ç³»ç»Ÿ"""
    
    def __init__(self):
        self.parameter_bounds = {}
        self.objective_history = []
        self.parameter_history = []
        self.best_parameters = {}
        self.best_objective = -np.inf
        
    def set_parameter_bounds(self, bounds: Dict[str, Tuple[float, float]]):
        """è®¾ç½®å‚æ•°è¾¹ç•Œ"""
        self.parameter_bounds = bounds
    
    def gaussian_process_surrogate(self, X: np.ndarray, y: np.ndarray, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """é«˜æ–¯è¿‡ç¨‹ä»£ç†æ¨¡å‹"""
        if len(X) == 0 or len(y) == 0:
            return np.zeros(len(X_test)), np.ones(len(X_test))
        
        # ç®€åŒ–çš„é«˜æ–¯è¿‡ç¨‹å®ç°
        K = np.exp(-0.5 * spatial.distance.cdist(X, X, 'sqeuclidean'))
        K_star = np.exp(-0.5 * spatial.distance.cdist(X_test, X, 'sqeuclidean'))
        K_star_star = np.exp(-0.5 * spatial.distance.cdist(X_test, X_test, 'sqeuclidean'))
        
        # æ·»åŠ å™ªå£°é¡¹é¿å…å¥‡å¼‚çŸ©é˜µ
        K += 1e-6 * np.eye(len(K))
        
        try:
            K_inv = np.linalg.inv(K)
            mu = K_star @ K_inv @ y
            sigma = np.diag(K_star_star - K_star @ K_inv @ K_star.T)
            sigma = np.maximum(sigma, 1e-10)  # ç¡®ä¿æ–¹å·®éè´Ÿ
        except np.linalg.LinAlgError:
            mu = np.mean(y) * np.ones(len(X_test))
            sigma = np.var(y) * np.ones(len(X_test))
        
        return mu, np.sqrt(sigma)
    
    def expected_improvement(self, mu: np.ndarray, sigma: np.ndarray, xi: float = 0.01) -> np.ndarray:
        """æœŸæœ›æ”¹è¿›è·å–å‡½æ•°"""
        if self.best_objective == -np.inf:
            return sigma
        
        improvement = mu - self.best_objective - xi
        Z = improvement / (sigma + 1e-10)
        
        # ä½¿ç”¨æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„CDFå’ŒPDF
        from scipy.stats import norm
        ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)
        return ei
    
    def optimize_parameters(self, objective_function, n_iterations: int = 50) -> Dict:
        """ä¼˜åŒ–å‚æ•°"""
        if not self.parameter_bounds:
            return {}
        
        param_names = list(self.parameter_bounds.keys())
        bounds = [self.parameter_bounds[name] for name in param_names]
        
        # åˆå§‹éšæœºé‡‡æ ·
        if len(self.parameter_history) == 0:
            for _ in range(5):
                random_params = {}
                for i, name in enumerate(param_names):
                    low, high = bounds[i]
                    random_params[name] = np.random.uniform(low, high)
                
                try:
                    obj_val = objective_function(random_params)
                    self.parameter_history.append(list(random_params.values()))
                    self.objective_history.append(obj_val)
                    
                    if obj_val > self.best_objective:
                        self.best_objective = obj_val
                        self.best_parameters = random_params.copy()
                except Exception:
                    continue
        
        # è´å¶æ–¯ä¼˜åŒ–è¿­ä»£
        for iteration in range(n_iterations):
            if len(self.parameter_history) < 2:
                continue
            
            X = np.array(self.parameter_history)
            y = np.array(self.objective_history)
            
            # ç”Ÿæˆå€™é€‰ç‚¹
            n_candidates = 100
            candidate_points = []
            for _ in range(n_candidates):
                candidate = []
                for low, high in bounds:
                    candidate.append(np.random.uniform(low, high))
                candidate_points.append(candidate)
            
            candidate_points = np.array(candidate_points)
            
            # é«˜æ–¯è¿‡ç¨‹é¢„æµ‹
            mu, sigma = self.gaussian_process_surrogate(X, y, candidate_points)
            
            # è®¡ç®—æœŸæœ›æ”¹è¿›
            ei = self.expected_improvement(mu, sigma)
            
            # é€‰æ‹©æœ€ä¼˜å€™é€‰ç‚¹
            best_candidate_idx = np.argmax(ei)
            best_candidate = candidate_points[best_candidate_idx]
            
            # è¯„ä¼°ç›®æ ‡å‡½æ•°
            best_params = {name: best_candidate[i] for i, name in enumerate(param_names)}
            
            try:
                obj_val = objective_function(best_params)
                self.parameter_history.append(list(best_params.values()))
                self.objective_history.append(obj_val)
                
                if obj_val > self.best_objective:
                    self.best_objective = obj_val
                    self.best_parameters = best_params.copy()
            except Exception:
                continue
        
        return self.best_parameters


class OnlineLearningEngine:
    """åœ¨çº¿å­¦ä¹ å¼•æ“ - å®æ—¶å­¦ä¹ ä¸é€‚åº”ç³»ç»Ÿ"""
    
    def __init__(self, learning_rate: float = 0.01):
        self.learning_rate = learning_rate
        self.model_weights = {}
        self.gradient_history = {}
        self.momentum = {}
        self.adaptive_lr = {}
        
    def update_model_weights(self, model_name: str, gradient: np.ndarray, use_momentum: bool = True):
        """æ›´æ–°æ¨¡å‹æƒé‡"""
        if model_name not in self.model_weights:
            self.model_weights[model_name] = np.random.normal(0, 0.1, len(gradient))
            self.momentum[model_name] = np.zeros_like(gradient)
            self.adaptive_lr[model_name] = self.learning_rate
        
        # è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆAdaGradé£æ ¼ï¼‰
        if model_name not in self.gradient_history:
            self.gradient_history[model_name] = []
        
        self.gradient_history[model_name].append(gradient)
        
        # è®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡
        gradient_squares = np.sum([g**2 for g in self.gradient_history[model_name][-10:]], axis=0)
        adaptive_lr = self.learning_rate / (np.sqrt(gradient_squares + 1e-8))
        
        # åŠ¨é‡æ›´æ–°
        if use_momentum:
            momentum_decay = 0.9
            self.momentum[model_name] = momentum_decay * self.momentum[model_name] + adaptive_lr * gradient
            self.model_weights[model_name] += self.momentum[model_name]
        else:
            self.model_weights[model_name] += adaptive_lr * gradient
    
    def get_learning_statistics(self) -> Dict:
        """è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for model_name in self.model_weights:
            if model_name in self.gradient_history and self.gradient_history[model_name]:
                recent_gradients = self.gradient_history[model_name][-10:]
                gradient_norm = np.mean([np.linalg.norm(g) for g in recent_gradients])
                gradient_variance = np.var([np.linalg.norm(g) for g in recent_gradients])
                
                stats[model_name] = {
                    'gradient_norm': gradient_norm,
                    'gradient_variance': gradient_variance,
                    'learning_stability': 1.0 / (1.0 + gradient_variance),
                    'convergence_indicator': gradient_norm < 0.01
                }
        
        return stats


class ParameterAdaptationSystem:
    """å‚æ•°è‡ªé€‚åº”ç³»ç»Ÿ - åŠ¨æ€å‚æ•°è°ƒæ•´"""
    
    def __init__(self):
        self.parameter_history = {}
        self.performance_history = {}
        self.adaptation_rules = {}
        
    def register_parameter(self, param_name: str, initial_value: float, 
                          bounds: Tuple[float, float], adaptation_rate: float = 0.1):
        """æ³¨å†Œå‚æ•°"""
        self.parameter_history[param_name] = [initial_value]
        self.performance_history[param_name] = []
        self.adaptation_rules[param_name] = {
            'bounds': bounds,
            'adaptation_rate': adaptation_rate,
            'current_value': initial_value,
            'best_value': initial_value,
            'best_performance': -np.inf
        }
    
    def update_parameter(self, param_name: str, performance_score: float) -> float:
        """æ›´æ–°å‚æ•°å€¼"""
        if param_name not in self.adaptation_rules:
            return 0.5  # é»˜è®¤å€¼
        
        rule = self.adaptation_rules[param_name]
        current_value = rule['current_value']
        
        # è®°å½•æ€§èƒ½å†å²
        self.performance_history[param_name].append(performance_score)
        
        # å¦‚æœæ€§èƒ½æå‡ï¼Œæ›´æ–°æœ€ä½³å€¼
        if performance_score > rule['best_performance']:
            rule['best_performance'] = performance_score
            rule['best_value'] = current_value
        
        # è‡ªé€‚åº”è°ƒæ•´ç­–ç•¥
        if len(self.performance_history[param_name]) >= 3:
            recent_performance = self.performance_history[param_name][-3:]
            performance_trend = np.polyfit(range(3), recent_performance, 1)[0]
            
            # æ ¹æ®æ€§èƒ½è¶‹åŠ¿è°ƒæ•´å‚æ•°
            if performance_trend > 0:
                # æ€§èƒ½æå‡ï¼Œç»§ç»­å½“å‰æ–¹å‘
                adjustment = rule['adaptation_rate'] * np.sign(np.random.randn())
            else:
                # æ€§èƒ½ä¸‹é™ï¼Œå‘æœ€ä½³å€¼é è¿‘
                adjustment = rule['adaptation_rate'] * np.sign(rule['best_value'] - current_value)
            
            # åº”ç”¨è¾¹ç•Œçº¦æŸ
            new_value = current_value + adjustment
            low, high = rule['bounds']
            new_value = max(low, min(high, new_value))
            
            rule['current_value'] = new_value
            self.parameter_history[param_name].append(new_value)
        
        return rule['current_value']


class ModelEvolutionTracker:
    """æ¨¡å‹æ¼”åŒ–è¿½è¸ªå™¨ - è¿½è¸ªæ¨¡å‹æ€§èƒ½æ¼”åŒ–"""
    
    def __init__(self):
        self.evolution_history = {}
        self.performance_metrics = {}
        self.complexity_metrics = {}
        
    def record_model_state(self, model_name: str, performance: float, 
                          complexity: float, timestamp: datetime = None):
        """è®°å½•æ¨¡å‹çŠ¶æ€"""
        if timestamp is None:
            timestamp = datetime.now()
        
        if model_name not in self.evolution_history:
            self.evolution_history[model_name] = []
            self.performance_metrics[model_name] = []
            self.complexity_metrics[model_name] = []
        
        self.evolution_history[model_name].append({
            'timestamp': timestamp,
            'performance': performance,
            'complexity': complexity,
            'efficiency': performance / (complexity + 1e-10)
        })
        
        self.performance_metrics[model_name].append(performance)
        self.complexity_metrics[model_name].append(complexity)
    
    def get_evolution_statistics(self) -> Dict:
        """è·å–æ¼”åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for model_name in self.evolution_history:
            if len(self.performance_metrics[model_name]) >= 2:
                performance_trend = np.polyfit(
                    range(len(self.performance_metrics[model_name])),
                    self.performance_metrics[model_name], 1
                )[0]
                
                complexity_trend = np.polyfit(
                    range(len(self.complexity_metrics[model_name])),
                    self.complexity_metrics[model_name], 1
                )[0]
                
                stats[model_name] = {
                    'performance_trend': performance_trend,
                    'complexity_trend': complexity_trend,
                    'current_performance': self.performance_metrics[model_name][-1],
                    'current_complexity': self.complexity_metrics[model_name][-1],
                    'evolution_stability': 1.0 / (1.0 + np.std(self.performance_metrics[model_name][-10:]))
                }
        
        return stats


class CognitiveBiasDetectionSystem:
    """è®¤çŸ¥åå·®æ£€æµ‹ç³»ç»Ÿ - é«˜çº§åå·®è¯†åˆ«ä¸é‡åŒ–"""
    
    def __init__(self):
        self.bias_detectors = self._initialize_bias_detectors()
        self.bias_interaction_matrix = np.zeros((len(CognitiveBias), len(CognitiveBias)))
        self.temporal_bias_patterns = {}
        
    def _initialize_bias_detectors(self) -> Dict:
        """åˆå§‹åŒ–åå·®æ£€æµ‹å™¨"""
        detectors = {}
        
        for bias in CognitiveBias:
            detectors[bias.name] = {
                'detector_function': self._get_bias_detector_function(bias),
                'threshold': 0.6,
                'sensitivity': 1.0,
                'historical_detections': []
            }
        
        return detectors
    
    def _get_bias_detector_function(self, bias: CognitiveBias):
        """è·å–ç‰¹å®šåå·®çš„æ£€æµ‹å‡½æ•°"""
        if bias == CognitiveBias.ANCHORING:
            return self._detect_anchoring_bias_advanced
        elif bias == CognitiveBias.AVAILABILITY:
            return self._detect_availability_bias_advanced
        elif bias == CognitiveBias.CONFIRMATION:
            return self._detect_confirmation_bias_advanced
        elif bias == CognitiveBias.REPRESENTATIVENESS:
            return self._detect_representativeness_bias
        elif bias == CognitiveBias.LOSS_AVERSION:
            return self._detect_loss_aversion_advanced
        elif bias == CognitiveBias.MENTAL_ACCOUNTING:
            return self._detect_mental_accounting_bias
        elif bias == CognitiveBias.OVERCONFIDENCE:
            return self._detect_overconfidence_advanced
        elif bias == CognitiveBias.HINDSIGHT:
            return self._detect_hindsight_bias
        elif bias == CognitiveBias.HOT_HAND:
            return self._detect_hot_hand_fallacy_advanced
        elif bias == CognitiveBias.GAMBLERS_FALLACY:
            return self._detect_gamblers_fallacy
        else:
            return self._default_bias_detector
    
    def _detect_representativeness_bias(self, categorization_data, base_rate_data=None):
        """
        ä»£è¡¨æ€§å¯å‘å¼åå·®æ£€æµ‹ï¼ˆRepresentativeness Heuristic Bias Detectionï¼‰
        è¿™æ˜¯ _detect_representativeness_heuristic_bias çš„ç®€åŒ–æ¥å£
        """
        return self._detect_representativeness_heuristic_bias(categorization_data, base_rate_data)
    
    def _detect_loss_aversion_advanced(self, choice_data, gains_losses_data, reference_points=None):
        """
        é«˜çº§æŸå¤±åŒæ¶åå·®æ£€æµ‹ï¼ˆAdvanced Loss Aversion Bias Detectionï¼‰
        è¿™æ˜¯ _detect_loss_aversion_bias çš„é«˜çº§ç‰ˆæœ¬
        """
        return self._detect_loss_aversion_bias(choice_data, gains_losses_data, reference_points)
    
    def _detect_mental_accounting_bias(self, financial_data, account_categories=None, mental_budgets=None):
        """
        å¿ƒç†è´¦æˆ·åå·®æ£€æµ‹ï¼ˆMental Accounting Bias Detectionï¼‰
        
        Args:
            financial_data: è´¢åŠ¡å†³ç­–æ•°æ®
            account_categories: è´¦æˆ·åˆ†ç±»æ•°æ® (å¯é€‰)
            mental_budgets: å¿ƒç†é¢„ç®—æ•°æ® (å¯é€‰)
        
        Returns:
            dict: å¿ƒç†è´¦æˆ·åå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            from collections import defaultdict
            
            if not financial_data or len(financial_data) < 5:
                return {'error': 'éœ€è¦è‡³å°‘5ä¸ªè´¢åŠ¡å†³ç­–æ•°æ®ç‚¹è¿›è¡Œå¿ƒç†è´¦æˆ·åå·®åˆ†æ'}
            
            # 1. èµ„é‡‘éæ›¿ä»£æ€§åˆ†æ (Fungibility Violation)
            fungibility_analysis = self._analyze_money_fungibility(financial_data, account_categories)
            
            # 2. æ¶ˆè´¹é¢„ç®—åˆ†å‰²åˆ†æ
            budget_segregation = self._analyze_budget_segregation(financial_data, mental_budgets)
            
            # 3. æŸç›Šåˆ†ç¦»è¯„ä¼°åˆ†æ
            loss_gain_separation = self._analyze_loss_gain_separation(financial_data)
            
            # 4. æ¥æºæ•ˆåº”åˆ†æ (Source Dependence)
            source_effect = self._analyze_money_source_effect(financial_data, account_categories)
            
            # 5. æ—¶é—´è´´ç°ä¸ä¸€è‡´æ€§åˆ†æ
            temporal_discounting = self._analyze_temporal_discounting_inconsistency(financial_data)
            
            # 6. å¿ƒç†è´¦æˆ·æ ‡è®°åˆ†æ
            account_labeling = self._analyze_mental_account_labeling(financial_data, account_categories)
            
            # ç»¼åˆå¿ƒç†è´¦æˆ·åå·®æŒ‡æ ‡
            bias_indicators = {
                'fungibility_violation_score': fungibility_analysis.get('violation_score', 0.5),
                'budget_segregation_score': budget_segregation.get('segregation_score', 0.5),
                'loss_gain_separation_score': loss_gain_separation.get('separation_score', 0.5),
                'source_effect_score': source_effect.get('effect_score', 0.5),
                'temporal_discounting_score': temporal_discounting.get('inconsistency_score', 0.5),
                'account_labeling_score': account_labeling.get('labeling_score', 0.5)
            }
            
            # è®¡ç®—ç»¼åˆå¿ƒç†è´¦æˆ·åå·®å¾—åˆ†
            mental_accounting_score = np.mean(list(bias_indicators.values()))
            
            return {
                'mental_accounting_bias_score': float(mental_accounting_score),
                'bias_strength': self._determine_bias_strength(mental_accounting_score),
                'bias_indicators': bias_indicators,
                'fungibility_analysis': fungibility_analysis,
                'budget_segregation_analysis': budget_segregation,
                'loss_gain_separation_analysis': loss_gain_separation,
                'source_effect_analysis': source_effect,
                'temporal_discounting_analysis': temporal_discounting,
                'account_labeling_analysis': account_labeling,
                'spending_pattern_analysis': self._analyze_spending_patterns(financial_data),
                'recommendations': self._generate_mental_accounting_recommendations(mental_accounting_score),
                'analysis_metadata': {
                    'sample_size': len(financial_data),
                    'analysis_date': self._get_current_timestamp(),
                    'confidence_level': self._calculate_analysis_confidence(len(financial_data))
                }
            }
            
        except Exception as e:
            return {'error': f'å¿ƒç†è´¦æˆ·åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}
    
    def _detect_overconfidence_advanced(self, confidence_ratings, actual_performance, calibration_data=None):
        """
        é«˜çº§è¿‡åº¦è‡ªä¿¡åå·®æ£€æµ‹ï¼ˆAdvanced Overconfidence Bias Detectionï¼‰
        è¿™æ˜¯ _detect_overconfidence_bias çš„é«˜çº§ç‰ˆæœ¬
        """
        return self._detect_overconfidence_bias(confidence_ratings, actual_performance, calibration_data)
    
    def _detect_hindsight_bias(self, prediction_data, outcome_data, memory_data=None):
        """
        åè§ä¹‹æ˜åå·®æ£€æµ‹ï¼ˆHindsight Bias Detectionï¼‰
        
        Args:
            prediction_data: é¢„æµ‹æ•°æ®
            outcome_data: ç»“æœæ•°æ®
            memory_data: è®°å¿†æ•°æ® (å¯é€‰)
        
        Returns:
            dict: åè§ä¹‹æ˜åå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            if not prediction_data or not outcome_data:
                return {'error': 'éœ€è¦é¢„æµ‹æ•°æ®å’Œç»“æœæ•°æ®è¿›è¡Œåè§ä¹‹æ˜åå·®åˆ†æ'}
            
            if len(prediction_data) != len(outcome_data):
                return {'error': 'é¢„æµ‹æ•°æ®å’Œç»“æœæ•°æ®é•¿åº¦ä¸åŒ¹é…'}
            
            # 1. é¢„æµ‹å›å¿†åå·®åˆ†æ
            prediction_recall_bias = self._analyze_prediction_recall_bias(
                prediction_data, outcome_data, memory_data
            )
            
            # 2. å¿…ç„¶æ€§é”™è§‰åˆ†æ (Inevitability Illusion)
            inevitability_illusion = self._analyze_inevitability_illusion(
                prediction_data, outcome_data
            )
            
            # 3. ç»“æœå¯é¢„è§æ€§é«˜ä¼°åˆ†æ
            foreseeability_overestimation = self._analyze_foreseeability_overestimation(
                prediction_data, outcome_data
            )
            
            # 4. è®°å¿†é‡æ„åˆ†æ (Memory Reconstruction)
            memory_reconstruction = self._analyze_memory_reconstruction(
                prediction_data, outcome_data, memory_data
            )
            
            # 5. è®¤çŸ¥å¤±è°ƒè§£å†³åˆ†æ
            cognitive_dissonance_resolution = self._analyze_cognitive_dissonance_resolution(
                prediction_data, outcome_data
            )
            
            # 6. æ—¶é—´è·ç¦»æ•ˆåº”åˆ†æ
            temporal_distance_effect = self._analyze_temporal_distance_effect(
                prediction_data, outcome_data
            )
            
            # ç»¼åˆåè§ä¹‹æ˜åå·®æŒ‡æ ‡
            bias_indicators = {
                'prediction_recall_bias_score': prediction_recall_bias.get('bias_score', 0.5),
                'inevitability_illusion_score': inevitability_illusion.get('illusion_score', 0.5),
                'foreseeability_overestimation_score': foreseeability_overestimation.get('overestimation_score', 0.5),
                'memory_reconstruction_score': memory_reconstruction.get('reconstruction_score', 0.5),
                'cognitive_dissonance_score': cognitive_dissonance_resolution.get('dissonance_score', 0.5),
                'temporal_distance_score': temporal_distance_effect.get('distance_effect_score', 0.5)
            }
            
            # è®¡ç®—ç»¼åˆåè§ä¹‹æ˜åå·®å¾—åˆ†
            hindsight_score = np.mean(list(bias_indicators.values()))
            
            return {
                'hindsight_bias_score': float(hindsight_score),
                'bias_strength': self._determine_bias_strength(hindsight_score),
                'bias_indicators': bias_indicators,
                'prediction_recall_analysis': prediction_recall_bias,
                'inevitability_illusion_analysis': inevitability_illusion,
                'foreseeability_analysis': foreseeability_overestimation,
                'memory_reconstruction_analysis': memory_reconstruction,
                'cognitive_dissonance_analysis': cognitive_dissonance_resolution,
                'temporal_distance_analysis': temporal_distance_effect,
                'accuracy_assessment': self._assess_prediction_accuracy(prediction_data, outcome_data),
                'recommendations': self._generate_hindsight_bias_recommendations(hindsight_score),
                'analysis_metadata': {
                    'sample_size': len(prediction_data),
                    'analysis_date': self._get_current_timestamp(),
                    'confidence_level': self._calculate_analysis_confidence(len(prediction_data))
                }
            }
            
        except Exception as e:
            return {'error': f'åè§ä¹‹æ˜åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}
    
    def _detect_hot_hand_fallacy_advanced(self, sequence_data, performance_data=None, expectation_data=None):
        """
        é«˜çº§çƒ­æ‰‹è°¬è¯¯æ£€æµ‹ï¼ˆAdvanced Hot Hand Fallacy Detectionï¼‰
        
        Args:
            sequence_data: åºåˆ—æ•°æ®ï¼ˆæˆåŠŸ/å¤±è´¥åºåˆ—ï¼‰
            performance_data: è¡¨ç°æ•°æ® (å¯é€‰)
            expectation_data: æœŸæœ›æ•°æ® (å¯é€‰)
        
        Returns:
            dict: çƒ­æ‰‹è°¬è¯¯æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            if not sequence_data or len(sequence_data) < 10:
                return {'error': 'éœ€è¦è‡³å°‘10ä¸ªåºåˆ—æ•°æ®ç‚¹è¿›è¡Œçƒ­æ‰‹è°¬è¯¯åˆ†æ'}
            
            # 1. è¿ç»­æˆåŠŸæœŸæœ›è¿‡é«˜åˆ†æ
            streak_expectation_analysis = self._analyze_streak_expectation_bias(
                sequence_data, expectation_data
            )
            
            # 2. åºåˆ—éšæœºæ€§è¯¯åˆ¤åˆ†æ
            randomness_misjudgment = self._analyze_sequence_randomness_misjudgment(sequence_data)
            
            # 3. æ¨¡å¼è¯†åˆ«è¿‡åº¦åˆ†æ
            pattern_over_recognition = self._analyze_pattern_over_recognition(sequence_data)
            
            # 4. æ¡ä»¶æ¦‚ç‡è¯¯ä¼°åˆ†æ
            conditional_probability_misestimation = self._analyze_conditional_probability_misestimation(
                sequence_data
            )
            
            # 5. å›å½’å¹³å‡å¿½è§†åˆ†æï¼ˆåœ¨è¿ç»­æˆåŠŸåï¼‰
            regression_neglect_after_streaks = self._analyze_regression_neglect_in_streaks(
                sequence_data, performance_data
            )
            
            # 6. å› æœå…³ç³»é”™è¯¯å½’å› åˆ†æ
            causal_attribution_error = self._analyze_causal_attribution_in_sequences(
                sequence_data, performance_data
            )
            
            # ç»¼åˆçƒ­æ‰‹è°¬è¯¯æŒ‡æ ‡
            bias_indicators = {
                'streak_expectation_bias_score': streak_expectation_analysis.get('bias_score', 0.5),
                'randomness_misjudgment_score': randomness_misjudgment.get('misjudgment_score', 0.5),
                'pattern_over_recognition_score': pattern_over_recognition.get('over_recognition_score', 0.5),
                'conditional_probability_error_score': conditional_probability_misestimation.get('error_score', 0.5),
                'regression_neglect_score': regression_neglect_after_streaks.get('neglect_score', 0.5),
                'causal_attribution_error_score': causal_attribution_error.get('attribution_error_score', 0.5)
            }
            
            # è®¡ç®—ç»¼åˆçƒ­æ‰‹è°¬è¯¯å¾—åˆ†
            hot_hand_score = np.mean(list(bias_indicators.values()))
            
            return {
                'hot_hand_fallacy_score': float(hot_hand_score),
                'fallacy_strength': self._determine_bias_strength(hot_hand_score),
                'bias_indicators': bias_indicators,
                'streak_expectation_analysis': streak_expectation_analysis,
                'randomness_misjudgment_analysis': randomness_misjudgment,
                'pattern_recognition_analysis': pattern_over_recognition,
                'conditional_probability_analysis': conditional_probability_misestimation,
                'regression_neglect_analysis': regression_neglect_after_streaks,
                'causal_attribution_analysis': causal_attribution_error,
                'sequence_statistics': self._calculate_sequence_statistics(sequence_data),
                'streak_analysis': self._analyze_streak_patterns(sequence_data),
                'recommendations': self._generate_hot_hand_fallacy_recommendations(hot_hand_score),
                'analysis_metadata': {
                    'sample_size': len(sequence_data),
                    'analysis_date': self._get_current_timestamp(),
                    'confidence_level': self._calculate_analysis_confidence(len(sequence_data))
                }
            }
            
        except Exception as e:
            return {'error': f'çƒ­æ‰‹è°¬è¯¯æ£€æµ‹å¤±è´¥: {str(e)}'}
    
    def _detect_gamblers_fallacy(self, sequence_data, probability_estimates=None, betting_data=None):
        """
        èµŒå¾’è°¬è¯¯æ£€æµ‹ï¼ˆGambler's Fallacy Detectionï¼‰
        
        Args:
            sequence_data: åºåˆ—æ•°æ®ï¼ˆéšæœºäº‹ä»¶åºåˆ—ï¼‰
            probability_estimates: æ¦‚ç‡ä¼°è®¡æ•°æ® (å¯é€‰)
            betting_data: æŠ•æ³¨æ•°æ® (å¯é€‰)
        
        Returns:
            dict: èµŒå¾’è°¬è¯¯æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            if not sequence_data or len(sequence_data) < 10:
                return {'error': 'éœ€è¦è‡³å°‘10ä¸ªåºåˆ—æ•°æ®ç‚¹è¿›è¡ŒèµŒå¾’è°¬è¯¯åˆ†æ'}
            
            # 1. è´Ÿç›¸å…³æœŸæœ›åˆ†æ
            negative_correlation_expectation = self._analyze_negative_correlation_expectation(
                sequence_data, probability_estimates
            )
            
            # 2. å‡å€¼å›å½’è¿‡åº¦æœŸæœ›åˆ†æ
            mean_reversion_over_expectation = self._analyze_mean_reversion_over_expectation(
                sequence_data
            )
            
            # 3. å°æ•°å®šå¾‹é”™è¯¯åº”ç”¨åˆ†æ
            law_of_small_numbers_misapplication = self._analyze_law_of_small_numbers_misapplication(
                sequence_data
            )
            
            # 4. éšæœºæ€§ä»£è¡¨æ€§è¯¯åˆ¤åˆ†æ
            randomness_representativeness_misjudgment = self._analyze_randomness_representativeness(
                sequence_data
            )
            
            # 5. ç‹¬ç«‹äº‹ä»¶ç›¸å…³æ€§é”™è§‰åˆ†æ
            independence_violation_illusion = self._analyze_independence_violation_illusion(
                sequence_data, probability_estimates
            )
            
            # 6. æŠ•æ³¨è¡Œä¸ºå˜åŒ–åˆ†æï¼ˆå¦‚æœæœ‰æŠ•æ³¨æ•°æ®ï¼‰
            betting_behavior_analysis = self._analyze_betting_behavior_changes(
                sequence_data, betting_data
            ) if betting_data else {'behavior_change_score': 0.5}
            
            # ç»¼åˆèµŒå¾’è°¬è¯¯æŒ‡æ ‡
            bias_indicators = {
                'negative_correlation_score': negative_correlation_expectation.get('correlation_score', 0.5),
                'mean_reversion_expectation_score': mean_reversion_over_expectation.get('expectation_score', 0.5),
                'small_numbers_law_score': law_of_small_numbers_misapplication.get('misapplication_score', 0.5),
                'randomness_misjudgment_score': randomness_representativeness_misjudgment.get('misjudgment_score', 0.5),
                'independence_violation_score': independence_violation_illusion.get('violation_score', 0.5),
                'betting_behavior_change_score': betting_behavior_analysis.get('behavior_change_score', 0.5)
            }
            
            # è®¡ç®—ç»¼åˆèµŒå¾’è°¬è¯¯å¾—åˆ†
            gamblers_fallacy_score = np.mean(list(bias_indicators.values()))
            
            return {
                'gamblers_fallacy_score': float(gamblers_fallacy_score),
                'fallacy_strength': self._determine_bias_strength(gamblers_fallacy_score),
                'bias_indicators': bias_indicators,
                'negative_correlation_analysis': negative_correlation_expectation,
                'mean_reversion_analysis': mean_reversion_over_expectation,
                'small_numbers_law_analysis': law_of_small_numbers_misapplication,
                'randomness_representativeness_analysis': randomness_representativeness_misjudgment,
                'independence_violation_analysis': independence_violation_illusion,
                'betting_behavior_analysis': betting_behavior_analysis,
                'sequence_pattern_analysis': self._analyze_sequence_patterns(sequence_data),
                'probability_calibration': self._assess_probability_calibration(sequence_data, probability_estimates),
                'recommendations': self._generate_gamblers_fallacy_recommendations(gamblers_fallacy_score),
                'analysis_metadata': {
                    'sample_size': len(sequence_data),
                    'analysis_date': self._get_current_timestamp(),
                    'confidence_level': self._calculate_analysis_confidence(len(sequence_data))
                }
            }
            
        except Exception as e:
            return {'error': f'èµŒå¾’è°¬è¯¯æ£€æµ‹å¤±è´¥: {str(e)}'}

    # ä»¥ä¸‹æ˜¯æ”¯æŒæ–¹æ³•çš„å®ç°
    
    def _analyze_money_fungibility(self, financial_data, account_categories):
        """åˆ†æèµ„é‡‘æ›¿ä»£æ€§è¿å"""
        try:
            fungibility_violations = []
            
            for i, decision in enumerate(financial_data):
                # æ£€æŸ¥åŒç­‰é‡‘é¢åœ¨ä¸åŒå¿ƒç†è´¦æˆ·ä¸­çš„ä¸åŒå¤„ç†æ–¹å¼
                amount = decision.get('amount', 0)
                category = decision.get('category', 'general')
                spending_willingness = decision.get('willingness_score', 0.5)
                
                # å¯»æ‰¾ç›¸ä¼¼é‡‘é¢ä½†ä¸åŒç±»åˆ«çš„å†³ç­–
                for j, other_decision in enumerate(financial_data):
                    if i != j:
                        other_amount = other_decision.get('amount', 0)
                        other_category = other_decision.get('category', 'general')
                        other_willingness = other_decision.get('willingness_score', 0.5)
                        
                        # å¦‚æœé‡‘é¢ç›¸ä¼¼ä½†ç±»åˆ«ä¸åŒï¼Œæ¯”è¾ƒæ”¯å‡ºæ„æ„¿å·®å¼‚
                        if (abs(amount - other_amount) / max(amount, other_amount, 1) < 0.2 and 
                            category != other_category):
                            willingness_diff = abs(spending_willingness - other_willingness)
                            if willingness_diff > 0.3:  # æ˜¾è‘—å·®å¼‚é˜ˆå€¼
                                fungibility_violations.append(willingness_diff)
            
            violation_score = np.mean(fungibility_violations) if fungibility_violations else 0.0
            
            return {
                'violation_score': float(violation_score),
                'violation_count': len(fungibility_violations),
                'violation_details': fungibility_violations[:5]  # å‰5ä¸ªè¿åæ¡ˆä¾‹
            }
        except Exception as e:
            return {'violation_score': 0.5, 'error': str(e)}

    def _analyze_budget_segregation(self, financial_data, mental_budgets):
        """åˆ†æé¢„ç®—åˆ†å‰²è¡Œä¸º"""
        try:
            if not mental_budgets:
                return {'segregation_score': 0.5, 'note': 'æ— å¿ƒç†é¢„ç®—æ•°æ®'}
            
            segregation_indicators = []
            
            for budget_category, budget_limit in mental_budgets.items():
                category_spending = [
                    d.get('amount', 0) for d in financial_data 
                    if d.get('category') == budget_category
                ]
                
                if category_spending:
                    total_spent = sum(category_spending)
                    # æ£€æŸ¥æ˜¯å¦ä¸¥æ ¼éµå®ˆé¢„ç®—è¾¹ç•Œï¼Œå³ä½¿æ€»ä½“è´¢åŠ¡çŠ¶å†µå…è®¸çµæ´»æ€§
                    budget_adherence = min(total_spent / budget_limit, 1.0) if budget_limit > 0 else 0
                    
                    # æ£€æŸ¥æ˜¯å¦æ‹’ç»æœ‰ä»·å€¼çš„è·¨ç±»åˆ«è°ƒç”¨
                    cross_category_rigidity = self._assess_cross_category_rigidity(
                        financial_data, budget_category, budget_limit
                    )
                    
                    segregation_indicators.append(budget_adherence * cross_category_rigidity)
            
            segregation_score = np.mean(segregation_indicators) if segregation_indicators else 0.0
            
            return {
                'segregation_score': float(segregation_score),
                'budget_adherence_analysis': segregation_indicators,
                'rigid_categories': len([s for s in segregation_indicators if s > 0.7])
            }
        except Exception as e:
            return {'segregation_score': 0.5, 'error': str(e)}

    def _assess_cross_category_rigidity(self, financial_data, category, budget_limit):
        """è¯„ä¼°è·¨ç±»åˆ«è°ƒç”¨çš„åˆšæ€§"""
        # æ£€æŸ¥åœ¨è¯¥ç±»åˆ«é¢„ç®—ç´§å¼ æ—¶ï¼Œæ˜¯å¦è€ƒè™‘å…¶ä»–ç±»åˆ«çš„ç›ˆä½™
        rigidity_score = 0.5  # é»˜è®¤ä¸­ç­‰åˆšæ€§
        
        category_decisions = [d for d in financial_data if d.get('category') == category]
        if not category_decisions:
            return rigidity_score
        
        # ç®€åŒ–çš„åˆšæ€§è¯„ä¼°ï¼šåŸºäºå†³ç­–ä¸€è‡´æ€§
        consistency_scores = []
        for decision in category_decisions:
            budget_utilization = decision.get('amount', 0) / budget_limit if budget_limit > 0 else 0
            decision_firmness = decision.get('firmness', 0.5)  # å†³ç­–åšå®šæ€§
            consistency_scores.append(budget_utilization * decision_firmness)
        
        return np.mean(consistency_scores) if consistency_scores else rigidity_score

    def _analyze_loss_gain_separation(self, financial_data):
        """åˆ†ææŸç›Šåˆ†ç¦»è¯„ä¼°"""
        try:
            separation_indicators = []
            
            for decision in financial_data:
                gains = decision.get('gains', [])
                losses = decision.get('losses', [])
                evaluation_mode = decision.get('evaluation_mode', 'integrated')  # 'separated' or 'integrated'
                
                if gains and losses:
                    # æ£€æŸ¥æ˜¯å¦å€¾å‘äºåˆ†ç¦»è¯„ä¼°æŸç›Š
                    if evaluation_mode == 'separated':
                        # åˆ†ç¦»è¯„ä¼°å¯èƒ½å¯¼è‡´éæœ€ä¼˜å†³ç­–
                        net_outcome = sum(gains) - sum(losses)
                        separated_evaluation = sum(gains) * 0.8 - sum(losses) * 1.2  # æŸå¤±åŒæ¶æƒé‡
                        
                        evaluation_difference = abs(net_outcome - separated_evaluation)
                        separation_indicators.append(evaluation_difference / max(abs(net_outcome), 1))
            
            separation_score = np.mean(separation_indicators) if separation_indicators else 0.0
            
            return {
                'separation_score': float(separation_score),
                'separated_evaluations': len([d for d in financial_data 
                                            if d.get('evaluation_mode') == 'separated']),
                'total_evaluations': len(financial_data)
            }
        except Exception as e:
            return {'separation_score': 0.5, 'error': str(e)}

    def _analyze_money_source_effect(self, financial_data, account_categories):
        """åˆ†æèµ„é‡‘æ¥æºæ•ˆåº”"""
        try:
            source_effects = []
            
            for decision in financial_data:
                amount = decision.get('amount', 0)
                source = decision.get('source', 'general')  # èµ„é‡‘æ¥æº
                spending_behavior = decision.get('spending_behavior', 0.5)
                
                # æ¯”è¾ƒç›¸åŒé‡‘é¢ä½†ä¸åŒæ¥æºçš„æ”¯å‡ºè¡Œä¸º
                for other_decision in financial_data:
                    other_amount = other_decision.get('amount', 0)
                    other_source = other_decision.get('source', 'general')
                    other_behavior = other_decision.get('spending_behavior', 0.5)
                    
                    if (abs(amount - other_amount) / max(amount, other_amount, 1) < 0.1 and 
                        source != other_source):
                        behavior_difference = abs(spending_behavior - other_behavior)
                        source_effects.append(behavior_difference)
            
            effect_score = np.mean(source_effects) if source_effects else 0.0
            
            return {
                'effect_score': float(effect_score),
                'source_dependent_decisions': len(source_effects),
                'effect_magnitude': effect_score
            }
        except Exception as e:
            return {'effect_score': 0.5, 'error': str(e)}
        
    def _analyze_temporal_discounting_inconsistency(self, financial_data):
        """åˆ†ææ—¶é—´è´´ç°ä¸ä¸€è‡´æ€§"""
        try:
            inconsistencies = []
            
            for decision in financial_data:
                immediate_value = decision.get('immediate_value', 0)
                delayed_value = decision.get('delayed_value', 0)
                delay_period = decision.get('delay_period', 1)
                choice = decision.get('choice', 'immediate')  # 'immediate' or 'delayed'
                
                if immediate_value > 0 and delayed_value > 0 and delay_period > 0:
                    # è®¡ç®—éšå«è´´ç°ç‡
                    implied_discount_rate = (delayed_value / immediate_value - 1) / delay_period
                    
                    # æ£€æŸ¥è´´ç°ç‡çš„ä¸€è‡´æ€§
                    inconsistencies.append({
                        'discount_rate': implied_discount_rate,
                        'choice': choice,
                        'delay': delay_period
                    })
            
            # åˆ†æè´´ç°ç‡å˜å¼‚æ€§
            if len(inconsistencies) > 1:
                discount_rates = [inc['discount_rate'] for inc in inconsistencies]
                inconsistency_score = np.std(discount_rates) / (np.mean(discount_rates) + 0.001)
            else:
                inconsistency_score = 0.0
            
            return {
                'inconsistency_score': float(min(inconsistency_score, 1.0)),
                'discount_rate_variance': float(np.var([inc['discount_rate'] for inc in inconsistencies])) if inconsistencies else 0.0,
                'sample_size': len(inconsistencies)
            }
        except Exception as e:
            return {'inconsistency_score': 0.5, 'error': str(e)}

    def _analyze_mental_account_labeling(self, financial_data, account_categories):
        """åˆ†æå¿ƒç†è´¦æˆ·æ ‡è®°æ•ˆåº”"""
        try:
            labeling_effects = []
            
            if not account_categories:
                return {'labeling_score': 0.5}
            
            for category in account_categories:
                category_decisions = [d for d in financial_data if d.get('category') == category]
                
                if len(category_decisions) > 1:
                    # åˆ†ææ ‡è®°å¯¹æ”¯å‡ºè¡Œä¸ºçš„å½±å“
                    spending_amounts = [d.get('amount', 0) for d in category_decisions]
                    spending_consistency = 1.0 - (np.std(spending_amounts) / (np.mean(spending_amounts) + 0.001))
                    labeling_effects.append(spending_consistency)
            
            labeling_score = np.mean(labeling_effects) if labeling_effects else 0.0
            
            return {
                'labeling_score': float(labeling_score),
                'labeled_categories': len(labeling_effects),
                'consistency_scores': labeling_effects
            }
        except Exception as e:
            return {'labeling_score': 0.5, 'error': str(e)}

    def _analyze_spending_patterns(self, financial_data):
        """åˆ†ææ”¯å‡ºæ¨¡å¼"""
        try:
            patterns = {
                'category_distribution': {},
                'amount_distribution': [],
                'temporal_patterns': {},
                'decision_consistency': 0.0
            }
            
            for decision in financial_data:
                category = decision.get('category', 'uncategorized')
                amount = decision.get('amount', 0)
                timestamp = decision.get('timestamp', 0)
                
                # ç±»åˆ«åˆ†å¸ƒ
                patterns['category_distribution'][category] = patterns['category_distribution'].get(category, 0) + 1
                
                # é‡‘é¢åˆ†å¸ƒ
                patterns['amount_distribution'].append(amount)
                
                # æ—¶é—´æ¨¡å¼ï¼ˆç®€åŒ–ï¼‰
                time_period = str(timestamp // 86400) if timestamp > 0 else 'unknown'  # æŒ‰å¤©åˆ†ç»„
                patterns['temporal_patterns'][time_period] = patterns['temporal_patterns'].get(time_period, 0) + 1
            
            # å†³ç­–ä¸€è‡´æ€§è¯„ä¼°
            if len(patterns['amount_distribution']) > 1:
                amount_consistency = 1.0 - (np.std(patterns['amount_distribution']) / 
                                          (np.mean(patterns['amount_distribution']) + 0.001))
                patterns['decision_consistency'] = float(max(0.0, amount_consistency))
            
            return patterns
        except Exception as e:
            return {'error': str(e)}

    def _generate_mental_accounting_recommendations(self, mental_accounting_score):
        """ç”Ÿæˆå¿ƒç†è´¦æˆ·åå·®æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if mental_accounting_score > 0.7:
            recommendations.extend([
                "é«˜åº¦å¿ƒç†è´¦æˆ·åå·®ï¼šå»ºè®®é‡‡ç”¨æ•´ä½“è´¢åŠ¡è§„åˆ’è§†è§’",
                "å®æ–½èµ„é‡‘æ± ç®¡ç†ç­–ç•¥ï¼Œå‡å°‘ä¸å¿…è¦çš„ç±»åˆ«åˆ†å‰²",
                "å®šæœŸå®¡æŸ¥å’Œè°ƒæ•´å¿ƒç†é¢„ç®—è¾¹ç•Œ",
                "è€ƒè™‘ä½¿ç”¨é‡åŒ–å·¥å…·è¯„ä¼°æŠ•èµ„å†³ç­–"
            ])
        elif mental_accounting_score > 0.5:
            recommendations.extend([
                "ä¸­ç­‰å¿ƒç†è´¦æˆ·åå·®ï¼šå»ºè®®æé«˜è´¢åŠ¡å†³ç­–çš„çµæ´»æ€§",
                "è·¨ç±»åˆ«èµ„æºè°ƒé…æ—¶è€ƒè™‘æ•´ä½“æ•ˆç›Š",
                "å‡å°‘å¯¹èµ„é‡‘æ¥æºçš„è¿‡åº¦ä¾èµ–"
            ])
        else:
            recommendations.extend([
                "å¿ƒç†è´¦æˆ·åå·®è¾ƒä½ï¼šä¿æŒç†æ€§çš„è´¢åŠ¡å†³ç­–ä¹ æƒ¯",
                "ç»§ç»­é‡‡ç”¨æ•´ä½“è´¢åŠ¡è§†è§’è¿›è¡Œå†³ç­–"
            ])
        
        return recommendations

    def _analyze_prediction_recall_bias(self, prediction_data, outcome_data, memory_data):
        """åˆ†æé¢„æµ‹å›å¿†åå·®"""
        try:
            recall_biases = []
            
            for i, (prediction, outcome) in enumerate(zip(prediction_data, outcome_data)):
                original_prediction = prediction.get('original_confidence', 0.5)
                recalled_prediction = prediction.get('recalled_confidence', original_prediction)
                actual_outcome = outcome.get('success', False)
                
                # è®¡ç®—å›å¿†åå·®ï¼šå®é™…é¢„æµ‹ä¸å›å¿†é¢„æµ‹çš„å·®å¼‚
                recall_bias = abs(original_prediction - recalled_prediction)
                
                # ç»“æœå·²çŸ¥åçš„å›å¿†åå·®æ¨¡å¼
                if actual_outcome:
                    # æˆåŠŸç»“æœï¼šå€¾å‘äºå›å¿†æ›´é«˜çš„é¢„æµ‹ç½®ä¿¡åº¦
                    if recalled_prediction > original_prediction:
                        recall_bias *= 1.5  # åŠ æƒ
                else:
                    # å¤±è´¥ç»“æœï¼šå€¾å‘äºå›å¿†æ›´ä½çš„é¢„æµ‹ç½®ä¿¡åº¦
                    if recalled_prediction < original_prediction:
                        recall_bias *= 1.5
                
                recall_biases.append(recall_bias)
            
            bias_score = np.mean(recall_biases) if recall_biases else 0.0
            
            return {
                'bias_score': float(min(bias_score, 1.0)),
                'recall_distortions': len([b for b in recall_biases if b > 0.1]),
                'average_distortion': float(bias_score)
            }
        except Exception as e:
            return {'bias_score': 0.5, 'error': str(e)}

    def _analyze_inevitability_illusion(self, prediction_data, outcome_data):
        """åˆ†æå¿…ç„¶æ€§é”™è§‰"""
        try:
            inevitability_scores = []
            
            for prediction, outcome in zip(prediction_data, outcome_data):
                post_outcome_inevitability = prediction.get('post_outcome_inevitability', 0.5)
                original_prediction_confidence = prediction.get('original_confidence', 0.5)
                actual_outcome = outcome.get('success', False)
                
                # å¦‚æœç»“æœå·²çŸ¥åè®¤ä¸ºç»“æœå¿…ç„¶ï¼Œä½†åŸå§‹é¢„æµ‹ç½®ä¿¡åº¦ä¸é«˜
                if actual_outcome and post_outcome_inevitability > 0.7 and original_prediction_confidence < 0.6:
                    inevitability_score = post_outcome_inevitability - original_prediction_confidence
                    inevitability_scores.append(inevitability_score)
                elif not actual_outcome and post_outcome_inevitability > 0.7:
                    # å¯¹å¤±è´¥ç»“æœä¹Ÿè®¤ä¸ºå¿…ç„¶
                    inevitability_scores.append(post_outcome_inevitability)
            
            illusion_score = np.mean(inevitability_scores) if inevitability_scores else 0.0
            
            return {
                'illusion_score': float(min(illusion_score, 1.0)),
                'inevitability_cases': len(inevitability_scores),
                'average_illusion_strength': float(illusion_score)
            }
        except Exception as e:
            return {'illusion_score': 0.5, 'error': str(e)}

    def _analyze_foreseeability_overestimation(self, prediction_data, outcome_data):
        """åˆ†æç»“æœå¯é¢„è§æ€§é«˜ä¼°"""
        try:
            overestimation_scores = []
            
            for prediction, outcome in zip(prediction_data, outcome_data):
                post_outcome_foreseeability = prediction.get('post_outcome_foreseeability', 0.5)
                pre_outcome_predictability = prediction.get('pre_outcome_predictability', 0.5)
                
                # äº‹åè®¤ä¸ºç»“æœæ¯”äº‹å‰æ›´å¯é¢„è§
                if post_outcome_foreseeability > pre_outcome_predictability:
                    overestimation = post_outcome_foreseeability - pre_outcome_predictability
                    overestimation_scores.append(overestimation)
            
            overestimation_score = np.mean(overestimation_scores) if overestimation_scores else 0.0
            
            return {
                'overestimation_score': float(min(overestimation_score, 1.0)),
                'overestimation_cases': len(overestimation_scores),
                'average_overestimation': float(overestimation_score)
            }
        except Exception as e:
            return {'overestimation_score': 0.5, 'error': str(e)}

    def _analyze_memory_reconstruction(self, prediction_data, outcome_data, memory_data):
        """åˆ†æè®°å¿†é‡æ„"""
        try:
            if not memory_data:
                return {'reconstruction_score': 0.5, 'note': 'æ— è®°å¿†æ•°æ®'}
            
            reconstruction_indicators = []
            
            for i, (prediction, outcome, memory) in enumerate(zip(prediction_data, outcome_data, memory_data)):
                original_details = memory.get('original_details', {})
                recalled_details = memory.get('recalled_details', {})
                
                # è®¡ç®—è®°å¿†é‡æ„ç¨‹åº¦
                if original_details and recalled_details:
                    detail_changes = 0
                    total_details = len(original_details)
                    
                    for key in original_details:
                        if key in recalled_details:
                            if original_details[key] != recalled_details[key]:
                                detail_changes += 1
                    
                    if total_details > 0:
                        reconstruction_ratio = detail_changes / total_details
                        reconstruction_indicators.append(reconstruction_ratio)
            
            reconstruction_score = np.mean(reconstruction_indicators) if reconstruction_indicators else 0.0
            
            return {
                'reconstruction_score': float(reconstruction_score),
                'reconstructed_memories': len(reconstruction_indicators),
                'average_reconstruction_ratio': float(reconstruction_score)
            }
        except Exception as e:
            return {'reconstruction_score': 0.5, 'error': str(e)}

    def _analyze_cognitive_dissonance_resolution(self, prediction_data, outcome_data):
        """åˆ†æè®¤çŸ¥å¤±è°ƒè§£å†³"""
        try:
            dissonance_resolutions = []
            
            for prediction, outcome in zip(prediction_data, outcome_data):
                original_confidence = prediction.get('original_confidence', 0.5)
                actual_outcome = outcome.get('success', False)
                post_outcome_explanation = prediction.get('post_outcome_explanation', '')
                
                # é«˜ç½®ä¿¡åº¦é¢„æµ‹ä½†ç»“æœç›¸åæ—¶çš„è®¤çŸ¥å¤±è°ƒ
                if ((original_confidence > 0.7 and not actual_outcome) or 
                    (original_confidence < 0.3 and actual_outcome)):
                    
                    # æ£€æŸ¥æ˜¯å¦é€šè¿‡ä¿®æ”¹è®°å¿†æˆ–è§£é‡Šæ¥è§£å†³å¤±è°ƒ
                    if post_outcome_explanation:
                        dissonance_resolution_score = len(post_outcome_explanation) / 100  # ç®€åŒ–è¯„ä¼°
                        dissonance_resolutions.append(min(dissonance_resolution_score, 1.0))
            
            dissonance_score = np.mean(dissonance_resolutions) if dissonance_resolutions else 0.0
            
            return {
                'dissonance_score': float(dissonance_score),
                'dissonance_cases': len(dissonance_resolutions),
                'resolution_patterns': dissonance_resolutions
            }
        except Exception as e:
            return {'dissonance_score': 0.5, 'error': str(e)}

    def _analyze_temporal_distance_effect(self, prediction_data, outcome_data):
        """åˆ†ææ—¶é—´è·ç¦»æ•ˆåº”"""
        try:
            temporal_effects = []
            
            for prediction, outcome in zip(prediction_data, outcome_data):
                prediction_time = prediction.get('prediction_timestamp', 0)
                outcome_time = outcome.get('outcome_timestamp', 0)
                recall_time = prediction.get('recall_timestamp', outcome_time)
                
                if prediction_time > 0 and outcome_time > 0 and recall_time > 0:
                    # è®¡ç®—æ—¶é—´è·ç¦»
                    prediction_outcome_distance = outcome_time - prediction_time
                    outcome_recall_distance = recall_time - outcome_time
                    
                    # æ—¶é—´è·ç¦»å¯¹åè§ä¹‹æ˜åå·®çš„å½±å“
                    # è·ç¦»è¶Šè¿‘ï¼Œåå·®å¯èƒ½è¶Šå¼º
                    distance_effect = 1.0 / (1.0 + outcome_recall_distance / 86400)  # æ ‡å‡†åŒ–åˆ°å¤©
                    temporal_effects.append(distance_effect)
            
            distance_effect_score = np.mean(temporal_effects) if temporal_effects else 0.0
            
            return {
                'distance_effect_score': float(distance_effect_score),
                'temporal_cases': len(temporal_effects),
                'average_distance_effect': float(distance_effect_score)
            }
        except Exception as e:
            return {'distance_effect_score': 0.5, 'error': str(e)}

    def _assess_prediction_accuracy(self, prediction_data, outcome_data):
        """è¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§"""
        try:
            accuracies = []
            
            for prediction, outcome in zip(prediction_data, outcome_data):
                predicted_probability = prediction.get('original_confidence', 0.5)
                actual_outcome = outcome.get('success', False)
                
                # è®¡ç®—å¸ƒé‡Œå°”å¾—åˆ†ï¼ˆBrier Scoreï¼‰
                brier_score = (predicted_probability - (1.0 if actual_outcome else 0.0)) ** 2
                accuracy = 1.0 - brier_score  # è½¬æ¢ä¸ºå‡†ç¡®æ€§åˆ†æ•°
                accuracies.append(accuracy)
            
            overall_accuracy = np.mean(accuracies) if accuracies else 0.0
            
            return {
                'overall_accuracy': float(overall_accuracy),
                'brier_score': float(1.0 - overall_accuracy),
                'sample_size': len(accuracies),
                'accuracy_variance': float(np.var(accuracies)) if accuracies else 0.0
            }
        except Exception as e:
            return {'overall_accuracy': 0.5, 'error': str(e)}

    def _generate_hindsight_bias_recommendations(self, hindsight_score):
        """ç”Ÿæˆåè§ä¹‹æ˜åå·®æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if hindsight_score > 0.7:
            recommendations.extend([
                "é«˜åº¦åè§ä¹‹æ˜åå·®ï¼šå»ºè®®è®°å½•åŸå§‹é¢„æµ‹å’Œæ¨ç†è¿‡ç¨‹",
                "åœ¨ç»“æœå·²çŸ¥å‰å†™ä¸‹é¢„æµ‹ä¾æ®å’Œä¸ç¡®å®šæ€§",
                "å®šæœŸå›é¡¾é¢„æµ‹è®°å½•ï¼Œå®¢è§‚è¯„ä¼°é¢„æµ‹èƒ½åŠ›",
                "ä½¿ç”¨å¤–éƒ¨è§†è§’è¿›è¡Œé¢„æµ‹éªŒè¯"
            ])
        elif hindsight_score > 0.5:
            recommendations.extend([
                "ä¸­ç­‰åè§ä¹‹æ˜åå·®ï¼šæé«˜å¯¹é¢„æµ‹è¿‡ç¨‹çš„æ„è¯†",
                "é¿å…åœ¨å·²çŸ¥ç»“æœåä¿®æ”¹å¯¹é¢„æµ‹çš„è®°å¿†",
                "åŸ¹å…»æ‰¿è®¤ä¸ç¡®å®šæ€§çš„ä¹ æƒ¯"
            ])
        else:
            recommendations.extend([
                "åè§ä¹‹æ˜åå·®è¾ƒä½ï¼šä¿æŒå®¢è§‚çš„é¢„æµ‹è¯„ä¼°ä¹ æƒ¯",
                "ç»§ç»­å‡†ç¡®è®°å½•å’Œå›é¡¾é¢„æµ‹è¿‡ç¨‹"
            ])
        
        return recommendations
    
    def _analyze_streak_expectation_bias(self, sequence_data, expectation_data):
        """åˆ†æè¿ç»­æˆåŠŸæœŸæœ›åå·®"""
        try:
            bias_indicators = []
            
            # è¯†åˆ«è¿ç»­æˆåŠŸåºåˆ—
            current_streak = 0
            for i, event in enumerate(sequence_data):
                success = event.get('success', False)
                expected_next_probability = event.get('expected_next_probability', 0.5)
                
                if success:
                    current_streak += 1
                else:
                    current_streak = 0
                
                # åœ¨è¿ç»­æˆåŠŸåï¼Œæ£€æŸ¥å¯¹ä¸‹æ¬¡æˆåŠŸçš„æœŸæœ›æ˜¯å¦è¿‡é«˜
                if current_streak >= 2 and i < len(sequence_data) - 1:
                    # æ¯”è¾ƒæœŸæœ›æ¦‚ç‡ä¸åŸºç¡€æ¦‚ç‡
                    base_probability = 0.5  # å‡è®¾åŸºç¡€æˆåŠŸæ¦‚ç‡
                    if expected_next_probability > base_probability * 1.3:  # é«˜å‡º30%
                        expectation_bias = expected_next_probability - base_probability
                        bias_indicators.append(expectation_bias)
            
            bias_score = np.mean(bias_indicators) if bias_indicators else 0.0
            
            return {
                'bias_score': float(min(bias_score, 1.0)),
                'biased_expectations': len(bias_indicators),
                'average_bias_magnitude': float(bias_score)
            }
        except Exception as e:
            return {'bias_score': 0.5, 'error': str(e)}

    def _analyze_sequence_randomness_misjudgment(self, sequence_data):
        """åˆ†æåºåˆ—éšæœºæ€§è¯¯åˆ¤"""
        try:
            misjudgments = []
            
            # è®¡ç®—å®é™…éšæœºæ€§æŒ‡æ ‡
            successes = [event.get('success', False) for event in sequence_data]
            runs = self._count_runs(successes)
            expected_runs = self._calculate_expected_runs(successes)
            
            # è®¡ç®—éšæœºæ€§åç¦»åº¦
            if expected_runs > 0:
                randomness_deviation = abs(runs - expected_runs) / expected_runs
            else:
                randomness_deviation = 0.0
            
            # æ£€æŸ¥ä¸»è§‚éšæœºæ€§åˆ¤æ–­
            for event in sequence_data:
                perceived_randomness = event.get('perceived_randomness', 0.5)
                # å¦‚æœå®é™…åºåˆ—æ¯”è¾ƒéšæœºä½†è¢«è®¤ä¸ºä¸éšæœºï¼Œæˆ–ç›¸å
                if randomness_deviation < 0.2:  # å®é™…æ¯”è¾ƒéšæœº
                    if perceived_randomness < 0.4:  # ä½†è¢«è®¤ä¸ºä¸éšæœº
                        misjudgments.append(0.4 - perceived_randomness)
                elif randomness_deviation > 0.5:  # å®é™…ä¸å¤ªéšæœº
                    if perceived_randomness > 0.6:  # ä½†è¢«è®¤ä¸ºéšæœº
                        misjudgments.append(perceived_randomness - 0.6)
            
            misjudgment_score = np.mean(misjudgments) if misjudgments else 0.0
            
            return {
                'misjudgment_score': float(min(misjudgment_score, 1.0)),
                'actual_randomness_deviation': float(randomness_deviation),
                'misjudgment_cases': len(misjudgments)
            }
        except Exception as e:
            return {'misjudgment_score': 0.5, 'error': str(e)}

    def _count_runs(self, binary_sequence):
        """è®¡ç®—æ¸¸ç¨‹æ•°"""
        if not binary_sequence:
            return 0
        
        runs = 1
        for i in range(1, len(binary_sequence)):
            if binary_sequence[i] != binary_sequence[i-1]:
                runs += 1
        return runs

    def _calculate_expected_runs(self, binary_sequence):
        """è®¡ç®—æœŸæœ›æ¸¸ç¨‹æ•°"""
        if not binary_sequence:
            return 0
        
        n = len(binary_sequence)
        n1 = sum(binary_sequence)  # æˆåŠŸæ¬¡æ•°
        n0 = n - n1  # å¤±è´¥æ¬¡æ•°
        
        if n1 == 0 or n0 == 0:
            return 1
        
        expected_runs = (2 * n1 * n0) / n + 1
        return expected_runs

    def _analyze_pattern_over_recognition(self, sequence_data):
        """åˆ†ææ¨¡å¼è¿‡åº¦è¯†åˆ«"""
        try:
            pattern_recognitions = []
            
            for i in range(len(sequence_data) - 2):
                # æ£€æŸ¥æ˜¯å¦è¯†åˆ«å‡ºä¸å­˜åœ¨çš„æ¨¡å¼
                current_pattern = [
                    sequence_data[i].get('success', False),
                    sequence_data[i+1].get('success', False),
                    sequence_data[i+2].get('success', False)
                ]
                
                pattern_confidence = sequence_data[i+2].get('pattern_confidence', 0.0)
                
                # å¦‚æœå¯¹éšæœºåºåˆ—æœ‰é«˜æ¨¡å¼è¯†åˆ«ä¿¡å¿ƒ
                if pattern_confidence > 0.6:
                    # æ£€æŸ¥æ¨¡å¼æ˜¯å¦çœŸçš„å­˜åœ¨ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
                    pattern_strength = self._assess_pattern_strength(current_pattern)
                    if pattern_strength < 0.3:  # æ¨¡å¼ä¸å¼ºä½†æœ‰é«˜ä¿¡å¿ƒ
                        over_recognition = pattern_confidence - pattern_strength
                        pattern_recognitions.append(over_recognition)
            
            over_recognition_score = np.mean(pattern_recognitions) if pattern_recognitions else 0.0
            
            return {
                'over_recognition_score': float(min(over_recognition_score, 1.0)),
                'over_recognition_cases': len(pattern_recognitions),
                'average_over_confidence': float(over_recognition_score)
            }
        except Exception as e:
            return {'over_recognition_score': 0.5, 'error': str(e)}

    def _assess_pattern_strength(self, pattern):
        """è¯„ä¼°æ¨¡å¼å¼ºåº¦"""
        # ç®€åŒ–çš„æ¨¡å¼å¼ºåº¦è¯„ä¼°
        if not pattern or len(pattern) < 2:
            return 0.0
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        if all(x == pattern[0] for x in pattern):
            return 1.0  # å®Œå…¨ä¸€è‡´
        elif len(set(pattern)) == len(pattern):
            return 0.0  # å®Œå…¨éšæœº
        else:
            return 0.5  # éƒ¨åˆ†æ¨¡å¼

    def _analyze_conditional_probability_misestimation(self, sequence_data):
        """åˆ†ææ¡ä»¶æ¦‚ç‡è¯¯ä¼°"""
        try:
            misestimations = []
            
            for i in range(1, len(sequence_data)):
                previous_success = sequence_data[i-1].get('success', False)
                estimated_next_probability = sequence_data[i-1].get('estimated_next_probability', 0.5)
                actual_next_success = sequence_data[i].get('success', False)
                
                # åŸºç¡€æ¦‚ç‡ï¼ˆå‡è®¾ä¸º0.5ï¼‰
                base_probability = 0.5
                
                # æ£€æŸ¥æ¡ä»¶æ¦‚ç‡ä¼°è®¡æ˜¯å¦åç¦»åŸºç¡€æ¦‚ç‡
                if previous_success:
                    # å‰ä¸€æ¬¡æˆåŠŸåï¼Œä¼°è®¡ä¸‹æ¬¡æˆåŠŸæ¦‚ç‡çš„åå·®
                    estimation_bias = abs(estimated_next_probability - base_probability)
                    if estimation_bias > 0.1:  # æ˜¾è‘—åå·®
                        misestimations.append(estimation_bias)
            
            error_score = np.mean(misestimations) if misestimations else 0.0
            
            return {
                'error_score': float(min(error_score, 1.0)),
                'misestimation_cases': len(misestimations),
                'average_error_magnitude': float(error_score)
            }
        except Exception as e:
            return {'error_score': 0.5, 'error': str(e)}

    def _analyze_regression_neglect_in_streaks(self, sequence_data, performance_data):
        """åˆ†æè¿ç»­æˆåŠŸåçš„å›å½’å¹³å‡å¿½è§†"""
        try:
            if not performance_data:
                return {'neglect_score': 0.5, 'note': 'æ— è¡¨ç°æ•°æ®'}
            
            neglect_indicators = []
            
            current_streak = 0
            for i, (event, performance) in enumerate(zip(sequence_data, performance_data)):
                success = event.get('success', False)
                performance_score = performance.get('score', 0.5)
                expected_next_performance = performance.get('expected_next_performance', 0.5)
                
                if success:
                    current_streak += 1
                else:
                    current_streak = 0
                
                # åœ¨è¿ç»­æˆåŠŸåï¼Œæ£€æŸ¥æ˜¯å¦å¿½è§†å›å½’å¹³å‡
                if current_streak >= 3:
                    # æœŸæœ›è¡¨ç°åº”è¯¥å›å½’å¹³å‡å€¼
                    mean_performance = 0.5  # å‡è®¾å¹³å‡è¡¨ç°
                    if expected_next_performance > mean_performance * 1.2:  # æœŸæœ›è¿‡é«˜
                        neglect_score = expected_next_performance - mean_performance
                        neglect_indicators.append(neglect_score)
            
            neglect_score = np.mean(neglect_indicators) if neglect_indicators else 0.0
            
            return {
                'neglect_score': float(min(neglect_score, 1.0)),
                'regression_neglect_cases': len(neglect_indicators),
                'average_neglect_magnitude': float(neglect_score)
            }
        except Exception as e:
            return {'neglect_score': 0.5, 'error': str(e)}

    def _analyze_causal_attribution_in_sequences(self, sequence_data, performance_data):
        """åˆ†æåºåˆ—ä¸­çš„å› æœå…³ç³»é”™è¯¯å½’å› """
        try:
            attribution_errors = []
            
            for i in range(1, len(sequence_data)):
                previous_success = sequence_data[i-1].get('success', False)
                current_success = sequence_data[i].get('success', False)
                causal_attribution = sequence_data[i].get('causal_attribution', 0.0)
                
                # æ£€æŸ¥æ˜¯å¦é”™è¯¯åœ°å°†å‰æ¬¡ç»“æœå½’å› ä¸ºå½“å‰ç»“æœçš„åŸå› 
                if previous_success and current_success and causal_attribution > 0.5:
                    # é«˜å› æœå½’å› åœ¨ç‹¬ç«‹äº‹ä»¶ä¸­æ˜¯é”™è¯¯çš„
                    attribution_errors.append(causal_attribution)
                elif not previous_success and not current_success and causal_attribution > 0.5:
                    # è¿ç»­å¤±è´¥çš„å› æœå½’å› ä¹Ÿå¯èƒ½æ˜¯é”™è¯¯çš„
                    attribution_errors.append(causal_attribution)
            
            attribution_error_score = np.mean(attribution_errors) if attribution_errors else 0.0
            
            return {
                'attribution_error_score': float(min(attribution_error_score, 1.0)),
                'error_cases': len(attribution_errors),
                'average_error_strength': float(attribution_error_score)
            }
        except Exception as e:
            return {'attribution_error_score': 0.5, 'error': str(e)}

    def _calculate_sequence_statistics(self, sequence_data):
        """è®¡ç®—åºåˆ—ç»Ÿè®¡ä¿¡æ¯"""
        try:
            successes = [event.get('success', False) for event in sequence_data]
            
            total_events = len(successes)
            success_count = sum(successes)
            success_rate = success_count / total_events if total_events > 0 else 0.0
            
            # è®¡ç®—æœ€é•¿è¿ç»­æˆåŠŸå’Œå¤±è´¥
            max_success_streak = 0
            max_failure_streak = 0
            current_success_streak = 0
            current_failure_streak = 0
            
            for success in successes:
                if success:
                    current_success_streak += 1
                    current_failure_streak = 0
                    max_success_streak = max(max_success_streak, current_success_streak)
                else:
                    current_failure_streak += 1
                    current_success_streak = 0
                    max_failure_streak = max(max_failure_streak, current_failure_streak)
            
            return {
                'total_events': total_events,
                'success_count': success_count,
                'success_rate': float(success_rate),
                'max_success_streak': max_success_streak,
                'max_failure_streak': max_failure_streak,
                'runs_count': self._count_runs(successes)
            }
        except Exception as e:
            return {'error': str(e)}

    def _analyze_streak_patterns(self, sequence_data):
        """åˆ†æè¿ç»­æ¨¡å¼"""
        try:
            successes = [event.get('success', False) for event in sequence_data]
            
            streaks = []
            current_streak = {'type': None, 'length': 0}
            
            for success in successes:
                if current_streak['type'] is None:
                    current_streak = {'type': 'success' if success else 'failure', 'length': 1}
                elif (success and current_streak['type'] == 'success') or (not success and current_streak['type'] == 'failure'):
                    current_streak['length'] += 1
                else:
                    streaks.append(current_streak.copy())
                    current_streak = {'type': 'success' if success else 'failure', 'length': 1}
            
            if current_streak['length'] > 0:
                streaks.append(current_streak)
            
            # åˆ†æè¿ç»­æ¨¡å¼ç»Ÿè®¡
            success_streaks = [s['length'] for s in streaks if s['type'] == 'success']
            failure_streaks = [s['length'] for s in streaks if s['type'] == 'failure']
            
            return {
                'total_streaks': len(streaks),
                'success_streaks': success_streaks,
                'failure_streaks': failure_streaks,
                'avg_success_streak': float(np.mean(success_streaks)) if success_streaks else 0.0,
                'avg_failure_streak': float(np.mean(failure_streaks)) if failure_streaks else 0.0,
                'longest_success_streak': max(success_streaks) if success_streaks else 0,
                'longest_failure_streak': max(failure_streaks) if failure_streaks else 0
            }
        except Exception as e:
            return {'error': str(e)}

    def _generate_hot_hand_fallacy_recommendations(self, hot_hand_score):
        """ç”Ÿæˆçƒ­æ‰‹è°¬è¯¯æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if hot_hand_score > 0.7:
            recommendations.extend([
                "é«˜åº¦çƒ­æ‰‹è°¬è¯¯ï¼šæé†’è‡ªå·±ç‹¬ç«‹äº‹ä»¶ä¸å—å†å²å½±å“",
                "ä½¿ç”¨ç»Ÿè®¡æ€ç»´åˆ†æè¿ç»­äº‹ä»¶",
                "é¿å…åœ¨è¿ç»­æˆåŠŸåè¿‡åº¦è‡ªä¿¡",
                "å»ºç«‹åŸºäºæ¦‚ç‡è€Œéæ¨¡å¼çš„å†³ç­–æ¡†æ¶"
            ])
        elif hot_hand_score > 0.5:
            recommendations.extend([
                "ä¸­ç­‰çƒ­æ‰‹è°¬è¯¯ï¼šå¢å¼ºå¯¹éšæœºæ€§çš„ç†è§£",
                "åœ¨è¿ç»­ç»“æœåä¿æŒå®¢è§‚åˆ¤æ–­",
                "é¿å…è¿‡åº¦è§£è¯»çŸ­æœŸæ¨¡å¼"
            ])
        else:
            recommendations.extend([
                "çƒ­æ‰‹è°¬è¯¯è¾ƒä½ï¼šä¿æŒå¯¹éšæœºäº‹ä»¶çš„ç†æ€§è®¤çŸ¥",
                "ç»§ç»­åŸºäºæ¦‚ç‡è¿›è¡Œå†³ç­–"
            ])
        
        return recommendations

    def _analyze_negative_correlation_expectation(self, sequence_data, probability_estimates):
        """åˆ†æè´Ÿç›¸å…³æœŸæœ›"""
        try:
            correlation_expectations = []
            
            for i in range(1, len(sequence_data)):
                previous_event = sequence_data[i-1].get('success', False)
                current_estimate = probability_estimates[i] if probability_estimates and i < len(probability_estimates) else 0.5
                
                # æ£€æŸ¥åœ¨å‰ä¸€äº‹ä»¶åæ˜¯å¦æœŸæœ›è´Ÿç›¸å…³
                base_probability = 0.5
                
                if previous_event:
                    # å‰æ¬¡æˆåŠŸåï¼Œå¦‚æœæœŸæœ›ä¸‹æ¬¡å¤±è´¥æ¦‚ç‡æ›´é«˜
                    if current_estimate < base_probability * 0.8:  # æœŸæœ›æ˜¾è‘—ä¸‹é™
                        negative_expectation = base_probability - current_estimate
                        correlation_expectations.append(negative_expectation)
                else:
                    # å‰æ¬¡å¤±è´¥åï¼Œå¦‚æœæœŸæœ›ä¸‹æ¬¡æˆåŠŸæ¦‚ç‡æ›´é«˜
                    if current_estimate > base_probability * 1.2:  # æœŸæœ›æ˜¾è‘—ä¸Šå‡
                        negative_expectation = current_estimate - base_probability
                        correlation_expectations.append(negative_expectation)
            
            correlation_score = np.mean(correlation_expectations) if correlation_expectations else 0.0
            
            return {
                'correlation_score': float(min(correlation_score, 1.0)),
                'negative_correlation_cases': len(correlation_expectations),
                'average_expectation_strength': float(correlation_score)
            }
        except Exception as e:
            return {'correlation_score': 0.5, 'error': str(e)}

    def _analyze_mean_reversion_over_expectation(self, sequence_data):
        """åˆ†æå‡å€¼å›å½’è¿‡åº¦æœŸæœ›"""
        try:
            reversion_expectations = []
            
            # å¯»æ‰¾åç¦»å¹³å‡çš„åºåˆ—æ®µ
            successes = [event.get('success', False) for event in sequence_data]
            running_average = []
            cumulative_successes = 0
            
            for i, success in enumerate(successes):
                cumulative_successes += success
                current_average = cumulative_successes / (i + 1)
                running_average.append(current_average)
                
                # å¦‚æœæœ€è¿‘çš„è¡¨ç°åç¦»é•¿æœŸå¹³å‡
                if i >= 4:  # è‡³å°‘5ä¸ªæ•°æ®ç‚¹
                    recent_average = sum(successes[i-4:i+1]) / 5
                    long_term_average = current_average
                    
                    deviation = abs(recent_average - long_term_average)
                    
                    # æ£€æŸ¥æ˜¯å¦è¿‡åº¦æœŸæœ›å›å½’
                    expected_reversion = sequence_data[i].get('expected_reversion', 0.0)
                    if deviation > 0.2 and expected_reversion > 0.7:
                        reversion_expectations.append(expected_reversion)
            
            reversion_score = np.mean(reversion_expectations) if reversion_expectations else 0.0
            
            return {
                'expectation_score': float(min(reversion_score, 1.0)),
                'over_expectation_cases': len(reversion_expectations),
                'average_expectation_strength': float(reversion_score)
            }
        except Exception as e:
            return {'expectation_score': 0.5, 'error': str(e)}

    def _analyze_law_of_small_numbers_misapplication(self, sequence_data):
        """åˆ†æå°æ•°å®šå¾‹é”™è¯¯åº”ç”¨"""
        try:
            misapplications = []
            
            # æ£€æŸ¥åœ¨å°æ ·æœ¬ä¸­æ˜¯å¦è¿‡åº¦ç›¸ä¿¡è§„å¾‹
            for window_size in [3, 5, 7]:
                for i in range(len(sequence_data) - window_size + 1):
                    window = sequence_data[i:i+window_size]
                    window_successes = [event.get('success', False) for event in window]
                    
                    success_rate = sum(window_successes) / len(window_successes)
                    confidence_in_pattern = window[-1].get('pattern_confidence', 0.0)
                    
                    # åœ¨å°æ ·æœ¬ä¸­é«˜åº¦ç›¸ä¿¡æ¨¡å¼
                    if len(window_successes) <= 5 and confidence_in_pattern > 0.7:
                        # æ£€æŸ¥æ¨¡å¼æ˜¯å¦çœŸçš„å¼º
                        if success_rate in [0.0, 1.0]:  # æç«¯ç»“æœ
                            misapplication_score = confidence_in_pattern
                            misapplications.append(misapplication_score)
            
            misapplication_score = np.mean(misapplications) if misapplications else 0.0
            
            return {
                'misapplication_score': float(min(misapplication_score, 1.0)),
                'misapplication_cases': len(misapplications),
                'average_misapplication_strength': float(misapplication_score)
            }
        except Exception as e:
            return {'misapplication_score': 0.5, 'error': str(e)}

    def _analyze_randomness_representativeness(self, sequence_data):
        """åˆ†æéšæœºæ€§ä»£è¡¨æ€§è¯¯åˆ¤"""
        try:
            misjudgments = []
            
            successes = [event.get('success', False) for event in sequence_data]
            
            # æ£€æŸ¥å¯¹"å…¸å‹"éšæœºåºåˆ—çš„è¯¯åˆ¤
            for i in range(len(sequence_data)):
                perceived_randomness = sequence_data[i].get('perceived_randomness', 0.5)
                
                # è®¡ç®—å½“å‰ä½ç½®å‰çš„å±€éƒ¨æ¨¡å¼
                if i >= 2:
                    local_pattern = successes[max(0, i-2):i+1]
                    pattern_variety = len(set(local_pattern))
                    
                    # å¦‚æœå±€éƒ¨æ¨¡å¼å•ä¸€ä½†è¢«è®¤ä¸ºå¾ˆéšæœºï¼Œæˆ–æ¨¡å¼å¤šæ ·ä½†è¢«è®¤ä¸ºä¸éšæœº
                    if pattern_variety == 1 and perceived_randomness > 0.6:
                        misjudgment = perceived_randomness - 0.3
                        misjudgments.append(misjudgment)
                    elif pattern_variety >= 2 and perceived_randomness < 0.4:
                        misjudgment = 0.6 - perceived_randomness
                        misjudgments.append(misjudgment)
            
            misjudgment_score = np.mean(misjudgments) if misjudgments else 0.0
            
            return {
                'misjudgment_score': float(min(misjudgment_score, 1.0)),
                'misjudgment_cases': len(misjudgments),
                'average_misjudgment_strength': float(misjudgment_score)
            }
        except Exception as e:
            return {'misjudgment_score': 0.5, 'error': str(e)}

    def _analyze_independence_violation_illusion(self, sequence_data, probability_estimates):
        """åˆ†æç‹¬ç«‹äº‹ä»¶ç›¸å…³æ€§é”™è§‰"""
        try:
            if not probability_estimates:
                return {'violation_score': 0.5, 'note': 'æ— æ¦‚ç‡ä¼°è®¡æ•°æ®'}
            
            violation_indicators = []
            
            for i in range(1, min(len(sequence_data), len(probability_estimates))):
                previous_outcome = sequence_data[i-1].get('success', False)
                current_estimate = probability_estimates[i]
                base_probability = 0.5
                
                # æ£€æŸ¥æ¦‚ç‡ä¼°è®¡æ˜¯å¦å—å‰ä¸€ç»“æœå½±å“
                estimate_deviation = abs(current_estimate - base_probability)
                
                if estimate_deviation > 0.1:  # æ˜¾è‘—åç¦»åŸºç¡€æ¦‚ç‡
                    # éªŒè¯è¿™ç§åç¦»æ˜¯å¦ä¸å‰ä¸€ç»“æœç›¸å…³
                    if previous_outcome and current_estimate > base_probability:
                        violation_indicators.append(estimate_deviation)
                    elif not previous_outcome and current_estimate < base_probability:
                        violation_indicators.append(estimate_deviation)
            
            violation_score = np.mean(violation_indicators) if violation_indicators else 0.0
            
            return {
                'violation_score': float(min(violation_score, 1.0)),
                'violation_cases': len(violation_indicators),
                'average_violation_strength': float(violation_score)
            }
        except Exception as e:
            return {'violation_score': 0.5, 'error': str(e)}

    def _analyze_betting_behavior_changes(self, sequence_data, betting_data):
        """åˆ†ææŠ•æ³¨è¡Œä¸ºå˜åŒ–"""
        try:
            if not betting_data:
                return {'behavior_change_score': 0.5, 'note': 'æ— æŠ•æ³¨æ•°æ®'}
            
            behavior_changes = []
            
            for i in range(1, min(len(sequence_data), len(betting_data))):
                previous_outcome = sequence_data[i-1].get('success', False)
                previous_bet = betting_data[i-1].get('amount', 0)
                current_bet = betting_data[i].get('amount', 0)
                
                # è®¡ç®—æŠ•æ³¨å˜åŒ–
                if previous_bet > 0:
                    bet_change_ratio = abs(current_bet - previous_bet) / previous_bet
                    
                    # æ£€æŸ¥æŠ•æ³¨å˜åŒ–æ˜¯å¦ä¸å‰ä¸€ç»“æœç›¸å…³ï¼ˆèµŒå¾’è°¬è¯¯çš„è¡¨ç°ï¼‰
                    if previous_outcome and current_bet < previous_bet * 0.8:
                        # èµ¢äº†ä¹‹åå‡å°‘æŠ•æ³¨ï¼ˆæœŸæœ›è¿æ°”ä¼šè½¬åï¼‰
                        behavior_changes.append(bet_change_ratio)
                    elif not previous_outcome and current_bet > previous_bet * 1.2:
                        # è¾“äº†ä¹‹åå¢åŠ æŠ•æ³¨ï¼ˆæœŸæœ›è¿æ°”ä¼šè½¬å¥½ï¼‰
                        behavior_changes.append(bet_change_ratio)
            
            behavior_change_score = np.mean(behavior_changes) if behavior_changes else 0.0
            
            return {
                'behavior_change_score': float(min(behavior_change_score, 1.0)),
                'significant_changes': len(behavior_changes),
                'average_change_magnitude': float(behavior_change_score)
            }
        except Exception as e:
            return {'behavior_change_score': 0.5, 'error': str(e)}

    def _analyze_sequence_patterns(self, sequence_data):
        """åˆ†æåºåˆ—æ¨¡å¼"""
        try:
            successes = [event.get('success', False) for event in sequence_data]
            
            # åŸºæœ¬ç»Ÿè®¡
            total_events = len(successes)
            success_count = sum(successes)
            
            # è½¬æ¢ç‚¹åˆ†æ
            transitions = []
            for i in range(1, len(successes)):
                if successes[i] != successes[i-1]:
                    transitions.append(i)
            
            # å‘¨æœŸæ€§åˆ†æï¼ˆç®€åŒ–ï¼‰
            periodicity_scores = []
            for period in [2, 3, 4, 5]:
                if total_events >= period * 2:
                    period_matches = 0
                    period_comparisons = 0
                    
                    for i in range(period, total_events):
                        if successes[i] == successes[i - period]:
                            period_matches += 1
                        period_comparisons += 1
                    
                    if period_comparisons > 0:
                        periodicity = period_matches / period_comparisons
                        periodicity_scores.append(periodicity)
            
            return {
                'total_events': total_events,
                'success_rate': float(success_count / total_events) if total_events > 0 else 0.0,
                'transition_count': len(transitions),
                'transition_rate': float(len(transitions) / max(total_events - 1, 1)),
                'periodicity_scores': periodicity_scores,
                'max_periodicity': float(max(periodicity_scores)) if periodicity_scores else 0.0
            }
        except Exception as e:
            return {'error': str(e)}

    def _assess_probability_calibration(self, sequence_data, probability_estimates):
        """è¯„ä¼°æ¦‚ç‡æ ¡å‡†"""
        try:
            if not probability_estimates:
                return {'calibration_score': 0.5, 'note': 'æ— æ¦‚ç‡ä¼°è®¡æ•°æ®'}
            
            calibration_errors = []
            
            for i, (event, estimate) in enumerate(zip(sequence_data, probability_estimates)):
                actual_outcome = 1.0 if event.get('success', False) else 0.0
                predicted_probability = estimate
                
                # è®¡ç®—å¸ƒé‡Œå°”å¾—åˆ†
                brier_score = (predicted_probability - actual_outcome) ** 2
                calibration_errors.append(brier_score)
            
            average_brier_score = np.mean(calibration_errors) if calibration_errors else 0.25
            calibration_score = 1.0 - average_brier_score  # è½¬æ¢ä¸ºæ ¡å‡†å¾—åˆ†
            
            return {
                'calibration_score': float(max(0.0, calibration_score)),
                'average_brier_score': float(average_brier_score),
                'sample_size': len(calibration_errors),
                'calibration_variance': float(np.var(calibration_errors)) if calibration_errors else 0.0
            }
        except Exception as e:
            return {'calibration_score': 0.5, 'error': str(e)}

    def _generate_gamblers_fallacy_recommendations(self, gamblers_fallacy_score):
        """ç”ŸæˆèµŒå¾’è°¬è¯¯æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if gamblers_fallacy_score > 0.7:
            recommendations.extend([
                "é«˜åº¦èµŒå¾’è°¬è¯¯ï¼šå¼ºåŒ–ç‹¬ç«‹äº‹ä»¶æ¦‚å¿µç†è§£",
                "æé†’è‡ªå·±æ¯æ¬¡äº‹ä»¶éƒ½æ˜¯ç‹¬ç«‹çš„",
                "é¿å…åŸºäºå†å²ç»“æœè°ƒæ•´ç­–ç•¥",
                "å­¦ä¹ æ¦‚ç‡è®ºåŸºç¡€çŸ¥è¯†",
                "ä½¿ç”¨å®¢è§‚æ•°æ®è€Œéç›´è§‰è¿›è¡Œå†³ç­–"
            ])
        elif gamblers_fallacy_score > 0.5:
            recommendations.extend([
                "ä¸­ç­‰èµŒå¾’è°¬è¯¯ï¼šæé«˜å¯¹éšæœºæ€§çš„è®¤çŸ¥",
                "åœ¨è¿ç»­ç»“æœåä¿æŒå†·é™",
                "é¿å…'è¡¥å¿'å¿ƒç†"
            ])
        else:
            recommendations.extend([
                "èµŒå¾’è°¬è¯¯è¾ƒä½ï¼šä¿æŒå¯¹ç‹¬ç«‹äº‹ä»¶çš„æ­£ç¡®ç†è§£",
                "ç»§ç»­åŸºäºå®¢è§‚æ¦‚ç‡è¿›è¡Œåˆ¤æ–­"
            ])
        
        return recommendations
    
    def _detect_confirmation_bias_advanced(self, decision_data, prior_beliefs=None, evidence_weights=None):
        """
        é«˜çº§ç¡®è®¤åå·®æ£€æµ‹ï¼ˆAdvanced Confirmation Bias Detectionï¼‰
        
        Args:
            decision_data: å†³ç­–æ•°æ®ï¼ŒåŒ…å«é€‰æ‹©å’Œç»“æœ
            prior_beliefs: å…ˆéªŒä¿¡å¿µæ•°æ® (å¯é€‰)
            evidence_weights: è¯æ®æƒé‡æ•°æ® (å¯é€‰)
        
        Returns:
            dict: ç¡®è®¤åå·®æ£€æµ‹ç»“æœ
            {
                'bias_score': float,           # åå·®å¾—åˆ† (0-1)
                'bias_strength': str,          # åå·®å¼ºåº¦æè¿°
                'bias_indicators': dict,       # å…·ä½“åå·®æŒ‡æ ‡
                'evidence_selectivity': dict,  # è¯æ®é€‰æ‹©æ€§åˆ†æ
                'belief_updating': dict,       # ä¿¡å¿µæ›´æ–°åˆ†æ
                'recommendations': list        # æ”¹è¿›å»ºè®®
            }
        """
        try:
            import numpy as np
            from collections import defaultdict
            
            # æ•°æ®éªŒè¯
            if not decision_data or len(decision_data) < 3:
                return {'error': 'éœ€è¦è‡³å°‘3ä¸ªå†³ç­–æ•°æ®ç‚¹è¿›è¡Œç¡®è®¤åå·®åˆ†æ'}
            
            # åˆå§‹åŒ–åˆ†æç»“æœ
            bias_indicators = {}
            evidence_selectivity = {}
            belief_updating = {}
            
            # 1. è¯æ®é€‰æ‹©æ€§åå·®åˆ†æ
            evidence_selectivity = self._analyze_evidence_selectivity(decision_data, prior_beliefs)
        
            # 2. ä¿¡æ¯å¤„ç†åå·®åˆ†æ
            information_processing_bias = self._analyze_information_processing_bias(decision_data)
            
            # 3. ä¿¡å¿µæŒç»­æ€§åˆ†æ
            belief_persistence = self._analyze_belief_persistence(decision_data, prior_beliefs)
            
            # 4. åé©³è¯æ®å¿½è§†åˆ†æ
            contradictory_evidence_neglect = self._analyze_contradictory_evidence_neglect(
                decision_data, evidence_weights
            )
            
            # 5. ç»¼åˆåå·®æŒ‡æ ‡
            bias_indicators = {
                'evidence_selectivity_score': evidence_selectivity.get('selectivity_score', 0.5),
                'information_processing_bias_score': information_processing_bias.get('bias_score', 0.5),
                'belief_persistence_score': belief_persistence.get('persistence_score', 0.5),
                'contradictory_neglect_score': contradictory_evidence_neglect.get('neglect_score', 0.5)
            }
            
            # 6. è®¡ç®—ç»¼åˆç¡®è®¤åå·®å¾—åˆ†
            bias_score = np.mean(list(bias_indicators.values()))
            
            # 7. ç¡®å®šåå·®å¼ºåº¦
            bias_strength = self._determine_bias_strength(bias_score)
            
            # 8. ä¿¡å¿µæ›´æ–°è´¨é‡åˆ†æ
            belief_updating = self._analyze_belief_updating_quality(decision_data, prior_beliefs)
            
            # 9. ç”Ÿæˆæ”¹è¿›å»ºè®®
            recommendations = self._generate_confirmation_bias_recommendations(
                bias_score, bias_indicators, evidence_selectivity, belief_updating
            )
        
            return {
                'bias_score': float(bias_score),
                'bias_strength': bias_strength,
                'bias_indicators': bias_indicators,
                'evidence_selectivity': evidence_selectivity,
                'information_processing_analysis': information_processing_bias,
                'belief_persistence_analysis': belief_persistence,
                'contradictory_evidence_analysis': contradictory_evidence_neglect,
                'belief_updating': belief_updating,
                'statistical_significance': self._test_bias_significance(bias_indicators),
                'mitigation_strategies': self._suggest_bias_mitigation_strategies(bias_score),
                'recommendations': recommendations,
                'analysis_metadata': {
                    'sample_size': len(decision_data),
                    'analysis_date': self._get_current_timestamp(),
                    'confidence_level': self._calculate_analysis_confidence(len(decision_data))
                }
            }
            
        except Exception as e:
            return {'error': f'é«˜çº§ç¡®è®¤åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}

    def _detect_anchoring_bias(self, decision_sequence, initial_anchors=None):
        """
        é”šå®šåå·®æ£€æµ‹ï¼ˆAnchoring Bias Detectionï¼‰
        
        Args:
            decision_sequence: å†³ç­–åºåˆ—æ•°æ®
            initial_anchors: åˆå§‹é”šç‚¹æ•°æ® (å¯é€‰)
        
        Returns:
            dict: é”šå®šåå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            if len(decision_sequence) < 3:
                return {'error': 'éœ€è¦è‡³å°‘3ä¸ªå†³ç­–ç‚¹è¿›è¡Œé”šå®šåå·®åˆ†æ'}
            
            # è¯†åˆ«æ½œåœ¨é”šç‚¹
            potential_anchors = self._identify_potential_anchors(decision_sequence, initial_anchors)
            
            # è®¡ç®—é”šå®šæ•ˆåº”å¼ºåº¦
            anchoring_effects = []
            for anchor in potential_anchors:
                effect_strength = self._calculate_anchoring_effect(decision_sequence, anchor)
                anchoring_effects.append(effect_strength)
            
            # åˆ†æè°ƒæ•´ä¸å……åˆ†
            insufficient_adjustment = self._analyze_insufficient_adjustment(
                decision_sequence, potential_anchors
            )
            
            # è®¡ç®—æ€»ä½“é”šå®šåå·®å¾—åˆ†
            anchoring_score = np.mean(anchoring_effects) if anchoring_effects else 0.5
        
            return {
                'anchoring_score': float(anchoring_score),
                'anchoring_strength': self._interpret_anchoring_strength(anchoring_score),
                'identified_anchors': potential_anchors,
                'anchoring_effects': anchoring_effects,
                'insufficient_adjustment_analysis': insufficient_adjustment,
                'temporal_pattern': self._analyze_temporal_anchoring_pattern(decision_sequence),
                'recommendations': self._generate_anchoring_bias_recommendations(anchoring_score)
            }
            
        except Exception as e:
            return {'error': f'é”šå®šåå·®æ£€æµ‹å¤±è´¥: {str(e)}'}

    def _detect_availability_heuristic_bias(self, memory_data, decision_data, recency_weights=None):
        """
        å¯å¾—æ€§å¯å‘å¼åå·®æ£€æµ‹ï¼ˆAvailability Heuristic Bias Detectionï¼‰
        
        Args:
            memory_data: è®°å¿†/ç»éªŒæ•°æ®
            decision_data: å†³ç­–æ•°æ®
            recency_weights: è¿‘æœŸæ€§æƒé‡ (å¯é€‰)
        
        Returns:
            dict: å¯å¾—æ€§å¯å‘å¼åå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            # åˆ†æè®°å¿†å¯å¾—æ€§å¯¹å†³ç­–çš„å½±å“
            memory_influence = self._analyze_memory_influence_on_decisions(memory_data, decision_data)
            
            # æ£€æµ‹è¿‘æœŸæ€§æ•ˆåº”
            recency_effect = self._detect_recency_effect(memory_data, decision_data, recency_weights)
            
            # åˆ†æç”ŸåŠ¨æ€§åå·®
            vividness_bias = self._analyze_vividness_bias(memory_data, decision_data)
            
            # é¢‘ç‡ä¸æ¦‚ç‡ä¼°è®¡åå·®
            frequency_probability_bias = self._analyze_frequency_probability_bias(
                memory_data, decision_data
            )
            
            # è®¡ç®—ç»¼åˆå¯å¾—æ€§åå·®å¾—åˆ†
            availability_components = [
                memory_influence.get('influence_score', 0.5),
                recency_effect.get('effect_strength', 0.5),
                vividness_bias.get('bias_score', 0.5),
                frequency_probability_bias.get('bias_score', 0.5)
            ]
            
            availability_score = np.mean(availability_components)
            
            return {
                'availability_bias_score': float(availability_score),
                'bias_strength': self._interpret_availability_bias_strength(availability_score),
                'memory_influence_analysis': memory_influence,
                'recency_effect_analysis': recency_effect,
                'vividness_bias_analysis': vividness_bias,
                'frequency_probability_analysis': frequency_probability_bias,
                'cognitive_shortcuts_detected': self._identify_cognitive_shortcuts(decision_data),
                'recommendations': self._generate_availability_bias_recommendations(availability_score)
            }
            
        except Exception as e:
            return {'error': f'å¯å¾—æ€§å¯å‘å¼åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}

    def _detect_representativeness_heuristic_bias(self, categorization_data, base_rate_data=None):
        """
        ä»£è¡¨æ€§å¯å‘å¼åå·®æ£€æµ‹ï¼ˆRepresentativeness Heuristic Bias Detectionï¼‰
        
        Args:
            categorization_data: åˆ†ç±»/åˆ¤æ–­æ•°æ®
            base_rate_data: åŸºç¡€æ¦‚ç‡æ•°æ® (å¯é€‰)
        
        Returns:
            dict: ä»£è¡¨æ€§å¯å‘å¼åå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            # åŸºç¡€æ¦‚ç‡å¿½è§†åˆ†æ
            base_rate_neglect = self._analyze_base_rate_neglect(categorization_data, base_rate_data)
            
            # æ ·æœ¬å¤§å°å¿½è§†åˆ†æ
            sample_size_neglect = self._analyze_sample_size_neglect(categorization_data)
            
            # å›å½’å¹³å‡å¿½è§†åˆ†æ
            regression_to_mean_neglect = self._analyze_regression_to_mean_neglect(categorization_data)
            
            # è”åˆäº‹ä»¶æ¦‚ç‡è¯¯åˆ¤åˆ†æ
            conjunction_fallacy = self._analyze_conjunction_fallacy(categorization_data)
            
            # è®¡ç®—ç»¼åˆä»£è¡¨æ€§åå·®å¾—åˆ†
            representativeness_components = [
                base_rate_neglect.get('neglect_score', 0.5),
                sample_size_neglect.get('neglect_score', 0.5),
                regression_to_mean_neglect.get('neglect_score', 0.5),
                conjunction_fallacy.get('fallacy_score', 0.5)
            ]
            
            representativeness_score = np.mean(representativeness_components)
        
            return {
                'representativeness_bias_score': float(representativeness_score),
                'bias_strength': self._interpret_representativeness_bias_strength(representativeness_score),
                'base_rate_neglect_analysis': base_rate_neglect,
                'sample_size_neglect_analysis': sample_size_neglect,
                'regression_to_mean_analysis': regression_to_mean_neglect,
                'conjunction_fallacy_analysis': conjunction_fallacy,
                'stereotype_reliance': self._analyze_stereotype_reliance(categorization_data),
                'recommendations': self._generate_representativeness_bias_recommendations(representativeness_score)
            }
            
        except Exception as e:
            return {'error': f'ä»£è¡¨æ€§å¯å‘å¼åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}

    def _detect_overconfidence_bias(self, confidence_ratings, actual_performance, calibration_data=None):
        """
        è¿‡åº¦è‡ªä¿¡åå·®æ£€æµ‹ï¼ˆOverconfidence Bias Detectionï¼‰
        
        Args:
            confidence_ratings: ç½®ä¿¡åº¦è¯„åˆ†æ•°æ®
            actual_performance: å®é™…è¡¨ç°æ•°æ®
            calibration_data: æ ¡å‡†æ•°æ® (å¯é€‰)
        
        Returns:
            dict: è¿‡åº¦è‡ªä¿¡åå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            # æ ¡å‡†åº¦åˆ†æ (Calibration)
            calibration_analysis = self._analyze_confidence_calibration(
                confidence_ratings, actual_performance
            )
        
            # è¿‡åº¦ç¡®å®šæ€§åˆ†æ (Overconfidence)
            overconfidence_analysis = self._analyze_overconfidence_levels(
                confidence_ratings, actual_performance
            )
            
            # å›°éš¾åº¦æ•ˆåº”åˆ†æ
            difficulty_effect = self._analyze_difficulty_effect_on_confidence(
                confidence_ratings, actual_performance
            )
            
            # å…ƒè®¤çŸ¥å‡†ç¡®æ€§åˆ†æ
            metacognitive_accuracy = self._analyze_metacognitive_accuracy(
                confidence_ratings, actual_performance
            )
            
            # è®¡ç®—è¿‡åº¦è‡ªä¿¡åå·®å¾—åˆ†
            overconfidence_score = self._calculate_overconfidence_score(
                calibration_analysis, overconfidence_analysis
            )
            
            return {
                'overconfidence_bias_score': float(overconfidence_score),
                'bias_strength': self._interpret_overconfidence_strength(overconfidence_score),
                'calibration_analysis': calibration_analysis,
                'overconfidence_analysis': overconfidence_analysis,
                'difficulty_effect_analysis': difficulty_effect,
                'metacognitive_accuracy_analysis': metacognitive_accuracy,
                'confidence_intervals': self._calculate_confidence_intervals(confidence_ratings),
                'better_than_average_effect': self._detect_better_than_average_effect(confidence_ratings),
                'recommendations': self._generate_overconfidence_bias_recommendations(overconfidence_score)
            }
            
        except Exception as e:
            return {'error': f'è¿‡åº¦è‡ªä¿¡åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}

    def _detect_loss_aversion_bias(self, choice_data, gains_losses_data, reference_points=None):
        """
        æŸå¤±åŒæ¶åå·®æ£€æµ‹ï¼ˆLoss Aversion Bias Detectionï¼‰
        
        Args:
            choice_data: é€‰æ‹©è¡Œä¸ºæ•°æ®
            gains_losses_data: æ”¶ç›ŠæŸå¤±æ•°æ®
            reference_points: å‚è€ƒç‚¹æ•°æ® (å¯é€‰)
        
        Returns:
            dict: æŸå¤±åŒæ¶åå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            # æŸå¤±åŒæ¶ç³»æ•°è®¡ç®—
            loss_aversion_coefficient = self._calculate_loss_aversion_coefficient(
                choice_data, gains_losses_data
            )
            
            # å‚è€ƒç‚¹ä¾èµ–åˆ†æ
            reference_point_dependence = self._analyze_reference_point_dependence(
                choice_data, reference_points
            )
            
            # ç¦€èµ‹æ•ˆåº”åˆ†æ
            endowment_effect = self._analyze_endowment_effect(choice_data)
            
            # æ¡†æ¶æ•ˆåº”åˆ†æ
            framing_effect = self._analyze_framing_effect(choice_data, gains_losses_data)
            
            # é£é™©åå¥½ä¸ä¸€è‡´æ€§åˆ†æ
            risk_preference_inconsistency = self._analyze_risk_preference_inconsistency(
                choice_data, gains_losses_data
            )
        
            # è®¡ç®—ç»¼åˆæŸå¤±åŒæ¶åå·®å¾—åˆ†
            loss_aversion_score = self._calculate_comprehensive_loss_aversion_score(
                loss_aversion_coefficient, reference_point_dependence, endowment_effect
            )
            
            return {
                'loss_aversion_bias_score': float(loss_aversion_score),
                'bias_strength': self._interpret_loss_aversion_strength(loss_aversion_score),
                'loss_aversion_coefficient': loss_aversion_coefficient,
                'reference_point_analysis': reference_point_dependence,
                'endowment_effect_analysis': endowment_effect,
                'framing_effect_analysis': framing_effect,
                'risk_preference_analysis': risk_preference_inconsistency,
                'prospect_theory_fit': self._assess_prospect_theory_fit(choice_data, gains_losses_data),
                'recommendations': self._generate_loss_aversion_bias_recommendations(loss_aversion_score)
            }
            
        except Exception as e:
            return {'error': f'æŸå¤±åŒæ¶åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}

    def _detect_sunk_cost_fallacy(self, investment_data, decision_points, cost_information):
        """
        æ²‰æ²¡æˆæœ¬è°¬è¯¯æ£€æµ‹ï¼ˆSunk Cost Fallacy Detectionï¼‰
        
        Args:
            investment_data: æŠ•èµ„/æŠ•å…¥æ•°æ®
            decision_points: å†³ç­–ç‚¹æ•°æ®
            cost_information: æˆæœ¬ä¿¡æ¯
        
        Returns:
            dict: æ²‰æ²¡æˆæœ¬è°¬è¯¯æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            # æ²‰æ²¡æˆæœ¬å½±å“åˆ†æ
            sunk_cost_influence = self._analyze_sunk_cost_influence(
                investment_data, decision_points, cost_information
            )
            
            # ç»§ç»­æŠ•èµ„å€¾å‘åˆ†æ
            escalation_of_commitment = self._analyze_escalation_of_commitment(
                investment_data, decision_points
            )
            
            # ç†æ€§å†³ç­–åç¦»åˆ†æ
            rational_decision_deviation = self._analyze_rational_decision_deviation(
                investment_data, decision_points, cost_information
            )
            
            # æ—¶é—´æŠ•å…¥æ•ˆåº”åˆ†æ
            time_investment_effect = self._analyze_time_investment_effect(
                investment_data, decision_points
            )
        
            # è®¡ç®—æ²‰æ²¡æˆæœ¬è°¬è¯¯å¾—åˆ†
            sunk_cost_fallacy_score = self._calculate_sunk_cost_fallacy_score(
                sunk_cost_influence, escalation_of_commitment, rational_decision_deviation
            )
            
            return {
                'sunk_cost_fallacy_score': float(sunk_cost_fallacy_score),
                'fallacy_strength': self._interpret_sunk_cost_fallacy_strength(sunk_cost_fallacy_score),
                'sunk_cost_influence_analysis': sunk_cost_influence,
                'escalation_of_commitment_analysis': escalation_of_commitment,
                'rational_deviation_analysis': rational_decision_deviation,
                'time_investment_analysis': time_investment_effect,
                'cost_sensitivity_analysis': self._analyze_cost_sensitivity(cost_information),
                'recommendations': self._generate_sunk_cost_fallacy_recommendations(sunk_cost_fallacy_score)
            }
            
        except Exception as e:
            return {'error': f'æ²‰æ²¡æˆæœ¬è°¬è¯¯æ£€æµ‹å¤±è´¥: {str(e)}'}

    def _detect_groupthink_bias(self, group_decision_data, individual_preferences=None, dissent_data=None):
        """
        ç¾¤ä½“æ€ç»´åå·®æ£€æµ‹ï¼ˆGroupthink Bias Detectionï¼‰
        
        Args:
            group_decision_data: ç¾¤ä½“å†³ç­–æ•°æ®
            individual_preferences: ä¸ªä½“åå¥½æ•°æ® (å¯é€‰)
            dissent_data: å¼‚è®®è¡¨è¾¾æ•°æ® (å¯é€‰)
        
        Returns:
            dict: ç¾¤ä½“æ€ç»´åå·®æ£€æµ‹ç»“æœ
        """
        try:
            import numpy as np
            
            # ä¸€è‡´æ€§å‹åŠ›åˆ†æ
            conformity_pressure = self._analyze_conformity_pressure(
                group_decision_data, individual_preferences
            )
            
            # å¼‚è®®æŠ‘åˆ¶åˆ†æ
            dissent_suppression = self._analyze_dissent_suppression(
                group_decision_data, dissent_data
            )
            
            # ä¿¡æ¯å¤šæ ·æ€§åˆ†æ
            information_diversity = self._analyze_information_diversity(group_decision_data)
            
            # ç¾¤ä½“æåŒ–åˆ†æ
            group_polarization = self._analyze_group_polarization(
                group_decision_data, individual_preferences
            )
            
            # é¢†å¯¼è€…å½±å“åˆ†æ
            leader_influence = self._analyze_leader_influence(group_decision_data)
        
            # è®¡ç®—ç¾¤ä½“æ€ç»´åå·®å¾—åˆ†
            groupthink_score = self._calculate_groupthink_score(
                conformity_pressure, dissent_suppression, information_diversity
            )
            
            return {
                'groupthink_bias_score': float(groupthink_score),
                'bias_strength': self._interpret_groupthink_strength(groupthink_score),
                'conformity_pressure_analysis': conformity_pressure,
                'dissent_suppression_analysis': dissent_suppression,
                'information_diversity_analysis': information_diversity,
                'group_polarization_analysis': group_polarization,
                'leader_influence_analysis': leader_influence,
                'decision_quality_assessment': self._assess_group_decision_quality(group_decision_data),
                'recommendations': self._generate_groupthink_bias_recommendations(groupthink_score)
            }
            
        except Exception as e:
            return {'error': f'ç¾¤ä½“æ€ç»´åå·®æ£€æµ‹å¤±è´¥: {str(e)}'}

    # è®¡ç®—åˆ†æç½®ä¿¡åº¦çš„è¾…åŠ©æ–¹æ³•
    def _calculate_analysis_confidence(self, sample_size):
        """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
        if sample_size >= 100:
            return "high"
        elif sample_size >= 30:
            return "medium"
        elif sample_size >= 10:
            return "low"
        else:
            return "very_low"
    
    def _detect_anchoring_bias_advanced(self, data_list: List[Dict]) -> float:
        """é«˜çº§é”šå®šåå·®æ£€æµ‹"""
        if len(data_list) < 8:
            return 0.0
        
        anchoring_scores = []
        
        # æ£€æµ‹å¤šä¸ªæ½œåœ¨é”šç‚¹
        anchor_candidates = [
            data_list[-1].get('tails', []),  # ç¬¬ä¸€æœŸä½œä¸ºé”šç‚¹
            data_list[len(data_list)//2].get('tails', []),  # ä¸­é—´æœŸä½œä¸ºé”šç‚¹
            set(range(0, 10, 2)),  # å¶æ•°é”šç‚¹
            set(range(1, 10, 2))   # å¥‡æ•°é”šç‚¹
        ]
        
        for anchor_tails in anchor_candidates:
            anchor_set = set(anchor_tails)
            if not anchor_set:
                continue
            
            influence_scores = []
            for i, period in enumerate(data_list[:-1]):
                current_tails = set(period.get('tails', []))
                overlap = len(current_tails.intersection(anchor_set))
                max_possible_overlap = min(len(current_tails), len(anchor_set))
                
                if max_possible_overlap > 0:
                    # è€ƒè™‘æ—¶é—´è¡°å‡
                    time_weight = np.exp(-0.1 * i)
                    influence_score = (overlap / max_possible_overlap) * time_weight
                    influence_scores.append(influence_score)
            
            if influence_scores:
                anchoring_scores.append(np.mean(influence_scores))
        
        return max(anchoring_scores) if anchoring_scores else 0.0
    
    def _detect_availability_bias_advanced(self, data_list: List[Dict]) -> float:
        """é«˜çº§å¯å¾—æ€§åå·®æ£€æµ‹"""
        if len(data_list) < 6:
            return 0.0
        
        # è®¡ç®—è®°å¿†è¡°å‡æƒé‡
        memory_weights = [np.exp(-0.2 * i) for i in range(len(data_list))]
        
        # åŠ æƒé¢‘ç‡è®¡ç®—
        weighted_tail_counts = defaultdict(float)
        total_weight = 0
        
        for i, period in enumerate(data_list):
            weight = memory_weights[i]
            total_weight += weight
            
            for tail in period.get('tails', []):
                weighted_tail_counts[tail] += weight
        
        # å½’ä¸€åŒ–åŠ æƒé¢‘ç‡
        if total_weight > 0:
            for tail in weighted_tail_counts:
                weighted_tail_counts[tail] /= total_weight
        
        # æ¯”è¾ƒåŠ æƒé¢‘ç‡ä¸å‡åŒ€åˆ†å¸ƒçš„åå·®
        uniform_prob = 1.0 / 10
        availability_bias = 0
        
        for tail in range(10):
            weighted_freq = weighted_tail_counts.get(tail, 0)
            # æ£€æŸ¥æ˜¯å¦è¿‡åº¦ä¾èµ–å®¹æ˜“å›å¿†çš„äº‹ä»¶
            if weighted_freq > uniform_prob * 1.5:  # æ¯”å‡åŒ€åˆ†å¸ƒé«˜50%
                availability_bias += (weighted_freq - uniform_prob) / uniform_prob
        
        return min(1.0, availability_bias / 10)  # å½’ä¸€åŒ–åˆ°[0,1]
    
    def _default_bias_detector(self, data_list: List[Dict]) -> float:
        """é»˜è®¤åå·®æ£€æµ‹å™¨"""
        return 0.0


class PerformanceMetricsTracker:
    """æ€§èƒ½æŒ‡æ ‡è¿½è¸ªå™¨ - å…¨é¢çš„æ€§èƒ½è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(self):
        self.prediction_history = []
        self.actual_results = []
        self.confidence_history = []
        self.timestamps = []
        
    def record_prediction(self, prediction: Dict, actual_result: List[int], 
                         confidence: float, timestamp: datetime = None):
        """è®°å½•é¢„æµ‹ç»“æœ"""
        if timestamp is None:
            timestamp = datetime.now()
        
        self.prediction_history.append(prediction)
        self.actual_results.append(actual_result)
        self.confidence_history.append(confidence)
        self.timestamps.append(timestamp)
    
    def calculate_comprehensive_metrics(self) -> Dict:
        """è®¡ç®—ç»¼åˆæ€§èƒ½æŒ‡æ ‡"""
        if len(self.prediction_history) == 0:
            return {}
        
        metrics = {}
        
        # åŸºç¡€å‡†ç¡®ç‡
        correct_predictions = 0
        total_predictions = len(self.prediction_history)
        
        for pred, actual in zip(self.prediction_history, self.actual_results):
            if any(t in actual for t in pred.get('recommended_tails', [])):
                correct_predictions += 1
        
        metrics['accuracy'] = correct_predictions / total_predictions
        
        # ç²¾ç¡®ç‡å’Œå¬å›ç‡
        true_positives = correct_predictions
        false_positives = total_predictions - correct_predictions
        false_negatives = 0  # éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
        
        if true_positives + false_positives > 0:
            metrics['precision'] = true_positives / (true_positives + false_positives)
        else:
            metrics['precision'] = 0.0
        
        # F1åˆ†æ•°
        if metrics['precision'] > 0:
            recall = metrics['accuracy']  # ç®€åŒ–çš„å¬å›ç‡è®¡ç®—
            metrics['f1_score'] = 2 * (metrics['precision'] * recall) / (metrics['precision'] + recall)
        else:
            metrics['f1_score'] = 0.0
        
        # ç½®ä¿¡åº¦æ ¡å‡†
        if self.confidence_history:
            avg_confidence = np.mean(self.confidence_history)
            confidence_accuracy_correlation = np.corrcoef(self.confidence_history, 
                [1 if any(t in actual for t in pred.get('recommended_tails', [])) else 0 
                 for pred, actual in zip(self.prediction_history, self.actual_results)])[0, 1]
            
            metrics['average_confidence'] = avg_confidence
            metrics['confidence_calibration'] = confidence_accuracy_correlation if not np.isnan(confidence_accuracy_correlation) else 0.0
        
        # æ—¶é—´åºåˆ—æ€§èƒ½
        if len(self.prediction_history) >= 10:
            recent_accuracy = sum(1 if any(t in actual for t in pred.get('recommended_tails', [])) else 0 
                                for pred, actual in zip(self.prediction_history[-10:], self.actual_results[-10:])) / 10
            metrics['recent_accuracy'] = recent_accuracy
            metrics['performance_trend'] = recent_accuracy - metrics['accuracy']
        
        return metrics


class ModelSelectionCriteria:
    """æ¨¡å‹é€‰æ‹©æ ‡å‡† - å¤šæ ‡å‡†æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©"""
    
    def __init__(self):
        self.criteria_weights = {
            'accuracy': 0.3,
            'complexity': 0.2,
            'interpretability': 0.2,
            'robustness': 0.15,
            'efficiency': 0.15
        }
    
    def evaluate_model(self, model_results: Dict, model_complexity: float) -> float:
        """è¯„ä¼°æ¨¡å‹ç»¼åˆå¾—åˆ†"""
        scores = {}
        
        # å‡†ç¡®æ€§å¾—åˆ†
        scores['accuracy'] = model_results.get('accuracy', 0.0)
        
        # å¤æ‚æ€§å¾—åˆ†ï¼ˆå¤æ‚æ€§è¶Šä½å¾—åˆ†è¶Šé«˜ï¼‰
        scores['complexity'] = max(0, 1.0 - model_complexity / 10.0)
        
        # å¯è§£é‡Šæ€§å¾—åˆ†ï¼ˆåŸºäºæ¨¡å‹ç±»å‹ï¼‰
        scores['interpretability'] = self._calculate_interpretability_score(model_results)
        
        # é²æ£’æ€§å¾—åˆ†
        scores['robustness'] = model_results.get('performance_stability', 0.5)
        
        # æ•ˆç‡å¾—åˆ†
        scores['efficiency'] = model_results.get('computational_efficiency', 0.5)
        
        # åŠ æƒç»¼åˆå¾—åˆ†
        total_score = sum(scores[criterion] * self.criteria_weights[criterion] 
                         for criterion in scores)
        
        return total_score
    
    def _calculate_interpretability_score(self, model_results: Dict) -> float:
        """è®¡ç®—å¯è§£é‡Šæ€§å¾—åˆ†"""
        model_type = model_results.get('model_type', 'unknown')
        
        interpretability_scores = {
            'linear': 0.9,
            'tree': 0.8,
            'rule_based': 0.85,
            'ensemble': 0.6,
            'neural_network': 0.3,
            'svm': 0.4,
            'unknown': 0.5
        }
        
        return interpretability_scores.get(model_type, 0.5)
    

    
    def _validate_and_preprocess_data(self, candidate_tails: List[int], data_list: List[Dict]) -> Dict:
        """æ•°æ®éªŒè¯ä¸é¢„å¤„ç†"""
        try:
            # åŸºç¡€éªŒè¯
            if not candidate_tails or not data_list:
                return {
                    'valid': False,
                    'error_response': {
                        'success': False,
                        'recommended_tails': [],
                        'confidence': 0.0,
                        'analysis_type': 'insufficient_data',
                        'error': 'Empty candidate tails or data list'
                    }
                }
            
            if len(data_list) < 10:
                return {
                    'valid': False,
                    'error_response': {
                        'success': False,
                        'recommended_tails': [],
                        'confidence': 0.0,
                        'analysis_type': 'insufficient_historical_data',
                        'error': f'Need at least 10 periods, got {len(data_list)}'
                    }
                }
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            data_quality_score = self._assess_data_quality(data_list)
            if data_quality_score < 0.6:
                print(f"âš ï¸ æ•°æ®è´¨é‡è¾ƒä½: {data_quality_score:.3f}")
            
            # æ•°æ®æ¸…æ´—
            cleaned_data = self._clean_and_normalize_data(data_list)
            
            # ç‰¹å¾çŸ©é˜µæ„å»º
            feature_matrix = self._build_comprehensive_feature_matrix(cleaned_data)
            
            # æ•°æ®å˜æ¢
            transformed_data = self._apply_data_transformations(cleaned_data, feature_matrix)
            
            return {
                'valid': True,
                'processed_data': transformed_data,
                'feature_matrix': feature_matrix,
                'data_quality_score': data_quality_score
            }
            
        except Exception as e:
            return {
                'valid': False,
                'error_response': {
                    'success': False,
                    'recommended_tails': [],
                    'confidence': 0.0,
                    'error': f'Data validation failed: {str(e)}'
                }
            }
    
    def _assess_data_quality(self, data_list: List[Dict]) -> float:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        quality_factors = []
        
        # 1. å®Œæ•´æ€§æ£€æŸ¥
        complete_periods = sum(1 for period in data_list 
                             if period.get('tails') and len(period['tails']) > 0)
        completeness = complete_periods / len(data_list)
        quality_factors.append(completeness)
        
        # 2. ä¸€è‡´æ€§æ£€æŸ¥
        tail_counts = [len(period.get('tails', [])) for period in data_list]
        if tail_counts:
            count_std = np.std(tail_counts)
            count_mean = np.mean(tail_counts)
            consistency = 1.0 / (1.0 + count_std / max(1, count_mean))
            quality_factors.append(consistency)
        
        # 3. åˆç†æ€§æ£€æŸ¥
        all_tails = []
        for period in data_list:
            all_tails.extend(period.get('tails', []))
        
        if all_tails:
            # æ£€æŸ¥å°¾æ•°åˆ†å¸ƒæ˜¯å¦åˆç†
            unique_tails = set(all_tails)
            tail_diversity = len(unique_tails) / 10.0  # æœ€å¤š10ä¸ªä¸åŒçš„å°¾æ•°
            quality_factors.append(tail_diversity)
        
        return np.mean(quality_factors) if quality_factors else 0.0
    
    def _clean_and_normalize_data(self, data_list: List[Dict]) -> List[Dict]:
        """æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–"""
        cleaned_data = []
        
        for period in data_list:
            cleaned_period = {}
            
            # æ¸…æ´—å°¾æ•°æ•°æ®
            raw_tails = period.get('tails', [])
            if isinstance(raw_tails, list):
                # ç¡®ä¿æ‰€æœ‰å°¾æ•°éƒ½åœ¨0-9èŒƒå›´å†…
                valid_tails = [tail for tail in raw_tails 
                             if isinstance(tail, int) and 0 <= tail <= 9]
                cleaned_period['tails'] = sorted(list(set(valid_tails)))  # å»é‡å¹¶æ’åº
            else:
                cleaned_period['tails'] = []
            
            # ä¿ç•™å…¶ä»–å­—æ®µ
            for key, value in period.items():
                if key != 'tails':
                    cleaned_period[key] = value
            
            # æ·»åŠ æ—¶é—´æˆ³ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if 'timestamp' not in cleaned_period:
                cleaned_period['timestamp'] = datetime.now()
            
            cleaned_data.append(cleaned_period)
        
        return cleaned_data
    
    def _build_comprehensive_feature_matrix(self, data_list: List[Dict]) -> np.ndarray:
        """æ„å»ºç»¼åˆç‰¹å¾çŸ©é˜µ"""
        n_periods = len(data_list)
        n_features = 150  # æ‰©å±•ç‰¹å¾ç»´åº¦
        
        feature_matrix = np.zeros((n_periods, n_features))
        
        for i, period in enumerate(data_list):
            tails = period.get('tails', [])
            feature_idx = 0
            
            # 1. åŸºç¡€ç‰¹å¾ (10ç»´) - å°¾æ•°one-hotç¼–ç 
            tail_vector = np.zeros(10)
            for tail in tails:
                tail_vector[tail] = 1
            feature_matrix[i, feature_idx:feature_idx+10] = tail_vector
            feature_idx += 10
            
            # 2. ç»Ÿè®¡ç‰¹å¾ (20ç»´)
            feature_matrix[i, feature_idx] = len(tails)  # å°¾æ•°æ•°é‡
            feature_matrix[i, feature_idx+1] = np.mean(tails) if tails else 5  # å¹³å‡å€¼
            feature_matrix[i, feature_idx+2] = np.std(tails) if len(tails) > 1 else 0  # æ ‡å‡†å·®
            feature_matrix[i, feature_idx+3] = max(tails) if tails else 0  # æœ€å¤§å€¼
            feature_matrix[i, feature_idx+4] = min(tails) if tails else 9  # æœ€å°å€¼
            feature_matrix[i, feature_idx+5] = max(tails) - min(tails) if tails else 0  # æå·®
            
            # å¥‡å¶æ•°ç»Ÿè®¡
            odd_count = sum(1 for tail in tails if tail % 2 == 1)
            even_count = len(tails) - odd_count
            feature_matrix[i, feature_idx+6] = odd_count
            feature_matrix[i, feature_idx+7] = even_count
            
            # å¤§å°æ•°ç»Ÿè®¡
            large_count = sum(1 for tail in tails if tail >= 5)
            small_count = len(tails) - large_count
            feature_matrix[i, feature_idx+8] = large_count
            feature_matrix[i, feature_idx+9] = small_count
            
            # è´¨æ•°ç»Ÿè®¡
            primes = {2, 3, 5, 7}
            prime_count = sum(1 for tail in tails if tail in primes)
            feature_matrix[i, feature_idx+10] = prime_count
            
            # è¿ç»­æ€§ç‰¹å¾
            if len(tails) > 1:
                sorted_tails = sorted(tails)
                consecutive_count = 0
                for j in range(len(sorted_tails) - 1):
                    if sorted_tails[j+1] - sorted_tails[j] == 1:
                        consecutive_count += 1
                feature_matrix[i, feature_idx+11] = consecutive_count
            
            # åˆ†æ•£åº¦ç‰¹å¾
            if len(tails) > 0:
                # è®¡ç®—åŸºå°¼ç³»æ•°
                tail_counts = np.bincount(tails, minlength=10)
                gini = self._calculate_gini_coefficient(tail_counts)
                feature_matrix[i, feature_idx+12] = gini
            
            # ä¿¡æ¯ç†µ
            if len(tails) > 0:
                tail_counts = np.bincount(tails, minlength=10)
                probabilities = tail_counts / np.sum(tail_counts)
                entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
                feature_matrix[i, feature_idx+13] = entropy
            
            # å…¶ä»–ç»Ÿè®¡ç‰¹å¾
            for k in range(6):  # å¡«å……å‰©ä½™ç»Ÿè®¡ç‰¹å¾
                feature_matrix[i, feature_idx+14+k] = np.random.normal(0, 0.1)
            
            feature_idx += 20
            
            # 3. æ—¶é—´åºåˆ—ç‰¹å¾ (30ç»´)
            if i > 0:
                # ä¸å‰ä¸€æœŸçš„å·®å¼‚
                prev_tails = set(data_list[i-1].get('tails', []))
                curr_tails = set(tails)
                
                jaccard_similarity = len(curr_tails.intersection(prev_tails)) / max(1, len(curr_tails.union(prev_tails)))
                feature_matrix[i, feature_idx] = jaccard_similarity
                
                # å˜åŒ–ç‡
                change_rate = abs(len(tails) - len(prev_tails)) / max(1, len(prev_tails))
                feature_matrix[i, feature_idx+1] = change_rate
                
                # æ–°å¢å°¾æ•°æ•°é‡
                new_tails = curr_tails - prev_tails
                feature_matrix[i, feature_idx+2] = len(new_tails)
                
                # æ¶ˆå¤±å°¾æ•°æ•°é‡
                disappeared_tails = prev_tails - curr_tails
                feature_matrix[i, feature_idx+3] = len(disappeared_tails)
            
            # æ»‘åŠ¨çª—å£ç‰¹å¾ (è€ƒè™‘å‰næœŸ)
            for window_size in [3, 5, 7]:
                window_start = max(0, i - window_size + 1)
                window_data = data_list[window_start:i+1]
                
                window_tails = []
                for period in window_data:
                    window_tails.extend(period.get('tails', []))
                
                if window_tails:
                    window_tail_counts = np.bincount(window_tails, minlength=10)
                    window_feature_idx = feature_idx + 4 + (window_size - 3) // 2 * 10
                    feature_matrix[i, window_feature_idx:window_feature_idx+10] = window_tail_counts / len(window_data)
            
            feature_idx += 30
            
            # 4. é¢‘åŸŸç‰¹å¾ (20ç»´) - åŸºäºå°æ³¢å˜æ¢
            if i >= 10:  # éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
                recent_counts = []
                for j in range(min(i+1, 20)):
                    period_tails = data_list[i-j].get('tails', []) if i-j >= 0 else []
                    recent_counts.append(len(period_tails))
                
                if len(recent_counts) >= 10:
                    # ç®€åŒ–çš„é¢‘åŸŸåˆ†æ
                    fft_result = np.fft.fft(recent_counts[:10])
                    fft_magnitude = np.abs(fft_result)
                    feature_matrix[i, feature_idx:feature_idx+10] = fft_magnitude[:10] / np.sum(fft_magnitude)
                
                # è¶‹åŠ¿ç‰¹å¾
                if len(recent_counts) >= 5:
                    trend = np.polyfit(range(5), recent_counts[-5:], 1)[0]
                    feature_matrix[i, feature_idx+10] = trend
                
                # å‘¨æœŸæ€§ç‰¹å¾
                autocorr_lags = [1, 2, 3, 4, 5]
                for lag_idx, lag in enumerate(autocorr_lags):
                    if len(recent_counts) > lag:
                        autocorr = np.corrcoef(recent_counts[:-lag], recent_counts[lag:])[0, 1]
                        feature_matrix[i, feature_idx+11+lag_idx] = autocorr if not np.isnan(autocorr) else 0
            
            # å¡«å……å‰©ä½™é¢‘åŸŸç‰¹å¾
            remaining_freq_features = 20 - 16  # 20 - (10 + 1 + 5)
            for k in range(remaining_freq_features):
                feature_matrix[i, feature_idx+16+k] = 0
            
            feature_idx += 20
            
            # 5. å¤æ‚æ€§ç‰¹å¾ (30ç»´)
            # Lempel-Zivå¤æ‚åº¦
            if len(tails) > 0:
                binary_sequence = ''.join('1' if t in tails else '0' for t in range(10))
                lz_complexity = self._calculate_lempel_ziv_complexity(binary_sequence)
                feature_matrix[i, feature_idx] = lz_complexity
            
            # åˆ†å½¢ç»´æ•°
            if i >= 5:
                recent_tail_sequences = []
                for j in range(min(i+1, 10)):
                    if i-j >= 0:
                        recent_tail_sequences.append(data_list[i-j].get('tails', []))
                
                if recent_tail_sequences:
                    fractal_dim = self._estimate_fractal_dimension(recent_tail_sequences)
                    feature_matrix[i, feature_idx+1] = fractal_dim
            
            # å…¶ä»–å¤æ‚æ€§ç‰¹å¾ï¼ˆå ä½ç¬¦ï¼‰
            for k in range(28):
                feature_matrix[i, feature_idx+2+k] = np.random.normal(0, 0.01)
            
            feature_idx += 30
            
            # 6. ç½‘ç»œç‰¹å¾ (40ç»´)
            if i >= 3:
                # æ„å»ºè½¬ç§»ç½‘ç»œ
                transitions = []
                for j in range(min(i, 5)):
                    if i-j-1 >= 0:
                        from_tails = data_list[i-j-1].get('tails', [])
                        to_tails = data_list[i-j].get('tails', [])
                        for from_tail in from_tails:
                            for to_tail in to_tails:
                                transitions.append((from_tail, to_tail))
                
                if transitions:
                    # è½¬ç§»çŸ©é˜µ
                    transition_matrix = np.zeros((10, 10))
                    for from_tail, to_tail in transitions:
                        transition_matrix[from_tail, to_tail] += 1
                    
                    # å½’ä¸€åŒ–
                    row_sums = transition_matrix.sum(axis=1)
                    transition_matrix = transition_matrix / (row_sums[:, np.newaxis] + 1e-10)
                    
                    # ç½‘ç»œç‰¹å¾ï¼šä¸­å¿ƒæ€§åº¦é‡
                    out_degrees = np.sum(transition_matrix > 0, axis=1)
                    in_degrees = np.sum(transition_matrix > 0, axis=0)
                    
                    feature_matrix[i, feature_idx:feature_idx+10] = out_degrees / 10.0
                    feature_matrix[i, feature_idx+10:feature_idx+20] = in_degrees / 10.0
                    
                    # ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
                    eigenvals, eigenvecs = np.linalg.eig(transition_matrix.T)
                    principal_eigenvec = np.real(eigenvecs[:, np.argmax(np.real(eigenvals))])
                    principal_eigenvec = np.abs(principal_eigenvec) / np.sum(np.abs(principal_eigenvec))
                    feature_matrix[i, feature_idx+20:feature_idx+30] = principal_eigenvec
            
            # å¡«å……å‰©ä½™ç½‘ç»œç‰¹å¾
            for k in range(10):
                feature_matrix[i, feature_idx+30+k] = 0
            
        return feature_matrix
    
    def _apply_data_transformations(self, data_list: List[Dict], feature_matrix: np.ndarray) -> List[Dict]:
        """åº”ç”¨æ•°æ®å˜æ¢"""
        transformed_data = []
        
        for i, period in enumerate(data_list):
            transformed_period = period.copy()
            
            # æ·»åŠ ç‰¹å¾å‘é‡
            transformed_period['feature_vector'] = feature_matrix[i]
            
            # æ·»åŠ æ—¶é—´æƒé‡ï¼ˆè¶Šæ–°çš„æ•°æ®æƒé‡è¶Šé«˜ï¼‰
            time_weight = np.exp(-0.1 * i)
            transformed_period['time_weight'] = time_weight
            
            # æ·»åŠ è´¨é‡æƒé‡
            tail_count = len(period.get('tails', []))
            quality_weight = min(1.0, tail_count / 7.0)  # å‡è®¾ç†æƒ³å°¾æ•°æ•°é‡ä¸º7
            transformed_period['quality_weight'] = quality_weight
            
            transformed_data.append(transformed_period)
        
        return transformed_data
    
    def _calculate_lempel_ziv_complexity(self, binary_string: str) -> float:
        """è®¡ç®—Lempel-Zivå¤æ‚åº¦"""
        if len(binary_string) <= 1:
            return 1.0
        
        complexity = 1
        i = 0
        while i < len(binary_string):
            j = 1
            while i + j <= len(binary_string):
                substring = binary_string[i:i+j]
                if substring in binary_string[:i]:
                    j += 1
                else:
                    break
            complexity += 1
            i += j
        
        # å½’ä¸€åŒ–
        max_complexity = len(binary_string)
        return complexity / max_complexity if max_complexity > 0 else 1.0
    
    def _estimate_fractal_dimension(self, tail_sequences: List[List[int]]) -> float:
        """ä¼°ç®—åˆ†å½¢ç»´æ•°"""
        if not tail_sequences:
            return 1.0
        
        # ç®€åŒ–çš„ç›’è®¡æ•°æ³•
        all_points = []
        for i, sequence in enumerate(tail_sequences):
            for j, tail in enumerate(sequence):
                all_points.append([i, j, tail])
        
        if len(all_points) < 4:
            return 1.0
        
        points = np.array(all_points)
        
        # ä¸åŒå°ºåº¦ä¸‹çš„ç›’è®¡æ•°
        scales = [1, 2, 4, 8]
        box_counts = []
        
        for scale in scales:
            # ç®€åŒ–çš„ç›’è®¡æ•°
            boxes = set()
            for point in points:
                box_coord = tuple((point // scale).astype(int))
                boxes.add(box_coord)
            box_counts.append(len(boxes))
        
        # çº¿æ€§å›å½’ä¼°ç®—åˆ†å½¢ç»´æ•°
        if len(box_counts) > 1:
            log_scales = np.log([1/s for s in scales])
            log_counts = np.log(box_counts)
            
            # å¤„ç†é›¶å€¼
            valid_indices = log_counts > -np.inf
            if np.sum(valid_indices) >= 2:
                slope = np.polyfit(log_scales[valid_indices], log_counts[valid_indices], 1)[0]
                return abs(slope)
        
        return 1.5  # é»˜è®¤åˆ†å½¢ç»´æ•°
    
    def _analyze_micro_behavior_patterns(self, processed_data: List[Dict], feature_matrix: np.ndarray) -> Dict:
        """
        å¾®è§‚è¡Œä¸ºåˆ†æ - ä¸ªä½“å†³ç­–å±‚é¢çš„è¡Œä¸ºæ¨¡å¼è¯†åˆ«
        åŸºäºä¸ªä½“é€‰æ‹©ç†è®ºå’Œå†³ç­–å¿ƒç†å­¦
        """
        print("   ğŸ” æ‰§è¡Œå¾®è§‚è¡Œä¸ºåˆ†æ...")
        
        try:
            micro_analysis = {
                'individual_decision_patterns': {},
                'choice_consistency_metrics': {},
                'behavioral_entropy': 0.0,
                'decision_tree_complexity': 0.0,
                'cognitive_load_indicators': {},
                'micro_adaptation_signals': {}
            }
            
            # 1. ä¸ªä½“å†³ç­–æ¨¡å¼è¯†åˆ«
            decision_patterns = self._identify_individual_decision_patterns(processed_data)
            micro_analysis['individual_decision_patterns'] = decision_patterns
            
            # 2. é€‰æ‹©ä¸€è‡´æ€§åˆ†æ
            consistency_metrics = self._calculate_choice_consistency_metrics(processed_data)
            micro_analysis['choice_consistency_metrics'] = consistency_metrics
            
            # 3. è¡Œä¸ºç†µè®¡ç®— - åŸºäºä¿¡æ¯è®º
            behavioral_entropy = self.information_analyzer.calculate_behavioral_entropy(
                [len(period.get('tails', [])) for period in processed_data]
            )
            micro_analysis['behavioral_entropy'] = behavioral_entropy
            
            # 4. å†³ç­–æ ‘å¤æ‚åº¦åˆ†æ
            if SKLEARN_AVAILABLE and len(processed_data) >= 20:
                complexity = self._analyze_decision_tree_complexity(processed_data, feature_matrix)
                micro_analysis['decision_tree_complexity'] = complexity
            
            # 5. è®¤çŸ¥è´Ÿè·æŒ‡æ ‡
            cognitive_load = self._calculate_cognitive_load_indicators(processed_data)
            micro_analysis['cognitive_load_indicators'] = cognitive_load
            
            # 6. å¾®è§‚é€‚åº”ä¿¡å·
            adaptation_signals = self._detect_micro_adaptation_signals(processed_data)
            micro_analysis['micro_adaptation_signals'] = adaptation_signals
            
            # 7. ä¸ªä½“å·®å¼‚åˆ†æ
            individual_differences = self._analyze_individual_differences(processed_data)
            micro_analysis['individual_differences'] = individual_differences
            
            print(f"      âœ“ å¾®è§‚åˆ†æå®Œæˆï¼Œè¡Œä¸ºç†µ: {behavioral_entropy:.4f}")
            return micro_analysis
            
        except Exception as e:
            print(f"      âŒ å¾®è§‚è¡Œä¸ºåˆ†æå¤±è´¥: {e}")
            return {'error': str(e), 'behavioral_entropy': 0.0}
    
    def _analyze_meso_behavior_patterns(self, processed_data: List[Dict], feature_matrix: np.ndarray) -> Dict:
        """
        ä¸­è§‚è¡Œä¸ºåˆ†æ - ç¾¤ä½“äº¤äº’å±‚é¢çš„è¡Œä¸ºæ¨¡å¼è¯†åˆ«
        åŸºäºç¤¾ä¼šç½‘ç»œç†è®ºå’Œç¾¤ä½“åŠ¨åŠ›å­¦
        """
        print("   ğŸŒ æ‰§è¡Œä¸­è§‚è¡Œä¸ºåˆ†æ...")
        
        try:
            meso_analysis = {
                'network_structure': {},
                'social_influence_patterns': {},
                'network_density': 0.0,
                'clustering_coefficients': {},
                'information_cascade_detection': {},
                'group_polarization_metrics': {}
            }
            
            # 1. æ„å»ºè¡Œä¸ºç½‘ç»œ
            behavior_sequences = [period.get('tails', []) for period in processed_data]
            network_structure = self.network_analyzer.build_behavior_network(behavior_sequences)
            meso_analysis['network_structure'] = network_structure
            meso_analysis['network_density'] = network_structure['density']
            
            # 2. ç¤¾ä¼šå½±å“æ¨¡å¼åˆ†æ
            influence_patterns = self._analyze_social_influence_patterns(processed_data)
            meso_analysis['social_influence_patterns'] = influence_patterns
            
            # 3. èšç±»ç³»æ•°åˆ†æ
            clustering_coefficients = self._calculate_network_clustering_coefficients(network_structure)
            meso_analysis['clustering_coefficients'] = clustering_coefficients
            
            # 4. ä¿¡æ¯çº§è”æ£€æµ‹
            cascade_detection = self._detect_information_cascades(processed_data)
            meso_analysis['information_cascade_detection'] = cascade_detection
            
            # 5. ç¾¤ä½“æåŒ–åº¦é‡
            polarization_metrics = self._calculate_group_polarization_metrics(processed_data)
            meso_analysis['group_polarization_metrics'] = polarization_metrics
            
            # 6. ç¤¾ä¼šå­¦ä¹ ä¿¡å·
            social_learning = self._detect_social_learning_signals(processed_data)
            meso_analysis['social_learning_signals'] = social_learning
            
            # 7. ç½‘ç»œä¸­å¿ƒæ€§åˆ†æ
            if SKLEARN_AVAILABLE:
                centrality_analysis = self._perform_network_centrality_analysis(network_structure)
                meso_analysis['centrality_analysis'] = centrality_analysis
            
            print(f"      âœ“ ä¸­è§‚åˆ†æå®Œæˆï¼Œç½‘ç»œå¯†åº¦: {network_structure['density']:.4f}")
            return meso_analysis
            
        except Exception as e:
            print(f"      âŒ ä¸­è§‚è¡Œä¸ºåˆ†æå¤±è´¥: {e}")
            return {'error': str(e), 'network_density': 0.0}
    
    def _analyze_social_influence_patterns(self, processed_data: List[Dict]) -> Dict:
        """
        ç¤¾ä¼šå½±å“æ¨¡å¼åˆ†æ - åŸºäºç¤¾ä¼šå¿ƒç†å­¦å’Œç½‘ç»œä¼ æ’­ç†è®º
        å®ç°Watts-Strogatzå°ä¸–ç•Œæ¨¡å‹å’ŒBarabÃ¡si-Albertæ— æ ‡åº¦ç½‘ç»œç†è®º
        """
        try:
            influence_analysis = {
                'influence_strength_matrix': np.zeros((10, 10)),  # å°¾æ•°é—´å½±å“å¼ºåº¦çŸ©é˜µ
                'cascade_probability': 0.0,                      # çº§è”ä¼ æ’­æ¦‚ç‡
                'social_proof_index': 0.0,                       # ç¤¾ä¼šè¯æ˜æŒ‡æ•°
                'conformity_pressure': 0.0,                      # ä»ä¼—å‹åŠ›
                'opinion_leadership_distribution': {},            # æ„è§é¢†è¢–åˆ†å¸ƒ
                'influence_diffusion_patterns': {},              # å½±å“æ‰©æ•£æ¨¡å¼
                'social_learning_signals': [],                   # ç¤¾ä¼šå­¦ä¹ ä¿¡å·
                'network_effect_strength': 0.0                   # ç½‘ç»œæ•ˆåº”å¼ºåº¦
            }
            
            if len(processed_data) < 10:
                return influence_analysis
            
            # 1. æ„å»ºå½±å“å¼ºåº¦çŸ©é˜µ - åŸºäºæ—¶é—´åºåˆ—ç›¸å…³æ€§
            for i in range(10):
                for j in range(10):
                    if i != j:
                        # è®¡ç®—å°¾æ•°iå¯¹å°¾æ•°jçš„å½±å“å¼ºåº¦
                        influence_strength = self._calculate_cross_tail_influence(
                            i, j, processed_data
                        )
                        influence_analysis['influence_strength_matrix'][i, j] = influence_strength
            
            # 2. çº§è”ä¼ æ’­åˆ†æ - åŸºäºGranovetteré˜ˆå€¼æ¨¡å‹
            cascade_events = self._detect_cascade_events(processed_data)
            influence_analysis['cascade_probability'] = self._calculate_cascade_probability(cascade_events)
            
            # 3. ç¤¾ä¼šè¯æ˜æŒ‡æ•°è®¡ç®— - åŸºäºCialdiniç¤¾ä¼šè¯æ˜ç†è®º
            social_proof = self._calculate_social_proof_index(processed_data)
            influence_analysis['social_proof_index'] = social_proof
            
            # 4. ä»ä¼—å‹åŠ›æµ‹é‡ - åŸºäºAschä»ä¼—å®éªŒç†è®º
            conformity_pressure = self._measure_conformity_pressure(processed_data)
            influence_analysis['conformity_pressure'] = conformity_pressure
            
            # 5. æ„è§é¢†è¢–è¯†åˆ« - åŸºäºKatz-Lazarsfeldä¸¤çº§ä¼ æ’­ç†è®º
            opinion_leaders = self._identify_opinion_leaders(
                processed_data, influence_analysis['influence_strength_matrix']
            )
            influence_analysis['opinion_leadership_distribution'] = opinion_leaders
            
            # 6. å½±å“æ‰©æ•£æ¨¡å¼åˆ†æ - åŸºäºRogersåˆ›æ–°æ‰©æ•£ç†è®º
            diffusion_patterns = self._analyze_influence_diffusion_patterns(processed_data)
            influence_analysis['influence_diffusion_patterns'] = diffusion_patterns
            
            # 7. ç¤¾ä¼šå­¦ä¹ ä¿¡å·æ£€æµ‹ - åŸºäºBanduraç¤¾ä¼šå­¦ä¹ ç†è®º
            learning_signals = self._detect_social_learning_signals(processed_data)
            influence_analysis['social_learning_signals'] = learning_signals
            
            # 8. ç½‘ç»œæ•ˆåº”å¼ºåº¦è®¡ç®— - åŸºäºMetcalfeç½‘ç»œæ•ˆåº”å®šå¾‹
            network_effect = self._calculate_network_effect_strength(
                influence_analysis['influence_strength_matrix']
            )
            influence_analysis['network_effect_strength'] = network_effect
            
            return influence_analysis
            
        except Exception as e:
            print(f"      âŒ ç¤¾ä¼šå½±å“æ¨¡å¼åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_cross_tail_influence(self, tail_i: int, tail_j: int, 
                                       processed_data: List[Dict]) -> float:
        """
        è®¡ç®—å°¾æ•°é—´äº¤å‰å½±å“å¼ºåº¦ - åŸºäºæ ¼å…°æ°å› æœæ£€éªŒå’Œä¿¡æ¯è®º
        """
        # æ„å»ºæ—¶é—´åºåˆ—
        series_i = []
        series_j = []
        
        for period in processed_data:
            tails = period.get('tails', [])
            series_i.append(1 if tail_i in tails else 0)
            series_j.append(1 if tail_j in tails else 0)
        
        if len(series_i) < 8:
            return 0.0
        
        # 1. æ»åç›¸å…³åˆ†æ
        max_influence = 0.0
        for lag in range(1, min(6, len(series_i) // 2)):
            if len(series_i) > lag:
                # è®¡ç®—tail_iæ»ålagæœŸå¯¹tail_jçš„å½±å“
                lagged_i = series_i[:-lag]
                current_j = series_j[lag:]
                
                if len(lagged_i) == len(current_j) and len(lagged_i) > 3:
                    correlation = np.corrcoef(lagged_i, current_j)[0, 1]
                    if not np.isnan(correlation):
                        influence = abs(correlation) * np.exp(-0.1 * lag)  # æ—¶é—´è¡°å‡
                        max_influence = max(max_influence, influence)
        
        # 2. äº’ä¿¡æ¯è®¡ç®—
        if len(series_i) >= 10:
            mutual_info = self.information_analyzer.calculate_mutual_information(
                series_i[:-1], series_j[1:]  # iå½±å“jçš„ä¸‹ä¸€æœŸ
            )
            max_influence = max(max_influence, mutual_info)
        
        # 3. æ¡ä»¶æ¦‚ç‡å½±å“
        conditional_influence = self._calculate_conditional_influence(series_i, series_j)
        
        return min(1.0, (max_influence + conditional_influence) / 2.0)
    
    def _calculate_conditional_influence(self, series_i: List[int], series_j: List[int]) -> float:
        """è®¡ç®—æ¡ä»¶æ¦‚ç‡å½±å“å¼ºåº¦"""
        if len(series_i) != len(series_j) or len(series_i) < 5:
            return 0.0
        
        # P(j=1|i=1) vs P(j=1|i=0)
        j_given_i_1 = []
        j_given_i_0 = []
        
        for k in range(len(series_i) - 1):
            if series_i[k] == 1:
                j_given_i_1.append(series_j[k + 1])
            else:
                j_given_i_0.append(series_j[k + 1])
        
        if len(j_given_i_1) == 0 or len(j_given_i_0) == 0:
            return 0.0
        
        prob_j_given_i_1 = np.mean(j_given_i_1)
        prob_j_given_i_0 = np.mean(j_given_i_0)
        
        # å½±å“å¼ºåº¦ = æ¡ä»¶æ¦‚ç‡å·®å¼‚
        influence = abs(prob_j_given_i_1 - prob_j_given_i_0)
        return influence
    
    def _detect_cascade_events(self, processed_data: List[Dict]) -> List[Dict]:
        """
        æ£€æµ‹çº§è”äº‹ä»¶ - åŸºäºWattsé˜ˆå€¼æ¨¡å‹
        """
        cascade_events = []
        
        if len(processed_data) < 5:
            return cascade_events
        
        # å¯»æ‰¾å¿«é€Ÿä¼ æ’­çš„æ¨¡å¼
        for i in range(len(processed_data) - 3):
            current_period = processed_data[i]
            future_periods = processed_data[i+1:i+4]  # æœªæ¥3æœŸ
            
            current_tails = set(current_period.get('tails', []))
            
            # æ£€æµ‹æ˜¯å¦æœ‰å°¾æ•°åœ¨åç»­æœŸé—´å¿«é€Ÿä¼ æ’­
            for tail in current_tails:
                spread_count = sum(1 for period in future_periods 
                                 if tail in period.get('tails', []))
                
                if spread_count >= 2:  # åœ¨æœªæ¥3æœŸä¸­å‡ºç°2æ¬¡ä»¥ä¸Š
                    cascade_strength = spread_count / 3.0
                    cascade_events.append({
                        'period_index': i,
                        'tail': tail,
                        'cascade_strength': cascade_strength,
                        'spread_speed': spread_count / len(future_periods)
                    })
        
        return cascade_events
    
    def _calculate_cascade_probability(self, cascade_events: List[Dict]) -> float:
        """è®¡ç®—çº§è”ä¼ æ’­æ¦‚ç‡"""
        if not cascade_events:
            return 0.0
        
        # åŸºäºå†å²çº§è”äº‹ä»¶è®¡ç®—æ€»ä½“æ¦‚ç‡
        total_strength = sum(event['cascade_strength'] for event in cascade_events)
        total_periods = len(cascade_events)
        
        if total_periods == 0:
            return 0.0
        
        cascade_probability = total_strength / total_periods
        return min(1.0, cascade_probability)
    
    def _calculate_social_proof_index(self, processed_data: List[Dict]) -> float:
        """
        è®¡ç®—ç¤¾ä¼šè¯æ˜æŒ‡æ•° - åŸºäºCialdiniç¤¾ä¼šå½±å“ç†è®º
        """
        if len(processed_data) < 8:
            return 0.0
        
        social_proof_indicators = []
        
        # 1. å¤šæ•°é€‰æ‹©æ•ˆåº”
        for period in processed_data:
            tails = period.get('tails', [])
            if tails:
                # æ£€æµ‹æ˜¯å¦é€‰æ‹©äº†"å¤šæ•°äººé€‰æ‹©"çš„å°¾æ•°
                tail_popularity = {}
                for recent_period in processed_data[:5]:  # æœ€è¿‘5æœŸçš„æµè¡Œåº¦
                    for tail in recent_period.get('tails', []):
                        tail_popularity[tail] = tail_popularity.get(tail, 0) + 1
                
                if tail_popularity:
                    popular_tails = [tail for tail, count in tail_popularity.items() 
                                   if count >= 3]  # åœ¨5æœŸä¸­å‡ºç°3æ¬¡ä»¥ä¸Š
                    
                    overlap_with_popular = len(set(tails).intersection(set(popular_tails)))
                    social_proof_score = overlap_with_popular / len(tails)
                    social_proof_indicators.append(social_proof_score)
        
        # 2. ä»ä¼—è¡Œä¸ºè¯†åˆ«
        conformity_scores = []
        for i in range(1, len(processed_data)):
            current_tails = set(processed_data[i].get('tails', []))
            prev_tails = set(processed_data[i-1].get('tails', []))
            
            if current_tails and prev_tails:
                conformity_score = len(current_tails.intersection(prev_tails)) / len(current_tails)
                conformity_scores.append(conformity_score)
        
        # 3. ç»¼åˆç¤¾ä¼šè¯æ˜æŒ‡æ•°
        all_indicators = social_proof_indicators + conformity_scores
        return np.mean(all_indicators) if all_indicators else 0.0
    
    def _measure_conformity_pressure(self, processed_data: List[Dict]) -> float:
        """
        æµ‹é‡ä»ä¼—å‹åŠ› - åŸºäºAschä»ä¼—å®éªŒç†è®º
        """
        if len(processed_data) < 6:
            return 0.0
        
        pressure_indicators = []
        
        # 1. é€‰æ‹©æ”¶æ•›æ€§åˆ†æ
        choice_diversity_over_time = []
        for period in processed_data:
            tails = period.get('tails', [])
            diversity = len(tails) / 10.0  # é€‰æ‹©å¤šæ ·æ€§
            choice_diversity_over_time.append(diversity)
        
        # å¤šæ ·æ€§é€’å‡è¡¨ç¤ºä»ä¼—å‹åŠ›å¢åŠ 
        if len(choice_diversity_over_time) >= 3:
            trend_slope = np.polyfit(range(len(choice_diversity_over_time)), 
                                   choice_diversity_over_time, 1)[0]
            conformity_pressure_from_trend = max(0.0, -trend_slope)  # è´Ÿè¶‹åŠ¿=ä»ä¼—å‹åŠ›
            pressure_indicators.append(conformity_pressure_from_trend)
        
        # 2. ç¾¤ä½“ä¸€è‡´æ€§å¢é•¿
        consistency_scores = []
        window_size = 3
        for i in range(len(processed_data) - window_size + 1):
            window_periods = processed_data[i:i + window_size]
            
            # è®¡ç®—çª—å£å†…çš„ä¸€è‡´æ€§
            all_tails_in_window = []
            for period in window_periods:
                all_tails_in_window.extend(period.get('tails', []))
            
            if all_tails_in_window:
                tail_counts = defaultdict(int)
                for tail in all_tails_in_window:
                    tail_counts[tail] += 1
                
                # ä½¿ç”¨åŸºå°¼ç³»æ•°è¡¡é‡é›†ä¸­åº¦ï¼ˆé›†ä¸­åº¦é«˜=ä¸€è‡´æ€§é«˜ï¼‰
                counts = list(tail_counts.values())
                gini = self._calculate_gini_coefficient(counts)
                consistency_scores.append(gini)
        
        if len(consistency_scores) >= 2:
            # ä¸€è‡´æ€§å¢é•¿è¶‹åŠ¿
            consistency_trend = np.polyfit(range(len(consistency_scores)), 
                                         consistency_scores, 1)[0]
            pressure_from_consistency = max(0.0, consistency_trend)
            pressure_indicators.append(pressure_from_consistency)
        
        # 3. å¼‚è®®è€…æ¯”ä¾‹ä¸‹é™
        dissenter_ratios = []
        for i in range(1, len(processed_data)):
            current_tails = set(processed_data[i].get('tails', []))
            prev_popular = self._get_popular_tails(processed_data[:i])
            
            if current_tails and prev_popular:
                dissenting_choices = current_tails - prev_popular
                dissenter_ratio = len(dissenting_choices) / len(current_tails)
                dissenter_ratios.append(dissenter_ratio)
        
        if len(dissenter_ratios) >= 2:
            dissenter_trend = np.polyfit(range(len(dissenter_ratios)), 
                                       dissenter_ratios, 1)[0]
            pressure_from_dissent = max(0.0, -dissenter_trend)  # å¼‚è®®è€…å‡å°‘=å‹åŠ›å¢åŠ 
            pressure_indicators.append(pressure_from_dissent)
        
        return np.mean(pressure_indicators) if pressure_indicators else 0.0
    
    def _get_popular_tails(self, data_slice: List[Dict]) -> set:
        """è·å–æµè¡Œå°¾æ•°"""
        tail_counts = defaultdict(int)
        for period in data_slice[-3:]:  # æœ€è¿‘3æœŸ
            for tail in period.get('tails', []):
                tail_counts[tail] += 1
        
        if not tail_counts:
            return set()
        
        max_count = max(tail_counts.values())
        popular_tails = {tail for tail, count in tail_counts.items() 
                        if count >= max_count * 0.7}  # å‡ºç°é¢‘ç‡è¾¾åˆ°æœ€é«˜é¢‘ç‡70%ä»¥ä¸Š
        return popular_tails
    
    def _calculate_network_clustering_coefficients(self, network_structure: Dict) -> Dict:
        """
        è®¡ç®—ç½‘ç»œèšç±»ç³»æ•° - åŸºäºå¤æ‚ç½‘ç»œç†è®ºçš„å±€éƒ¨èšç±»åˆ†æ
        å®ç°Watts-Strogatzèšç±»ç³»æ•°å’Œå±€éƒ¨èšç±»åº¦é‡
        """
        try:
            clustering_analysis = {
                'global_clustering_coefficient': 0.0,      # å…¨å±€èšç±»ç³»æ•°
                'local_clustering_coefficients': {},       # å„èŠ‚ç‚¹å±€éƒ¨èšç±»ç³»æ•°
                'average_clustering': 0.0,                 # å¹³å‡èšç±»ç³»æ•°
                'clustering_distribution': {},             # èšç±»ç³»æ•°åˆ†å¸ƒ
                'small_world_indicator': 0.0,              # å°ä¸–ç•Œç½‘ç»œæŒ‡æ ‡
                'network_modularity': 0.0                  # ç½‘ç»œæ¨¡å—åº¦
            }
            
            adjacency_matrix = network_structure.get('adjacency_matrix')
            if adjacency_matrix is None or adjacency_matrix.size == 0:
                return clustering_analysis
            
            n_nodes = adjacency_matrix.shape[0]  # 10ä¸ªå°¾æ•°èŠ‚ç‚¹
            
            # 1. è®¡ç®—å„èŠ‚ç‚¹çš„å±€éƒ¨èšç±»ç³»æ•°
            local_clustering = []
            for i in range(n_nodes):
                clustering_coeff = self._calculate_node_clustering_coefficient(adjacency_matrix, i)
                clustering_analysis['local_clustering_coefficients'][i] = clustering_coeff
                local_clustering.append(clustering_coeff)
            
            # 2. è®¡ç®—å…¨å±€èšç±»ç³»æ•°
            global_clustering = self._calculate_global_clustering_coefficient(adjacency_matrix)
            clustering_analysis['global_clustering_coefficient'] = global_clustering
            
            # 3. è®¡ç®—å¹³å‡èšç±»ç³»æ•°
            if local_clustering:
                clustering_analysis['average_clustering'] = np.mean(local_clustering)
            
            # 4. èšç±»ç³»æ•°åˆ†å¸ƒåˆ†æ
            if local_clustering:
                clustering_distribution = self._analyze_clustering_distribution(local_clustering)
                clustering_analysis['clustering_distribution'] = clustering_distribution
            
            # 5. å°ä¸–ç•Œç½‘ç»œæŒ‡æ ‡è®¡ç®—
            small_world_indicator = self._calculate_small_world_indicator(
                adjacency_matrix, clustering_analysis['average_clustering']
            )
            clustering_analysis['small_world_indicator'] = small_world_indicator
            
            # 6. ç½‘ç»œæ¨¡å—åº¦è®¡ç®—
            modularity = self._calculate_network_modularity(adjacency_matrix)
            clustering_analysis['network_modularity'] = modularity
            
            return clustering_analysis
            
        except Exception as e:
            print(f"      âŒ ç½‘ç»œèšç±»ç³»æ•°è®¡ç®—å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_node_clustering_coefficient(self, adj_matrix: np.ndarray, node: int) -> float:
        """
        è®¡ç®—å•ä¸ªèŠ‚ç‚¹çš„å±€éƒ¨èšç±»ç³»æ•°
        åŸºäºèŠ‚ç‚¹é‚»å±…é—´è¿æ¥å¯†åº¦çš„Watts-Strogatzå®šä¹‰
        """
        try:
            # è·å–èŠ‚ç‚¹çš„é‚»å±…
            neighbors = np.where(adj_matrix[node] > 0)[0]
            k = len(neighbors)
            
            if k < 2:
                return 0.0  # å°‘äº2ä¸ªé‚»å±…æ— æ³•å½¢æˆä¸‰è§’å½¢
            
            # è®¡ç®—é‚»å±…é—´çš„å®é™…è¿æ¥æ•°
            actual_edges = 0
            for i in range(len(neighbors)):
                for j in range(i + 1, len(neighbors)):
                    if adj_matrix[neighbors[i], neighbors[j]] > 0:
                        actual_edges += 1
            
            # è®¡ç®—å¯èƒ½çš„æœ€å¤§è¿æ¥æ•°
            max_possible_edges = k * (k - 1) // 2
            
            # èšç±»ç³»æ•° = å®é™…è¿æ¥æ•° / æœ€å¤§å¯èƒ½è¿æ¥æ•°
            clustering_coefficient = actual_edges / max_possible_edges
            return clustering_coefficient
            
        except Exception as e:
            return 0.0
    
    def _calculate_global_clustering_coefficient(self, adj_matrix: np.ndarray) -> float:
        """
        è®¡ç®—å…¨å±€èšç±»ç³»æ•° - åŸºäºä¸‰å…ƒç»„åˆ†æ
        """
        try:
            n_nodes = adj_matrix.shape[0]
            total_triangles = 0
            total_triplets = 0
            
            # éå†æ‰€æœ‰å¯èƒ½çš„ä¸‰å…ƒç»„
            for i in range(n_nodes):
                for j in range(i + 1, n_nodes):
                    for k in range(j + 1, n_nodes):
                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¿æ¥
                        edge_ij = adj_matrix[i, j] > 0
                        edge_jk = adj_matrix[j, k] > 0  
                        edge_ik = adj_matrix[i, k] > 0
                        
                        # è®¡ç®—è¿æ¥æ•°
                        edge_count = sum([edge_ij, edge_jk, edge_ik])
                        
                        if edge_count >= 2:
                            total_triplets += 1
                            if edge_count == 3:
                                total_triangles += 1
            
            # å…¨å±€èšç±»ç³»æ•° = 3 * ä¸‰è§’å½¢æ•° / ä¸‰å…ƒç»„æ•°
            if total_triplets > 0:
                global_clustering = (3 * total_triangles) / total_triplets
            else:
                global_clustering = 0.0
            
            return global_clustering
            
        except Exception as e:
            return 0.0
    
    def _analyze_clustering_distribution(self, clustering_coefficients: List[float]) -> Dict:
        """åˆ†æèšç±»ç³»æ•°åˆ†å¸ƒç‰¹å¾"""
        try:
            if not clustering_coefficients:
                return {}
            
            distribution_analysis = {
                'mean': np.mean(clustering_coefficients),
                'std': np.std(clustering_coefficients),
                'min': np.min(clustering_coefficients),
                'max': np.max(clustering_coefficients),
                'median': np.median(clustering_coefficients),
                'quartiles': {
                    'q25': np.percentile(clustering_coefficients, 25),
                    'q75': np.percentile(clustering_coefficients, 75)
                }
            }
            
            # åˆ†å¸ƒç±»å‹åˆ†æ
            if distribution_analysis['std'] < 0.1:
                distribution_analysis['distribution_type'] = 'uniform'
            elif distribution_analysis['max'] - distribution_analysis['min'] > 0.7:
                distribution_analysis['distribution_type'] = 'heterogeneous'
            else:
                distribution_analysis['distribution_type'] = 'moderate'
            
            return distribution_analysis
            
        except Exception as e:
            return {'error': str(e)}
    
    def _calculate_small_world_indicator(self, adj_matrix: np.ndarray, avg_clustering: float) -> float:
        """
        è®¡ç®—å°ä¸–ç•Œç½‘ç»œæŒ‡æ ‡ - åŸºäºWatts-Strogatzå°ä¸–ç•Œç†è®º
        """
        try:
            if avg_clustering == 0:
                return 0.0
            
            # è®¡ç®—å¹³å‡è·¯å¾„é•¿åº¦
            avg_path_length = self._calculate_average_path_length_optimized(adj_matrix)
            
            if avg_path_length == 0 or avg_path_length == np.inf:
                return 0.0
            
            # å°ä¸–ç•ŒæŒ‡æ ‡ï¼šèšç±»ç³»æ•°é«˜ä¸”è·¯å¾„é•¿åº¦çŸ­
            # ä½¿ç”¨ä¿®æ­£çš„å°ä¸–ç•Œç³»æ•°å…¬å¼
            n_nodes = adj_matrix.shape[0]
            random_clustering = np.sum(adj_matrix > 0) / (n_nodes * (n_nodes - 1))  # éšæœºç½‘ç»œèšç±»ç³»æ•°
            random_path_length = np.log(n_nodes) / np.log(np.sum(adj_matrix > 0) / n_nodes + 1)  # éšæœºç½‘ç»œè·¯å¾„é•¿åº¦
            
            if random_clustering > 0 and random_path_length > 0:
                clustering_ratio = avg_clustering / random_clustering
                path_ratio = avg_path_length / random_path_length
                
                small_world_coefficient = clustering_ratio / path_ratio
                return min(10.0, small_world_coefficient)  # é™åˆ¶ä¸Šç•Œ
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_average_path_length_optimized(self, adj_matrix: np.ndarray) -> float:
        """ä¼˜åŒ–çš„å¹³å‡è·¯å¾„é•¿åº¦è®¡ç®—"""
        try:
            n = adj_matrix.shape[0]
            
            # ä½¿ç”¨Floyd-Warshallç®—æ³•è®¡ç®—æœ€çŸ­è·¯å¾„
            dist_matrix = np.full((n, n), np.inf)
            
            # åˆå§‹åŒ–ç›´æ¥è¿æ¥çš„è·ç¦»
            for i in range(n):
                for j in range(n):
                    if i == j:
                        dist_matrix[i, j] = 0
                    elif adj_matrix[i, j] > 0:
                        dist_matrix[i, j] = 1
            
            # Floyd-Warshallç®—æ³•
            for k in range(n):
                for i in range(n):
                    for j in range(n):
                        if dist_matrix[i, k] + dist_matrix[k, j] < dist_matrix[i, j]:
                            dist_matrix[i, j] = dist_matrix[i, k] + dist_matrix[k, j]
            
            # è®¡ç®—å¹³å‡è·¯å¾„é•¿åº¦ï¼ˆæ’é™¤æ— ç©·å¤§å€¼ï¼‰
            finite_distances = dist_matrix[dist_matrix != np.inf]
            finite_distances = finite_distances[finite_distances > 0]  # æ’é™¤è‡ªç¯
            
            if len(finite_distances) > 0:
                return np.mean(finite_distances)
            else:
                return np.inf
                
        except Exception as e:
            return np.inf
    
    def _calculate_network_modularity(self, adj_matrix: np.ndarray) -> float:
        """
        è®¡ç®—ç½‘ç»œæ¨¡å—åº¦ - åŸºäºNewmanæ¨¡å—åº¦å®šä¹‰
        """
        try:
            n = adj_matrix.shape[0]
            total_edges = np.sum(adj_matrix > 0) / 2  # æ— å‘å›¾è¾¹æ•°
            
            if total_edges == 0:
                return 0.0
            
            # ç®€åŒ–çš„ç¤¾åŒºæ£€æµ‹ï¼šåŸºäºç›¸ä¼¼æ€§èšç±»
            communities = self._detect_simple_communities(adj_matrix)
            
            # è®¡ç®—æ¨¡å—åº¦
            modularity = 0.0
            for community in communities:
                for i in community:
                    for j in community:
                        if i != j:
                            # å®é™…è¿æ¥
                            actual_edge = 1 if adj_matrix[i, j] > 0 else 0
                            
                            # æœŸæœ›è¿æ¥ï¼ˆåŸºäºåº¦çš„æœŸæœ›ï¼‰
                            degree_i = np.sum(adj_matrix[i] > 0)
                            degree_j = np.sum(adj_matrix[j] > 0)
                            expected_edge = (degree_i * degree_j) / (2 * total_edges)
                            
                            modularity += (actual_edge - expected_edge)
            
            # å½’ä¸€åŒ–
            modularity = modularity / (2 * total_edges) if total_edges > 0 else 0.0
            return modularity
            
        except Exception as e:
            return 0.0
    
    def _detect_simple_communities(self, adj_matrix: np.ndarray) -> List[List[int]]:
        """ç®€å•çš„ç¤¾åŒºæ£€æµ‹ç®—æ³•"""
        try:
            n = adj_matrix.shape[0]
            communities = []
            visited = set()
            
            # ä½¿ç”¨æ·±åº¦ä¼˜å…ˆæœç´¢æ£€æµ‹è¿é€šç»„ä»¶
            for i in range(n):
                if i not in visited:
                    community = []
                    stack = [i]
                    
                    while stack:
                        node = stack.pop()
                        if node not in visited:
                            visited.add(node)
                            community.append(node)
                            
                            # æ·»åŠ é‚»å±…èŠ‚ç‚¹
                            neighbors = np.where(adj_matrix[node] > 0)[0]
                            for neighbor in neighbors:
                                if neighbor not in visited:
                                    stack.append(neighbor)
                    
                    if community:
                        communities.append(community)
            
            return communities
            
        except Exception as e:
            return [[i] for i in range(adj_matrix.shape[0])]  # å›é€€ï¼šæ¯ä¸ªèŠ‚ç‚¹ä¸ºç‹¬ç«‹ç¤¾åŒº
    
    def _detect_information_cascades(self, processed_data: List[Dict]) -> Dict:
        """
        æ£€æµ‹ä¿¡æ¯çº§è”äº‹ä»¶ - åŸºäºBikhchandaniä¿¡æ¯çº§è”ç†è®º
        å®ç°å¤šå±‚æ¬¡çº§è”æ£€æµ‹å’Œä¼ æ’­åŠ¨åŠ›å­¦åˆ†æ
        """
        try:
            cascade_analysis = {
                'cascade_events': [],                      # æ£€æµ‹åˆ°çš„çº§è”äº‹ä»¶
                'cascade_strength_distribution': {},       # çº§è”å¼ºåº¦åˆ†å¸ƒ
                'temporal_cascade_patterns': {},           # æ—¶é—´çº§è”æ¨¡å¼
                'cascade_propagation_speed': {},           # çº§è”ä¼ æ’­é€Ÿåº¦
                'information_diffusion_rate': 0.0,        # ä¿¡æ¯æ‰©æ•£ç‡
                'cascade_network_structure': {},           # çº§è”ç½‘ç»œç»“æ„
                'critical_cascade_threshold': 0.0          # ä¸´ç•Œçº§è”é˜ˆå€¼
            }
            
            if len(processed_data) < 8:
                return cascade_analysis
            
            # 1. åŸºäºé˜ˆå€¼æ¨¡å‹çš„çº§è”æ£€æµ‹
            threshold_cascades = self._detect_threshold_based_cascades(processed_data)
            cascade_analysis['cascade_events'].extend(threshold_cascades)
            
            # 2. åŸºäºå½±å“ä¼ æ’­çš„çº§è”æ£€æµ‹
            influence_cascades = self._detect_influence_propagation_cascades(processed_data)
            cascade_analysis['cascade_events'].extend(influence_cascades)
            
            # 3. åŸºäºæ—¶é—´åºåˆ—çš„çº§è”æ£€æµ‹
            temporal_cascades = self._detect_temporal_sequence_cascades(processed_data)
            cascade_analysis['cascade_events'].extend(temporal_cascades)
            
            # 4. çº§è”å¼ºåº¦åˆ†å¸ƒåˆ†æ
            if cascade_analysis['cascade_events']:
                strength_distribution = self._analyze_cascade_strength_distribution(
                    cascade_analysis['cascade_events']
                )
                cascade_analysis['cascade_strength_distribution'] = strength_distribution
            
            # 5. æ—¶é—´æ¨¡å¼åˆ†æ
            temporal_patterns = self._analyze_temporal_cascade_patterns(
                cascade_analysis['cascade_events'], processed_data
            )
            cascade_analysis['temporal_cascade_patterns'] = temporal_patterns
            
            # 6. ä¼ æ’­é€Ÿåº¦åˆ†æ
            propagation_speed = self._analyze_cascade_propagation_speed(
                cascade_analysis['cascade_events']
            )
            cascade_analysis['cascade_propagation_speed'] = propagation_speed
            
            # 7. ä¿¡æ¯æ‰©æ•£ç‡è®¡ç®—
            diffusion_rate = self._calculate_information_diffusion_rate(
                cascade_analysis['cascade_events'], processed_data
            )
            cascade_analysis['information_diffusion_rate'] = diffusion_rate
            
            # 8. çº§è”ç½‘ç»œç»“æ„åˆ†æ
            network_structure = self._analyze_cascade_network_structure(
                cascade_analysis['cascade_events']
            )
            cascade_analysis['cascade_network_structure'] = network_structure
            
            # 9. ä¸´ç•Œé˜ˆå€¼ä¼°ç®—
            critical_threshold = self._estimate_critical_cascade_threshold(
                cascade_analysis['cascade_events'], processed_data
            )
            cascade_analysis['critical_cascade_threshold'] = critical_threshold
            
            return cascade_analysis
            
        except Exception as e:
            print(f"      âŒ ä¿¡æ¯çº§è”æ£€æµ‹å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _detect_threshold_based_cascades(self, processed_data: List[Dict]) -> List[Dict]:
        """
        åŸºäºé˜ˆå€¼æ¨¡å‹çš„çº§è”æ£€æµ‹ - Granovetteré˜ˆå€¼æ¨¡å‹å®ç°
        """
        threshold_cascades = []
        
        try:
            # è®¾å®šä¸åŒçš„é˜ˆå€¼æ°´å¹³
            thresholds = [0.3, 0.5, 0.7]  # 30%, 50%, 70%çš„é‡‡ç”¨é˜ˆå€¼
            
            for threshold in thresholds:
                # æ£€æµ‹æ¯ä¸ªå°¾æ•°çš„é˜ˆå€¼çº§è”
                for tail in range(10):
                    cascade_event = self._analyze_tail_threshold_cascade(
                        tail, threshold, processed_data
                    )
                    if cascade_event and cascade_event['cascade_detected']:
                        threshold_cascades.append(cascade_event)
            
            return threshold_cascades
            
        except Exception as e:
            return []
    
    def _analyze_tail_threshold_cascade(self, tail: int, threshold: float, 
                                       processed_data: List[Dict]) -> Dict:
        """åˆ†æç‰¹å®šå°¾æ•°çš„é˜ˆå€¼çº§è”"""
        try:
            cascade_event = {
                'cascade_type': 'threshold_based',
                'target_tail': tail,
                'threshold': threshold,
                'cascade_detected': False,
                'cascade_strength': 0.0,
                'trigger_period': -1,
                'affected_periods': [],
                'propagation_path': []
            }
            
            # è¿½è¸ªè¯¥å°¾æ•°çš„ä¼ æ’­è¿‡ç¨‹
            adoption_sequence = []
            for i, period in enumerate(processed_data):
                if tail in period.get('tails', []):
                    adoption_sequence.append(i)
            
            if len(adoption_sequence) < 3:
                return cascade_event
            
            # åˆ†ææ˜¯å¦ç¬¦åˆé˜ˆå€¼çº§è”æ¨¡å¼
            for i in range(len(adoption_sequence) - 2):
                start_period = adoption_sequence[i]
                subsequent_periods = adoption_sequence[i+1:i+3]
                
                # æ£€æŸ¥åç»­é‡‡ç”¨æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
                if len(subsequent_periods) >= 2:
                    # è®¡ç®—é‡‡ç”¨å¯†åº¦
                    time_span = subsequent_periods[-1] - start_period + 1
                    adoption_density = len(subsequent_periods) / time_span
                    
                    if adoption_density >= threshold:
                        cascade_event['cascade_detected'] = True
                        cascade_event['cascade_strength'] = adoption_density
                        cascade_event['trigger_period'] = start_period
                        cascade_event['affected_periods'] = subsequent_periods
                        cascade_event['propagation_path'] = adoption_sequence[i:i+3]
                        break
            
            return cascade_event
            
        except Exception as e:
            return {}
    
    def _detect_influence_propagation_cascades(self, processed_data: List[Dict]) -> List[Dict]:
        """
        æ£€æµ‹åŸºäºå½±å“ä¼ æ’­çš„çº§è” - åŸºäºç¤¾ä¼šå½±å“ç†è®º
        """
        influence_cascades = []
        
        try:
            # æ„å»ºå½±å“ç½‘ç»œ
            influence_network = self._build_tail_influence_network(processed_data)
            
            # æ£€æµ‹å½±å“ä¼ æ’­çº§è”
            for source_tail in range(10):
                for target_tail in range(10):
                    if source_tail != target_tail:
                        cascade_event = self._analyze_influence_cascade_pair(
                            source_tail, target_tail, influence_network, processed_data
                        )
                        if cascade_event and cascade_event['cascade_detected']:
                            influence_cascades.append(cascade_event)
            
            return influence_cascades
            
        except Exception as e:
            return []
    
    def _build_tail_influence_network(self, processed_data: List[Dict]) -> np.ndarray:
        """æ„å»ºå°¾æ•°é—´å½±å“ç½‘ç»œ"""
        try:
            influence_matrix = np.zeros((10, 10))
            
            # è®¡ç®—æ¯å¯¹å°¾æ•°é—´çš„æ—¶åºå½±å“å…³ç³»
            for i in range(10):
                for j in range(10):
                    if i != j:
                        influence_strength = self._calculate_temporal_influence(
                            i, j, processed_data
                        )
                        influence_matrix[i, j] = influence_strength
            
            return influence_matrix
            
        except Exception as e:
            return np.zeros((10, 10))
    
    def _calculate_temporal_influence(self, source_tail: int, target_tail: int, 
                                    processed_data: List[Dict]) -> float:
        """è®¡ç®—æ—¶åºå½±å“å¼ºåº¦"""
        try:
            influence_events = 0
            total_opportunities = 0
            
            # åˆ†æsource_tailå‡ºç°åtarget_tailçš„å“åº”
            for i in range(len(processed_data) - 2):
                if source_tail in processed_data[i].get('tails', []):
                    total_opportunities += 1
                    
                    # æ£€æŸ¥åç»­1-2æœŸå†…target_tailæ˜¯å¦å‡ºç°
                    for j in range(1, min(3, len(processed_data) - i)):
                        if target_tail in processed_data[i + j].get('tails', []):
                            influence_events += 1
                            break
            
            if total_opportunities > 0:
                return influence_events / total_opportunities
            else:
                return 0.0
                
        except Exception as e:
            return 0.0

    def _analyze_influence_cascade_pair(self, source_tail: int, target_tail: int, 
                                       influence_network: np.ndarray, processed_data: List[Dict]) -> Dict:
        """åˆ†æå½±å“çº§è”å¯¹"""
        try:
            cascade_event = {
                'cascade_type': 'influence_propagation',
                'source_tail': source_tail,
                'target_tail': target_tail,
                'cascade_detected': False,
                'influence_strength': 0.0,
                'propagation_delay': 0,
                'cascade_episodes': []
            }
            
            influence_strength = influence_network[source_tail, target_tail]
            if influence_strength < 0.3:  # å½±å“å¼ºåº¦é˜ˆå€¼
                return cascade_event
            
            # æ£€æµ‹çº§è”äº‹ä»¶
            cascade_episodes = []
            for i in range(len(processed_data) - 3):
                if source_tail in processed_data[i].get('tails', []):
                    # æ£€æŸ¥åç»­æœŸé—´target_tailçš„å“åº”
                    for delay in range(1, 4):
                        if i + delay < len(processed_data):
                            if target_tail in processed_data[i + delay].get('tails', []):
                                cascade_episodes.append({
                                    'trigger_period': i,
                                    'response_period': i + delay,
                                    'delay': delay
                                })
                                break
            
            if len(cascade_episodes) >= 2:  # è‡³å°‘2æ¬¡çº§è”äº‹ä»¶
                cascade_event['cascade_detected'] = True
                cascade_event['influence_strength'] = influence_strength
                cascade_event['cascade_episodes'] = cascade_episodes
                avg_delay = np.mean([ep['delay'] for ep in cascade_episodes])
                cascade_event['propagation_delay'] = avg_delay
            
            return cascade_event
            
        except Exception as e:
            return {}
    
    def _detect_temporal_sequence_cascades(self, processed_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹æ—¶é—´åºåˆ—çº§è”"""
        try:
            temporal_cascades = []
            
            # æ£€æµ‹è¿ç»­ä¼ æ’­æ¨¡å¼
            for tail in range(10):
                cascade_sequences = self._find_cascade_sequences(tail, processed_data)
                for sequence in cascade_sequences:
                    if sequence['sequence_length'] >= 3:
                        temporal_cascades.append({
                            'cascade_type': 'temporal_sequence',
                            'target_tail': tail,
                            'cascade_detected': True,
                            'sequence_length': sequence['sequence_length'],
                            'sequence_periods': sequence['periods'],
                            'sequence_strength': sequence['strength']
                        })
            
            return temporal_cascades
            
        except Exception as e:
            return []
    
    def _find_cascade_sequences(self, tail: int, processed_data: List[Dict]) -> List[Dict]:
        """æŸ¥æ‰¾çº§è”åºåˆ—"""
        sequences = []
        current_sequence = []
        
        for i, period in enumerate(processed_data):
            if tail in period.get('tails', []):
                current_sequence.append(i)
            else:
                if len(current_sequence) >= 2:
                    # æ£€æŸ¥åºåˆ—çš„è¿ç»­æ€§
                    gaps = [current_sequence[j+1] - current_sequence[j] 
                           for j in range(len(current_sequence)-1)]
                    if all(gap <= 2 for gap in gaps):  # æœ€å¤§é—´éš”2æœŸ
                        strength = len(current_sequence) / (current_sequence[-1] - current_sequence[0] + 1)
                        sequences.append({
                            'periods': current_sequence.copy(),
                            'sequence_length': len(current_sequence),
                            'strength': strength
                        })
                current_sequence = []
        
        # å¤„ç†æœ€åä¸€ä¸ªåºåˆ—
        if len(current_sequence) >= 2:
            gaps = [current_sequence[j+1] - current_sequence[j] 
                   for j in range(len(current_sequence)-1)]
            if all(gap <= 2 for gap in gaps):
                strength = len(current_sequence) / (current_sequence[-1] - current_sequence[0] + 1)
                sequences.append({
                    'periods': current_sequence.copy(),
                    'sequence_length': len(current_sequence),
                    'strength': strength
                })
        
        return sequences
    
    def _analyze_cascade_strength_distribution(self, cascade_events: List[Dict]) -> Dict:
        """åˆ†æçº§è”å¼ºåº¦åˆ†å¸ƒ"""
        try:
            if not cascade_events:
                return {}
            
            strengths = []
            for event in cascade_events:
                if 'cascade_strength' in event:
                    strengths.append(event['cascade_strength'])
                elif 'influence_strength' in event:
                    strengths.append(event['influence_strength'])
                elif 'sequence_strength' in event:
                    strengths.append(event['sequence_strength'])
            
            if not strengths:
                return {}
            
            distribution = {
                'mean_strength': np.mean(strengths),
                'std_strength': np.std(strengths),
                'min_strength': np.min(strengths),
                'max_strength': np.max(strengths),
                'strength_range': np.max(strengths) - np.min(strengths),
                'strength_bins': self._create_strength_bins(strengths)
            }
            
            return distribution
            
        except Exception as e:
            return {}
    
    def _create_strength_bins(self, strengths: List[float]) -> Dict:
        """åˆ›å»ºå¼ºåº¦åˆ†ç®±"""
        try:
            bins = {
                'weak': (0.0, 0.3),
                'moderate': (0.3, 0.6),
                'strong': (0.6, 0.8),
                'very_strong': (0.8, 1.0)
            }
            
            bin_counts = {bin_name: 0 for bin_name in bins}
            
            for strength in strengths:
                for bin_name, (low, high) in bins.items():
                    if low <= strength < high:
                        bin_counts[bin_name] += 1
                        break
            
            return bin_counts
            
        except Exception as e:
            return {}
    
    def _analyze_temporal_cascade_patterns(self, cascade_events: List[Dict], 
                                         processed_data: List[Dict]) -> Dict:
        """åˆ†ææ—¶é—´çº§è”æ¨¡å¼"""
        try:
            if not cascade_events:
                return {}
            
            patterns = {
                'cascade_frequency_over_time': {},
                'peak_cascade_periods': [],
                'cascade_clustering_in_time': {},
                'seasonal_cascade_patterns': {}
            }
            
            # æŒ‰æ—¶é—´åˆ†æçº§è”é¢‘ç‡
            cascade_timeline = [0] * len(processed_data)
            for event in cascade_events:
                if 'trigger_period' in event:
                    period = event['trigger_period']
                    if 0 <= period < len(cascade_timeline):
                        cascade_timeline[period] += 1
            
            patterns['cascade_frequency_over_time'] = cascade_timeline
            
            # è¯†åˆ«å³°å€¼æœŸ
            if cascade_timeline:
                max_cascades = max(cascade_timeline)
                peak_periods = [i for i, count in enumerate(cascade_timeline) 
                              if count == max_cascades and count > 0]
                patterns['peak_cascade_periods'] = peak_periods
            
            # åˆ†æçº§è”èšé›†æ€§
            clustering_analysis = self._analyze_cascade_clustering(cascade_timeline)
            patterns['cascade_clustering_in_time'] = clustering_analysis
            
            return patterns
            
        except Exception as e:
            return {}
    
    def _analyze_cascade_clustering(self, cascade_timeline: List[int]) -> Dict:
        """åˆ†æçº§è”çš„æ—¶é—´èšé›†æ€§"""
        try:
            if not cascade_timeline or sum(cascade_timeline) == 0:
                return {}
            
            # è®¡ç®—çº§è”äº‹ä»¶é—´çš„æ—¶é—´é—´éš”
            cascade_periods = [i for i, count in enumerate(cascade_timeline) if count > 0]
            
            if len(cascade_periods) < 2:
                return {'clustering_detected': False}
            
            intervals = [cascade_periods[i+1] - cascade_periods[i] 
                        for i in range(len(cascade_periods)-1)]
            
            mean_interval = np.mean(intervals)
            std_interval = np.std(intervals)
            
            # æ£€æµ‹èšé›†æ€§ï¼šé—´éš”å°äºå¹³å‡å€¼çš„äº‹ä»¶æ¯”ä¾‹
            short_intervals = sum(1 for interval in intervals if interval < mean_interval)
            clustering_ratio = short_intervals / len(intervals)
            
            return {
                'clustering_detected': clustering_ratio > 0.6,
                'clustering_ratio': clustering_ratio,
                'mean_interval': mean_interval,
                'interval_variance': std_interval ** 2
            }
            
        except Exception as e:
            return {}
    
    def _analyze_cascade_propagation_speed(self, cascade_events: List[Dict]) -> Dict:
        """åˆ†æçº§è”ä¼ æ’­é€Ÿåº¦"""
        try:
            if not cascade_events:
                return {}
            
            propagation_delays = []
            for event in cascade_events:
                if 'propagation_delay' in event:
                    propagation_delays.append(event['propagation_delay'])
                elif 'cascade_episodes' in event:
                    for episode in event['cascade_episodes']:
                        if 'delay' in episode:
                            propagation_delays.append(episode['delay'])
            
            if not propagation_delays:
                return {}
            
            speed_analysis = {
                'average_delay': np.mean(propagation_delays),
                'median_delay': np.median(propagation_delays),
                'min_delay': np.min(propagation_delays),
                'max_delay': np.max(propagation_delays),
                'delay_variance': np.var(propagation_delays),
                'speed_distribution': self._categorize_propagation_speeds(propagation_delays)
            }
            
            return speed_analysis
            
        except Exception as e:
            return {}
    
    def _categorize_propagation_speeds(self, delays: List[float]) -> Dict:
        """åˆ†ç±»ä¼ æ’­é€Ÿåº¦"""
        try:
            categories = {
                'immediate': 0,     # å»¶è¿Ÿ <= 1
                'fast': 0,          # å»¶è¿Ÿ 1-2
                'moderate': 0,      # å»¶è¿Ÿ 2-3
                'slow': 0           # å»¶è¿Ÿ > 3
            }
            
            for delay in delays:
                if delay <= 1:
                    categories['immediate'] += 1
                elif delay <= 2:
                    categories['fast'] += 1
                elif delay <= 3:
                    categories['moderate'] += 1
                else:
                    categories['slow'] += 1
            
            return categories
            
        except Exception as e:
            return {}
    
    def _calculate_information_diffusion_rate(self, cascade_events: List[Dict], 
                                            processed_data: List[Dict]) -> float:
        """è®¡ç®—ä¿¡æ¯æ‰©æ•£ç‡"""
        try:
            if not cascade_events or not processed_data:
                return 0.0
            
            total_diffusion_events = len(cascade_events)
            total_possible_events = len(processed_data) * 10  # æ¯æœŸæ¯ä¸ªå°¾æ•°éƒ½å¯èƒ½è§¦å‘çº§è”
            
            diffusion_rate = total_diffusion_events / total_possible_events
            return diffusion_rate
            
        except Exception as e:
            return 0.0
    
    def _analyze_cascade_network_structure(self, cascade_events: List[Dict]) -> Dict:
        """åˆ†æçº§è”ç½‘ç»œç»“æ„"""
        try:
            if not cascade_events:
                return {}
            
            # æ„å»ºçº§è”ç½‘ç»œ
            cascade_network = np.zeros((10, 10))
            
            for event in cascade_events:
                if event.get('cascade_type') == 'influence_propagation':
                    source = event.get('source_tail')
                    target = event.get('target_tail')
                    if source is not None and target is not None:
                        cascade_network[source, target] += 1
            
            # åˆ†æç½‘ç»œç‰¹å¾
            network_analysis = {
                'total_cascade_links': np.sum(cascade_network > 0),
                'cascade_density': np.sum(cascade_network > 0) / (10 * 9),  # æ’é™¤å¯¹è§’çº¿
                'dominant_cascade_pairs': self._find_dominant_cascade_pairs(cascade_network),
                'cascade_hubs': self._identify_cascade_hubs(cascade_network),
                'cascade_clusters': self._identify_cascade_clusters(cascade_network)
            }
            
            return network_analysis
            
        except Exception as e:
            return {}
    
    def _find_dominant_cascade_pairs(self, cascade_network: np.ndarray) -> List[Dict]:
        """æ‰¾åˆ°ä¸»å¯¼çº§è”å¯¹"""
        try:
            pairs = []
            threshold = np.mean(cascade_network[cascade_network > 0]) if np.any(cascade_network > 0) else 0
            
            for i in range(10):
                for j in range(10):
                    if cascade_network[i, j] > threshold:
                        pairs.append({
                            'source': i,
                            'target': j,
                            'strength': cascade_network[i, j]
                        })
            
            # æŒ‰å¼ºåº¦æ’åº
            pairs.sort(key=lambda x: x['strength'], reverse=True)
            return pairs[:5]  # è¿”å›å‰5ä¸ª
            
        except Exception as e:
            return []
    
    def _identify_cascade_hubs(self, cascade_network: np.ndarray) -> Dict:
        """è¯†åˆ«çº§è”æ¢çº½"""
        try:
            out_degrees = np.sum(cascade_network > 0, axis=1)  # å‡ºåº¦
            in_degrees = np.sum(cascade_network > 0, axis=0)   # å…¥åº¦
            
            # è¯†åˆ«é«˜å‡ºåº¦èŠ‚ç‚¹ï¼ˆçº§è”æºï¼‰
            max_out_degree = np.max(out_degrees)
            cascade_sources = [i for i, degree in enumerate(out_degrees) 
                             if degree == max_out_degree and degree > 0]
            
            # è¯†åˆ«é«˜å…¥åº¦èŠ‚ç‚¹ï¼ˆçº§è”ç›®æ ‡ï¼‰
            max_in_degree = np.max(in_degrees)
            cascade_targets = [i for i, degree in enumerate(in_degrees) 
                             if degree == max_in_degree and degree > 0]
            
            return {
                'cascade_sources': cascade_sources,
                'cascade_targets': cascade_targets,
                'max_out_degree': max_out_degree,
                'max_in_degree': max_in_degree
            }
            
        except Exception as e:
            return {}
    
    def _identify_cascade_clusters(self, cascade_network: np.ndarray) -> List[List[int]]:
        """è¯†åˆ«çº§è”èšç±»"""
        try:
            # ä½¿ç”¨ç®€å•çš„è¿é€šç»„ä»¶ç®—æ³•
            clusters = []
            visited = set()
            
            for i in range(10):
                if i not in visited:
                    cluster = []
                    stack = [i]
                    
                    while stack:
                        node = stack.pop()
                        if node not in visited:
                            visited.add(node)
                            cluster.append(node)
                            
                            # æ·»åŠ æœ‰çº§è”å…³ç³»çš„é‚»å±…
                            for j in range(10):
                                if (cascade_network[node, j] > 0 or cascade_network[j, node] > 0) and j not in visited:
                                    stack.append(j)
                    
                    if len(cluster) > 1:
                        clusters.append(cluster)
            
            return clusters
            
        except Exception as e:
            return []
    
    def _estimate_critical_cascade_threshold(self, cascade_events: List[Dict], 
                                           processed_data: List[Dict]) -> float:
        """ä¼°ç®—ä¸´ç•Œçº§è”é˜ˆå€¼"""
        try:
            if not cascade_events:
                return 0.0
            
            # åˆ†æçº§è”å‘ç”Ÿçš„æ¡ä»¶
            cascade_conditions = []
            
            for event in cascade_events:
                if 'trigger_period' in event:
                    period_idx = event['trigger_period']
                    if 0 <= period_idx < len(processed_data):
                        # åˆ†æè¯¥æœŸçš„æ¡ä»¶
                        period_data = processed_data[period_idx]
                        tail_count = len(period_data.get('tails', []))
                        cascade_conditions.append(tail_count / 10.0)  # å½’ä¸€åŒ–
            
            if cascade_conditions:
                # ä¸´ç•Œé˜ˆå€¼ä¼°ç®—ä¸ºçº§è”æ¡ä»¶çš„ä¸‹å››åˆ†ä½æ•°
                critical_threshold = np.percentile(cascade_conditions, 25)
                return critical_threshold
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_group_polarization_metrics(self, processed_data: List[Dict]) -> Dict:
        """
        è®¡ç®—ç¾¤ä½“æåŒ–åº¦é‡ - åŸºäºç¾¤ä½“å†³ç­–ç†è®º
        å®ç°Moscoviciç¾¤ä½“æåŒ–ç†è®ºçš„é‡åŒ–åˆ†æ
        """
        try:
            polarization_analysis = {
                'polarization_index': 0.0,              # æåŒ–æŒ‡æ•°
                'consensus_convergence': 0.0,            # å…±è¯†æ”¶æ•›åº¦
                'opinion_dispersion': 0.0,               # æ„è§åˆ†æ•£åº¦
                'extremity_shift': 0.0,                  # æç«¯æ€§è½¬ç§»
                'polarization_trend': 'stable',          # æåŒ–è¶‹åŠ¿
                'group_fragmentation': 0.0               # ç¾¤ä½“åˆ†åŒ–åº¦
            }
            
            if len(processed_data) < 10:
                return polarization_analysis
            
            # 1. è®¡ç®—æ„è§åˆ†æ•£åº¦å˜åŒ–
            dispersion_timeline = []
            for period in processed_data[:20]:
                tails = period.get('tails', [])
                if tails:
                    # ä½¿ç”¨æ ‡å‡†å·®è¡¡é‡åˆ†æ•£åº¦
                    dispersion = np.std(tails) if len(tails) > 1 else 0
                    dispersion_timeline.append(dispersion)
            
            if len(dispersion_timeline) >= 2:
                polarization_analysis['opinion_dispersion'] = np.mean(dispersion_timeline)
                
                # æåŒ–è¶‹åŠ¿åˆ†æ
                early_dispersion = np.mean(dispersion_timeline[-5:])  # æœ€è¿‘5æœŸ
                late_dispersion = np.mean(dispersion_timeline[:5])    # æ—©æœŸ5æœŸ
                
                if early_dispersion > late_dispersion * 1.2:
                    polarization_analysis['polarization_trend'] = 'increasing'
                elif early_dispersion < late_dispersion * 0.8:
                    polarization_analysis['polarization_trend'] = 'decreasing'
                else:
                    polarization_analysis['polarization_trend'] = 'stable'
            
            # 2. è®¡ç®—æç«¯æ€§è½¬ç§»
            extremity_shifts = []
            for i in range(len(processed_data) - 1):
                current_tails = processed_data[i].get('tails', [])
                next_tails = processed_data[i + 1].get('tails', [])
                
                if current_tails and next_tails:
                    # è®¡ç®—é€‰æ‹©çš„æç«¯ç¨‹åº¦ï¼ˆåç¦»5çš„ç¨‹åº¦ï¼‰
                    current_extremity = np.mean([abs(tail - 5) for tail in current_tails])
                    next_extremity = np.mean([abs(tail - 5) for tail in next_tails])
                    
                    extremity_shifts.append(next_extremity - current_extremity)
            
            if extremity_shifts:
                polarization_analysis['extremity_shift'] = np.mean(extremity_shifts)
            
            # 3. å…±è¯†æ”¶æ•›åº¦åˆ†æ
            consensus_timeline = []
            for period in processed_data[:15]:
                tails = period.get('tails', [])
                if tails:
                    # ä½¿ç”¨ç†µçš„è´Ÿå€¼è¡¡é‡å…±è¯†ç¨‹åº¦
                    tail_counts = np.bincount(tails, minlength=10)
                    probabilities = tail_counts / np.sum(tail_counts)
                    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
                    consensus = 1.0 - (entropy / np.log2(10))  # å½’ä¸€åŒ–
                    consensus_timeline.append(consensus)
            
            if len(consensus_timeline) >= 2:
                polarization_analysis['consensus_convergence'] = np.mean(consensus_timeline)
            
            # 4. ç¾¤ä½“åˆ†åŒ–åº¦
            if len(processed_data) >= 10:
                fragmentation = self._calculate_group_fragmentation(processed_data[:10])
                polarization_analysis['group_fragmentation'] = fragmentation
            
            # 5. ç»¼åˆæåŒ–æŒ‡æ•°
            polarization_index = (
                polarization_analysis['opinion_dispersion'] * 0.3 +
                abs(polarization_analysis['extremity_shift']) * 0.3 +
                (1.0 - polarization_analysis['consensus_convergence']) * 0.2 +
                polarization_analysis['group_fragmentation'] * 0.2
            )
            polarization_analysis['polarization_index'] = polarization_index
            
            return polarization_analysis
            
        except Exception as e:
            print(f"      âŒ ç¾¤ä½“æåŒ–åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_group_fragmentation(self, recent_data: List[Dict]) -> float:
        """è®¡ç®—ç¾¤ä½“åˆ†åŒ–åº¦"""
        try:
            if len(recent_data) < 3:
                return 0.0
            
            # åˆ†æé€‰æ‹©æ¨¡å¼çš„ä¸€è‡´æ€§
            all_patterns = []
            for period in recent_data:
                tails = sorted(period.get('tails', []))
                pattern = tuple(tails)
                all_patterns.append(pattern)
            
            # è®¡ç®—å”¯ä¸€æ¨¡å¼æ¯”ä¾‹
            unique_patterns = len(set(all_patterns))
            total_patterns = len(all_patterns)
            
            fragmentation = unique_patterns / total_patterns
            return fragmentation
            
        except Exception as e:
            return 0.0
    
    def _detect_social_learning_signals(self, processed_data: List[Dict]) -> List[Dict]:
        """
        æ£€æµ‹ç¤¾ä¼šå­¦ä¹ ä¿¡å· - åŸºäºBanduraç¤¾ä¼šå­¦ä¹ ç†è®º
        """
        try:
            learning_signals = []
            
            if len(processed_data) < 6:
                return learning_signals
            
            # 1. æ¨¡ä»¿å­¦ä¹ ä¿¡å·æ£€æµ‹
            imitation_signals = self._detect_imitation_learning(processed_data)
            learning_signals.extend(imitation_signals)
            
            # 2. è§‚å¯Ÿå­¦ä¹ ä¿¡å·æ£€æµ‹
            observational_signals = self._detect_observational_learning(processed_data)
            learning_signals.extend(observational_signals)
            
            # 3. è¯•é”™å­¦ä¹ ä¿¡å·æ£€æµ‹
            trial_error_signals = self._detect_trial_error_learning(processed_data)
            learning_signals.extend(trial_error_signals)
            
            return learning_signals
            
        except Exception as e:
            return []
    
    def _detect_imitation_learning(self, processed_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹æ¨¡ä»¿å­¦ä¹ ä¿¡å·"""
        signals = []
        
        try:
            # æ£€æµ‹é‡å¤æ¨¡å¼ï¼ˆæ¨¡ä»¿å‰æœŸæˆåŠŸæ¨¡å¼ï¼‰
            for i in range(2, len(processed_data)):
                current_tails = set(processed_data[i].get('tails', []))
                
                # æ£€æŸ¥æ˜¯å¦æ¨¡ä»¿å‰æœŸæ¨¡å¼
                for j in range(max(0, i-5), i):
                    prev_tails = set(processed_data[j].get('tails', []))
                    
                    if current_tails and prev_tails:
                        similarity = len(current_tails.intersection(prev_tails)) / len(current_tails.union(prev_tails))
                        
                        if similarity >= 0.7:  # é«˜ç›¸ä¼¼åº¦è¡¨ç¤ºæ¨¡ä»¿
                            signals.append({
                                'signal_type': 'imitation_learning',
                                'current_period': i,
                                'imitated_period': j,
                                'similarity': similarity,
                                'lag': i - j
                            })
                            break
            
            return signals
            
        except Exception as e:
            return []
    
    def _detect_observational_learning(self, processed_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹è§‚å¯Ÿå­¦ä¹ ä¿¡å·"""
        signals = []
        
        try:
            # æ£€æµ‹é€æ­¥è°ƒæ•´æ¨¡å¼ï¼ˆè§‚å¯Ÿåçš„æ¸è¿›æ”¹å˜ï¼‰
            for i in range(3, len(processed_data)):
                sequence = processed_data[i-3:i+1]
                
                # åˆ†æ4æœŸçš„å˜åŒ–æ¨¡å¼
                changes = []
                for j in range(len(sequence) - 1):
                    current = set(sequence[j].get('tails', []))
                    next_period = set(sequence[j+1].get('tails', []))
                    
                    if current and next_period:
                        change_rate = 1 - len(current.intersection(next_period)) / len(current.union(next_period))
                        changes.append(change_rate)
                
                if len(changes) >= 2:
                    # æ£€æµ‹æ˜¯å¦ä¸ºæ¸è¿›å¼å­¦ä¹ ï¼ˆå˜åŒ–ç‡é€æ¸å‡å°ï¼‰
                    if all(changes[j] >= changes[j+1] for j in range(len(changes)-1)):
                        avg_change = np.mean(changes)
                        if 0.2 <= avg_change <= 0.6:  # é€‚åº¦å˜åŒ–
                            signals.append({
                                'signal_type': 'observational_learning',
                                'period_sequence': list(range(i-3, i+1)),
                                'change_pattern': changes,
                                'learning_intensity': 1 - avg_change
                            })
            
            return signals
            
        except Exception as e:
            return []
    
    def _detect_trial_error_learning(self, processed_data: List[Dict]) -> List[Dict]:
        """æ£€æµ‹è¯•é”™å­¦ä¹ ä¿¡å·"""
        signals = []
        
        try:
            # æ£€æµ‹æ¢ç´¢-åˆ©ç”¨æ¨¡å¼
            for i in range(4, len(processed_data)):
                recent_sequence = processed_data[i-4:i+1]
                
                # åˆ†æå˜åŒ–ç¨‹åº¦
                change_scores = []
                for j in range(len(recent_sequence) - 1):
                    current = set(recent_sequence[j].get('tails', []))
                    next_period = set(recent_sequence[j+1].get('tails', []))
                    
                    if current and next_period:
                        overlap = len(current.intersection(next_period))
                        union = len(current.union(next_period))
                        change_score = 1 - (overlap / union)
                        change_scores.append(change_score)
                
                if len(change_scores) >= 3:
                    # æ£€æµ‹è¯•é”™æ¨¡å¼ï¼šé«˜å˜åŒ–åè¶‹äºç¨³å®š
                    early_changes = change_scores[:2]
                    late_changes = change_scores[2:]
                    
                    if np.mean(early_changes) > 0.6 and np.mean(late_changes) < 0.4:
                        signals.append({
                            'signal_type': 'trial_error_learning',
                            'exploration_phase': early_changes,
                            'exploitation_phase': late_changes,
                            'learning_efficiency': np.mean(early_changes) - np.mean(late_changes)
                        })
            
            return signals
            
        except Exception as e:
            return []
    
    def _perform_network_centrality_analysis(self, network_structure: Dict) -> Dict:
        """
        æ‰§è¡Œç½‘ç»œä¸­å¿ƒæ€§åˆ†æ - åŸºäºå¤æ‚ç½‘ç»œç†è®º
        å®ç°å¤šç§ä¸­å¿ƒæ€§åº¦é‡å’Œç½‘ç»œæ‹“æ‰‘åˆ†æ
        """
        try:
            centrality_analysis = {
                'degree_centrality': {},          # åº¦ä¸­å¿ƒæ€§
                'betweenness_centrality': {},     # ä»‹æ•°ä¸­å¿ƒæ€§
                'closeness_centrality': {},       # ç´§å¯†ä¸­å¿ƒæ€§
                'eigenvector_centrality': {},     # ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
                'pagerank_centrality': {},        # PageRankä¸­å¿ƒæ€§
                'network_centralization': 0.0,    # ç½‘ç»œä¸­å¿ƒåŒ–ç¨‹åº¦
                'central_nodes': [],              # ä¸­å¿ƒèŠ‚ç‚¹
                'peripheral_nodes': []            # è¾¹ç¼˜èŠ‚ç‚¹
            }
            
            adjacency_matrix = network_structure.get('adjacency_matrix')
            if adjacency_matrix is None or adjacency_matrix.size == 0:
                return centrality_analysis
            
            n_nodes = adjacency_matrix.shape[0]
            
            # 1. åº¦ä¸­å¿ƒæ€§è®¡ç®—
            degree_centrality = self._calculate_degree_centrality(adjacency_matrix)
            centrality_analysis['degree_centrality'] = degree_centrality
            
            # 2. ä»‹æ•°ä¸­å¿ƒæ€§è®¡ç®—
            betweenness_centrality = self._calculate_betweenness_centrality(adjacency_matrix)
            centrality_analysis['betweenness_centrality'] = betweenness_centrality
            
            # 3. ç´§å¯†ä¸­å¿ƒæ€§è®¡ç®—
            closeness_centrality = self._calculate_closeness_centrality(adjacency_matrix)
            centrality_analysis['closeness_centrality'] = closeness_centrality
            
            # 4. ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§è®¡ç®—
            eigenvector_centrality = self._calculate_eigenvector_centrality(adjacency_matrix)
            centrality_analysis['eigenvector_centrality'] = eigenvector_centrality
            
            # 5. PageRankä¸­å¿ƒæ€§è®¡ç®—
            pagerank_centrality = self._calculate_pagerank_centrality(adjacency_matrix)
            centrality_analysis['pagerank_centrality'] = pagerank_centrality
            
            # 6. ç½‘ç»œä¸­å¿ƒåŒ–ç¨‹åº¦
            centralization = self._calculate_network_centralization(degree_centrality)
            centrality_analysis['network_centralization'] = centralization
            
            # 7. è¯†åˆ«ä¸­å¿ƒå’Œè¾¹ç¼˜èŠ‚ç‚¹
            central_nodes, peripheral_nodes = self._identify_central_peripheral_nodes(
                degree_centrality, betweenness_centrality
            )
            centrality_analysis['central_nodes'] = central_nodes
            centrality_analysis['peripheral_nodes'] = peripheral_nodes
            
            return centrality_analysis
            
        except Exception as e:
            print(f"      âŒ ç½‘ç»œä¸­å¿ƒæ€§åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_degree_centrality(self, adj_matrix: np.ndarray) -> Dict:
        """è®¡ç®—åº¦ä¸­å¿ƒæ€§"""
        try:
            n_nodes = adj_matrix.shape[0]
            degree_centrality = {}
            
            for i in range(n_nodes):
                # è®¡ç®—èŠ‚ç‚¹çš„åº¦ï¼ˆå‡ºåº¦+å…¥åº¦ï¼‰
                out_degree = np.sum(adj_matrix[i] > 0)
                in_degree = np.sum(adj_matrix[:, i] > 0)
                total_degree = out_degree + in_degree
                
                # å½’ä¸€åŒ–åº¦ä¸­å¿ƒæ€§
                normalized_centrality = total_degree / (2 * (n_nodes - 1)) if n_nodes > 1 else 0
                
                degree_centrality[i] = {
                    'degree': total_degree,
                    'normalized_centrality': normalized_centrality,
                    'out_degree': out_degree,
                    'in_degree': in_degree
                }
            
            return degree_centrality
            
        except Exception as e:
            return {}
    
    def _calculate_betweenness_centrality(self, adj_matrix: np.ndarray) -> Dict:
        """è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§"""
        try:
            n_nodes = adj_matrix.shape[0]
            betweenness = {i: 0.0 for i in range(n_nodes)}
            
            # å¯¹æ¯å¯¹èŠ‚ç‚¹è®¡ç®—æœ€çŸ­è·¯å¾„
            for s in range(n_nodes):
                for t in range(n_nodes):
                    if s != t:
                        # æ‰¾åˆ°såˆ°tçš„æ‰€æœ‰æœ€çŸ­è·¯å¾„
                        paths = self._find_shortest_paths(adj_matrix, s, t)
                        if paths:
                            # è®¡ç®—æ¯ä¸ªä¸­é—´èŠ‚ç‚¹åœ¨æœ€çŸ­è·¯å¾„ä¸­çš„å‡ºç°é¢‘ç‡
                            for path in paths:
                                for node in path[1:-1]:  # æ’é™¤èµ·ç‚¹å’Œç»ˆç‚¹
                                    betweenness[node] += 1.0 / len(paths)
            
            # å½’ä¸€åŒ–
            max_betweenness = ((n_nodes - 1) * (n_nodes - 2)) / 2 if n_nodes > 2 else 1
            for node in betweenness:
                betweenness[node] = betweenness[node] / max_betweenness
            
            return betweenness
            
        except Exception as e:
            return {}
    
    def _find_shortest_paths(self, adj_matrix: np.ndarray, source: int, target: int) -> List[List[int]]:
        """æŸ¥æ‰¾æœ€çŸ­è·¯å¾„ï¼ˆç®€åŒ–ç‰ˆBFSï¼‰"""
        try:
            if source == target:
                return [[source]]
            
            n_nodes = adj_matrix.shape[0]
            visited = set()
            queue = [(source, [source])]
            all_paths = []
            min_length = float('inf')
            
            while queue:
                current_node, path = queue.pop(0)
                
                if len(path) > min_length:
                    continue
                
                if current_node == target:
                    if len(path) < min_length:
                        min_length = len(path)
                        all_paths = [path]
                    elif len(path) == min_length:
                        all_paths.append(path)
                    continue
                
                if current_node in visited:
                    continue
                
                visited.add(current_node)
                
                # éå†é‚»å±…
                for neighbor in range(n_nodes):
                    if adj_matrix[current_node, neighbor] > 0 and neighbor not in path:
                        new_path = path + [neighbor]
                        queue.append((neighbor, new_path))
            
            return all_paths
            
        except Exception as e:
            return []
    
    def _calculate_closeness_centrality(self, adj_matrix: np.ndarray) -> Dict:
        """è®¡ç®—ç´§å¯†ä¸­å¿ƒæ€§"""
        try:
            n_nodes = adj_matrix.shape[0]
            closeness = {}
            
            # è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹å¯¹ä¹‹é—´çš„æœ€çŸ­è·ç¦»
            dist_matrix = self._floyd_warshall(adj_matrix)
            
            for i in range(n_nodes):
                distances = []
                for j in range(n_nodes):
                    if i != j and dist_matrix[i, j] != np.inf:
                        distances.append(dist_matrix[i, j])
                
                if distances:
                    # ç´§å¯†ä¸­å¿ƒæ€§ = 1 / å¹³å‡è·ç¦»
                    avg_distance = np.mean(distances)
                    closeness[i] = 1.0 / avg_distance if avg_distance > 0 else 0.0
                else:
                    closeness[i] = 0.0
            
            return closeness
            
        except Exception as e:
            return {}
    
    def _floyd_warshall(self, adj_matrix: np.ndarray) -> np.ndarray:
        """Floyd-Warshallæœ€çŸ­è·¯å¾„ç®—æ³•"""
        try:
            n = adj_matrix.shape[0]
            dist = np.full((n, n), np.inf)
            
            # åˆå§‹åŒ–è·ç¦»çŸ©é˜µ
            for i in range(n):
                for j in range(n):
                    if i == j:
                        dist[i, j] = 0
                    elif adj_matrix[i, j] > 0:
                        dist[i, j] = 1  # å‡è®¾æ‰€æœ‰è¾¹æƒé‡ä¸º1
            
            # Floyd-Warshallç®—æ³•
            for k in range(n):
                for i in range(n):
                    for j in range(n):
                        if dist[i, k] + dist[k, j] < dist[i, j]:
                            dist[i, j] = dist[i, k] + dist[k, j]
            
            return dist
            
        except Exception as e:
            n = adj_matrix.shape[0]
            return np.full((n, n), np.inf)
    
    def _calculate_eigenvector_centrality(self, adj_matrix: np.ndarray) -> Dict:
        """è®¡ç®—ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§"""
        try:
            n_nodes = adj_matrix.shape[0]
            
            # ç¡®ä¿çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼ˆæ— å‘å›¾ï¼‰
            symmetric_matrix = (adj_matrix + adj_matrix.T) / 2
            
            try:
                # è®¡ç®—æœ€å¤§ç‰¹å¾å€¼å’Œå¯¹åº”çš„ç‰¹å¾å‘é‡
                eigenvalues, eigenvectors = np.linalg.eig(symmetric_matrix)
                max_eigenvalue_index = np.argmax(eigenvalues.real)
                principal_eigenvector = eigenvectors[:, max_eigenvalue_index].real
                
                # å½’ä¸€åŒ–åˆ°[0,1]
                if np.max(principal_eigenvector) > np.min(principal_eigenvector):
                    normalized_eigenvector = (principal_eigenvector - np.min(principal_eigenvector)) / (np.max(principal_eigenvector) - np.min(principal_eigenvector))
                else:
                    normalized_eigenvector = np.ones(n_nodes) / n_nodes
                
                eigenvector_centrality = {i: float(normalized_eigenvector[i]) for i in range(n_nodes)}
                
            except np.linalg.LinAlgError:
                # å¦‚æœç‰¹å¾å€¼è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åº¦ä¸­å¿ƒæ€§ä½œä¸ºè¿‘ä¼¼
                degrees = np.sum(symmetric_matrix > 0, axis=1)
                max_degree = np.max(degrees) if np.max(degrees) > 0 else 1
                eigenvector_centrality = {i: float(degrees[i] / max_degree) for i in range(n_nodes)}
            
            return eigenvector_centrality
            
        except Exception as e:
            n_nodes = adj_matrix.shape[0]
            return {i: 0.0 for i in range(n_nodes)}
    
    def _calculate_pagerank_centrality(self, adj_matrix: np.ndarray, damping: float = 0.85, max_iter: int = 100) -> Dict:
        """è®¡ç®—PageRankä¸­å¿ƒæ€§"""
        try:
            n_nodes = adj_matrix.shape[0]
            
            # åˆå§‹åŒ–PageRankå€¼
            pagerank = np.ones(n_nodes) / n_nodes
            
            # æ„å»ºè½¬ç§»çŸ©é˜µ
            transition_matrix = adj_matrix.copy()
            for i in range(n_nodes):
                row_sum = np.sum(transition_matrix[i])
                if row_sum > 0:
                    transition_matrix[i] /= row_sum
                else:
                    # å¤„ç†æ²¡æœ‰å‡ºè¾¹çš„èŠ‚ç‚¹
                    transition_matrix[i] = 1.0 / n_nodes
            
            # è¿­ä»£è®¡ç®—PageRank
            for iteration in range(max_iter):
                new_pagerank = (1 - damping) / n_nodes + damping * transition_matrix.T.dot(pagerank)
                
                # æ£€æŸ¥æ”¶æ•›
                if np.allclose(pagerank, new_pagerank, rtol=1e-6):
                    break
                
                pagerank = new_pagerank
            
            # å½’ä¸€åŒ–
            pagerank = pagerank / np.sum(pagerank)
            
            return {i: float(pagerank[i]) for i in range(n_nodes)}
            
        except Exception as e:
            n_nodes = adj_matrix.shape[0]
            return {i: 1.0/n_nodes for i in range(n_nodes)}
    
    def _calculate_network_centralization(self, degree_centrality: Dict) -> float:
        """è®¡ç®—ç½‘ç»œä¸­å¿ƒåŒ–ç¨‹åº¦"""
        try:
            if not degree_centrality:
                return 0.0
            
            centralities = [node_data['normalized_centrality'] for node_data in degree_centrality.values()]
            
            if not centralities:
                return 0.0
            
            max_centrality = max(centralities)
            
            # è®¡ç®—ä¸­å¿ƒåŒ–æŒ‡æ•°
            numerator = sum(max_centrality - c for c in centralities)
            n_nodes = len(centralities)
            
            if n_nodes <= 2:
                return 0.0
            
            # ç†è®ºæœ€å¤§å€¼ï¼ˆæ˜Ÿå½¢ç½‘ç»œï¼‰
            max_possible = (n_nodes - 1) * (max_centrality - 1/(n_nodes-1))
            
            centralization = numerator / max_possible if max_possible > 0 else 0.0
            
            return min(1.0, centralization)
            
        except Exception as e:
            return 0.0
    
    def _identify_central_peripheral_nodes(self, degree_centrality: Dict, 
                                         betweenness_centrality: Dict) -> Tuple[List[int], List[int]]:
        """è¯†åˆ«ä¸­å¿ƒå’Œè¾¹ç¼˜èŠ‚ç‚¹"""
        try:
            if not degree_centrality or not betweenness_centrality:
                return [], []
            
            # ç»¼åˆåº¦ä¸­å¿ƒæ€§å’Œä»‹æ•°ä¸­å¿ƒæ€§
            combined_scores = {}
            for node in degree_centrality:
                degree_score = degree_centrality[node]['normalized_centrality']
                betweenness_score = betweenness_centrality.get(node, 0.0)
                combined_scores[node] = (degree_score + betweenness_score) / 2
            
            if not combined_scores:
                return [], []
            
            # è®¡ç®—é˜ˆå€¼
            scores = list(combined_scores.values())
            mean_score = np.mean(scores)
            std_score = np.std(scores)
            
            central_threshold = mean_score + 0.5 * std_score
            peripheral_threshold = mean_score - 0.5 * std_score
            
            central_nodes = [node for node, score in combined_scores.items() 
                           if score >= central_threshold]
            peripheral_nodes = [node for node, score in combined_scores.items() 
                              if score <= peripheral_threshold]
            
            return central_nodes, peripheral_nodes
            
        except Exception as e:
            return [], []
    
    def _analyze_macro_behavior_patterns(self, processed_data: List[Dict], feature_matrix: np.ndarray) -> Dict:
        """
        å®è§‚è¡Œä¸ºåˆ†æ - ç³»ç»Ÿæ¶Œç°å±‚é¢çš„è¡Œä¸ºæ¨¡å¼è¯†åˆ«
        åŸºäºå¤æ‚ç³»ç»Ÿç†è®ºå’Œæ¶Œç°ç†è®º
        """
        print("   ğŸŒ æ‰§è¡Œå®è§‚è¡Œä¸ºåˆ†æ...")
        
        try:
            macro_analysis = {
                'system_complexity': 0.0,
                'emergence_patterns': {},
                'phase_transitions': {},
                'critical_phenomena': {},
                'collective_behavior_modes': {},
                'system_resilience_metrics': {}
            }
            
            # 1. ç³»ç»Ÿå¤æ‚åº¦è®¡ç®—
            system_complexity = self._calculate_system_complexity(processed_data, feature_matrix)
            macro_analysis['system_complexity'] = system_complexity
            
            # 2. æ¶Œç°æ¨¡å¼è¯†åˆ«
            emergence_patterns = self._identify_emergence_patterns(processed_data)
            macro_analysis['emergence_patterns'] = emergence_patterns
            
            # 3. ç›¸å˜æ£€æµ‹
            phase_transitions = self._detect_phase_transitions(processed_data)
            macro_analysis['phase_transitions'] = phase_transitions
            
            # 4. ä¸´ç•Œç°è±¡åˆ†æ
            critical_phenomena = self._analyze_critical_phenomena(processed_data)
            macro_analysis['critical_phenomena'] = critical_phenomena
            
            # 5. é›†ä½“è¡Œä¸ºæ¨¡å¼
            collective_modes = self._identify_collective_behavior_modes(processed_data)
            macro_analysis['collective_behavior_modes'] = collective_modes
            
            # 6. ç³»ç»ŸéŸ§æ€§åº¦é‡
            resilience_metrics = self._calculate_system_resilience_metrics(processed_data)
            macro_analysis['system_resilience_metrics'] = resilience_metrics
            
            # 7. æ··æ²Œåˆ†æ
            if len(processed_data) >= 30:
                chaos_analysis = self._perform_chaos_analysis(processed_data)
                macro_analysis['chaos_analysis'] = chaos_analysis
            
            # 8. åˆ†å½¢åˆ†æ
            fractal_analysis = self._perform_comprehensive_fractal_analysis(processed_data)
            macro_analysis['fractal_analysis'] = fractal_analysis
            
            print(f"      âœ“ å®è§‚åˆ†æå®Œæˆï¼Œç³»ç»Ÿå¤æ‚åº¦: {system_complexity:.4f}")
            return macro_analysis
            
        except Exception as e:
            print(f"      âŒ å®è§‚è¡Œä¸ºåˆ†æå¤±è´¥: {e}")
            return {'error': str(e), 'system_complexity': 0.0}

    def _calculate_system_complexity(self, processed_data: List[Dict], feature_matrix: np.ndarray) -> float:
        """
        è®¡ç®—ç³»ç»Ÿå¤æ‚åº¦ - åŸºäºå¤æ‚é€‚åº”ç³»ç»Ÿç†è®º
        å®ç°å¤šç»´åº¦å¤æ‚æ€§åº¦é‡å’Œç³»ç»Ÿæ¼”åŒ–åˆ†æ
        """
        try:
            if len(processed_data) < 15 or feature_matrix.size == 0:
                return 0.0
            
            complexity_factors = []
            
            # 1. ä¿¡æ¯ç†µå¤æ‚åº¦
            entropy_complexity = self._calculate_entropy_complexity(processed_data)
            complexity_factors.append(entropy_complexity)
            
            # 2. æ‹“æ‰‘å¤æ‚åº¦
            topology_complexity = self._calculate_topology_complexity(processed_data)
            complexity_factors.append(topology_complexity)
            
            # 3. åŠ¨æ€å¤æ‚åº¦
            dynamic_complexity = self._calculate_dynamic_complexity(processed_data)
            complexity_factors.append(dynamic_complexity)
            
            # 4. ç‰¹å¾ç©ºé—´å¤æ‚åº¦
            if feature_matrix.ndim == 2 and feature_matrix.shape[0] > 5:
                feature_complexity = self._calculate_feature_space_complexity(feature_matrix)
                complexity_factors.append(feature_complexity)
            
            # 5. æ—¶é—´åºåˆ—å¤æ‚åº¦
            temporal_complexity = self._calculate_temporal_complexity(processed_data)
            complexity_factors.append(temporal_complexity)
            
            # 6. ç›¸äº’ä½œç”¨å¤æ‚åº¦
            interaction_complexity = self._calculate_interaction_complexity(processed_data)
            complexity_factors.append(interaction_complexity)
            
            # ç»¼åˆå¤æ‚åº¦è®¡ç®—
            if complexity_factors:
                system_complexity = np.mean(complexity_factors)
                return min(1.0, system_complexity)
            
            return 0.0
            
        except Exception as e:
            print(f"      âŒ ç³»ç»Ÿå¤æ‚åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_entropy_complexity(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—åŸºäºç†µçš„å¤æ‚åº¦"""
        try:
            # è®¡ç®—é€‰æ‹©æ¨¡å¼çš„ç†µ
            patterns = []
            for period in processed_data[:20]:
                tails = tuple(sorted(period.get('tails', [])))
                patterns.append(tails)
            
            if not patterns:
                return 0.0
            
            # è®¡ç®—æ¨¡å¼åˆ†å¸ƒçš„ç†µ
            from collections import Counter
            pattern_counts = Counter(patterns)
            total_patterns = len(patterns)
            
            entropy = 0.0
            for count in pattern_counts.values():
                probability = count / total_patterns
                if probability > 0:
                    entropy -= probability * np.log2(probability)
            
            # å½’ä¸€åŒ–åˆ°[0,1]
            max_entropy = np.log2(total_patterns) if total_patterns > 1 else 1
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            
            return normalized_entropy
            
        except Exception as e:
            return 0.0
    
    def _calculate_topology_complexity(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—æ‹“æ‰‘å¤æ‚åº¦"""
        try:
            # æ„å»ºçŠ¶æ€è½¬ç§»ç½‘ç»œ
            state_transitions = []
            for i in range(len(processed_data) - 1):
                current_state = tuple(sorted(processed_data[i].get('tails', [])))
                next_state = tuple(sorted(processed_data[i+1].get('tails', [])))
                state_transitions.append((current_state, next_state))
            
            if not state_transitions:
                return 0.0
            
            # è®¡ç®—ç½‘ç»œçš„å¤æ‚æ€§æŒ‡æ ‡
            unique_states = set()
            transition_counts = {}
            
            for current, next_state in state_transitions:
                unique_states.add(current)
                unique_states.add(next_state)
                
                transition = (current, next_state)
                transition_counts[transition] = transition_counts.get(transition, 0) + 1
            
            # ç½‘ç»œå¤æ‚åº¦ = çŠ¶æ€å¤šæ ·æ€§ Ã— è½¬ç§»å¤šæ ·æ€§
            state_diversity = len(unique_states)
            transition_diversity = len(transition_counts)
            
            # å½’ä¸€åŒ–
            max_states = min(2**10, len(processed_data))  # ç†è®ºæœ€å¤§çŠ¶æ€æ•°
            max_transitions = max_states * (max_states - 1)  # ç†è®ºæœ€å¤§è½¬ç§»æ•°
            
            topology_complexity = (
                (state_diversity / max_states) * 0.5 +
                (transition_diversity / min(max_transitions, 100)) * 0.5
            )
            
            return min(1.0, topology_complexity)
            
        except Exception as e:
            return 0.0
    
    def _calculate_dynamic_complexity(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—åŠ¨æ€å¤æ‚åº¦"""
        try:
            if len(processed_data) < 10:
                return 0.0
            
            # è®¡ç®—ç³»ç»ŸçŠ¶æ€çš„æ—¶é—´æ¼”åŒ–å¤æ‚æ€§
            complexity_indicators = []
            
            # 1. å˜åŒ–ç‡çš„å˜åŒ–ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰
            change_rates = []
            for i in range(len(processed_data) - 1):
                current_tails = set(processed_data[i].get('tails', []))
                next_tails = set(processed_data[i+1].get('tails', []))
                
                if current_tails and next_tails:
                    change_rate = 1 - len(current_tails.intersection(next_tails)) / len(current_tails.union(next_tails))
                    change_rates.append(change_rate)
            
            if len(change_rates) >= 3:
                # è®¡ç®—å˜åŒ–ç‡çš„æ–¹å·®ï¼ˆåŠ¨æ€å¤æ‚æ€§æŒ‡æ ‡ï¼‰
                change_variance = np.var(change_rates)
                complexity_indicators.append(change_variance)
            
            # 2. å‘¨æœŸæ€§çš„å¤æ‚åº¦
            if len(processed_data) >= 15:
                periodicity_complexity = self._calculate_periodicity_complexity(processed_data[:15])
                complexity_indicators.append(periodicity_complexity)
            
            # 3. å¯é¢„æµ‹æ€§çš„å¤æ‚åº¦ï¼ˆåå‘æŒ‡æ ‡ï¼‰
            predictability = self._calculate_predictability_score(processed_data[:12])
            complexity_indicators.append(1.0 - predictability)  # ä½å¯é¢„æµ‹æ€§ = é«˜å¤æ‚åº¦
            
            if complexity_indicators:
                return np.mean(complexity_indicators)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_periodicity_complexity(self, data_sequence: List[Dict]) -> float:
        """è®¡ç®—å‘¨æœŸæ€§å¤æ‚åº¦"""
        try:
            # åˆ†ææ¯ä¸ªå°¾æ•°çš„å‘¨æœŸæ€§æ¨¡å¼
            tail_periodicities = []
            
            for tail in range(10):
                appearances = [1 if tail in period.get('tails', []) else 0 
                             for period in data_sequence]
                
                if sum(appearances) > 0:
                    # ä½¿ç”¨è‡ªç›¸å…³åˆ†æå‘¨æœŸæ€§
                    autocorrelations = []
                    for lag in range(1, min(8, len(appearances)//2)):
                        if len(appearances) > lag:
                            correlation = np.corrcoef(appearances[:-lag], appearances[lag:])[0, 1]
                            if not np.isnan(correlation):
                                autocorrelations.append(abs(correlation))
                    
                    if autocorrelations:
                        periodicity_strength = max(autocorrelations)
                        tail_periodicities.append(periodicity_strength)
            
            if tail_periodicities:
                # å‘¨æœŸæ€§å¤æ‚åº¦ = å‘¨æœŸæ€§å¼ºåº¦çš„å˜å¼‚ç¨‹åº¦
                periodicity_complexity = np.std(tail_periodicities)
                return min(1.0, periodicity_complexity)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_predictability_score(self, data_sequence: List[Dict]) -> float:
        """è®¡ç®—å¯é¢„æµ‹æ€§è¯„åˆ†"""
        try:
            if len(data_sequence) < 8:
                return 0.5
            
            # ä½¿ç”¨ç®€å•çš„é©¬å°”å¯å¤«é“¾æ¨¡å‹è¯„ä¼°å¯é¢„æµ‹æ€§
            transition_matrix = np.zeros((10, 10))
            
            # æ„å»ºè½¬ç§»æ¦‚ç‡çŸ©é˜µ
            for i in range(len(data_sequence) - 1):
                current_tails = data_sequence[i].get('tails', [])
                next_tails = data_sequence[i+1].get('tails', [])
                
                for current_tail in current_tails:
                    for next_tail in next_tails:
                        transition_matrix[current_tail, next_tail] += 1
            
            # å½’ä¸€åŒ–
            for i in range(10):
                row_sum = np.sum(transition_matrix[i])
                if row_sum > 0:
                    transition_matrix[i] /= row_sum
            
            # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡
            correct_predictions = 0
            total_predictions = 0
            
            for i in range(len(data_sequence) - 2):
                current_tails = data_sequence[i].get('tails', [])
                actual_next_tails = set(data_sequence[i+1].get('tails', []))
                
                if current_tails and actual_next_tails:
                    # æ ¹æ®è½¬ç§»çŸ©é˜µé¢„æµ‹
                    predicted_probabilities = {}
                    for tail in range(10):
                        prob = 0
                        for current_tail in current_tails:
                            prob += transition_matrix[current_tail, tail]
                        predicted_probabilities[tail] = prob / len(current_tails)
                    
                    # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å°¾æ•°ä½œä¸ºé¢„æµ‹
                    predicted_tail = max(predicted_probabilities.items(), key=lambda x: x[1])[0]
                    
                    if predicted_tail in actual_next_tails:
                        correct_predictions += 1
                    total_predictions += 1
            
            if total_predictions > 0:
                predictability = correct_predictions / total_predictions
                return predictability
            
            return 0.5
            
        except Exception as e:
            return 0.5
    
    def _calculate_feature_space_complexity(self, feature_matrix: np.ndarray) -> float:
        """è®¡ç®—ç‰¹å¾ç©ºé—´å¤æ‚åº¦"""
        try:
            if feature_matrix.shape[0] < 3:
                return 0.0
            
            complexity_factors = []
            
            # 1. ç‰¹å¾ç©ºé—´çš„ç»´åº¦æœ‰æ•ˆæ€§
            effective_dimensions = 0
            for col in range(feature_matrix.shape[1]):
                column = feature_matrix[:, col]
                if np.std(column) > 1e-6:  # æœ‰æ„ä¹‰çš„å˜åŒ–
                    effective_dimensions += 1
            
            dimension_complexity = effective_dimensions / feature_matrix.shape[1]
            complexity_factors.append(dimension_complexity)
            
            # 2. ç‰¹å¾ç›¸å…³æ€§å¤æ‚åº¦
            try:
                correlation_matrix = np.corrcoef(feature_matrix.T)
                # è®¡ç®—ç›¸å…³æ€§çš„åˆ†å¸ƒå¤æ‚åº¦
                correlations = correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]
                correlations = correlations[~np.isnan(correlations)]
                
                if len(correlations) > 0:
                    correlation_entropy = -np.sum(
                        correlations * np.log2(np.abs(correlations) + 1e-10)
                    ) / len(correlations)
                    complexity_factors.append(min(1.0, correlation_entropy / 10))
            except:
                pass
            
            # 3. æ•°æ®ç‚¹åˆ†å¸ƒå¤æ‚åº¦
            if SKLEARN_AVAILABLE:
                try:
                    from sklearn.neighbors import NearestNeighbors
                    nn = NearestNeighbors(n_neighbors=min(5, feature_matrix.shape[0]))
                    nn.fit(feature_matrix)
                    distances, _ = nn.kneighbors(feature_matrix)
                    
                    # è®¡ç®—è·ç¦»åˆ†å¸ƒçš„å¤æ‚åº¦
                    mean_distances = np.mean(distances, axis=1)
                    distance_complexity = np.std(mean_distances) / (np.mean(mean_distances) + 1e-10)
                    complexity_factors.append(min(1.0, distance_complexity))
                except:
                    pass
            
            if complexity_factors:
                return np.mean(complexity_factors)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_temporal_complexity(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—æ—¶é—´åºåˆ—å¤æ‚åº¦"""
        try:
            if len(processed_data) < 10:
                return 0.0
            
            # æ„å»ºæ—¶é—´åºåˆ—
            time_series_data = []
            for period in processed_data[:20]:
                tails = period.get('tails', [])
                # å°†å°¾æ•°é›†åˆè½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾
                features = [
                    len(tails),                                    # æ•°é‡
                    np.mean(tails) if tails else 5,               # å¹³å‡å€¼
                    np.std(tails) if len(tails) > 1 else 0,       # æ ‡å‡†å·®
                    max(tails) if tails else 0,                   # æœ€å¤§å€¼
                    min(tails) if tails else 9                    # æœ€å°å€¼
                ]
                time_series_data.append(features)
            
            time_series_matrix = np.array(time_series_data)
            
            complexity_measures = []
            
            # 1. æ—¶é—´åºåˆ—çš„LyapunovæŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if len(time_series_data) >= 10:
                lyapunov_complexity = self.chaos_analyzer.calculate_lyapunov_exponent(
                    time_series_matrix[:, 0], embedding_dim=3, delay=1
                )
                complexity_measures.append(min(1.0, abs(lyapunov_complexity)))
            
            # 2. æ ·æœ¬ç†µ
            for col in range(time_series_matrix.shape[1]):
                series = time_series_matrix[:, col]
                sample_entropy = self._calculate_sample_entropy(series)
                complexity_measures.append(sample_entropy)
            
            # 3. å°æ³¢ç†µ
            if len(time_series_data) >= 8:
                wavelet_result = self.wavelet_analyzer.morlet_wavelet_transform(time_series_matrix[:, 0])
                if 'power_spectrum' in wavelet_result and wavelet_result['power_spectrum'].size > 0:
                    wavelet_entropy = self.wavelet_analyzer._calculate_frequency_entropy(
                        wavelet_result['power_spectrum']
                    )
                    complexity_measures.append(wavelet_entropy / 10)  # å½’ä¸€åŒ–
            
            if complexity_measures:
                return np.mean(complexity_measures)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_sample_entropy(self, time_series: np.ndarray, m: int = 2, r: float = 0.2) -> float:
        """è®¡ç®—æ ·æœ¬ç†µ"""
        try:
            if len(time_series) < m + 1:
                return 0.0
            
            N = len(time_series)
            
            def _maxdist(xi, xj, m):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])
            
            def _phi(m):
                patterns = np.array([time_series[i:i + m] for i in range(N - m + 1)])
                C = np.zeros(N - m + 1)
                
                for i in range(N - m + 1):
                    template_i = patterns[i]
                    for j in range(N - m + 1):
                        if _maxdist(template_i, patterns[j], m) <= r:
                            C[i] += 1.0
                
                phi = np.mean(np.log(C / (N - m + 1.0)))
                return phi
            
            return _phi(m) - _phi(m + 1)
            
        except Exception as e:
            return 0.0
    
    def _calculate_interaction_complexity(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—ç›¸äº’ä½œç”¨å¤æ‚åº¦"""
        try:
            if len(processed_data) < 10:
                return 0.0
            
            # åˆ†æå°¾æ•°é—´çš„ç›¸äº’ä½œç”¨æ¨¡å¼
            interaction_matrix = np.zeros((10, 10))
            
            # è®¡ç®—å°¾æ•°å…±ç°é¢‘ç‡
            for period in processed_data[:15]:
                tails = period.get('tails', [])
                for i in range(len(tails)):
                    for j in range(i + 1, len(tails)):
                        tail_a, tail_b = tails[i], tails[j]
                        interaction_matrix[tail_a, tail_b] += 1
                        interaction_matrix[tail_b, tail_a] += 1
            
            # å½’ä¸€åŒ–
            max_interactions = len(processed_data[:15])
            if max_interactions > 0:
                interaction_matrix = interaction_matrix / max_interactions
            
            # è®¡ç®—ç›¸äº’ä½œç”¨å¤æ‚åº¦
            complexity_factors = []
            
            # 1. ç›¸äº’ä½œç”¨åˆ†å¸ƒçš„ç†µ
            interactions = interaction_matrix[interaction_matrix > 0]
            if len(interactions) > 0:
                interaction_entropy = -np.sum(interactions * np.log2(interactions + 1e-10))
                normalized_entropy = interaction_entropy / np.log2(len(interactions))
                complexity_factors.append(normalized_entropy)
            
            # 2. ç›¸äº’ä½œç”¨çš„ä¸å¯¹ç§°æ€§
            asymmetry = np.sum(np.abs(interaction_matrix - interaction_matrix.T))
            max_asymmetry = np.sum(interaction_matrix + interaction_matrix.T)
            if max_asymmetry > 0:
                asymmetry_complexity = asymmetry / max_asymmetry
                complexity_factors.append(asymmetry_complexity)
            
            # 3. ç›¸äº’ä½œç”¨çš„å±‚æ¬¡æ€§
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å±‚æ¬¡ç»“æ„ï¼ˆæŸäº›å°¾æ•°ç»„åˆæ›´é¢‘ç¹ï¼‰
            if len(interactions) > 1:
                interaction_variance = np.var(interactions)
                interaction_mean = np.mean(interactions)
                if interaction_mean > 0:
                    hierarchy_complexity = interaction_variance / interaction_mean
                    complexity_factors.append(min(1.0, hierarchy_complexity))
            
            if complexity_factors:
                return np.mean(complexity_factors)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _identify_emergence_patterns(self, processed_data: List[Dict]) -> Dict:
        """
        è¯†åˆ«æ¶Œç°æ¨¡å¼ - åŸºäºå¤æ‚ç³»ç»Ÿæ¶Œç°ç†è®º
        å®ç°å¤šå±‚æ¬¡æ¶Œç°ç°è±¡æ£€æµ‹å’Œåˆ†æ
        """
        try:
            emergence_analysis = {
                'emergence_detected': False,
                'emergence_patterns': [],
                'emergence_strength': 0.0,
                'emergence_types': [],
                'macro_micro_correlation': 0.0,
                'self_organization_indicators': {},
                'phase_coherence': 0.0
            }
            
            if len(processed_data) < 20:
                return emergence_analysis
            
            # 1. æ£€æµ‹å®è§‚-å¾®è§‚å…³è”æ¶Œç°
            macro_micro_patterns = self._detect_macro_micro_emergence(processed_data)
            if macro_micro_patterns['emergence_detected']:
                emergence_analysis['emergence_patterns'].append(macro_micro_patterns)
                emergence_analysis['emergence_types'].append('macro_micro')
            
            # 2. æ£€æµ‹è‡ªç»„ç»‡æ¶Œç°
            self_organization = self._detect_self_organization_emergence(processed_data)
            emergence_analysis['self_organization_indicators'] = self_organization
            if self_organization.get('emergence_detected', False):
                emergence_analysis['emergence_patterns'].append(self_organization)
                emergence_analysis['emergence_types'].append('self_organization')
            
            # 3. æ£€æµ‹é›†ä½“è¡Œä¸ºæ¶Œç°
            collective_emergence = self._detect_collective_behavior_emergence(processed_data)
            if collective_emergence.get('emergence_detected', False):
                emergence_analysis['emergence_patterns'].append(collective_emergence)
                emergence_analysis['emergence_types'].append('collective_behavior')
            
            # 4. æ£€æµ‹ç›¸å¹²æ€§æ¶Œç°
            phase_coherence = self._detect_phase_coherence_emergence(processed_data)
            emergence_analysis['phase_coherence'] = phase_coherence
            if phase_coherence > 0.7:
                emergence_analysis['emergence_patterns'].append({
                    'type': 'phase_coherence',
                    'strength': phase_coherence
                })
                emergence_analysis['emergence_types'].append('phase_coherence')
            
            # 5. ç»¼åˆæ¶Œç°è¯„ä¼°
            if emergence_analysis['emergence_patterns']:
                emergence_analysis['emergence_detected'] = True
                pattern_strengths = [p.get('strength', 0) for p in emergence_analysis['emergence_patterns'] 
                                   if 'strength' in p]
                if pattern_strengths:
                    emergence_analysis['emergence_strength'] = np.mean(pattern_strengths)
            
            return emergence_analysis
            
        except Exception as e:
            print(f"      âŒ æ¶Œç°æ¨¡å¼è¯†åˆ«å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _detect_macro_micro_emergence(self, processed_data: List[Dict]) -> Dict:
        """æ£€æµ‹å®è§‚-å¾®è§‚æ¶Œç°"""
        try:
            # å¾®è§‚æŒ‡æ ‡ï¼šä¸ªä½“å°¾æ•°è¡Œä¸º
            micro_indicators = []
            for tail in range(10):
                tail_appearances = [1 if tail in period.get('tails', []) else 0 
                                  for period in processed_data[:15]]
                micro_indicators.append(tail_appearances)
            
            # å®è§‚æŒ‡æ ‡ï¼šæ•´ä½“ç³»ç»Ÿè¡Œä¸º
            macro_indicators = []
            for period in processed_data[:15]:
                tails = period.get('tails', [])
                macro_features = [
                    len(tails),                           # æ€»æ•°é‡
                    np.mean(tails) if tails else 5,       # å¹³å‡å€¼
                    np.std(tails) if len(tails) > 1 else 0  # åˆ†æ•£åº¦
                ]
                macro_indicators.append(macro_features)
            
            # åˆ†æå¾®è§‚å’Œå®è§‚çš„å…³è”æ€§
            correlations = []
            for i, micro_series in enumerate(micro_indicators):
                for j, macro_feature in enumerate(zip(*macro_indicators)):
                    if len(micro_series) == len(macro_feature) and len(micro_series) > 3:
                        try:
                            correlation = np.corrcoef(micro_series, macro_feature)[0, 1]
                            if not np.isnan(correlation):
                                correlations.append(abs(correlation))
                        except:
                            continue
            
            emergence_pattern = {
                'type': 'macro_micro',
                'emergence_detected': False,
                'strength': 0.0,
                'correlations': correlations
            }
            
            if correlations:
                max_correlation = max(correlations)
                avg_correlation = np.mean(correlations)
                
                # æ¶Œç°åˆ¤æ–­ï¼šå¼ºç›¸å…³ä½†éçº¿æ€§
                if max_correlation > 0.6 and avg_correlation > 0.3:
                    emergence_pattern['emergence_detected'] = True
                    emergence_pattern['strength'] = avg_correlation
            
            return emergence_pattern
            
        except Exception as e:
            return {'type': 'macro_micro', 'emergence_detected': False}
    
    def _detect_self_organization_emergence(self, processed_data: List[Dict]) -> Dict:
        """æ£€æµ‹è‡ªç»„ç»‡æ¶Œç°"""
        try:
            self_org_indicators = {
                'emergence_detected': False,
                'pattern_formation': 0.0,
                'spontaneous_order': 0.0,
                'adaptive_behavior': 0.0,
                'symmetry_breaking': 0.0
            }
            
            # 1. æ¨¡å¼å½¢æˆæ£€æµ‹
            pattern_formation = self._detect_pattern_formation(processed_data)
            self_org_indicators['pattern_formation'] = pattern_formation
            
            # 2. è‡ªå‘ç§©åºæ£€æµ‹
            spontaneous_order = self._detect_spontaneous_order(processed_data)
            self_org_indicators['spontaneous_order'] = spontaneous_order
            
            # 3. è‡ªé€‚åº”è¡Œä¸ºæ£€æµ‹
            adaptive_behavior = self._detect_adaptive_behavior(processed_data)
            self_org_indicators['adaptive_behavior'] = adaptive_behavior
            
            # 4. å¯¹ç§°æ€§ç ´ç¼ºæ£€æµ‹
            symmetry_breaking = self._detect_symmetry_breaking(processed_data)
            self_org_indicators['symmetry_breaking'] = symmetry_breaking
            
            # ç»¼åˆè¯„ä¼°
            indicators = [pattern_formation, spontaneous_order, adaptive_behavior, symmetry_breaking]
            avg_indicator = np.mean(indicators)
            
            if avg_indicator > 0.6 and max(indicators) > 0.7:
                self_org_indicators['emergence_detected'] = True
                self_org_indicators['strength'] = avg_indicator
            
            return self_org_indicators
            
        except Exception as e:
            return {'emergence_detected': False}
    
    def _detect_pattern_formation(self, processed_data: List[Dict]) -> float:
        """æ£€æµ‹æ¨¡å¼å½¢æˆ"""
        try:
            # åˆ†ææ˜¯å¦å½¢æˆç¨³å®šçš„é€‰æ‹©æ¨¡å¼
            if len(processed_data) < 10:
                return 0.0
            
            # è®¡ç®—ç›¸é‚»æœŸé—´çš„ç›¸ä¼¼æ€§
            similarities = []
            for i in range(len(processed_data) - 1):
                current_tails = set(processed_data[i].get('tails', []))
                next_tails = set(processed_data[i+1].get('tails', []))
                
                if current_tails and next_tails:
                    similarity = len(current_tails.intersection(next_tails)) / len(current_tails.union(next_tails))
                    similarities.append(similarity)
            
            if not similarities:
                return 0.0
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é€’å¢çš„æ¨¡å¼å½¢æˆè¶‹åŠ¿
            if len(similarities) >= 5:
                early_similarity = np.mean(similarities[:3])
                late_similarity = np.mean(similarities[-3:])
                
                pattern_formation_strength = max(0, late_similarity - early_similarity)
                return min(1.0, pattern_formation_strength * 2)  # æ”¾å¤§æ•ˆæœ
            
            return np.mean(similarities)
            
        except Exception as e:
            return 0.0
    
    def _detect_spontaneous_order(self, processed_data: List[Dict]) -> float:
        """æ£€æµ‹è‡ªå‘ç§©åº"""
        try:
            # åˆ†æç³»ç»Ÿæ˜¯å¦ä»æ— åºèµ°å‘æœ‰åº
            order_timeline = []
            
            for period in processed_data[:12]:
                tails = period.get('tails', [])
                if tails:
                    # è®¡ç®—æœ‰åºåº¦ï¼ˆåŸºäºå°¾æ•°çš„åˆ†å¸ƒï¼‰
                    tail_counts = np.bincount(tails, minlength=10)
                    # ä½¿ç”¨åŸºå°¼ç³»æ•°è¡¡é‡æœ‰åºç¨‹åº¦
                    gini = self._calculate_gini_coefficient(tail_counts)
                    order_timeline.append(gini)
            
            if len(order_timeline) < 5:
                return 0.0
            
            # æ£€æŸ¥æœ‰åºåº¦çš„å¢é•¿è¶‹åŠ¿
            if len(order_timeline) >= 6:
                early_order = np.mean(order_timeline[:3])
                late_order = np.mean(order_timeline[-3:])
                
                spontaneous_order = max(0, late_order - early_order)
                return min(1.0, spontaneous_order * 3)  # æ”¾å¤§è¶‹åŠ¿
            
            return np.mean(order_timeline)
            
        except Exception as e:
            return 0.0
    
    def _detect_adaptive_behavior(self, processed_data: List[Dict]) -> float:
        """æ£€æµ‹è‡ªé€‚åº”è¡Œä¸º"""
        try:
            # åˆ†æç³»ç»Ÿæ˜¯å¦è¡¨ç°å‡ºå­¦ä¹ å’Œé€‚åº”
            adaptation_signals = []
            
            # æ£€æŸ¥é€‰æ‹©ç­–ç•¥çš„å˜åŒ–å’Œä¼˜åŒ–
            for i in range(3, len(processed_data)):
                recent_window = processed_data[i-3:i+1]
                
                # åˆ†æé€‰æ‹©å¤šæ ·æ€§çš„å˜åŒ–
                diversities = []
                for period in recent_window:
                    tails = period.get('tails', [])
                    diversity = len(tails) / 10.0 if tails else 0
                    diversities.append(diversity)
                
                if len(diversities) >= 3:
                    # æ£€æŸ¥æ˜¯å¦æœ‰é€‚åº”æ€§è°ƒæ•´ï¼ˆå…ˆæ¢ç´¢ååˆ©ç”¨ï¼‰
                    exploration_phase = diversities[:2]
                    exploitation_phase = diversities[2:]
                    
                    if (np.mean(exploration_phase) > 0.5 and 
                        np.mean(exploitation_phase) < np.mean(exploration_phase)):
                        adaptation_strength = np.mean(exploration_phase) - np.mean(exploitation_phase)
                        adaptation_signals.append(adaptation_strength)
            
            if adaptation_signals:
                return min(1.0, np.mean(adaptation_signals) * 2)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _detect_symmetry_breaking(self, processed_data: List[Dict]) -> float:
        """æ£€æµ‹å¯¹ç§°æ€§ç ´ç¼º"""
        try:
            # åˆ†æç³»ç»Ÿæ˜¯å¦æ‰“ç ´åˆå§‹çš„å¯¹ç§°æ€§
            if len(processed_data) < 10:
                return 0.0
            
            # è®¡ç®—å°¾æ•°åˆ†å¸ƒçš„å¯¹ç§°æ€§å˜åŒ–
            symmetry_timeline = []
            
            for period in processed_data[:10]:
                tails = period.get('tails', [])
                if tails:
                    # è®¡ç®—åˆ†å¸ƒçš„å¯¹ç§°æ€§
                    tail_counts = np.bincount(tails, minlength=10)
                    
                    # æ£€æŸ¥åˆ†å¸ƒçš„åæ–œåº¦
                    if np.sum(tail_counts) > 0:
                        probabilities = tail_counts / np.sum(tail_counts)
                        
                        # è®¡ç®—ç›¸å¯¹äºä¸­å¿ƒ(4.5)çš„åæ–œ
                        weighted_sum = sum(i * prob for i, prob in enumerate(probabilities))
                        asymmetry = abs(weighted_sum - 4.5) / 4.5
                        symmetry_timeline.append(asymmetry)
            
            if len(symmetry_timeline) >= 5:
                # æ£€æŸ¥å¯¹ç§°æ€§ç ´ç¼ºçš„è¶‹åŠ¿
                early_asymmetry = np.mean(symmetry_timeline[:3])
                late_asymmetry = np.mean(symmetry_timeline[-3:])
                
                symmetry_breaking = max(0, late_asymmetry - early_asymmetry)
                return min(1.0, symmetry_breaking * 3)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _detect_collective_behavior_emergence(self, processed_data: List[Dict]) -> Dict:
        """æ£€æµ‹é›†ä½“è¡Œä¸ºæ¶Œç°"""
        try:
            collective_emergence = {
                'emergence_detected': False,
                'synchronization': 0.0,
                'collective_modes': [],
                'emergence_threshold': 0.0
            }
            
            if len(processed_data) < 12:
                return collective_emergence
            
            # 1. åŒæ­¥åŒ–æ£€æµ‹
            synchronization = self._detect_synchronization(processed_data)
            collective_emergence['synchronization'] = synchronization
            
            # 2. é›†ä½“æ¨¡å¼è¯†åˆ«
            collective_modes = self._identify_collective_modes(processed_data)
            collective_emergence['collective_modes'] = collective_modes
            
            # 3. æ¶Œç°é˜ˆå€¼ä¼°ç®—
            emergence_threshold = self._estimate_emergence_threshold(processed_data)
            collective_emergence['emergence_threshold'] = emergence_threshold
            
            # ç»¼åˆåˆ¤æ–­
            if (synchronization > 0.6 or 
                len(collective_modes) >= 2 or 
                emergence_threshold > 0.7):
                collective_emergence['emergence_detected'] = True
                collective_emergence['strength'] = max(synchronization, emergence_threshold)
            
            return collective_emergence
            
        except Exception as e:
            return {'emergence_detected': False}
    
    def _detect_synchronization(self, processed_data: List[Dict]) -> float:
        """æ£€æµ‹åŒæ­¥åŒ–ç°è±¡"""
        try:
            # åˆ†æå°¾æ•°å‡ºç°çš„åŒæ­¥æ€§
            sync_scores = []
            
            for i in range(len(processed_data) - 2):
                window = processed_data[i:i+3]
                
                # åˆ†æ3æœŸå†…çš„å°¾æ•°åŒæ­¥å‡ºç°æ¨¡å¼
                sync_patterns = {}
                for period in window:
                    for tail in period.get('tails', []):
                        sync_patterns[tail] = sync_patterns.get(tail, 0) + 1
                
                # è®¡ç®—åŒæ­¥å¼ºåº¦
                if sync_patterns:
                    max_sync = max(sync_patterns.values())
                    sync_score = max_sync / 3.0  # å½’ä¸€åŒ–
                    sync_scores.append(sync_score)
            
            if sync_scores:
                return np.mean(sync_scores)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _identify_collective_modes(self, processed_data: List[Dict]) -> List[Dict]:
        """è¯†åˆ«é›†ä½“è¡Œä¸ºæ¨¡å¼"""
        try:
            modes = []
            
            # è¯†åˆ«å‘¨æœŸæ€§é›†ä½“è¡Œä¸º
            periodic_mode = self._detect_periodic_collective_mode(processed_data)
            if periodic_mode['detected']:
                modes.append(periodic_mode)
            
            # è¯†åˆ«æ³¢åŠ¨æ€§é›†ä½“è¡Œä¸º
            wave_mode = self._detect_wave_collective_mode(processed_data)
            if wave_mode['detected']:
                modes.append(wave_mode)
            
            return modes
            
        except Exception as e:
            return []
    
    def _detect_periodic_collective_mode(self, processed_data: List[Dict]) -> Dict:
        """æ£€æµ‹å‘¨æœŸæ€§é›†ä½“æ¨¡å¼"""
        try:
            mode = {'detected': False, 'period': 0, 'strength': 0.0}
            
            # åˆ†ææ•´ä½“é€‰æ‹©æ•°é‡çš„å‘¨æœŸæ€§
            choice_counts = [len(period.get('tails', [])) for period in processed_data[:15]]
            
            if len(choice_counts) >= 8:
                # æ£€æµ‹ä¸åŒå‘¨æœŸé•¿åº¦
                for period_length in range(2, 6):
                    correlations = []
                    for lag in range(period_length, len(choice_counts), period_length):
                        if lag < len(choice_counts):
                            base_seq = choice_counts[:len(choice_counts)-lag]
                            shifted_seq = choice_counts[lag:]
                            min_len = min(len(base_seq), len(shifted_seq))
                            
                            if min_len > 2:
                                correlation = np.corrcoef(base_seq[:min_len], shifted_seq[:min_len])[0, 1]
                                if not np.isnan(correlation):
                                    correlations.append(abs(correlation))
                    
                    if correlations and np.mean(correlations) > 0.6:
                        mode['detected'] = True
                        mode['period'] = period_length
                        mode['strength'] = np.mean(correlations)
                        break
            
            return mode
            
        except Exception as e:
            return {'detected': False}
    
    def _detect_wave_collective_mode(self, processed_data: List[Dict]) -> Dict:
        """æ£€æµ‹æ³¢åŠ¨æ€§é›†ä½“æ¨¡å¼"""
        try:
            mode = {'detected': False, 'wavelength': 0, 'amplitude': 0.0}
            
            # åˆ†æé€‰æ‹©æ¨¡å¼çš„æ³¢åŠ¨æ€§
            if len(processed_data) < 10:
                return mode
            
            # è®¡ç®—é€‰æ‹©ä¸­å¿ƒçš„å˜åŒ–
            choice_centers = []
            for period in processed_data[:12]:
                tails = period.get('tails', [])
                if tails:
                    center = np.mean(tails)
                    choice_centers.append(center)
            
            if len(choice_centers) >= 6:
                # æ£€æµ‹æ³¢åŠ¨æ¨¡å¼
                differences = [choice_centers[i+1] - choice_centers[i] 
                             for i in range(len(choice_centers)-1)]
                
                # å¯»æ‰¾æ³¢åŠ¨å‘¨æœŸ
                for wavelength in range(2, 5):
                    wave_correlations = []
                    for i in range(len(differences) - wavelength):
                        segment1 = differences[i:i+wavelength]
                        segment2 = differences[i+wavelength:i+2*wavelength]
                        
                        if len(segment1) == len(segment2) and len(segment1) > 1:
                            correlation = np.corrcoef(segment1, segment2)[0, 1]
                            if not np.isnan(correlation):
                                wave_correlations.append(abs(correlation))
                    
                    if wave_correlations and np.mean(wave_correlations) > 0.5:
                        mode['detected'] = True
                        mode['wavelength'] = wavelength
                        mode['amplitude'] = np.std(choice_centers)
                        break
            
            return mode
            
        except Exception as e:
            return {'detected': False}
    
    def _estimate_emergence_threshold(self, processed_data: List[Dict]) -> float:
        """ä¼°ç®—æ¶Œç°é˜ˆå€¼"""
        try:
            # åˆ†æç³»ç»Ÿè¡Œä¸ºçš„è´¨å˜ç‚¹
            if len(processed_data) < 10:
                return 0.0
            
            # è®¡ç®—ç³»ç»ŸçŠ¶æ€çš„å˜åŒ–ç‡
            state_changes = []
            for i in range(len(processed_data) - 1):
                current_state = set(processed_data[i].get('tails', []))
                next_state = set(processed_data[i+1].get('tails', []))
                
                if current_state and next_state:
                    change_rate = 1 - len(current_state.intersection(next_state)) / len(current_state.union(next_state))
                    state_changes.append(change_rate)
            
            if len(state_changes) >= 5:
                # å¯»æ‰¾å˜åŒ–ç‡çš„çªå˜ç‚¹
                for i in range(2, len(state_changes) - 2):
                    before_avg = np.mean(state_changes[:i])
                    after_avg = np.mean(state_changes[i:])
                    
                    if abs(after_avg - before_avg) > 0.3:  # æ˜¾è‘—å˜åŒ–
                        # è®¡ç®—æ¶Œç°å¼ºåº¦
                        emergence_strength = abs(after_avg - before_avg)
                        return min(1.0, emergence_strength * 2)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _detect_phase_coherence_emergence(self, processed_data: List[Dict]) -> float:
        """æ£€æµ‹ç›¸å¹²æ€§æ¶Œç°"""
        try:
            if len(processed_data) < 8:
                return 0.0
            
            # åˆ†æä¸åŒå°¾æ•°çš„ç›¸ä½å…³ç³»
            phase_coherence_scores = []
            
            # å¯¹æ¯å¯¹å°¾æ•°è®¡ç®—ç›¸ä½å…³ç³»
            for tail_a in range(10):
                for tail_b in range(tail_a + 1, 10):
                    # æ„å»ºæ¯ä¸ªå°¾æ•°çš„å‡ºç°åºåˆ—
                    sequence_a = [1 if tail_a in period.get('tails', []) else 0 
                                 for period in processed_data[:10]]
                    sequence_b = [1 if tail_b in period.get('tails', []) else 0 
                                 for period in processed_data[:10]]
                    
                    # è®¡ç®—ç›¸ä½ç›¸å¹²æ€§
                    if sum(sequence_a) > 0 and sum(sequence_b) > 0:
                        # ä½¿ç”¨äº’ç›¸å…³åˆ†æç›¸ä½å…³ç³»
                        cross_correlation = np.correlate(sequence_a, sequence_b, mode='valid')[0]
                        max_possible = min(sum(sequence_a), sum(sequence_b))
                        
                        if max_possible > 0:
                            coherence = abs(cross_correlation) / max_possible
                            phase_coherence_scores.append(coherence)
            
            if phase_coherence_scores:
                return np.mean(phase_coherence_scores)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _detect_phase_transitions(self, processed_data: List[Dict]) -> Dict:
        """
        æ£€æµ‹ç›¸å˜ç°è±¡ - åŸºäºç»Ÿè®¡ç‰©ç†å­¦ç›¸å˜ç†è®º
        å®ç°Landauç›¸å˜ç†è®ºå’Œä¸´ç•Œç°è±¡åˆ†æ
        """
        try:
            phase_transition_analysis = {
                'transitions_detected': [],
                'critical_points': [],
                'order_parameters': {},
                'phase_diagram': {},
                'transition_types': [],
                'hysteresis_detected': False
            }
            
            if len(processed_data) < 15:
                return phase_transition_analysis
            
            # 1. å®šä¹‰åºå‚é‡ï¼ˆorder parameterï¼‰
            order_parameters = self._calculate_order_parameters(processed_data)
            phase_transition_analysis['order_parameters'] = order_parameters
            
            # 2. æ£€æµ‹ä¸€é˜¶ç›¸å˜
            first_order_transitions = self._detect_first_order_transitions(order_parameters)
            phase_transition_analysis['transitions_detected'].extend(first_order_transitions)
            
            # 3. æ£€æµ‹äºŒé˜¶ç›¸å˜ï¼ˆè¿ç»­ç›¸å˜ï¼‰
            second_order_transitions = self._detect_second_order_transitions(order_parameters)
            phase_transition_analysis['transitions_detected'].extend(second_order_transitions)
            
            # 4. è¯†åˆ«ä¸´ç•Œç‚¹
            critical_points = self._identify_critical_points(order_parameters)
            phase_transition_analysis['critical_points'] = critical_points
            
            # 5. æ„å»ºç›¸å›¾
            phase_diagram = self._construct_phase_diagram(processed_data, order_parameters)
            phase_transition_analysis['phase_diagram'] = phase_diagram
            
            # 6. æ£€æµ‹æ»å›ç°è±¡
            hysteresis = self._detect_hysteresis_behavior(order_parameters)
            phase_transition_analysis['hysteresis_detected'] = hysteresis
            
            # 7. åˆ†ç±»ç›¸å˜ç±»å‹
            transition_types = self._classify_transition_types(phase_transition_analysis['transitions_detected'])
            phase_transition_analysis['transition_types'] = transition_types
            
            return phase_transition_analysis
            
        except Exception as e:
            print(f"      âŒ ç›¸å˜æ£€æµ‹å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_order_parameters(self, processed_data: List[Dict]) -> Dict:
        """è®¡ç®—åºå‚é‡"""
        try:
            order_params = {
                'concentration_parameter': [],    # é›†ä¸­åº¦åºå‚é‡
                'symmetry_parameter': [],         # å¯¹ç§°æ€§åºå‚é‡
                'coherence_parameter': [],        # ç›¸å¹²æ€§åºå‚é‡
                'complexity_parameter': []        # å¤æ‚åº¦åºå‚é‡
            }
            
            for period in processed_data[:20]:
                tails = period.get('tails', [])
                
                if tails:
                    # 1. é›†ä¸­åº¦åºå‚é‡
                    tail_counts = np.bincount(tails, minlength=10)
                    gini = self._calculate_gini_coefficient(tail_counts)
                    order_params['concentration_parameter'].append(gini)
                    
                    # 2. å¯¹ç§°æ€§åºå‚é‡
                    center_of_mass = np.mean(tails)
                    symmetry = abs(center_of_mass - 4.5) / 4.5
                    order_params['symmetry_parameter'].append(symmetry)
                    
                    # 3. ç›¸å¹²æ€§åºå‚é‡
                    if len(tails) > 1:
                        coherence = 1.0 - np.std(tails) / 3.0  # å½’ä¸€åŒ–æ ‡å‡†å·®
                        order_params['coherence_parameter'].append(max(0, coherence))
                    else:
                        order_params['coherence_parameter'].append(1.0)
                    
                    # 4. å¤æ‚åº¦åºå‚é‡
                    if len(tail_counts) > 0:
                        probabilities = tail_counts / np.sum(tail_counts)
                        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
                        complexity = entropy / np.log2(10)  # å½’ä¸€åŒ–ç†µ
                        order_params['complexity_parameter'].append(complexity)
                    else:
                        order_params['complexity_parameter'].append(0.0)
                else:
                    # å¤„ç†ç©ºæœŸçš„æƒ…å†µ
                    order_params['concentration_parameter'].append(0.0)
                    order_params['symmetry_parameter'].append(0.0)
                    order_params['coherence_parameter'].append(0.0)
                    order_params['complexity_parameter'].append(0.0)
            
            return order_params
            
        except Exception as e:
            return {}
    
    def _detect_first_order_transitions(self, order_parameters: Dict) -> List[Dict]:
        """æ£€æµ‹ä¸€é˜¶ç›¸å˜ï¼ˆä¸è¿ç»­ç›¸å˜ï¼‰"""
        try:
            transitions = []
            
            for param_name, param_values in order_parameters.items():
                if len(param_values) < 5:
                    continue
                
                # æ£€æµ‹è·³è·ƒæ€§å˜åŒ–
                for i in range(1, len(param_values)):
                    change = abs(param_values[i] - param_values[i-1])
                    
                    # ä¸€é˜¶ç›¸å˜çš„ç‰¹å¾ï¼šçªç„¶çš„å¤§å¹…å˜åŒ–
                    if change > 0.3:  # é˜ˆå€¼å¯è°ƒ
                        # éªŒè¯æ˜¯å¦ä¸ºçœŸæ­£çš„ç›¸å˜
                        if self._validate_phase_transition(param_values, i):
                            transitions.append({
                                'type': 'first_order',
                                'parameter': param_name,
                                'transition_point': i,
                                'jump_magnitude': change,
                                'before_value': param_values[i-1],
                                'after_value': param_values[i]
                            })
            
            return transitions
            
        except Exception as e:
            return []
    
    def _detect_second_order_transitions(self, order_parameters: Dict) -> List[Dict]:
        """æ£€æµ‹äºŒé˜¶ç›¸å˜ï¼ˆè¿ç»­ç›¸å˜ï¼‰"""
        try:
            transitions = []
            
            for param_name, param_values in order_parameters.items():
                if len(param_values) < 8:
                    continue
                
                # æ£€æµ‹æ¸å˜æ€§è´¨å˜
                gradients = [param_values[i+1] - param_values[i] 
                           for i in range(len(param_values)-1)]
                
                # å¯»æ‰¾æ¢¯åº¦å˜åŒ–çš„è½¬æŠ˜ç‚¹
                for i in range(2, len(gradients)-2):
                    before_gradient = np.mean(gradients[i-2:i])
                    after_gradient = np.mean(gradients[i:i+2])
                    
                    gradient_change = abs(after_gradient - before_gradient)
                    
                    # äºŒé˜¶ç›¸å˜ç‰¹å¾ï¼šæ¢¯åº¦çš„æ˜¾è‘—å˜åŒ–
                    if gradient_change > 0.1:
                        transitions.append({
                            'type': 'second_order',
                            'parameter': param_name,
                            'transition_point': i,
                            'gradient_change': gradient_change,
                            'critical_behavior': self._analyze_critical_behavior(param_values, i)
                        })
            
            return transitions
            
        except Exception as e:
            return []
    
    def _validate_phase_transition(self, param_values: List[float], transition_point: int) -> bool:
        """éªŒè¯ç›¸å˜çš„çœŸå®æ€§"""
        try:
            # æ£€æŸ¥ç›¸å˜å‰åçš„ç¨³å®šæ€§
            window_size = 2
            
            if transition_point < window_size or transition_point >= len(param_values) - window_size:
                return False
            
            before_values = param_values[transition_point-window_size:transition_point]
            after_values = param_values[transition_point:transition_point+window_size]
            
            # ç›¸å˜å‰ååº”è¯¥ç›¸å¯¹ç¨³å®š
            before_stability = np.std(before_values) < 0.1
            after_stability = np.std(after_values) < 0.1
            
            # ç›¸å˜å‰åçš„å€¼åº”è¯¥æ˜¾è‘—ä¸åŒ
            significant_difference = abs(np.mean(before_values) - np.mean(after_values)) > 0.2
            
            return before_stability and after_stability and significant_difference
            
        except Exception as e:
            return False
    
    def _analyze_critical_behavior(self, param_values: List[float], critical_point: int) -> Dict:
        """åˆ†æä¸´ç•Œè¡Œä¸º"""
        try:
            critical_analysis = {
                'critical_exponent': 0.0,
                'correlation_length': 0.0,
                'fluctuation_amplitude': 0.0
            }
            
            if critical_point < 3 or critical_point >= len(param_values) - 3:
                return critical_analysis
            
            # åˆ†æä¸´ç•Œç‚¹é™„è¿‘çš„è¡Œä¸º
            vicinity = param_values[critical_point-3:critical_point+4]
            
            # è®¡ç®—ä¸´ç•ŒæŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            distances = [abs(i - 3) + 0.1 for i in range(len(vicinity))]  # è·ç¦»ä¸´ç•Œç‚¹çš„è·ç¦»
            values = [abs(v - param_values[critical_point]) + 1e-10 for v in vicinity]
            
            if len(distances) == len(values) and len(distances) > 3:
                # æ‹Ÿåˆå¹‚å¾‹å…³ç³» value ~ distance^beta
                log_distances = np.log(distances)
                log_values = np.log(values)
                
                try:
                    slope = np.polyfit(log_distances, log_values, 1)[0]
                    critical_analysis['critical_exponent'] = abs(slope)
                except:
                    pass
            
            # è®¡ç®—ç›¸å…³é•¿åº¦
            autocorr_values = []
            for lag in range(1, min(5, len(param_values) - critical_point)):
                if critical_point + lag < len(param_values):
                    correlation = abs(param_values[critical_point] - param_values[critical_point + lag])
                    autocorr_values.append(correlation)
            
            if autocorr_values:
                correlation_length = np.mean(autocorr_values)
                critical_analysis['correlation_length'] = correlation_length
            
            # è®¡ç®—æ¶¨è½å¹…åº¦
            fluctuation_amplitude = np.std(vicinity)
            critical_analysis['fluctuation_amplitude'] = fluctuation_amplitude
            
            return critical_analysis
            
        except Exception as e:
            return {}
    
    def _identify_critical_points(self, order_parameters: Dict) -> List[Dict]:
        """è¯†åˆ«ä¸´ç•Œç‚¹"""
        try:
            critical_points = []
            
            for param_name, param_values in order_parameters.items():
                if len(param_values) < 6:
                    continue
                
                # å¯»æ‰¾äºŒé˜¶å¯¼æ•°çš„æå€¼ç‚¹
                first_derivatives = [param_values[i+1] - param_values[i] 
                                   for i in range(len(param_values)-1)]
                
                if len(first_derivatives) < 3:
                    continue
                
                second_derivatives = [first_derivatives[i+1] - first_derivatives[i] 
                                    for i in range(len(first_derivatives)-1)]
                
                # å¯»æ‰¾äºŒé˜¶å¯¼æ•°çš„æå€¼
                for i in range(1, len(second_derivatives)-1):
                    if (second_derivatives[i] > second_derivatives[i-1] and 
                        second_derivatives[i] > second_derivatives[i+1]):
                        # æ‰¾åˆ°æå¤§å€¼ç‚¹
                        critical_points.append({
                            'parameter': param_name,
                            'position': i + 1,  # è°ƒæ•´ç´¢å¼•
                            'type': 'maximum',
                            'curvature': second_derivatives[i],
                            'order_parameter_value': param_values[i+1] if i+1 < len(param_values) else 0
                        })
                    elif (second_derivatives[i] < second_derivatives[i-1] and 
                          second_derivatives[i] < second_derivatives[i+1]):
                        # æ‰¾åˆ°æå°å€¼ç‚¹
                        critical_points.append({
                            'parameter': param_name,
                            'position': i + 1,
                            'type': 'minimum',
                            'curvature': abs(second_derivatives[i]),
                            'order_parameter_value': param_values[i+1] if i+1 < len(param_values) else 0
                        })
            
            return critical_points
            
        except Exception as e:
            return []
    
    def _construct_phase_diagram(self, processed_data: List[Dict], order_parameters: Dict) -> Dict:
        """æ„å»ºç›¸å›¾"""
        try:
            phase_diagram = {
                'phases_identified': [],
                'phase_boundaries': [],
                'phase_regions': {},
                'stability_analysis': {}
            }
            
            if not order_parameters:
                return phase_diagram
            
            # åŸºäºåºå‚é‡è¯†åˆ«ä¸åŒç›¸
            concentration_params = order_parameters.get('concentration_parameter', [])
            symmetry_params = order_parameters.get('symmetry_parameter', [])
            
            if len(concentration_params) < 5 or len(symmetry_params) < 5:
                return phase_diagram
            
            # å®šä¹‰ç›¸çš„åˆ†ç±»æ ‡å‡†
            phases = []
            for i in range(min(len(concentration_params), len(symmetry_params))):
                conc = concentration_params[i]
                symm = symmetry_params[i]
                
                # åŸºäºä¸¤ä¸ªåºå‚é‡å®šä¹‰ç›¸
                if conc > 0.7 and symm > 0.5:
                    phase = 'ordered_asymmetric'
                elif conc > 0.7 and symm <= 0.5:
                    phase = 'ordered_symmetric'
                elif conc <= 0.3 and symm <= 0.3:
                    phase = 'disordered_symmetric'
                elif conc <= 0.3 and symm > 0.3:
                    phase = 'disordered_asymmetric'
                else:
                    phase = 'intermediate'
                
                phases.append(phase)
            
            # è¯†åˆ«ç›¸è¾¹ç•Œ
            phase_boundaries = []
            for i in range(len(phases) - 1):
                if phases[i] != phases[i + 1]:
                    phase_boundaries.append({
                        'position': i,
                        'from_phase': phases[i],
                        'to_phase': phases[i + 1],
                        'transition_type': 'boundary'
                    })
            
            phase_diagram['phases_identified'] = list(set(phases))
            phase_diagram['phase_boundaries'] = phase_boundaries
            
            # åˆ†æç›¸åŒºåŸŸ
            from collections import Counter
            phase_counts = Counter(phases)
            total_points = len(phases)
            
            for phase, count in phase_counts.items():
                phase_diagram['phase_regions'][phase] = {
                    'stability': count / total_points,
                    'duration': count,
                    'dominance': count == max(phase_counts.values())
                }
            
            return phase_diagram
            
        except Exception as e:
            return {}
    
    def _detect_hysteresis_behavior(self, order_parameters: Dict) -> bool:
        """æ£€æµ‹æ»å›è¡Œä¸º"""
        try:
            # æ£€æŸ¥åºå‚é‡æ˜¯å¦è¡¨ç°å‡ºæ»å›ç‰¹æ€§
            for param_name, param_values in order_parameters.items():
                if len(param_values) < 10:
                    continue
                
                # åˆ†æä¸Šå‡å’Œä¸‹é™é˜¶æ®µ
                gradients = [param_values[i+1] - param_values[i] 
                           for i in range(len(param_values)-1)]
                
                # è¯†åˆ«ä¸Šå‡å’Œä¸‹é™è¶‹åŠ¿
                increasing_phases = []
                decreasing_phases = []
                
                current_trend = None
                trend_start = 0
                
                for i, gradient in enumerate(gradients):
                    if gradient > 0.05:  # ä¸Šå‡
                        if current_trend != 'increasing':
                            if current_trend == 'decreasing':
                                decreasing_phases.append((trend_start, i))
                            current_trend = 'increasing'
                            trend_start = i
                    elif gradient < -0.05:  # ä¸‹é™
                        if current_trend != 'decreasing':
                            if current_trend == 'increasing':
                                increasing_phases.append((trend_start, i))
                            current_trend = 'decreasing'
                            trend_start = i
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ»å›ç¯
                if len(increasing_phases) >= 1 and len(decreasing_phases) >= 1:
                    # ç®€åŒ–çš„æ»å›æ£€æµ‹ï¼šæ£€æŸ¥ç›¸åŒå‚æ•°å€¼ä¸‹çš„ä¸åŒå“åº”
                    for inc_start, inc_end in increasing_phases:
                        for dec_start, dec_end in decreasing_phases:
                            if inc_end < dec_start:  # ç¡®ä¿æ—¶é—´é¡ºåº
                                inc_values = param_values[inc_start:inc_end+1]
                                dec_values = param_values[dec_start:dec_end+1]
                                
                                # æ£€æŸ¥é‡å åŒºåŸŸçš„ä¸åŒå“åº”
                                if (max(min(inc_values), min(dec_values)) < 
                                    min(max(inc_values), max(dec_values))):
                                    return True
            
            return False
            
        except Exception as e:
            return False
    
    def _classify_transition_types(self, transitions: List[Dict]) -> List[str]:
        """åˆ†ç±»ç›¸å˜ç±»å‹"""
        try:
            types = []
            
            for transition in transitions:
                transition_type = transition.get('type', 'unknown')
                parameter = transition.get('parameter', '')
                
                # åŸºäºå‚æ•°ç±»å‹å’Œå˜åŒ–ç‰¹å¾åˆ†ç±»
                if transition_type == 'first_order':
                    if 'concentration' in parameter:
                        types.append('clustering_transition')
                    elif 'symmetry' in parameter:
                        types.append('symmetry_breaking')
                    else:
                        types.append('discontinuous_transition')
                elif transition_type == 'second_order':
                    if 'coherence' in parameter:
                        types.append('coherence_transition')
                    elif 'complexity' in parameter:
                        types.append('complexity_transition')
                    else:
                        types.append('continuous_transition')
                else:
                    types.append('unknown_transition')
            
            return list(set(types))  # å»é‡
            
        except Exception as e:
            return []
    
    def _analyze_critical_phenomena(self, processed_data: List[Dict]) -> Dict:
        """
        åˆ†æä¸´ç•Œç°è±¡ - åŸºäºä¸´ç•Œç‚¹ç†è®ºå’Œæ ‡åº¦å¾‹
        å®ç°Wilsoné‡æ•´åŒ–ç¾¤ç†è®ºçš„ä¸´ç•Œè¡Œä¸ºåˆ†æ
        """
        try:
            critical_analysis = {
                'critical_points_detected': [],
                'scaling_behavior': {},
                'universality_class': '',
                'critical_exponents': {},
                'correlation_functions': {},
                'finite_size_effects': {}
            }
            
            if len(processed_data) < 15:
                return critical_analysis
            
            # 1. ä¸´ç•Œç‚¹æ£€æµ‹
            critical_points = self._detect_critical_points_advanced(processed_data)
            critical_analysis['critical_points_detected'] = critical_points
            
            # 2. æ ‡åº¦è¡Œä¸ºåˆ†æ
            scaling_behavior = self._analyze_scaling_behavior(processed_data, critical_points)
            critical_analysis['scaling_behavior'] = scaling_behavior
            
            # 3. ä¸´ç•ŒæŒ‡æ•°è®¡ç®—
            critical_exponents = self._calculate_critical_exponents(processed_data, critical_points)
            critical_analysis['critical_exponents'] = critical_exponents
            
            # 4. ç›¸å…³å‡½æ•°åˆ†æ
            correlation_functions = self._analyze_correlation_functions(processed_data)
            critical_analysis['correlation_functions'] = correlation_functions
            
            # 5. æ™®é€‚æ€§ç±»åˆ«ç¡®å®š
            universality_class = self._determine_universality_class(critical_exponents)
            critical_analysis['universality_class'] = universality_class
            
            # 6. æœ‰é™å°ºå¯¸æ•ˆåº”
            finite_size_effects = self._analyze_finite_size_effects(processed_data)
            critical_analysis['finite_size_effects'] = finite_size_effects
            
            return critical_analysis
            
        except Exception as e:
            print(f"      âŒ ä¸´ç•Œç°è±¡åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _detect_critical_points_advanced(self, processed_data: List[Dict]) -> List[Dict]:
        """é«˜çº§ä¸´ç•Œç‚¹æ£€æµ‹"""
        try:
            critical_points = []
            
            # è®¡ç®—å¤šç§åºå‚é‡
            order_params = self._calculate_order_parameters(processed_data)
            
            for param_name, param_values in order_params.items():
                if len(param_values) < 8:
                    continue
                
                # ä½¿ç”¨å¤šç§æ–¹æ³•æ£€æµ‹ä¸´ç•Œç‚¹
                
                # æ–¹æ³•1ï¼šæ¯”çƒ­å®¹å³°å€¼æ£€æµ‹
                specific_heat = self._calculate_specific_heat_analog(param_values)
                heat_peaks = self._find_peaks(specific_heat)
                
                # æ–¹æ³•2ï¼šç£åŒ–ç‡å³°å€¼æ£€æµ‹  
                susceptibility = self._calculate_susceptibility_analog(param_values)
                suscept_peaks = self._find_peaks(susceptibility)
                
                # æ–¹æ³•3ï¼šBinderç´¯ç§¯é‡åˆ†æ
                binder_cumulant = self._calculate_binder_cumulant(param_values)
                binder_crossings = self._find_binder_crossings(binder_cumulant)
                
                # ç»¼åˆåˆ¤æ–­ä¸´ç•Œç‚¹
                all_candidates = set(heat_peaks + suscept_peaks + binder_crossings)
                
                for candidate in all_candidates:
                    if self._validate_critical_point(param_values, candidate):
                        critical_points.append({
                            'parameter': param_name,
                            'position': candidate,
                            'detection_methods': [],
                            'strength': self._calculate_criticality_strength(param_values, candidate)
                        })
                        
                        # è®°å½•æ£€æµ‹æ–¹æ³•
                        if candidate in heat_peaks:
                            critical_points[-1]['detection_methods'].append('specific_heat')
                        if candidate in suscept_peaks:
                            critical_points[-1]['detection_methods'].append('susceptibility')
                        if candidate in binder_crossings:
                            critical_points[-1]['detection_methods'].append('binder_cumulant')
            
            return critical_points
            
        except Exception as e:
            return []
    
    def _calculate_specific_heat_analog(self, param_values: List[float]) -> List[float]:
        """è®¡ç®—æ¯”çƒ­å®¹ç±»ä¼¼ç‰©"""
        try:
            if len(param_values) < 5:
                return []
            
            # æ¯”çƒ­å®¹ ~ èƒ½é‡æ¶¨è½ ~ å‚æ•°æ¶¨è½çš„å¯¼æ•°
            fluctuations = []
            window_size = 3
            
            for i in range(window_size, len(param_values) - window_size):
                window = param_values[i-window_size:i+window_size+1]
                variance = np.var(window)
                fluctuations.append(variance)
            
            return fluctuations
            
        except Exception as e:
            return []
    
    def _calculate_susceptibility_analog(self, param_values: List[float]) -> List[float]:
        """è®¡ç®—ç£åŒ–ç‡ç±»ä¼¼ç‰©"""
        try:
            if len(param_values) < 5:
                return []
            
            # ç£åŒ–ç‡ ~ ç£åŒ–å¼ºåº¦æ¶¨è½ ~ å‚æ•°å¯¹å¤–åœºçš„å“åº”
            susceptibility = []
            
            for i in range(2, len(param_values) - 2):
                # è®¡ç®—å±€éƒ¨å“åº”
                local_gradient = (param_values[i+1] - param_values[i-1]) / 2
                local_curvature = param_values[i+1] - 2*param_values[i] + param_values[i-1]
                
                # ç£åŒ–ç‡è¿‘ä¼¼
                response = abs(local_gradient) + abs(local_curvature)
                susceptibility.append(response)
            
            return susceptibility
            
        except Exception as e:
            return []
    
    def _calculate_binder_cumulant(self, param_values: List[float]) -> List[float]:
        """è®¡ç®—Binderç´¯ç§¯é‡"""
        try:
            if len(param_values) < 6:
                return []
            
            binder_values = []
            window_size = 3
            
            for i in range(window_size, len(param_values) - window_size):
                window = param_values[i-window_size:i+window_size+1]
                
                if len(window) > 2:
                    mean = np.mean(window)
                    second_moment = np.mean([(x - mean)**2 for x in window])
                    fourth_moment = np.mean([(x - mean)**4 for x in window])
                    
                    if second_moment > 1e-10:
                        binder = 1 - fourth_moment / (3 * second_moment**2)
                        binder_values.append(binder)
                    else:
                        binder_values.append(0.0)
                else:
                    binder_values.append(0.0)
            
            return binder_values
            
        except Exception as e:
            return []
    
    def _find_peaks(self, values: List[float]) -> List[int]:
        """å¯»æ‰¾å³°å€¼"""
        try:
            if len(values) < 3:
                return []
            
            peaks = []
            for i in range(1, len(values) - 1):
                if (values[i] > values[i-1] and values[i] > values[i+1] and 
                    values[i] > np.mean(values) + np.std(values)):
                    peaks.append(i)
            
            return peaks
            
        except Exception as e:
            return []
    
    def _find_binder_crossings(self, binder_values: List[float]) -> List[int]:
        """å¯»æ‰¾Binderç´¯ç§¯é‡äº¤å‰ç‚¹"""
        try:
            crossings = []
            target_value = 2.0/3.0  # ä¸‰ç»´Isingæ¨¡å‹çš„æ™®é€‚å€¼
            
            for i in range(len(binder_values) - 1):
                if ((binder_values[i] - target_value) * (binder_values[i+1] - target_value) < 0):
                    crossings.append(i)
            
            return crossings
            
        except Exception as e:
            return []
    
    def _validate_critical_point(self, param_values: List[float], candidate: int) -> bool:
        """éªŒè¯ä¸´ç•Œç‚¹"""
        try:
            if candidate < 2 or candidate >= len(param_values) - 2:
                return False
            
            # æ£€æŸ¥ä¸´ç•Œç‚¹é™„è¿‘çš„è¡Œä¸º
            vicinity = param_values[candidate-2:candidate+3]
            
            # ä¸´ç•Œç‚¹åº”è¯¥æ˜¯æŸç§æå€¼æˆ–è½¬æŠ˜ç‚¹
            center_value = vicinity[2]  # å€™é€‰ç‚¹çš„å€¼
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå±€éƒ¨æå€¼æˆ–å¼ºå˜åŒ–ç‚¹
            is_local_max = all(center_value >= v for v in vicinity)
            is_local_min = all(center_value <= v for v in vicinity)
            
            # æ£€æŸ¥å˜åŒ–ç‡
            left_gradient = center_value - vicinity[1]
            right_gradient = vicinity[3] - center_value
            gradient_change = abs(right_gradient - left_gradient)
            
            return is_local_max or is_local_min or gradient_change > 0.2
            
        except Exception as e:
            return False
    
    def _calculate_criticality_strength(self, param_values: List[float], position: int) -> float:
        """è®¡ç®—ä¸´ç•Œæ€§å¼ºåº¦"""
        try:
            if position < 2 or position >= len(param_values) - 2:
                return 0.0
            
            # åŸºäºå±€éƒ¨å˜åŒ–ç‡è®¡ç®—å¼ºåº¦
            vicinity = param_values[position-2:position+3]
            center = vicinity[2]
            
            # è®¡ç®—å±€éƒ¨æ–¹å·®
            local_variance = np.var(vicinity)
            
            # è®¡ç®—æ¢¯åº¦å˜åŒ–
            left_grad = center - vicinity[1] 
            right_grad = vicinity[3] - center
            gradient_change = abs(right_grad - left_grad)
            
            # ç»¼åˆå¼ºåº¦
            strength = (local_variance + gradient_change) / 2
            return min(1.0, strength)
            
        except Exception as e:
            return 0.0
    
    def _analyze_scaling_behavior(self, processed_data: List[Dict], critical_points: List[Dict]) -> Dict:
        """åˆ†ææ ‡åº¦è¡Œä¸º"""
        try:
            scaling_analysis = {
                'scaling_laws_detected': [],
                'scaling_functions': {},
                'data_collapse_quality': 0.0,
                'scaling_regions': []
            }
            
            if not critical_points:
                return scaling_analysis
            
            # å¯¹æ¯ä¸ªä¸´ç•Œç‚¹åˆ†ææ ‡åº¦è¡Œä¸º
            for cp in critical_points:
                position = cp['position']
                parameter = cp['parameter']
                
                # æ„å»ºæ ‡åº¦å˜é‡
                scaling_vars = self._construct_scaling_variables(processed_data, position)
                
                # æ£€æµ‹å¹‚å¾‹æ ‡åº¦
                power_laws = self._detect_power_law_scaling(scaling_vars)
                scaling_analysis['scaling_laws_detected'].extend(power_laws)
                
                # åˆ†ææ ‡åº¦å‡½æ•°
                scaling_func = self._analyze_scaling_function(scaling_vars, position)
                scaling_analysis['scaling_functions'][f'{parameter}_{position}'] = scaling_func
            
            return scaling_analysis
            
        except Exception as e:
            return {}
    
    def _construct_scaling_variables(self, processed_data: List[Dict], critical_position: int) -> Dict:
        """æ„å»ºæ ‡åº¦å˜é‡"""
        try:
            scaling_vars = {
                'reduced_distance': [],    # çº¦åŒ–è·ç¦» t = (T - Tc)/Tc
                'order_parameter': [],     # åºå‚é‡
                'correlation_length': [],  # ç›¸å…³é•¿åº¦
                'system_size': len(processed_data)
            }
            
            # è®¡ç®—çº¦åŒ–è·ç¦»
            for i, period in enumerate(processed_data):
                distance_from_critical = abs(i - critical_position) / len(processed_data)
                scaling_vars['reduced_distance'].append(distance_from_critical)
                
                # è®¡ç®—åºå‚é‡
                tails = period.get('tails', [])
                if tails:
                    tail_counts = np.bincount(tails, minlength=10)
                    order_param = self._calculate_gini_coefficient(tail_counts)
                    scaling_vars['order_parameter'].append(order_param)
                else:
                    scaling_vars['order_parameter'].append(0.0)
            
            return scaling_vars
            
        except Exception as e:
            return {}
    
    def _detect_power_law_scaling(self, scaling_vars: Dict) -> List[Dict]:
        """æ£€æµ‹å¹‚å¾‹æ ‡åº¦"""
        try:
            power_laws = []
            
            distances = scaling_vars.get('reduced_distance', [])
            order_params = scaling_vars.get('order_parameter', [])
            
            if len(distances) != len(order_params) or len(distances) < 6:
                return power_laws
            
            # æ’é™¤é›¶ç‚¹å’Œä¸´ç•Œç‚¹
            valid_indices = [i for i, (d, o) in enumerate(zip(distances, order_params)) 
                           if d > 0.01 and o > 0.01]
            
            if len(valid_indices) < 4:
                return power_laws
            
            valid_distances = [distances[i] for i in valid_indices]
            valid_order_params = [order_params[i] for i in valid_indices]
            
            # æ‹Ÿåˆå¹‚å¾‹ order_param ~ distance^beta
            log_distances = np.log(valid_distances)
            log_order_params = np.log(valid_order_params)
            
            try:
                slope, intercept = np.polyfit(log_distances, log_order_params, 1)
                
                # è®¡ç®—æ‹Ÿåˆè´¨é‡
                predicted = slope * np.array(log_distances) + intercept
                r_squared = 1 - np.sum((log_order_params - predicted)**2) / np.sum((log_order_params - np.mean(log_order_params))**2)
                
                if r_squared > 0.7:  # è‰¯å¥½æ‹Ÿåˆ
                    power_laws.append({
                        'exponent': abs(slope),
                        'fit_quality': r_squared,
                        'scaling_type': 'order_parameter',
                        'critical_exponent_type': 'beta'
                    })
            except:
                pass
            
            return power_laws
            
        except Exception as e:
            return []
    
    def _analyze_scaling_function(self, scaling_vars: Dict, critical_position: int) -> Dict:
        """åˆ†ææ ‡åº¦å‡½æ•°"""
        try:
            scaling_func = {
                'function_type': 'unknown',
                'scaling_form': '',
                'collapse_quality': 0.0
            }
            
            distances = scaling_vars.get('reduced_distance', [])
            order_params = scaling_vars.get('order_parameter', [])
            
            if len(distances) < 6:
                return scaling_func
            
            # å°è¯•ä¸åŒçš„æ ‡åº¦å½¢å¼
            forms = [
                ('power_law', lambda x, a, b: a * np.power(x + 1e-10, b)),
                ('exponential', lambda x, a, b: a * np.exp(-b * x)),
                ('stretched_exponential', lambda x, a, b, c: a * np.exp(-b * np.power(x, c)))
            ]
            
            best_fit = 0.0
            best_form = 'unknown'
            
            for form_name, form_func in forms:
                try:
                    # ç®€åŒ–çš„æ‹Ÿåˆè¯„ä¼°
                    x_data = np.array(distances[1:])  # æ’é™¤é›¶ç‚¹
                    y_data = np.array(order_params[1:])
                    
                    if len(x_data) >= 3:
                        # ä½¿ç”¨ç›¸å…³ç³»æ•°ä½œä¸ºæ‹Ÿåˆè´¨é‡çš„ç®€å•åº¦é‡
                        if form_name == 'power_law':
                            log_x = np.log(x_data + 1e-10)
                            log_y = np.log(y_data + 1e-10)
                            correlation = abs(np.corrcoef(log_x, log_y)[0, 1])
                        else:
                            correlation = abs(np.corrcoef(x_data, y_data)[0, 1])
                        
                        if not np.isnan(correlation) and correlation > best_fit:
                            best_fit = correlation
                            best_form = form_name
                except:
                    continue
            
            scaling_func['function_type'] = best_form
            scaling_func['collapse_quality'] = best_fit
            
            return scaling_func
            
        except Exception as e:
            return {}
    
    def _calculate_critical_exponents(self, processed_data: List[Dict], critical_points: List[Dict]) -> Dict:
        """è®¡ç®—ä¸´ç•ŒæŒ‡æ•°"""
        try:
            exponents = {
                'beta': 0.0,      # åºå‚é‡æŒ‡æ•°
                'gamma': 0.0,     # ç£åŒ–ç‡æŒ‡æ•°  
                'nu': 0.0,        # ç›¸å…³é•¿åº¦æŒ‡æ•°
                'alpha': 0.0      # æ¯”çƒ­æŒ‡æ•°
            }
            
            if not critical_points:
                return exponents
            
            # é€‰æ‹©æœ€å¼ºçš„ä¸´ç•Œç‚¹è¿›è¡Œåˆ†æ
            strongest_cp = max(critical_points, key=lambda x: x.get('strength', 0))
            cp_position = strongest_cp['position']
            
            # è®¡ç®—å„ç§ç‰©ç†é‡
            order_params = self._calculate_order_parameters(processed_data)
            
            for param_name, param_values in order_params.items():
                if len(param_values) < cp_position + 3 or cp_position < 3:
                    continue
                
                # è®¡ç®—Î²æŒ‡æ•°ï¼ˆåºå‚é‡ï¼‰
                beta_exp = self._calculate_beta_exponent(param_values, cp_position)
                if beta_exp > 0:
                    exponents['beta'] = max(exponents['beta'], beta_exp)
                
                # è®¡ç®—Î³æŒ‡æ•°ï¼ˆç£åŒ–ç‡ï¼‰
                susceptibility = self._calculate_susceptibility_analog(param_values)
                if susceptibility:
                    gamma_exp = self._calculate_gamma_exponent(susceptibility, cp_position)
                    if gamma_exp > 0:
                        exponents['gamma'] = max(exponents['gamma'], gamma_exp)
                
                # è®¡ç®—Î±æŒ‡æ•°ï¼ˆæ¯”çƒ­ï¼‰
                specific_heat = self._calculate_specific_heat_analog(param_values)
                if specific_heat:
                    alpha_exp = self._calculate_alpha_exponent(specific_heat, cp_position)
                    if alpha_exp > 0:
                        exponents['alpha'] = max(exponents['alpha'], alpha_exp)
            
            return exponents
            
        except Exception as e:
            return {}
    
    def _calculate_beta_exponent(self, param_values: List[float], cp_position: int) -> float:
        """è®¡ç®—Î²æŒ‡æ•°"""
        try:
            # Î²æŒ‡æ•°ï¼šåºå‚é‡ ~ |t|^Î²
            distances = []
            order_values = []
            
            for i in range(len(param_values)):
                if i != cp_position and abs(i - cp_position) <= 5:
                    distance = abs(i - cp_position) / len(param_values)
                    if distance > 0 and param_values[i] > 0:
                        distances.append(distance)
                        order_values.append(param_values[i])
            
            if len(distances) < 3:
                return 0.0
            
            # æ‹Ÿåˆ log(order) ~ Î² * log(distance)
            log_distances = np.log(distances)
            log_orders = np.log(order_values)
            
            try:
                slope = np.polyfit(log_distances, log_orders, 1)[0]
                return abs(slope)
            except:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def _calculate_gamma_exponent(self, susceptibility: List[float], cp_position: int) -> float:
        """è®¡ç®—Î³æŒ‡æ•°"""
        try:
            if cp_position >= len(susceptibility):
                return 0.0
            
            # Î³æŒ‡æ•°ï¼šç£åŒ–ç‡ ~ |t|^(-Î³)
            distances = []
            suscept_values = []
            
            for i in range(len(susceptibility)):
                distance_from_cp = abs(i - cp_position)
                if 1 <= distance_from_cp <= 4 and susceptibility[i] > 0:
                    distance = distance_from_cp / len(susceptibility)
                    distances.append(distance)
                    suscept_values.append(susceptibility[i])
            
            if len(distances) < 3:
                return 0.0
            
            # æ‹Ÿåˆ log(susceptibility) ~ -Î³ * log(distance)
            log_distances = np.log(distances)
            log_suscept = np.log(suscept_values)
            
            try:
                slope = np.polyfit(log_distances, log_suscept, 1)[0]
                return abs(slope)  # Î³åº”ä¸ºæ­£å€¼
            except:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def _calculate_alpha_exponent(self, specific_heat: List[float], cp_position: int) -> float:
        """è®¡ç®—Î±æŒ‡æ•°"""
        try:
            if cp_position >= len(specific_heat):
                return 0.0
            
            # Î±æŒ‡æ•°ï¼šæ¯”çƒ­ ~ |t|^(-Î±)
            distances = []
            heat_values = []
            
            for i in range(len(specific_heat)):
                distance_from_cp = abs(i - cp_position)
                if 1 <= distance_from_cp <= 3 and specific_heat[i] > 0:
                    distance = distance_from_cp / len(specific_heat)
                    distances.append(distance)
                    heat_values.append(specific_heat[i])
            
            if len(distances) < 3:
                return 0.0
            
            # æ‹Ÿåˆ log(heat) ~ -Î± * log(distance)  
            log_distances = np.log(distances)
            log_heat = np.log(heat_values)
            
            try:
                slope = np.polyfit(log_distances, log_heat, 1)[0]
                return abs(slope)
            except:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def _analyze_correlation_functions(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æç›¸å…³å‡½æ•°"""
        try:
            correlation_analysis = {
                'spatial_correlations': {},
                'temporal_correlations': {},
                'correlation_length': 0.0,
                'correlation_decay': 'unknown'
            }
            
            if len(processed_data) < 10:
                return correlation_analysis
            
            # æ—¶é—´ç›¸å…³å‡½æ•°
            temporal_corr = self._calculate_temporal_correlations(processed_data)
            correlation_analysis['temporal_correlations'] = temporal_corr
            
            # ä¼°ç®—ç›¸å…³é•¿åº¦
            correlation_length = self._estimate_correlation_length(temporal_corr)
            correlation_analysis['correlation_length'] = correlation_length
            
            # åˆ†æè¡°å‡ç±»å‹
            decay_type = self._analyze_correlation_decay(temporal_corr)
            correlation_analysis['correlation_decay'] = decay_type
            
            return correlation_analysis
            
        except Exception as e:
            return {}
    
    def _calculate_temporal_correlations(self, processed_data: List[Dict]) -> Dict:
        """è®¡ç®—æ—¶é—´ç›¸å…³å‡½æ•°"""
        try:
            correlations = {}
            
            # ä¸ºæ¯ä¸ªå°¾æ•°è®¡ç®—è‡ªç›¸å…³å‡½æ•°
            for tail in range(10):
                sequence = [1 if tail in period.get('tails', []) else 0 
                           for period in processed_data]
                
                autocorrelations = []
                for lag in range(1, min(8, len(sequence)//2)):
                    if len(sequence) > lag:
                        correlation = np.corrcoef(sequence[:-lag], sequence[lag:])[0, 1]
                        if not np.isnan(correlation):
                            autocorrelations.append(correlation)
                        else:
                            autocorrelations.append(0.0)
                
                correlations[f'tail_{tail}'] = autocorrelations
            
            # è®¡ç®—å¹³å‡ç›¸å…³å‡½æ•°
            if correlations:
                max_len = max(len(corr) for corr in correlations.values())
                avg_correlation = []
                
                for lag in range(max_len):
                    lag_values = []
                    for corr_list in correlations.values():
                        if lag < len(corr_list):
                            lag_values.append(abs(corr_list[lag]))
                    
                    if lag_values:
                        avg_correlation.append(np.mean(lag_values))
                
                correlations['average'] = avg_correlation
            
            return correlations
            
        except Exception as e:
            return {}
    
    def _estimate_correlation_length(self, temporal_correlations: Dict) -> float:
        """ä¼°ç®—ç›¸å…³é•¿åº¦"""
        try:
            avg_corr = temporal_correlations.get('average', [])
            if not avg_corr:
                return 0.0
            
            # å¯»æ‰¾ç›¸å…³å‡½æ•°è¡°å‡åˆ°1/eçš„è·ç¦»
            threshold = 1.0 / np.e
            
            for i, corr in enumerate(avg_corr):
                if corr < threshold:
                    return float(i + 1)
            
            # å¦‚æœæ²¡æœ‰è¡°å‡åˆ°é˜ˆå€¼ï¼Œè¿”å›è§‚å¯Ÿåˆ°çš„æœ€å¤§é•¿åº¦
            return float(len(avg_corr))
            
        except Exception as e:
            return 0.0
    
    def _analyze_correlation_decay(self, temporal_correlations: Dict) -> str:
        """åˆ†æç›¸å…³è¡°å‡ç±»å‹"""
        try:
            avg_corr = temporal_correlations.get('average', [])
            if len(avg_corr) < 4:
                return 'unknown'
            
            # æ£€æŸ¥ä¸åŒè¡°å‡æ¨¡å¼
            
            # æŒ‡æ•°è¡°å‡ï¼šcorr ~ exp(-r/Î¾)
            log_corr = [np.log(c + 1e-10) for c in avg_corr]
            exp_correlation = abs(np.corrcoef(range(len(log_corr)), log_corr)[0, 1])
            
            # å¹‚å¾‹è¡°å‡ï¼šcorr ~ r^(-Î·)
            log_distances = [np.log(i + 1) for i in range(len(avg_corr))]
            log_corr_power = [np.log(c + 1e-10) for c in avg_corr]
            power_correlation = abs(np.corrcoef(log_distances, log_corr_power)[0, 1])
            
            # åˆ¤æ–­è¡°å‡ç±»å‹
            if not np.isnan(exp_correlation) and not np.isnan(power_correlation):
                if exp_correlation > power_correlation and exp_correlation > 0.7:
                    return 'exponential'
                elif power_correlation > exp_correlation and power_correlation > 0.7:
                    return 'power_law'
                else:
                    return 'mixed'
            
            return 'unknown'
            
        except Exception as e:
            return 'unknown'
    
    def _determine_universality_class(self, critical_exponents: Dict) -> str:
        """ç¡®å®šæ™®é€‚æ€§ç±»åˆ«"""
        try:
            beta = critical_exponents.get('beta', 0)
            gamma = critical_exponents.get('gamma', 0)
            alpha = critical_exponents.get('alpha', 0)
            
            # ä¸å·²çŸ¥æ™®é€‚æ€§ç±»åˆ«æ¯”è¾ƒ
            universality_classes = {
                'mean_field': {'beta': 0.5, 'gamma': 1.0, 'alpha': 0.0},
                '2d_ising': {'beta': 0.125, 'gamma': 1.75, 'alpha': 0.0},
                '3d_ising': {'beta': 0.325, 'gamma': 1.24, 'alpha': 0.11},
                'xy_model': {'beta': 0.35, 'gamma': 1.32, 'alpha': -0.01},
                'heisenberg': {'beta': 0.37, 'gamma': 1.39, 'alpha': -0.13}
            }
            
            min_distance = float('inf')
            best_class = 'unknown'
            
            for class_name, theoretical_exponents in universality_classes.items():
                # è®¡ç®—æŒ‡æ•°ç©ºé—´ä¸­çš„è·ç¦»
                distance = 0
                count = 0
                
                for exp_name in ['beta', 'gamma', 'alpha']:
                    if critical_exponents.get(exp_name, 0) > 0:
                        theoretical = theoretical_exponents[exp_name]
                        experimental = critical_exponents[exp_name]
                        distance += (theoretical - experimental) ** 2
                        count += 1
                
                if count > 0:
                    distance = np.sqrt(distance / count)
                    if distance < min_distance:
                        min_distance = distance
                        best_class = class_name
            
            # åªæœ‰å½“è·ç¦»è¶³å¤Ÿå°æ—¶æ‰ç¡®å®šç±»åˆ«
            if min_distance < 0.3:
                return best_class
            else:
                return 'unknown'
                
        except Exception as e:
            return 'unknown'
    
    def _analyze_finite_size_effects(self, processed_data: List[Dict]) -> Dict:
        """åˆ†ææœ‰é™å°ºå¯¸æ•ˆåº”"""
        try:
            finite_size_analysis = {
                'size_effects_detected': False,
                'scaling_corrections': {},
                'effective_system_size': 0,
                'boundary_effects': 0.0
            }
            
            system_size = len(processed_data)
            finite_size_analysis['effective_system_size'] = system_size
            
            # æ£€æµ‹è¾¹ç•Œæ•ˆåº”
            boundary_effects = self._detect_boundary_effects(processed_data)
            finite_size_analysis['boundary_effects'] = boundary_effects
            
            # åˆ†æå°ºå¯¸æ ‡åº¦ä¿®æ­£
            if system_size < 50:  # å°ç³»ç»Ÿ
                scaling_corrections = self._analyze_size_scaling_corrections(processed_data)
                finite_size_analysis['scaling_corrections'] = scaling_corrections
                
                if scaling_corrections.get('correction_magnitude', 0) > 0.1:
                    finite_size_analysis['size_effects_detected'] = True
            
            return finite_size_analysis
            
        except Exception as e:
            return {}
    
    def _detect_boundary_effects(self, processed_data: List[Dict]) -> float:
        """æ£€æµ‹è¾¹ç•Œæ•ˆåº”"""
        try:
            if len(processed_data) < 10:
                return 0.0
            
            # æ¯”è¾ƒé¦–æœ«ä¸¤ç«¯çš„è¡Œä¸ºä¸ä¸­é—´éƒ¨åˆ†çš„å·®å¼‚
            edge_size = min(3, len(processed_data) // 5)
            
            start_edge = processed_data[:edge_size]
            end_edge = processed_data[-edge_size:]
            middle_part = processed_data[edge_size:-edge_size]
            
            if not middle_part:
                return 0.0
            
            # è®¡ç®—å„éƒ¨åˆ†çš„ç‰¹å¾
            def calc_features(data_slice):
                features = []
                for period in data_slice:
                    tails = period.get('tails', [])
                    features.append([
                        len(tails),
                        np.mean(tails) if tails else 5,
                        np.std(tails) if len(tails) > 1 else 0
                    ])
                return np.mean(features, axis=0) if features else [0, 0, 0]
            
            start_features = calc_features(start_edge)
            end_features = calc_features(end_edge)
            middle_features = calc_features(middle_part)
            
            # è®¡ç®—è¾¹ç•Œæ•ˆåº”å¼ºåº¦
            start_diff = np.linalg.norm(np.array(start_features) - np.array(middle_features))
            end_diff = np.linalg.norm(np.array(end_features) - np.array(middle_features))
            
            boundary_effect = (start_diff + end_diff) / 2
            return min(1.0, boundary_effect / 5.0)  # å½’ä¸€åŒ–
            
        except Exception as e:
            return 0.0
    
    def _analyze_size_scaling_corrections(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æå°ºå¯¸æ ‡åº¦ä¿®æ­£"""
        try:
            corrections = {
                'correction_magnitude': 0.0,
                'correction_type': 'unknown',
                'extrapolated_infinite_size': {}
            }
            
            system_size = len(processed_data)
            
            # åˆ†æåºå‚é‡çš„å°ºå¯¸ä¾èµ–æ€§
            order_params = self._calculate_order_parameters(processed_data)
            
            for param_name, param_values in order_params.items():
                if param_values:
                    avg_param = np.mean(param_values)
                    
                    # ä¼°ç®—æ— ç©·å¤§ç³»ç»Ÿçš„å¤–æ¨å€¼
                    # ä½¿ç”¨ç®€å•çš„1/Lä¿®æ­£ï¼šparam(L) = param(âˆ) + a/L
                    correction_estimate = avg_param * (1.0 / system_size)
                    infinite_size_estimate = avg_param - correction_estimate
                    
                    corrections['extrapolated_infinite_size'][param_name] = infinite_size_estimate
                    
                    # ä¿®æ­£å¹…åº¦
                    correction_magnitude = abs(correction_estimate / avg_param) if avg_param != 0 else 0
                    corrections['correction_magnitude'] = max(corrections['correction_magnitude'], correction_magnitude)
            
            # ç¡®å®šä¿®æ­£ç±»å‹
            if corrections['correction_magnitude'] > 0.2:
                corrections['correction_type'] = 'strong'
            elif corrections['correction_magnitude'] > 0.05:
                corrections['correction_type'] = 'moderate'
            else:
                corrections['correction_type'] = 'weak'
            
            return corrections
            
        except Exception as e:
            return {}
    
    def _identify_collective_behavior_modes(self, processed_data: List[Dict]) -> Dict:
        """
        è¯†åˆ«é›†ä½“è¡Œä¸ºæ¨¡å¼ - åŸºäºé›†ä½“æ™ºèƒ½å’Œç¾¤ä½“åŠ¨åŠ›å­¦ç†è®º
        å®ç°Vicsekæ¨¡å‹ã€Boidsæ¨¡å‹å’Œé›†ç¾¤è¡Œä¸ºåˆ†æ
        """
        try:
            collective_analysis = {
                'flocking_behavior': {},
                'swarming_patterns': {},
                'consensus_dynamics': {},
                'leader_follower_dynamics': {},
                'collective_memory_effects': {},
                'phase_synchronization': {}
            }
            
            if len(processed_data) < 12:
                return collective_analysis
            
            # 1. ç¾¤é›†è¡Œä¸ºåˆ†æ
            flocking_behavior = self._analyze_flocking_behavior(processed_data)
            collective_analysis['flocking_behavior'] = flocking_behavior
            
            # 2. é›†ç¾¤æ¨¡å¼è¯†åˆ«
            swarming_patterns = self._identify_swarming_patterns(processed_data)
            collective_analysis['swarming_patterns'] = swarming_patterns
            
            # 3. å…±è¯†åŠ¨åŠ›å­¦
            consensus_dynamics = self._analyze_consensus_dynamics(processed_data)
            collective_analysis['consensus_dynamics'] = consensus_dynamics
            
            # 4. é¢†å¯¼è€…-è·Ÿéšè€…åŠ¨åŠ›å­¦
            leader_follower = self._analyze_leader_follower_dynamics(processed_data)
            collective_analysis['leader_follower_dynamics'] = leader_follower
            
            # 5. é›†ä½“è®°å¿†æ•ˆåº”
            collective_memory = self._analyze_collective_memory_effects(processed_data)
            collective_analysis['collective_memory_effects'] = collective_memory
            
            # 6. ç›¸ä½åŒæ­¥åˆ†æ
            phase_sync = self._analyze_phase_synchronization(processed_data)
            collective_analysis['phase_synchronization'] = phase_sync
            
            return collective_analysis
            
        except Exception as e:
            print(f"      âŒ é›†ä½“è¡Œä¸ºæ¨¡å¼è¯†åˆ«å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_flocking_behavior(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æç¾¤é›†è¡Œä¸º - åŸºäºVicsekæ¨¡å‹"""
        try:
            flocking_analysis = {
                'alignment_strength': 0.0,
                'cohesion_measure': 0.0,
                'separation_behavior': 0.0,
                'flock_stability': 0.0,
                'collective_velocity': 0.0
            }
            
            # å°†å°¾æ•°é€‰æ‹©æ˜ å°„ä¸º"ä¸ªä½“"åœ¨é€‰æ‹©ç©ºé—´ä¸­çš„ä½ç½®å’Œé€Ÿåº¦
            individual_positions = []
            individual_velocities = []
            
            for i, period in enumerate(processed_data[:15]):
                tails = period.get('tails', [])
                if tails:
                    # ä½ç½®ï¼šé€‰æ‹©çš„é‡å¿ƒ
                    position = np.mean(tails)
                    individual_positions.append(position)
                    
                    # é€Ÿåº¦ï¼šç›¸å¯¹äºå‰ä¸€æœŸçš„ä½ç½®å˜åŒ–
                    if i > 0 and individual_positions:
                        velocity = position - individual_positions[i-1]
                        individual_velocities.append(velocity)
            
            if len(individual_positions) < 3:
                return flocking_analysis
            
            # 1. å¯¹é½å¼ºåº¦ï¼ˆé€Ÿåº¦æ–¹å‘çš„ä¸€è‡´æ€§ï¼‰
            if len(individual_velocities) >= 3:
                velocity_directions = [1 if v >= 0 else -1 for v in individual_velocities]
                alignment_strength = abs(np.mean(velocity_directions))
                flocking_analysis['alignment_strength'] = alignment_strength
            
            # 2. èšé›†æ€§åº¦é‡ï¼ˆä½ç½®çš„èšé›†ç¨‹åº¦ï¼‰
            position_variance = np.var(individual_positions)
            max_variance = (9.0 - 0.0) ** 2 / 4  # æœ€å¤§å¯èƒ½æ–¹å·®
            cohesion_measure = 1.0 - (position_variance / max_variance)
            flocking_analysis['cohesion_measure'] = max(0.0, cohesion_measure)
            
            # 3. åˆ†ç¦»è¡Œä¸ºï¼ˆé¿å…è¿‡åº¦èšé›†ï¼‰
            if len(individual_positions) >= 4:
                neighbor_distances = []
                for i in range(len(individual_positions) - 1):
                    distance = abs(individual_positions[i+1] - individual_positions[i])
                    neighbor_distances.append(distance)
                
                if neighbor_distances:
                    avg_separation = np.mean(neighbor_distances)
                    optimal_separation = 2.0  # ç†æƒ³åˆ†ç¦»è·ç¦»
                    separation_behavior = 1.0 - abs(avg_separation - optimal_separation) / optimal_separation
                    flocking_analysis['separation_behavior'] = max(0.0, separation_behavior)
            
            # 4. ç¾¤ä½“ç¨³å®šæ€§
            if len(individual_positions) >= 5:
                position_changes = [abs(individual_positions[i+1] - individual_positions[i]) 
                                  for i in range(len(individual_positions)-1)]
                stability = 1.0 - (np.std(position_changes) / (np.mean(position_changes) + 1e-10))
                flocking_analysis['flock_stability'] = max(0.0, min(1.0, stability))
            
            # 5. é›†ä½“é€Ÿåº¦
            if individual_velocities:
                collective_velocity = abs(np.mean(individual_velocities))
                flocking_analysis['collective_velocity'] = collective_velocity
            
            return flocking_analysis
            
        except Exception as e:
            return {}
    
    def _identify_swarming_patterns(self, processed_data: List[Dict]) -> Dict:
        """è¯†åˆ«é›†ç¾¤æ¨¡å¼"""
        try:
            swarming_analysis = {
                'swarm_formation_detected': False,
                'swarm_density': 0.0,
                'emergence_threshold': 0.0,
                'swarm_coherence': 0.0,
                'collective_decision_making': 0.0
            }
            
            # åˆ†æé€‰æ‹©çš„é›†ç¾¤ç‰¹å¾
            choice_densities = []
            choice_coherences = []
            
            for period in processed_data[:12]:
                tails = period.get('tails', [])
                
                if tails:
                    # å¯†åº¦ï¼šé€‰æ‹©çš„é›†ä¸­ç¨‹åº¦
                    tail_counts = np.bincount(tails, minlength=10)
                    gini_coefficient = self._calculate_gini_coefficient(tail_counts)
                    choice_densities.append(gini_coefficient)
                    
                    # ç›¸å¹²æ€§ï¼šé€‰æ‹©çš„ç©ºé—´ç›¸å…³æ€§
                    if len(tails) > 1:
                        coherence = 1.0 - (np.std(tails) / 3.0)  # å½’ä¸€åŒ–æ ‡å‡†å·®
                        choice_coherences.append(max(0.0, coherence))
                    else:
                        choice_coherences.append(1.0)  # å•é€‰æ‹©å…·æœ‰å®Œå…¨ç›¸å¹²æ€§
            
            if choice_densities and choice_coherences:
                # é›†ç¾¤å¯†åº¦
                avg_density = np.mean(choice_densities)
                swarming_analysis['swarm_density'] = avg_density
                
                # é›†ç¾¤ç›¸å¹²æ€§
                avg_coherence = np.mean(choice_coherences)
                swarming_analysis['swarm_coherence'] = avg_coherence
                
                # é›†ç¾¤å½¢æˆæ£€æµ‹
                if avg_density > 0.6 and avg_coherence > 0.7:
                    swarming_analysis['swarm_formation_detected'] = True
                
                # æ¶Œç°é˜ˆå€¼
                density_variance = np.var(choice_densities)
                if avg_density > 0:
                    emergence_threshold = density_variance / avg_density
                    swarming_analysis['emergence_threshold'] = min(1.0, emergence_threshold)
                
                # é›†ä½“å†³ç­–èƒ½åŠ›
                decision_consistency = 1.0 - np.std(choice_coherences)
                swarming_analysis['collective_decision_making'] = max(0.0, decision_consistency)
            
            return swarming_analysis
            
        except Exception as e:
            return {}
    
    def _analyze_consensus_dynamics(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æå…±è¯†åŠ¨åŠ›å­¦"""
        try:
            consensus_analysis = {
                'consensus_formation_rate': 0.0,
                'consensus_stability': 0.0,
                'opinion_diversity': 0.0,
                'convergence_time': 0,
                'consensus_quality': 0.0
            }
            
            if len(processed_data) < 8:
                return consensus_analysis
            
            # è®¡ç®—æ¯æœŸçš„æ„è§å¤šæ ·æ€§
            diversity_timeline = []
            consensus_indicators = []
            
            for period in processed_data[:12]:
                tails = period.get('tails', [])
                
                if tails:
                    # æ„è§å¤šæ ·æ€§ï¼ˆåŸºäºç†µï¼‰
                    tail_counts = np.bincount(tails, minlength=10)
                    probabilities = tail_counts / np.sum(tail_counts)
                    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
                    normalized_entropy = entropy / np.log2(10)
                    diversity_timeline.append(normalized_entropy)
                    
                    # å…±è¯†æŒ‡æ ‡ï¼ˆåŸºäºé›†ä¸­åº¦ï¼‰
                    gini = self._calculate_gini_coefficient(tail_counts)
                    consensus_indicators.append(gini)
                else:
                    diversity_timeline.append(1.0)  # æœ€å¤§å¤šæ ·æ€§
                    consensus_indicators.append(0.0)  # æ— å…±è¯†
            
            if len(diversity_timeline) >= 3:
                # 1. å…±è¯†å½¢æˆé€Ÿç‡
                if len(diversity_timeline) >= 6:
                    early_diversity = np.mean(diversity_timeline[:3])
                    late_diversity = np.mean(diversity_timeline[-3:])
                    formation_rate = max(0, early_diversity - late_diversity)
                    consensus_analysis['consensus_formation_rate'] = formation_rate
                
                # 2. å…±è¯†ç¨³å®šæ€§
                consensus_variance = np.var(consensus_indicators)
                consensus_mean = np.mean(consensus_indicators)
                if consensus_mean > 0:
                    stability = 1.0 - (consensus_variance / consensus_mean)
                    consensus_analysis['consensus_stability'] = max(0.0, stability)
                
                # 3. å¹³å‡æ„è§å¤šæ ·æ€§
                consensus_analysis['opinion_diversity'] = np.mean(diversity_timeline)
                
                # 4. æ”¶æ•›æ—¶é—´ä¼°ç®—
                convergence_threshold = 0.3
                convergence_time = 0
                for i, diversity in enumerate(diversity_timeline):
                    if diversity < convergence_threshold:
                        convergence_time = i + 1
                        break
                consensus_analysis['convergence_time'] = convergence_time
                
                # 5. å…±è¯†è´¨é‡
                final_consensus = consensus_indicators[-1] if consensus_indicators else 0
                consensus_quality = final_consensus * consensus_analysis['consensus_stability']
                consensus_analysis['consensus_quality'] = consensus_quality
            
            return consensus_analysis
            
        except Exception as e:
            return {}
    
    def _analyze_leader_follower_dynamics(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æé¢†å¯¼è€…-è·Ÿéšè€…åŠ¨åŠ›å­¦"""
        try:
            leadership_analysis = {
                'leader_identification': {},
                'follower_response_time': 0.0,
                'leadership_effectiveness': 0.0,
                'hierarchy_strength': 0.0,
                'influence_propagation': {}
            }
            
            if len(processed_data) < 8:
                return leadership_analysis
            
            # è¯†åˆ«æ½œåœ¨çš„"é¢†å¯¼è€…"å°¾æ•°
            leader_scores = {}
            influence_patterns = {}
            
            for lead_tail in range(10):
                lead_score = 0.0
                influence_events = []
                
                # åˆ†æè¯¥å°¾æ•°çš„é¢†å¯¼ç‰¹å¾
                for i in range(len(processed_data) - 3):
                    current_period = processed_data[i]
                    
                    if lead_tail in current_period.get('tails', []):
                        # æ£€æŸ¥åç»­æœŸé—´å…¶ä»–å°¾æ•°çš„å“åº”
                        response_count = 0
                        for follow_period in processed_data[i+1:i+3]:
                            follow_tails = follow_period.get('tails', [])
                            if follow_tails:
                                # è®¡ç®—è·Ÿéšå“åº”
                                response_strength = len(set(current_period.get('tails', [])).intersection(set(follow_tails)))
                                response_count += response_strength
                        
                        if response_count > 0:
                            lead_score += response_count
                            influence_events.append({
                                'period': i,
                                'response_strength': response_count
                            })
                
                leader_scores[lead_tail] = lead_score
                influence_patterns[lead_tail] = influence_events
            
            # è¯†åˆ«æœ€å¼ºçš„é¢†å¯¼è€…
            if leader_scores:
                top_leader = max(leader_scores.items(), key=lambda x: x[1])
                leadership_analysis['leader_identification'] = {
                    'primary_leader': top_leader[0],
                    'leadership_strength': top_leader[1],
                    'all_scores': leader_scores
                }
                
                # åˆ†æè·Ÿéšè€…å“åº”æ—¶é—´
                leader_events = influence_patterns.get(top_leader[0], [])
                if leader_events:
                    response_times = [1.5]  # å¹³å‡å“åº”å»¶è¿Ÿ
                    leadership_analysis['follower_response_time'] = np.mean(response_times)
                
                # é¢†å¯¼æ•ˆåŠ›
                total_possible_influence = len(processed_data) * 9  # æ¯æœŸæœ€å¤šå½±å“9ä¸ªå…¶ä»–å°¾æ•°
                actual_influence = top_leader[1]
                if total_possible_influence > 0:
                    effectiveness = actual_influence / total_possible_influence
                    leadership_analysis['leadership_effectiveness'] = min(1.0, effectiveness)
                
                # å±‚æ¬¡ç»“æ„å¼ºåº¦
                sorted_scores = sorted(leader_scores.values(), reverse=True)
                if len(sorted_scores) >= 2 and sorted_scores[1] > 0:
                    hierarchy_strength = (sorted_scores[0] - sorted_scores[1]) / sorted_scores[0]
                    leadership_analysis['hierarchy_strength'] = hierarchy_strength
                
                # å½±å“ä¼ æ’­åˆ†æ
                leadership_analysis['influence_propagation'] = {
                    'propagation_speed': leadership_analysis['follower_response_time'],
                    'propagation_reach': len([s for s in leader_scores.values() if s > 0]) / 10,
                    'propagation_persistence': len(leader_events) / len(processed_data) if leader_events else 0
                }
            
            return leadership_analysis
            
        except Exception as e:
            return {}
    
    def _analyze_collective_memory_effects(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æé›†ä½“è®°å¿†æ•ˆåº”"""
        try:
            memory_analysis = {
                'memory_length': 0,
                'memory_strength': 0.0,
                'memory_decay_rate': 0.0,
                'pattern_recall_ability': 0.0,
                'collective_learning_rate': 0.0
            }
            
            if len(processed_data) < 10:
                return memory_analysis
            
            # åˆ†æå†å²æ¨¡å¼çš„é‡ç°
            pattern_recalls = []
            memory_strengths = []
            
            for current_idx in range(3, len(processed_data)):
                current_tails = set(processed_data[current_idx].get('tails', []))
                
                if not current_tails:
                    continue
                
                # æ£€æŸ¥å†å²ç›¸ä¼¼æ¨¡å¼
                max_similarity = 0.0
                best_recall_distance = 0
                
                for hist_idx in range(current_idx):
                    hist_tails = set(processed_data[hist_idx].get('tails', []))
                    
                    if hist_tails:
                        # è®¡ç®—ç›¸ä¼¼åº¦
                        intersection = len(current_tails.intersection(hist_tails))
                        union = len(current_tails.union(hist_tails))
                        similarity = intersection / union if union > 0 else 0
                        
                        if similarity > max_similarity:
                            max_similarity = similarity
                            best_recall_distance = current_idx - hist_idx
                
                if max_similarity > 0.5:  # æ˜¾è‘—ç›¸ä¼¼
                    pattern_recalls.append({
                        'similarity': max_similarity,
                        'distance': best_recall_distance,
                        'current_period': current_idx
                    })
                    memory_strengths.append(max_similarity)
            
            if pattern_recalls:
                # 1. è®°å¿†é•¿åº¦ï¼ˆæœ€è¿œçš„æœ‰æ•ˆå›å¿†ï¼‰
                max_memory_distance = max(recall['distance'] for recall in pattern_recalls)
                memory_analysis['memory_length'] = max_memory_distance
                
                # 2. è®°å¿†å¼ºåº¦ï¼ˆå¹³å‡ç›¸ä¼¼åº¦ï¼‰
                avg_memory_strength = np.mean(memory_strengths)
                memory_analysis['memory_strength'] = avg_memory_strength
                
                # 3. è®°å¿†è¡°å‡ç‡
                if len(pattern_recalls) >= 3:
                    distances = [recall['distance'] for recall in pattern_recalls]
                    similarities = [recall['similarity'] for recall in pattern_recalls]
                    
                    # æ‹Ÿåˆè¡°å‡æ¨¡å‹ï¼šsimilarity ~ exp(-Î» * distance)
                    if len(distances) == len(similarities):
                        try:
                            log_similarities = [np.log(s + 1e-10) for s in similarities]
                            slope = np.polyfit(distances, log_similarities, 1)[0]
                            decay_rate = abs(slope)
                            memory_analysis['memory_decay_rate'] = min(1.0, decay_rate)
                        except:
                            pass
                
                # 4. æ¨¡å¼å›å¿†èƒ½åŠ›
                recall_success_rate = len(pattern_recalls) / (len(processed_data) - 3)
                memory_analysis['pattern_recall_ability'] = recall_success_rate
                
                # 5. é›†ä½“å­¦ä¹ ç‡
                if len(pattern_recalls) >= 2:
                    early_recalls = [r for r in pattern_recalls if r['current_period'] <= len(processed_data) // 2]
                    late_recalls = [r for r in pattern_recalls if r['current_period'] > len(processed_data) // 2]
                    
                    if early_recalls and late_recalls:
                        early_strength = np.mean([r['similarity'] for r in early_recalls])
                        late_strength = np.mean([r['similarity'] for r in late_recalls])
                        learning_rate = max(0, late_strength - early_strength)
                        memory_analysis['collective_learning_rate'] = learning_rate
            
            return memory_analysis
            
        except Exception as e:
            return {}
    
    def _analyze_phase_synchronization(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æç›¸ä½åŒæ­¥"""
        try:
            sync_analysis = {
                'global_synchronization': 0.0,
                'pairwise_synchronization': {},
                'synchronization_clusters': [],
                'phase_coherence': 0.0,
                'synchronization_stability': 0.0
            }
            
            if len(processed_data) < 8:
                return sync_analysis
            
            # ä¸ºæ¯ä¸ªå°¾æ•°æ„å»ºç›¸ä½æ—¶é—´åºåˆ—
            phase_series = {}
            for tail in range(10):
                appearances = [1 if tail in period.get('tails', []) else 0 
                             for period in processed_data[:12]]
                
                # å°†å‡ºç°åºåˆ—è½¬æ¢ä¸ºç›¸ä½ä¿¡æ¯
                phase_series[tail] = self._convert_to_phase_series(appearances)
            
            # 1. è®¡ç®—å…¨å±€åŒæ­¥
            all_phases = list(phase_series.values())
            if all_phases and len(all_phases[0]) > 0:
                global_sync = self._calculate_global_phase_synchronization(all_phases)
                sync_analysis['global_synchronization'] = global_sync
            
            # 2. è®¡ç®—ä¸¤ä¸¤åŒæ­¥
            pairwise_sync = {}
            for tail_a in range(10):
                for tail_b in range(tail_a + 1, 10):
                    if tail_a in phase_series and tail_b in phase_series:
                        sync_strength = self._calculate_pairwise_synchronization(
                            phase_series[tail_a], phase_series[tail_b]
                        )
                        pairwise_sync[f'{tail_a}_{tail_b}'] = sync_strength
            
            sync_analysis['pairwise_synchronization'] = pairwise_sync
            
            # 3. è¯†åˆ«åŒæ­¥é›†ç¾¤
            sync_clusters = self._identify_synchronization_clusters(pairwise_sync)
            sync_analysis['synchronization_clusters'] = sync_clusters
            
            # 4. ç›¸ä½ç›¸å¹²æ€§
            if all_phases:
                phase_coherence = self._calculate_phase_coherence(all_phases)
                sync_analysis['phase_coherence'] = phase_coherence
            
            # 5. åŒæ­¥ç¨³å®šæ€§
            if len(processed_data) >= 10:
                stability = self._calculate_synchronization_stability(phase_series, processed_data)
                sync_analysis['synchronization_stability'] = stability
            
            return sync_analysis
            
        except Exception as e:
            return {}
    
    def _convert_to_phase_series(self, appearances: List[int]) -> List[float]:
        """å°†å‡ºç°åºåˆ—è½¬æ¢ä¸ºç›¸ä½åºåˆ—"""
        try:
            phases = []
            cumulative_phase = 0.0
            
            for i, appeared in enumerate(appearances):
                if appeared:
                    # å‡ºç°æ—¶ç›¸ä½å¢åŠ 
                    cumulative_phase += np.pi / 2
                else:
                    # æœªå‡ºç°æ—¶ç›¸ä½ç¼“æ…¢å¢åŠ 
                    cumulative_phase += np.pi / 8
                
                # ä¿æŒç›¸ä½åœ¨[0, 2Ï€]èŒƒå›´å†…
                normalized_phase = cumulative_phase % (2 * np.pi)
                phases.append(normalized_phase)
            
            return phases
            
        except Exception as e:
            return []
    
    def _calculate_global_phase_synchronization(self, all_phases: List[List[float]]) -> float:
        """è®¡ç®—å…¨å±€ç›¸ä½åŒæ­¥"""
        try:
            if not all_phases or not all_phases[0]:
                return 0.0
            
            # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„ç›¸ä½åŒæ­¥æŒ‡æ•°
            sync_indices = []
            time_length = min(len(phases) for phases in all_phases)
            
            for t in range(time_length):
                # è®¡ç®—è¯¥æ—¶åˆ»æ‰€æœ‰æŒ¯å­çš„å¹³å‡ç›¸ä½å‘é‡
                complex_sum = 0.0
                for phases in all_phases:
                    if t < len(phases):
                        complex_sum += np.exp(1j * phases[t])
                
                # åŒæ­¥æŒ‡æ•°ï¼š|å¹³å‡ç›¸ä½å‘é‡|
                sync_index = abs(complex_sum) / len(all_phases)
                sync_indices.append(sync_index)
            
            return np.mean(sync_indices) if sync_indices else 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_pairwise_synchronization(self, phases_a: List[float], phases_b: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸¤ç›¸ä½åŒæ­¥"""
        try:
            if not phases_a or not phases_b:
                return 0.0
            
            min_length = min(len(phases_a), len(phases_b))
            if min_length < 3:
                return 0.0
            
            # è®¡ç®—ç›¸ä½å·®çš„ç¨³å®šæ€§
            phase_differences = []
            for i in range(min_length):
                diff = abs(phases_a[i] - phases_b[i])
                # å¤„ç†ç›¸ä½ç¼ ç»•
                diff = min(diff, 2*np.pi - diff)
                phase_differences.append(diff)
            
            # åŒæ­¥å¼ºåº¦ï¼šç›¸ä½å·®çš„ä¸€è‡´æ€§
            if phase_differences:
                mean_diff = np.mean(phase_differences)
                std_diff = np.std(phase_differences)
                
                # ä½æ–¹å·®è¡¨ç¤ºé«˜åŒæ­¥
                if std_diff > 0:
                    sync_strength = 1.0 / (1.0 + std_diff)
                else:
                    sync_strength = 1.0
                
                return min(1.0, sync_strength)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _identify_synchronization_clusters(self, pairwise_sync: Dict) -> List[List[int]]:
        """è¯†åˆ«åŒæ­¥é›†ç¾¤"""
        try:
            # æ„å»ºåŒæ­¥ç½‘ç»œ
            sync_threshold = 0.6
            sync_edges = []
            
            for pair_key, sync_strength in pairwise_sync.items():
                if sync_strength > sync_threshold:
                    tail_a, tail_b = map(int, pair_key.split('_'))
                    sync_edges.append((tail_a, tail_b))
            
            # ä½¿ç”¨è¿é€šç»„ä»¶ç®—æ³•æ‰¾åˆ°åŒæ­¥é›†ç¾¤
            clusters = []
            visited = set()
            
            for tail in range(10):
                if tail not in visited:
                    cluster = []
                    stack = [tail]
                    
                    while stack:
                        current = stack.pop()
                        if current not in visited:
                            visited.add(current)
                            cluster.append(current)
                            
                            # æ·»åŠ åŒæ­¥é‚»å±…
                            for edge in sync_edges:
                                if edge[0] == current and edge[1] not in visited:
                                    stack.append(edge[1])
                                elif edge[1] == current and edge[0] not in visited:
                                    stack.append(edge[0])
                    
                    if len(cluster) > 1:  # åªä¿ç•™çœŸæ­£çš„é›†ç¾¤
                        clusters.append(sorted(cluster))
            
            return clusters
            
        except Exception as e:
            return []
    
    def _calculate_phase_coherence(self, all_phases: List[List[float]]) -> float:
        """è®¡ç®—ç›¸ä½ç›¸å¹²æ€§"""
        try:
            if not all_phases:
                return 0.0
            
            # è®¡ç®—æ‰€æœ‰æ—¶åˆ»çš„ç›¸ä½åˆ†å¸ƒç›¸å¹²æ€§
            coherence_values = []
            time_length = min(len(phases) for phases in all_phases)
            
            for t in range(time_length):
                current_phases = [phases[t] for phases in all_phases if t < len(phases)]
                
                if len(current_phases) >= 2:
                    # è®¡ç®—ç›¸ä½åˆ†å¸ƒçš„ç›¸å¹²æ€§
                    complex_phases = [np.exp(1j * phase) for phase in current_phases]
                    mean_complex = np.mean(complex_phases)
                    coherence = abs(mean_complex)
                    coherence_values.append(coherence)
            
            return np.mean(coherence_values) if coherence_values else 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_synchronization_stability(self, phase_series: Dict, processed_data: List[Dict]) -> float:
        """è®¡ç®—åŒæ­¥ç¨³å®šæ€§"""
        try:
            if len(processed_data) < 8:
                return 0.0
            
            # åˆ†æåŒæ­¥å¼ºåº¦éšæ—¶é—´çš„å˜åŒ–
            window_size = 4
            sync_strengths = []
            
            for start in range(len(processed_data) - window_size + 1):
                window_phases = {}
                for tail, phases in phase_series.items():
                    if start + window_size <= len(phases):
                        window_phases[tail] = phases[start:start + window_size]
                
                if len(window_phases) >= 2:
                    # è®¡ç®—çª—å£å†…çš„å¹³å‡åŒæ­¥å¼ºåº¦
                    pairwise_syncs = []
                    for tail_a in window_phases:
                        for tail_b in window_phases:
                            if tail_a < tail_b:
                                sync = self._calculate_pairwise_synchronization(
                                    window_phases[tail_a], window_phases[tail_b]
                                )
                                pairwise_syncs.append(sync)
                    
                    if pairwise_syncs:
                        avg_sync = np.mean(pairwise_syncs)
                        sync_strengths.append(avg_sync)
            
            # ç¨³å®šæ€§ï¼šåŒæ­¥å¼ºåº¦çš„ä¸€è‡´æ€§
            if len(sync_strengths) >= 2:
                stability = 1.0 - (np.std(sync_strengths) / (np.mean(sync_strengths) + 1e-10))
                return max(0.0, stability)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_system_resilience_metrics(self, processed_data: List[Dict]) -> Dict:
        """
        è®¡ç®—ç³»ç»ŸéŸ§æ€§åº¦é‡ - åŸºäºå¤æ‚ç³»ç»ŸéŸ§æ€§ç†è®º
        å®ç°HollingéŸ§æ€§æ¨¡å‹å’Œé€‚åº”æ€§å¾ªç¯ç†è®º
        """
        try:
            resilience_analysis = {
                'adaptive_capacity': 0.0,
                'resistance_to_perturbation': 0.0,
                'recovery_speed': 0.0,
                'system_robustness': 0.0,
                'transformation_potential': 0.0,
                'resilience_stability_tradeoff': 0.0
            }
            
            if len(processed_data) < 15:
                return resilience_analysis
            
            # 1. è‡ªé€‚åº”èƒ½åŠ›åˆ†æ
            adaptive_capacity = self._calculate_adaptive_capacity(processed_data)
            resilience_analysis['adaptive_capacity'] = adaptive_capacity
            
            # 2. æŠ—æ‰°åŠ¨èƒ½åŠ›
            resistance = self._calculate_resistance_to_perturbation(processed_data)
            resilience_analysis['resistance_to_perturbation'] = resistance
            
            # 3. æ¢å¤é€Ÿåº¦
            recovery_speed = self._calculate_recovery_speed(processed_data)
            resilience_analysis['recovery_speed'] = recovery_speed
            
            # 4. ç³»ç»Ÿé²æ£’æ€§
            robustness = self._calculate_system_robustness(processed_data)
            resilience_analysis['system_robustness'] = robustness
            
            # 5. è½¬åŒ–æ½œåŠ›
            transformation_potential = self._calculate_transformation_potential(processed_data)
            resilience_analysis['transformation_potential'] = transformation_potential
            
            # 6. éŸ§æ€§-ç¨³å®šæ€§æƒè¡¡
            stability_tradeoff = self._calculate_resilience_stability_tradeoff(
                adaptive_capacity, resistance, recovery_speed
            )
            resilience_analysis['resilience_stability_tradeoff'] = stability_tradeoff
            
            return resilience_analysis
            
        except Exception as e:
            print(f"      âŒ ç³»ç»ŸéŸ§æ€§åº¦é‡è®¡ç®—å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_adaptive_capacity(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—è‡ªé€‚åº”èƒ½åŠ›"""
        try:
            # åˆ†æç³»ç»Ÿå¯¹å˜åŒ–çš„é€‚åº”èƒ½åŠ›
            adaptation_indicators = []
            
            # 1. é€‰æ‹©ç­–ç•¥çš„å¤šæ ·æ€§å˜åŒ–
            diversity_changes = []
            for i in range(1, len(processed_data)):
                current_tails = processed_data[i].get('tails', [])
                prev_tails = processed_data[i-1].get('tails', [])
                
                if current_tails and prev_tails:
                    current_diversity = len(current_tails) / 10.0
                    prev_diversity = len(prev_tails) / 10.0
                    diversity_change = abs(current_diversity - prev_diversity)
                    diversity_changes.append(diversity_change)
            
            if diversity_changes:
                avg_diversity_change = np.mean(diversity_changes)
                adaptation_indicators.append(avg_diversity_change)
            
            # 2. å“åº”æ¨¡å¼çš„çµæ´»æ€§
            response_flexibility = self._calculate_response_flexibility(processed_data)
            adaptation_indicators.append(response_flexibility)
            
            # 3. å­¦ä¹ æ•ˆåº”å¼ºåº¦
            learning_strength = self._calculate_learning_effect_strength(processed_data)
            adaptation_indicators.append(learning_strength)
            
            if adaptation_indicators:
                return np.mean(adaptation_indicators)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_response_flexibility(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—å“åº”çµæ´»æ€§"""
        try:
            if len(processed_data) < 8:
                return 0.0
            
            # åˆ†æç³»ç»Ÿå“åº”æ¨¡å¼çš„å˜åŒ–èƒ½åŠ›
            response_patterns = []
            
            for i in range(len(processed_data) - 2):
                window = processed_data[i:i+3]
                
                # æå–3æœŸçª—å£çš„å“åº”æ¨¡å¼
                pattern_features = []
                for period in window:
                    tails = period.get('tails', [])
                    features = [
                        len(tails),  # é€‰æ‹©æ•°é‡
                        np.mean(tails) if tails else 5,  # é€‰æ‹©ä¸­å¿ƒ
                        np.std(tails) if len(tails) > 1 else 0  # é€‰æ‹©åˆ†æ•£åº¦
                    ]
                    pattern_features.extend(features)
                
                response_patterns.append(pattern_features)
            
            if len(response_patterns) < 3:
                return 0.0
            
            # è®¡ç®—æ¨¡å¼å˜åŒ–çš„å¤šæ ·æ€§
            pattern_distances = []
            for i in range(len(response_patterns) - 1):
                distance = np.linalg.norm(
                    np.array(response_patterns[i]) - np.array(response_patterns[i+1])
                )
                pattern_distances.append(distance)
            
            if pattern_distances:
                flexibility = np.std(pattern_distances) / (np.mean(pattern_distances) + 1e-10)
                return min(1.0, flexibility)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_learning_effect_strength(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—å­¦ä¹ æ•ˆåº”å¼ºåº¦"""
        try:
            if len(processed_data) < 10:
                return 0.0
            
            # åˆ†ææ˜¯å¦å­˜åœ¨å­¦ä¹ æ•ˆåº”ï¼ˆæ€§èƒ½éšæ—¶é—´æ”¹å–„ï¼‰
            performance_timeline = []
            
            for period in processed_data:
                tails = period.get('tails', [])
                if tails:
                    # ä½¿ç”¨é€‰æ‹©çš„ä¸€è‡´æ€§ä½œä¸ºæ€§èƒ½æŒ‡æ ‡
                    consistency = 1.0 - (np.std(tails) / 3.0) if len(tails) > 1 else 1.0
                    performance_timeline.append(max(0.0, consistency))
                else:
                    performance_timeline.append(0.0)
            
            if len(performance_timeline) < 6:
                return 0.0
            
            # è®¡ç®—æ€§èƒ½è¶‹åŠ¿
            time_indices = list(range(len(performance_timeline)))
            try:
                slope = np.polyfit(time_indices, performance_timeline, 1)[0]
                learning_strength = max(0.0, slope)  # æ­£æ–œç‡è¡¨ç¤ºå­¦ä¹ æ”¹è¿›
                return min(1.0, learning_strength * 10)  # æ”¾å¤§æ•ˆæœ
            except:
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def _calculate_resistance_to_perturbation(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—æŠ—æ‰°åŠ¨èƒ½åŠ›"""
        try:
            # åˆ†æç³»ç»Ÿå¯¹"æ‰°åŠ¨"çš„æŠµæŠ—èƒ½åŠ›
            # è¿™é‡Œå°†å¤§å¹…åº¦çš„é€‰æ‹©å˜åŒ–è§†ä¸ºæ‰°åŠ¨
            
            perturbation_responses = []
            
            for i in range(1, len(processed_data)):
                current_tails = set(processed_data[i].get('tails', []))
                prev_tails = set(processed_data[i-1].get('tails', []))
                
                if current_tails and prev_tails:
                    # è®¡ç®—å˜åŒ–ç¨‹åº¦
                    overlap = len(current_tails.intersection(prev_tails))
                    union = len(current_tails.union(prev_tails))
                    change_magnitude = 1.0 - (overlap / union) if union > 0 else 1.0
                    
                    # å¦‚æœå˜åŒ–å¾ˆå¤§ï¼ˆæ‰°åŠ¨ï¼‰ï¼Œæ£€æŸ¥ç³»ç»Ÿçš„æŠµæŠ—åŠ›
                    if change_magnitude > 0.6:  # å¤§å˜åŒ–è¢«è®¤ä¸ºæ˜¯æ‰°åŠ¨
                        # æ£€æŸ¥åç»­æœŸé—´çš„æ¢å¤æƒ…å†µ
                        if i + 1 < len(processed_data):
                            next_tails = set(processed_data[i+1].get('tails', []))
                            
                            if next_tails:
                                # è®¡ç®—å‘åŸçŠ¶æ€çš„å›å½’ç¨‹åº¦
                                recovery_overlap = len(prev_tails.intersection(next_tails))
                                recovery_union = len(prev_tails.union(next_tails))
                                recovery_rate = recovery_overlap / recovery_union if recovery_union > 0 else 0
                                
                                perturbation_responses.append(recovery_rate)
            
            if perturbation_responses:
                # æŠ—æ‰°åŠ¨èƒ½åŠ›ï¼šå¹³å‡æ¢å¤ç‡
                return np.mean(perturbation_responses)
            
            # å¦‚æœæ²¡æœ‰æ˜æ˜¾æ‰°åŠ¨ï¼Œè¿”å›ä¸­ç­‰å€¼
            return 0.5
            
        except Exception as e:
            return 0.0
    
    def _calculate_recovery_speed(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—æ¢å¤é€Ÿåº¦"""
        try:
            # åˆ†æç³»ç»Ÿä»æ‰°åŠ¨ä¸­æ¢å¤çš„é€Ÿåº¦
            recovery_times = []
            
            for i in range(len(processed_data) - 4):
                # æ£€æµ‹æ˜¯å¦å­˜åœ¨æ‰°åŠ¨
                baseline_state = set(processed_data[i].get('tails', []))
                disturbed_state = set(processed_data[i+1].get('tails', []))
                
                if baseline_state and disturbed_state:
                    # è®¡ç®—æ‰°åŠ¨å¼ºåº¦
                    disturbance_strength = 1.0 - (
                        len(baseline_state.intersection(disturbed_state)) / 
                        len(baseline_state.union(disturbed_state))
                    )
                    
                    if disturbance_strength > 0.5:  # æ˜¾è‘—æ‰°åŠ¨
                        # å¯»æ‰¾æ¢å¤æ—¶é—´
                        recovery_time = 0
                        for j in range(i+2, min(i+5, len(processed_data))):
                            recovery_state = set(processed_data[j].get('tails', []))
                            
                            if recovery_state:
                                # è®¡ç®—ä¸åŸºçº¿çŠ¶æ€çš„ç›¸ä¼¼åº¦
                                similarity = (
                                    len(baseline_state.intersection(recovery_state)) / 
                                    len(baseline_state.union(recovery_state))
                                )
                                
                                if similarity > 0.7:  # è®¤ä¸ºå·²æ¢å¤
                                    recovery_time = j - (i + 1)
                                    break
                        
                        if recovery_time > 0:
                            recovery_times.append(recovery_time)
            
            if recovery_times:
                # æ¢å¤é€Ÿåº¦ï¼šæ¢å¤æ—¶é—´çš„å€’æ•°
                avg_recovery_time = np.mean(recovery_times)
                recovery_speed = 1.0 / (avg_recovery_time + 1)  # é¿å…é™¤é›¶
                return min(1.0, recovery_speed)
            
            return 0.5  # é»˜è®¤ä¸­ç­‰æ¢å¤é€Ÿåº¦
            
        except Exception as e:
            return 0.0
    
    def _calculate_system_robustness(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—ç³»ç»Ÿé²æ£’æ€§"""
        try:
            # åˆ†æç³»ç»Ÿåœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶çš„ç¨³å®šæ€§
            robustness_indicators = []
            
            # 1. é€‰æ‹©ä¸€è‡´æ€§
            choice_consistencies = []
            for period in processed_data:
                tails = period.get('tails', [])
                if tails:
                    # é€‰æ‹©çš„å†…éƒ¨ä¸€è‡´æ€§
                    std_choice = np.std(tails) if len(tails) > 1 else 0
                    consistency = 1.0 - (std_choice / 3.0)  # å½’ä¸€åŒ–
                    choice_consistencies.append(max(0.0, consistency))
            
            if choice_consistencies:
                avg_consistency = np.mean(choice_consistencies)
                robustness_indicators.append(avg_consistency)
            
            # 2. ç³»ç»ŸçŠ¶æ€çš„ç¨³å®šæ€§
            state_stability = self._calculate_state_stability(processed_data)
            robustness_indicators.append(state_stability)
            
            # 3. å“åº”çš„å¯é¢„æµ‹æ€§
            response_predictability = self._calculate_response_predictability(processed_data)
            robustness_indicators.append(response_predictability)
            
            if robustness_indicators:
                return np.mean(robustness_indicators)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_state_stability(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—çŠ¶æ€ç¨³å®šæ€§"""
        try:
            if len(processed_data) < 5:
                return 0.0
            
            # è®¡ç®—ç³»ç»ŸçŠ¶æ€å‘é‡çš„å˜åŒ–
            state_changes = []
            
            for i in range(len(processed_data) - 1):
                current_state = self._encode_system_state(processed_data[i])
                next_state = self._encode_system_state(processed_data[i+1])
                
                # è®¡ç®—çŠ¶æ€å˜åŒ–å¹…åº¦
                state_change = np.linalg.norm(np.array(current_state) - np.array(next_state))
                state_changes.append(state_change)
            
            if state_changes:
                # ç¨³å®šæ€§ï¼šçŠ¶æ€å˜åŒ–çš„åå‘æŒ‡æ ‡
                avg_change = np.mean(state_changes)
                max_possible_change = np.sqrt(3)  # æœ€å¤§å¯èƒ½çš„çŠ¶æ€å˜åŒ–
                stability = 1.0 - (avg_change / max_possible_change)
                return max(0.0, stability)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _encode_system_state(self, period_data: Dict) -> List[float]:
        """ç¼–ç ç³»ç»ŸçŠ¶æ€ä¸ºå‘é‡"""
        try:
            tails = period_data.get('tails', [])
            
            if tails:
                state_vector = [
                    len(tails) / 10.0,  # é€‰æ‹©æ•°é‡
                    np.mean(tails) / 9.0,  # é€‰æ‹©ä¸­å¿ƒï¼ˆå½’ä¸€åŒ–ï¼‰
                    (np.std(tails) / 3.0) if len(tails) > 1 else 0  # é€‰æ‹©åˆ†æ•£åº¦
                ]
            else:
                state_vector = [0.0, 0.5, 0.0]  # é»˜è®¤çŠ¶æ€
            
            return state_vector
            
        except Exception as e:
            return [0.0, 0.5, 0.0]
    
    def _calculate_response_predictability(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—å“åº”å¯é¢„æµ‹æ€§"""
        try:
            if len(processed_data) < 8:
                return 0.0
            
            # ä½¿ç”¨ç®€å•çš„é©¬å°”å¯å¤«æ¨¡å‹è¯„ä¼°å¯é¢„æµ‹æ€§
            prediction_accuracies = []
            
            for i in range(3, len(processed_data) - 1):
                # ä½¿ç”¨å‰3æœŸé¢„æµ‹ä¸‹ä¸€æœŸ
                context = processed_data[i-3:i]
                actual_next = set(processed_data[i].get('tails', []))
                
                if not actual_next:
                    continue
                
                # ç®€å•é¢„æµ‹ç­–ç•¥ï¼šå¯»æ‰¾å†å²ç›¸ä¼¼æ¨¡å¼
                best_match_similarity = 0.0
                predicted_next = set()
                
                for j in range(i):
                    if j + 3 < i:
                        hist_context = processed_data[j:j+3]
                        hist_next = set(processed_data[j+3].get('tails', []))
                        
                        # è®¡ç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦
                        context_similarity = self._calculate_context_similarity(context, hist_context)
                        
                        if context_similarity > best_match_similarity:
                            best_match_similarity = context_similarity
                            predicted_next = hist_next
                
                # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡
                if predicted_next and actual_next:
                    intersection = len(predicted_next.intersection(actual_next))
                    union = len(predicted_next.union(actual_next))
                    accuracy = intersection / union if union > 0 else 0
                    prediction_accuracies.append(accuracy)
            
            if prediction_accuracies:
                return np.mean(prediction_accuracies)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_context_similarity(self, context1: List[Dict], context2: List[Dict]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦"""
        try:
            if len(context1) != len(context2):
                return 0.0
            
            similarities = []
            for p1, p2 in zip(context1, context2):
                tails1 = set(p1.get('tails', []))
                tails2 = set(p2.get('tails', []))
                
                if tails1 and tails2:
                    intersection = len(tails1.intersection(tails2))
                    union = len(tails1.union(tails2))
                    similarity = intersection / union if union > 0 else 0
                    similarities.append(similarity)
                elif not tails1 and not tails2:
                    similarities.append(1.0)  # éƒ½ä¸ºç©ºæ—¶å®Œå…¨ç›¸ä¼¼
                else:
                    similarities.append(0.0)  # ä¸€ä¸ªä¸ºç©ºä¸€ä¸ªä¸ä¸ºç©º
            
            return np.mean(similarities) if similarities else 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_transformation_potential(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—è½¬åŒ–æ½œåŠ›"""
        try:
            # åˆ†æç³»ç»Ÿè¿›è¡Œæ ¹æœ¬æ€§å˜åŒ–çš„æ½œåŠ›
            transformation_indicators = []
            
            # 1. åˆ›æ–°æ€§å˜åŒ–é¢‘ç‡
            innovation_frequency = self._calculate_innovation_frequency(processed_data)
            transformation_indicators.append(innovation_frequency)
            
            # 2. çŠ¶æ€ç©ºé—´æ¢ç´¢ç¨‹åº¦
            exploration_breadth = self._calculate_exploration_breadth(processed_data)
            transformation_indicators.append(exploration_breadth)
            
            # 3. çªç°è¡Œä¸ºå€¾å‘
            emergence_tendency = self._calculate_emergence_tendency(processed_data)
            transformation_indicators.append(emergence_tendency)
            
            if transformation_indicators:
                return np.mean(transformation_indicators)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_innovation_frequency(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—åˆ›æ–°é¢‘ç‡"""
        try:
            if len(processed_data) < 5:
                return 0.0
            
            innovations = 0
            total_transitions = 0
            
            for i in range(len(processed_data) - 1):
                current_tails = set(processed_data[i].get('tails', []))
                next_tails = set(processed_data[i+1].get('tails', []))
                
                if current_tails and next_tails:
                    total_transitions += 1
                    
                    # åˆ›æ–°å®šä¹‰ï¼šå‡ºç°å®Œå…¨æ–°çš„é€‰æ‹©ç»„åˆ
                    if len(current_tails.intersection(next_tails)) == 0:
                        innovations += 1
                    # æˆ–è€…é€‰æ‹©æ¨¡å¼å‘ç”Ÿæ ¹æœ¬æ€§æ”¹å˜
                    elif len(current_tails.symmetric_difference(next_tails)) > len(current_tails.union(next_tails)) * 0.7:
                        innovations += 1
            
            if total_transitions > 0:
                return innovations / total_transitions
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_exploration_breadth(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—æ¢ç´¢å¹¿åº¦"""
        try:
            # åˆ†æç³»ç»Ÿæ¢ç´¢äº†å¤šå°‘ä¸åŒçš„çŠ¶æ€ç©ºé—´
            unique_states = set()
            
            for period in processed_data:
                tails = tuple(sorted(period.get('tails', [])))
                unique_states.add(tails)
            
            # ç†è®ºæœ€å¤§çŠ¶æ€æ•°ï¼ˆç®€åŒ–ä¼°ç®—ï¼‰
            max_possible_states = min(2**10, len(processed_data) * 5)  # ä¿å®ˆä¼°ç®—
            
            exploration_breadth = len(unique_states) / max_possible_states
            return min(1.0, exploration_breadth)
            
        except Exception as e:
            return 0.0
    
    def _calculate_emergence_tendency(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—çªç°å€¾å‘"""
        try:
            if len(processed_data) < 8:
                return 0.0
            
            # æ£€æµ‹æ˜¯å¦å­˜åœ¨çªç°çš„å®è§‚æ¨¡å¼
            emergence_indicators = []
            
            # 1. å¤æ‚æ€§å¢é•¿
            complexity_timeline = []
            for period in processed_data:
                tails = period.get('tails', [])
                if tails:
                    # ä½¿ç”¨ç†µåº¦é‡å¤æ‚æ€§
                    tail_counts = np.bincount(tails, minlength=10)
                    probabilities = tail_counts / np.sum(tail_counts)
                    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
                    complexity_timeline.append(entropy)
                else:
                    complexity_timeline.append(0.0)
            
            if len(complexity_timeline) >= 5:
                early_complexity = np.mean(complexity_timeline[:3])
                late_complexity = np.mean(complexity_timeline[-3:])
                complexity_growth = max(0, late_complexity - early_complexity)
                emergence_indicators.append(complexity_growth)
            
            # 2. éçº¿æ€§å“åº”å¼ºåº¦
            nonlinearity = self._calculate_nonlinear_response_strength(processed_data)
            emergence_indicators.append(nonlinearity)
            
            if emergence_indicators:
                return np.mean(emergence_indicators)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_nonlinear_response_strength(self, processed_data: List[Dict]) -> float:
        """è®¡ç®—éçº¿æ€§å“åº”å¼ºåº¦"""
        try:
            if len(processed_data) < 6:
                return 0.0
            
            # åˆ†æè¾“å…¥å˜åŒ–ä¸è¾“å‡ºå˜åŒ–çš„éçº¿æ€§å…³ç³»
            input_changes = []
            output_changes = []
            
            for i in range(len(processed_data) - 2):
                # è¾“å…¥ï¼šå½“å‰çŠ¶æ€
                current_state = self._encode_system_state(processed_data[i])
                next_state = self._encode_system_state(processed_data[i+1])
                response_state = self._encode_system_state(processed_data[i+2])
                
                # è¾“å…¥å˜åŒ–
                input_change = np.linalg.norm(np.array(next_state) - np.array(current_state))
                input_changes.append(input_change)
                
                # è¾“å‡ºå˜åŒ–ï¼ˆå“åº”ï¼‰
                output_change = np.linalg.norm(np.array(response_state) - np.array(next_state))
                output_changes.append(output_change)
            
            if len(input_changes) >= 4 and len(output_changes) >= 4:
                # æ£€æµ‹éçº¿æ€§å…³ç³»
                try:
                    # çº¿æ€§ç›¸å…³æ€§
                    linear_correlation = abs(np.corrcoef(input_changes, output_changes)[0, 1])
                    
                    # éçº¿æ€§å¼ºåº¦ï¼š1 - çº¿æ€§ç›¸å…³æ€§
                    nonlinearity = 1.0 - linear_correlation if not np.isnan(linear_correlation) else 0.5
                    return nonlinearity
                except:
                    return 0.5
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_resilience_stability_tradeoff(self, adaptive_capacity: float, 
                                               resistance: float, recovery_speed: float) -> float:
        """è®¡ç®—éŸ§æ€§-ç¨³å®šæ€§æƒè¡¡"""
        try:
            # éŸ§æ€§ç»„ä»¶
            resilience_score = (adaptive_capacity + recovery_speed) / 2
            
            # ç¨³å®šæ€§ç»„ä»¶
            stability_score = resistance
            
            # æƒè¡¡åˆ†æï¼šé«˜éŸ§æ€§å¯èƒ½ä»¥ç¨³å®šæ€§ä¸ºä»£ä»·
            if resilience_score > 0 and stability_score > 0:
                # ç†æƒ³æƒ…å†µï¼šæ—¢æœ‰éŸ§æ€§åˆæœ‰ç¨³å®šæ€§
                if resilience_score > 0.7 and stability_score > 0.7:
                    return 1.0  # æœ€ä½³æƒè¡¡
                # æƒè¡¡æƒ…å†µï¼šéŸ§æ€§å’Œç¨³å®šæ€§æ­¤æ¶ˆå½¼é•¿
                else:
                    balance_score = 1.0 - abs(resilience_score - stability_score)
                    return balance_score
            
            return 0.5  # é»˜è®¤ä¸­ç­‰æƒè¡¡
            
        except Exception as e:
            return 0.0

    def _perform_chaos_analysis(self, processed_data: List[Dict]) -> Dict:
        """
        æ‰§è¡Œæ··æ²Œåˆ†æ - åŸºäºéçº¿æ€§åŠ¨åŠ›å­¦ç†è®º
        å®ç°Lorenzæ··æ²Œç†è®ºã€LyapunovæŒ‡æ•°è®¡ç®—å’Œç›¸ç©ºé—´é‡æ„
        """
        try:
            chaos_analysis = {
                'lyapunov_exponents': {},
                'phase_space_reconstruction': {},
                'strange_attractors': {},
                'bifurcation_analysis': {},
                'chaos_indicators': {},
                'nonlinear_prediction': {}
            }
            
            if len(processed_data) < 20:
                print(f"      âš ï¸ æ•°æ®é‡ä¸è¶³({len(processed_data)}æœŸ)ï¼Œæ··æ²Œåˆ†æéœ€è¦è‡³å°‘20æœŸæ•°æ®")
                return chaos_analysis
            
            # æ„å»ºæ—¶é—´åºåˆ—ç”¨äºæ··æ²Œåˆ†æ
            time_series = self._construct_chaos_time_series(processed_data)
            
            if not time_series or len(time_series) < 15:
                print(f"      âš ï¸ æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥æˆ–é•¿åº¦ä¸è¶³")
                return chaos_analysis
            
            # 1. LyapunovæŒ‡æ•°è®¡ç®—
            lyapunov_results = self._calculate_lyapunov_exponents_comprehensive(time_series)
            chaos_analysis['lyapunov_exponents'] = lyapunov_results
            
            # 2. ç›¸ç©ºé—´é‡æ„
            phase_space = self._perform_phase_space_reconstruction(time_series)
            chaos_analysis['phase_space_reconstruction'] = phase_space
            
            # 3. å¥‡å¼‚å¸å¼•å­è¯†åˆ«
            attractors = self._identify_strange_attractors(phase_space, time_series)
            chaos_analysis['strange_attractors'] = attractors
            
            # 4. åˆ†å²”åˆ†æ
            bifurcation_analysis = self._perform_bifurcation_analysis(time_series)
            chaos_analysis['bifurcation_analysis'] = bifurcation_analysis
            
            # 5. æ··æ²ŒæŒ‡æ ‡è®¡ç®—
            chaos_indicators = self._calculate_comprehensive_chaos_indicators(time_series, phase_space)
            chaos_analysis['chaos_indicators'] = chaos_indicators
            
            # 6. éçº¿æ€§é¢„æµ‹èƒ½åŠ›
            prediction_analysis = self._analyze_nonlinear_prediction_capability(time_series)
            chaos_analysis['nonlinear_prediction'] = prediction_analysis
            
            return chaos_analysis
            
        except Exception as e:
            print(f"      âŒ æ··æ²Œåˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}
    
    def _construct_chaos_time_series(self, processed_data: List[Dict]) -> List[float]:
        """æ„å»ºç”¨äºæ··æ²Œåˆ†æçš„æ—¶é—´åºåˆ—"""
        try:
            time_series = []
            
            for period in processed_data:
                tails = period.get('tails', [])
                
                if tails:
                    # æ„å»ºå¤šç»´ç‰¹å¾ä½œä¸ºæ··æ²Œåˆ†æçš„åŸºç¡€
                    features = [
                        len(tails),  # é€‰æ‹©æ•°é‡
                        np.mean(tails),  # é€‰æ‹©ä¸­å¿ƒ
                        np.std(tails) if len(tails) > 1 else 0,  # é€‰æ‹©åˆ†æ•£åº¦
                        max(tails) - min(tails) if len(tails) > 1 else 0,  # é€‰æ‹©èŒƒå›´
                        len(set(tails)) / len(tails) if len(tails) > 0 else 0  # é€‰æ‹©å”¯ä¸€æ€§
                    ]
                    
                    # ä½¿ç”¨ä¸»æˆåˆ†æˆ–åŠ æƒç»„åˆåˆ›å»ºå•ä¸€æ—¶é—´åºåˆ—
                    composite_value = (
                        features[0] * 0.3 +  # æ•°é‡æƒé‡
                        features[1] * 0.3 +  # ä¸­å¿ƒæƒé‡  
                        features[2] * 0.2 +  # åˆ†æ•£åº¦æƒé‡
                        features[3] * 0.1 +  # èŒƒå›´æƒé‡
                        features[4] * 0.1    # å”¯ä¸€æ€§æƒé‡
                    )
                    
                    time_series.append(composite_value)
                else:
                    # å¤„ç†ç©ºæœŸçš„æƒ…å†µ
                    time_series.append(0.0)
            
            # æ•°æ®é¢„å¤„ç†ï¼šå»é™¤è¶‹åŠ¿å’Œå¼‚å¸¸å€¼
            if len(time_series) > 3:
                time_series = self._preprocess_chaos_time_series(time_series)
            
            return time_series
            
        except Exception as e:
            print(f"      âŒ æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {e}")
            return []
    
    def _preprocess_chaos_time_series(self, time_series: List[float]) -> List[float]:
        """é¢„å¤„ç†æ··æ²Œæ—¶é—´åºåˆ—"""
        try:
            series = np.array(time_series)
            
            # 1. å¼‚å¸¸å€¼å¤„ç†
            Q1 = np.percentile(series, 25)
            Q3 = np.percentile(series, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # ç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼
            series = np.clip(series, lower_bound, upper_bound)
            
            # 2. å»è¶‹åŠ¿å¤„ç†ï¼ˆå»é™¤çº¿æ€§è¶‹åŠ¿ï¼‰
            if len(series) > 5:
                x = np.arange(len(series))
                try:
                    slope, intercept = np.polyfit(x, series, 1)
                    trend = slope * x + intercept
                    series = series - trend
                except:
                    pass  # å¦‚æœæ‹Ÿåˆå¤±è´¥ï¼Œä¿æŒåŸåºåˆ—
            
            # 3. æ ‡å‡†åŒ–
            if np.std(series) > 1e-10:
                series = (series - np.mean(series)) / np.std(series)
            
            return series.tolist()
            
        except Exception as e:
            print(f"      âš ï¸ æ—¶é—´åºåˆ—é¢„å¤„ç†å¤±è´¥: {e}")
            return time_series
    
    def _calculate_lyapunov_exponents_comprehensive(self, time_series: List[float]) -> Dict:
        """è®¡ç®—LyapunovæŒ‡æ•°çš„ç»¼åˆåˆ†æ"""
        try:
            lyapunov_results = {
                'largest_lyapunov_exponent': 0.0,
                'lyapunov_spectrum': [],
                'chaos_classification': 'regular',
                'embedding_dimension': 3,
                'calculation_method': 'rosenstein',
                'confidence_interval': (0.0, 0.0)
            }
            
            if len(time_series) < 10:
                return lyapunov_results
            
            series = np.array(time_series)
            
            # ä½¿ç”¨å¤šç§æ–¹æ³•è®¡ç®—LyapunovæŒ‡æ•°
            methods_results = []
            
            # æ–¹æ³•1ï¼šRosensteinç®—æ³•ï¼ˆæ”¹è¿›ç‰ˆï¼‰
            rosenstein_result = self._calculate_lyapunov_rosenstein(series)
            if rosenstein_result is not None:
                methods_results.append(rosenstein_result)
            
            # æ–¹æ³•2ï¼šWolfç®—æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
            wolf_result = self._calculate_lyapunov_wolf(series)
            if wolf_result is not None:
                methods_results.append(wolf_result)
            
            # æ–¹æ³•3ï¼šåŸºäºç›¸ç©ºé—´çš„ç›´æ¥æ–¹æ³•
            direct_result = self._calculate_lyapunov_direct(series)
            if direct_result is not None:
                methods_results.append(direct_result)
            
            if methods_results:
                # ç»¼åˆå¤šç§æ–¹æ³•çš„ç»“æœ
                largest_lyapunov = np.mean(methods_results)
                lyapunov_results['largest_lyapunov_exponent'] = largest_lyapunov
                
                # ç½®ä¿¡åŒºé—´
                if len(methods_results) > 1:
                    std_lyapunov = np.std(methods_results)
                    lyapunov_results['confidence_interval'] = (
                        largest_lyapunov - std_lyapunov,
                        largest_lyapunov + std_lyapunov
                    )
                
                # åˆ†ç±»æ··æ²Œæ€§è´¨
                if largest_lyapunov > 0.1:
                    lyapunov_results['chaos_classification'] = 'chaotic'
                elif largest_lyapunov > 0.0:
                    lyapunov_results['chaos_classification'] = 'edge_of_chaos'
                elif largest_lyapunov > -0.1:
                    lyapunov_results['chaos_classification'] = 'quasiperiodic'
                else:
                    lyapunov_results['chaos_classification'] = 'regular'
            
            return lyapunov_results
            
        except Exception as e:
            print(f"      âŒ LyapunovæŒ‡æ•°è®¡ç®—å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_lyapunov_rosenstein(self, series: np.ndarray, embedding_dim: int = 3, delay: int = 1) -> float:
        """Rosensteinç®—æ³•è®¡ç®—æœ€å¤§LyapunovæŒ‡æ•°"""
        try:
            if len(series) < embedding_dim + delay * (embedding_dim - 1) + 10:
                return None
            
            # ç›¸ç©ºé—´é‡æ„
            embedded = self._embed_time_series(series, embedding_dim, delay)
            if embedded.shape[0] < 10:
                return None
            
            # å¯»æ‰¾æœ€è¿‘é‚»
            divergences = []
            
            for i in range(embedded.shape[0] - 5):
                # è®¡ç®—åˆ°æ‰€æœ‰å…¶ä»–ç‚¹çš„è·ç¦»
                distances = np.linalg.norm(embedded - embedded[i], axis=1)
                
                # æ’é™¤è‡ªå·±å’Œè¿‡è¿‘çš„æ—¶é—´ç‚¹
                valid_indices = np.where((distances > 0) & (np.arange(len(distances)) < i - 5))[0]
                
                if len(valid_indices) > 0:
                    # æ‰¾åˆ°æœ€è¿‘é‚»
                    nearest_idx = valid_indices[np.argmin(distances[valid_indices])]
                    
                    # è®¡ç®—å‘æ•£
                    max_evolution = min(5, len(series) - max(i, nearest_idx) - 1)
                    if max_evolution > 2:
                        for j in range(1, max_evolution):
                            if i + j < len(series) and nearest_idx + j < len(series):
                                divergence = abs(series[i + j] - series[nearest_idx + j])
                                if divergence > 1e-10:
                                    divergences.append(np.log(divergence))
            
            if len(divergences) > 5:
                # çº¿æ€§æ‹Ÿåˆæ±‚æ–œç‡
                time_steps = list(range(len(divergences)))
                try:
                    slope = np.polyfit(time_steps, divergences, 1)[0]
                    return max(0.0, slope)  # LyapunovæŒ‡æ•°åº”ä¸ºæ­£å€¼è¡¨ç¤ºæ··æ²Œ
                except:
                    return None
            
            return None
            
        except Exception as e:
            return None
    
    def _calculate_lyapunov_wolf(self, series: np.ndarray, embedding_dim: int = 3) -> float:
        """Wolfç®—æ³•è®¡ç®—LyapunovæŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            if len(series) < embedding_dim * 5:
                return None
            
            # ç›¸ç©ºé—´é‡æ„
            embedded = self._embed_time_series(series, embedding_dim, 1)
            if embedded.shape[0] < 10:
                return None
            
            lyapunov_sum = 0.0
            evolution_count = 0
            
            for i in range(embedded.shape[0] - 3):
                # å¯»æ‰¾æœ€è¿‘é‚»
                distances = np.linalg.norm(embedded - embedded[i], axis=1)
                distances[max(0, i-2):i+3] = np.inf  # æ’é™¤æ—¶é—´ä¸Šç›¸è¿‘çš„ç‚¹
                
                if np.min(distances) < np.inf:
                    nearest_idx = np.argmin(distances)
                    
                    # è®¡ç®—åˆå§‹åˆ†ç¦»
                    initial_separation = distances[nearest_idx]
                    
                    if initial_separation > 1e-10:
                        # æ¼”åŒ–ä¸€æ­¥
                        if i + 1 < embedded.shape[0] and nearest_idx + 1 < embedded.shape[0]:
                            evolved_separation = np.linalg.norm(
                                embedded[i + 1] - embedded[nearest_idx + 1]
                            )
                            
                            if evolved_separation > 1e-10:
                                lyapunov_contribution = np.log(evolved_separation / initial_separation)
                                lyapunov_sum += lyapunov_contribution
                                evolution_count += 1
            
            if evolution_count > 0:
                return lyapunov_sum / evolution_count
            
            return None
            
        except Exception as e:
            return None
    
    def _calculate_lyapunov_direct(self, series: np.ndarray) -> float:
        """åŸºäºç›¸ç©ºé—´çš„ç›´æ¥Lyapunovè®¡ç®—"""
        try:
            if len(series) < 15:
                return None
            
            # è®¡ç®—å±€éƒ¨å‘æ•£ç‡
            divergence_rates = []
            
            for i in range(len(series) - 5):
                window = series[i:i+5]
                
                if len(window) == 5:
                    # è®¡ç®—å±€éƒ¨çº¿æ€§å‘æ•£
                    differences = np.diff(window)
                    if len(differences) > 1:
                        # ä½¿ç”¨å·®åˆ†çš„æ ‡å‡†å·®ä½œä¸ºå‘æ•£åº¦é‡
                        local_divergence = np.std(differences)
                        if local_divergence > 1e-10:
                            divergence_rates.append(np.log(local_divergence + 1e-10))
            
            if len(divergence_rates) > 3:
                # è®¡ç®—å¹³å‡å‘æ•£ç‡
                mean_divergence = np.mean(divergence_rates)
                return mean_divergence
            
            return None
            
        except Exception as e:
            return None
    
    def _embed_time_series(self, series: np.ndarray, embedding_dim: int, delay: int) -> np.ndarray:
        """æ—¶é—´åºåˆ—çš„ç›¸ç©ºé—´åµŒå…¥"""
        try:
            n = len(series)
            embedded_length = n - delay * (embedding_dim - 1)
            
            if embedded_length <= 0:
                return np.array([])
            
            embedded = np.zeros((embedded_length, embedding_dim))
            
            for i in range(embedded_length):
                for j in range(embedding_dim):
                    embedded[i, j] = series[i + j * delay]
            
            return embedded
            
        except Exception as e:
            return np.array([])
    
    def _perform_phase_space_reconstruction(self, time_series: List[float]) -> Dict:
        """æ‰§è¡Œç›¸ç©ºé—´é‡æ„"""
        try:
            phase_space_analysis = {
                'optimal_embedding_dimension': 3,
                'optimal_delay': 1,
                'embedding_quality': 0.0,
                'phase_portrait_features': {},
                'attractor_dimension': 0.0,
                'reconstruction_error': 0.0
            }
            
            if len(time_series) < 10:
                return phase_space_analysis
            
            series = np.array(time_series)
            
            # 1. ç¡®å®šæœ€ä¼˜å»¶è¿Ÿæ—¶é—´
            optimal_delay = self._calculate_optimal_delay(series)
            phase_space_analysis['optimal_delay'] = optimal_delay
            
            # 2. ç¡®å®šæœ€ä¼˜åµŒå…¥ç»´æ•°
            optimal_dim = self._calculate_optimal_embedding_dimension(series, optimal_delay)
            phase_space_analysis['optimal_embedding_dimension'] = optimal_dim
            
            # 3. æ‰§è¡Œç›¸ç©ºé—´é‡æ„
            embedded = self._embed_time_series(series, optimal_dim, optimal_delay)
            
            if embedded.shape[0] > 5:
                # 4. åˆ†æç›¸ç©ºé—´ç‰¹å¾
                phase_features = self._analyze_phase_space_features(embedded)
                phase_space_analysis['phase_portrait_features'] = phase_features
                
                # 5. ä¼°ç®—å¸å¼•å­ç»´æ•°
                attractor_dim = self._estimate_attractor_dimension(embedded)
                phase_space_analysis['attractor_dimension'] = attractor_dim
                
                # 6. è®¡ç®—é‡æ„è´¨é‡
                reconstruction_quality = self._assess_reconstruction_quality(series, embedded, optimal_delay)
                phase_space_analysis['embedding_quality'] = reconstruction_quality
            
            return phase_space_analysis
            
        except Exception as e:
            print(f"      âŒ ç›¸ç©ºé—´é‡æ„å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_optimal_delay(self, series: np.ndarray) -> int:
        """è®¡ç®—æœ€ä¼˜å»¶è¿Ÿæ—¶é—´ï¼ˆåŸºäºäº’ä¿¡æ¯æˆ–è‡ªç›¸å…³ï¼‰"""
        try:
            max_delay = min(10, len(series) // 3)
            if max_delay < 2:
                return 1
            
            # ä½¿ç”¨è‡ªç›¸å…³å‡½æ•°å¯»æ‰¾æœ€ä¼˜å»¶è¿Ÿ
            autocorrelations = []
            
            for delay in range(1, max_delay + 1):
                if len(series) > delay:
                    correlation = np.corrcoef(series[:-delay], series[delay:])[0, 1]
                    if not np.isnan(correlation):
                        autocorrelations.append(abs(correlation))
                    else:
                        autocorrelations.append(0.0)
            
            if autocorrelations:
                # å¯»æ‰¾ç¬¬ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼
                for i in range(1, len(autocorrelations) - 1):
                    if (autocorrelations[i] < autocorrelations[i-1] and 
                        autocorrelations[i] < autocorrelations[i+1]):
                        return i + 1
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å±€éƒ¨æœ€å°å€¼ï¼Œä½¿ç”¨è‡ªç›¸å…³é™åˆ°1/eçš„ç‚¹
                threshold = 1.0 / np.e
                for i, corr in enumerate(autocorrelations):
                    if corr < threshold:
                        return i + 1
            
            return 1  # é»˜è®¤å»¶è¿Ÿ
            
        except Exception as e:
            return 1
    
    def _calculate_optimal_embedding_dimension(self, series: np.ndarray, delay: int) -> int:
        """è®¡ç®—æœ€ä¼˜åµŒå…¥ç»´æ•°ï¼ˆåŸºäºå‡æœ€è¿‘é‚»æ–¹æ³•ï¼‰"""
        try:
            max_dim = min(8, len(series) // (2 * delay))
            if max_dim < 2:
                return 3
            
            # ç®€åŒ–çš„å‡æœ€è¿‘é‚»åˆ†æ
            false_neighbor_percentages = []
            
            for dim in range(2, max_dim + 1):
                embedded = self._embed_time_series(series, dim, delay)
                
                if embedded.shape[0] < 5:
                    break
                
                false_neighbors = 0
                total_neighbors = 0
                
                for i in range(embedded.shape[0]):
                    # å¯»æ‰¾æœ€è¿‘é‚»
                    distances = np.linalg.norm(embedded - embedded[i], axis=1)
                    distances[i] = np.inf  # æ’é™¤è‡ªå·±
                    
                    if np.min(distances) < np.inf:
                        nearest_idx = np.argmin(distances)
                        
                        # æ£€æŸ¥åœ¨é«˜ç»´ç©ºé—´ä¸­æ˜¯å¦ä»ç„¶æ˜¯è¿‘é‚»
                        if dim < max_dim:
                            # ç®€åŒ–æ£€æŸ¥ï¼šæ¯”è¾ƒæ—¶é—´åºåˆ—å€¼
                            if (i < len(series) and nearest_idx < len(series)):
                                time_distance = abs(series[i] - series[nearest_idx])
                                spatial_distance = distances[nearest_idx]
                                
                                # å¦‚æœæ—¶é—´è·ç¦»ç›¸å¯¹ç©ºé—´è·ç¦»è¿‡å¤§ï¼Œè®¤ä¸ºæ˜¯å‡è¿‘é‚»
                                if spatial_distance > 0 and time_distance / spatial_distance > 2.0:
                                    false_neighbors += 1
                                
                                total_neighbors += 1
                
                if total_neighbors > 0:
                    false_percentage = false_neighbors / total_neighbors
                    false_neighbor_percentages.append(false_percentage)
                else:
                    false_neighbor_percentages.append(1.0)
            
            # é€‰æ‹©å‡æœ€è¿‘é‚»æ¯”ä¾‹è¶³å¤Ÿä½çš„æœ€å°ç»´æ•°
            for i, percentage in enumerate(false_neighbor_percentages):
                if percentage < 0.1:  # 10%é˜ˆå€¼
                    return i + 2
            
            return 3  # é»˜è®¤ç»´æ•°
            
        except Exception as e:
            return 3
    
    def _analyze_phase_space_features(self, embedded: np.ndarray) -> Dict:
        """åˆ†æç›¸ç©ºé—´ç‰¹å¾"""
        try:
            features = {
                'trajectory_length': 0.0,
                'bounding_box_volume': 0.0,
                'trajectory_density': 0.0,
                'recurrence_rate': 0.0,
                'geometric_complexity': 0.0
            }
            
            if embedded.shape[0] < 3:
                return features
            
            # 1. è½¨é“é•¿åº¦
            trajectory_length = 0.0
            for i in range(embedded.shape[0] - 1):
                trajectory_length += np.linalg.norm(embedded[i+1] - embedded[i])
            features['trajectory_length'] = trajectory_length
            
            # 2. è¾¹ç•Œæ¡†ä½“ç§¯
            if embedded.shape[1] > 0:
                ranges = []
                for dim in range(embedded.shape[1]):
                    dim_range = np.max(embedded[:, dim]) - np.min(embedded[:, dim])
                    ranges.append(max(dim_range, 1e-10))
                
                bounding_volume = np.prod(ranges)
                features['bounding_box_volume'] = bounding_volume
                
                # 3. è½¨é“å¯†åº¦
                if bounding_volume > 0:
                    features['trajectory_density'] = trajectory_length / bounding_volume
            
            # 4. å›å½’ç‡
            recurrence_count = 0
            total_pairs = 0
            threshold = np.std(embedded.flatten()) * 0.1
            
            for i in range(embedded.shape[0]):
                for j in range(i + 1, embedded.shape[0]):
                    distance = np.linalg.norm(embedded[i] - embedded[j])
                    if distance < threshold:
                        recurrence_count += 1
                    total_pairs += 1
            
            if total_pairs > 0:
                features['recurrence_rate'] = recurrence_count / total_pairs
            
            # 5. å‡ ä½•å¤æ‚åº¦
            if embedded.shape[0] > 3:
                # åŸºäºä¸»æˆåˆ†åˆ†æçš„å¤æ‚åº¦
                try:
                    centered = embedded - np.mean(embedded, axis=0)
                    cov_matrix = np.cov(centered.T)
                    eigenvalues = np.linalg.eigvals(cov_matrix)
                    eigenvalues = eigenvalues[eigenvalues > 1e-10]
                    
                    if len(eigenvalues) > 1:
                        # ä½¿ç”¨ç‰¹å¾å€¼çš„ç†µä½œä¸ºå¤æ‚åº¦åº¦é‡
                        eigenvalues = eigenvalues / np.sum(eigenvalues)
                        complexity = -np.sum(eigenvalues * np.log2(eigenvalues + 1e-10))
                        features['geometric_complexity'] = complexity / np.log2(len(eigenvalues))
                except:
                    pass
            
            return features
            
        except Exception as e:
            return {}
    
    def _estimate_attractor_dimension(self, embedded: np.ndarray) -> float:
        """ä¼°ç®—å¸å¼•å­ç»´æ•°ï¼ˆå…³è”ç»´æ•°ï¼‰"""
        try:
            if embedded.shape[0] < 10:
                return 0.0
            
            # è®¡ç®—å…³è”ç§¯åˆ†
            distances = []
            for i in range(embedded.shape[0]):
                for j in range(i + 1, embedded.shape[0]):
                    distance = np.linalg.norm(embedded[i] - embedded[j])
                    distances.append(distance)
            
            distances = np.array(distances)
            distances = distances[distances > 1e-10]
            
            if len(distances) < 10:
                return 0.0
            
            # é€‰æ‹©ä¸€ç³»åˆ—åŠå¾„
            min_distance = np.min(distances)
            max_distance = np.max(distances)
            
            radii = np.logspace(np.log10(min_distance), np.log10(max_distance), 20)
            
            correlation_integrals = []
            for radius in radii:
                count = np.sum(distances <= radius)
                correlation_integral = count / len(distances)
                correlation_integrals.append(correlation_integral + 1e-10)
            
            # æ‹Ÿåˆå…³è”ç»´æ•°
            valid_indices = np.where(np.array(correlation_integrals) > 1e-10)[0]
            if len(valid_indices) > 5:
                log_radii = np.log(radii[valid_indices])
                log_corr = np.log(correlation_integrals)[valid_indices]
                
                try:
                    slope = np.polyfit(log_radii, log_corr, 1)[0]
                    return max(0.0, slope)
                except:
                    return 0.0
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _assess_reconstruction_quality(self, original_series: np.ndarray, embedded: np.ndarray, delay: int) -> float:
        """è¯„ä¼°é‡æ„è´¨é‡"""
        try:
            if embedded.shape[0] < 5:
                return 0.0
            
            # ä½¿ç”¨é¢„æµ‹èƒ½åŠ›è¯„ä¼°é‡æ„è´¨é‡
            prediction_errors = []
            
            for i in range(embedded.shape[0] - 1):
                # å¯»æ‰¾æœ€è¿‘é‚»
                distances = np.linalg.norm(embedded - embedded[i], axis=1)
                distances[i] = np.inf
                
                if np.min(distances) < np.inf:
                    nearest_idx = np.argmin(distances)
                    
                    # é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
                    actual_next_idx = i + 1
                    predicted_next_idx = nearest_idx + 1
                    
                    if (actual_next_idx < len(original_series) and 
                        predicted_next_idx < len(original_series)):
                        
                        actual_next = original_series[actual_next_idx]
                        predicted_next = original_series[predicted_next_idx]
                        
                        error = abs(actual_next - predicted_next)
                        prediction_errors.append(error)
            
            if prediction_errors:
                mean_error = np.mean(prediction_errors)
                series_std = np.std(original_series)
                
                if series_std > 0:
                    # æ ‡å‡†åŒ–é¢„æµ‹è¯¯å·®
                    normalized_error = mean_error / series_std
                    quality = max(0.0, 1.0 - normalized_error)
                    return min(1.0, quality)
            
            return 0.5  # é»˜è®¤ä¸­ç­‰è´¨é‡
            
        except Exception as e:
            return 0.0
    
    def _identify_strange_attractors(self, phase_space: Dict, time_series: List[float]) -> Dict:
        """è¯†åˆ«å¥‡å¼‚å¸å¼•å­"""
        try:
            attractor_analysis = {
                'attractor_type': 'unknown',
                'attractor_detected': False,
                'attractor_characteristics': {},
                'basin_of_attraction': {},
                'stability_analysis': {}
            }
            
            if not phase_space or len(time_series) < 15:
                return attractor_analysis
            
            embedding_dim = phase_space.get('optimal_embedding_dimension', 3)
            delay = phase_space.get('optimal_delay', 1)
            
            series = np.array(time_series)
            embedded = self._embed_time_series(series, embedding_dim, delay)
            
            if embedded.shape[0] < 10:
                return attractor_analysis
            
            # 1. åˆ†æå¸å¼•å­ç±»å‹
            attractor_type = self._classify_attractor_type(embedded, series)
            attractor_analysis['attractor_type'] = attractor_type
            
            # 2. æ£€æµ‹æ˜¯å¦å­˜åœ¨å¥‡å¼‚å¸å¼•å­
            is_strange = self._detect_strange_attractor_properties(embedded)
            attractor_analysis['attractor_detected'] = is_strange
            
            # 3. åˆ†æå¸å¼•å­ç‰¹å¾
            characteristics = self._analyze_attractor_characteristics(embedded)
            attractor_analysis['attractor_characteristics'] = characteristics
            
            # 4. åˆ†æå¸å¼•åŸŸ
            basin_analysis = self._analyze_basin_of_attraction(embedded)
            attractor_analysis['basin_of_attraction'] = basin_analysis
            
            # 5. ç¨³å®šæ€§åˆ†æ
            stability = self._analyze_attractor_stability(embedded, series)
            attractor_analysis['stability_analysis'] = stability
            
            return attractor_analysis
            
        except Exception as e:
            print(f"      âŒ å¥‡å¼‚å¸å¼•å­è¯†åˆ«å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _classify_attractor_type(self, embedded: np.ndarray, series: np.ndarray) -> str:
        """åˆ†ç±»å¸å¼•å­ç±»å‹"""
        try:
            if embedded.shape[0] < 8:
                return 'insufficient_data'
            
            # åˆ†æè½¨é“çš„å‘¨æœŸæ€§å’Œå¤æ‚æ€§
            
            # 1. æ£€æŸ¥å›ºå®šç‚¹
            if self._is_fixed_point_attractor(embedded):
                return 'fixed_point'
            
            # 2. æ£€æŸ¥æé™ç¯
            if self._is_limit_cycle_attractor(embedded, series):
                return 'limit_cycle'
            
            # 3. æ£€æŸ¥å‡†å‘¨æœŸ
            if self._is_quasiperiodic_attractor(series):
                return 'quasiperiodic'
            
            # 4. æ£€æŸ¥æ··æ²Œå¸å¼•å­
            if self._is_chaotic_attractor(embedded, series):
                return 'chaotic'
            
            return 'unknown'
            
        except Exception as e:
            return 'unknown'
    
    def _is_fixed_point_attractor(self, embedded: np.ndarray) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå›ºå®šç‚¹å¸å¼•å­"""
        try:
            # è®¡ç®—è½¨é“çš„æ–¹å·®
            variances = np.var(embedded, axis=0)
            
            # å¦‚æœæ‰€æœ‰ç»´åº¦çš„æ–¹å·®éƒ½å¾ˆå°ï¼Œè®¤ä¸ºæ˜¯å›ºå®šç‚¹
            return all(var < 0.01 for var in variances)
            
        except Exception as e:
            return False
    
    def _is_limit_cycle_attractor(self, embedded: np.ndarray, series: np.ndarray) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæé™ç¯å¸å¼•å­"""
        try:
            # ä½¿ç”¨è‡ªç›¸å…³æ£€æµ‹å‘¨æœŸæ€§
            if len(series) < 10:
                return False
            
            # è®¡ç®—è‡ªç›¸å…³å‡½æ•°
            autocorr_threshold = 0.7
            max_lag = min(len(series) // 3, 10)
            
            for lag in range(2, max_lag):
                if len(series) > lag:
                    correlation = np.corrcoef(series[:-lag], series[lag:])[0, 1]
                    if not np.isnan(correlation) and abs(correlation) > autocorr_threshold:
                        return True
            
            return False
            
        except Exception as e:
            return False
    
    def _is_quasiperiodic_attractor(self, series: np.ndarray) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå‡†å‘¨æœŸå¸å¼•å­"""
        try:
            if len(series) < 15:
                return False
            
            # ä½¿ç”¨FFTåˆ†æé¢‘è°±ç‰¹å¾
            try:
                fft = np.fft.fft(series)
                power_spectrum = np.abs(fft) ** 2
                
                # å¯»æ‰¾ä¸»è¦é¢‘ç‡æˆåˆ†
                peak_indices = []
                threshold = np.max(power_spectrum) * 0.1
                
                for i in range(1, len(power_spectrum) // 2):
                    if (power_spectrum[i] > threshold and 
                        power_spectrum[i] > power_spectrum[i-1] and 
                        power_spectrum[i] > power_spectrum[i+1]):
                        peak_indices.append(i)
                
                # å‡†å‘¨æœŸè¿åŠ¨é€šå¸¸æœ‰å¤šä¸ªä¸ç›¸å…³çš„é¢‘ç‡æˆåˆ†
                return len(peak_indices) >= 2
                
            except:
                return False
                
        except Exception as e:
            return False
    
    def _is_chaotic_attractor(self, embedded: np.ndarray, series: np.ndarray) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ··æ²Œå¸å¼•å­"""
        try:
            # ç»¼åˆå¤šä¸ªæ··æ²ŒæŒ‡æ ‡
            chaos_indicators = []
            
            # 1. æ£€æŸ¥è½¨é“çš„ä¸è§„åˆ™æ€§
            if len(series) > 5:
                differences = np.diff(series)
                irregularity = np.std(differences) / (np.mean(np.abs(differences)) + 1e-10)
                chaos_indicators.append(irregularity > 0.5)
            
            # 2. æ£€æŸ¥ç›¸ç©ºé—´è½¨é“çš„å¤æ‚æ€§
            if embedded.shape[0] > 5:
                # è®¡ç®—è½¨é“é•¿åº¦ä¸è¾¹ç•Œæ¡†çš„æ¯”ç‡
                trajectory_length = 0.0
                for i in range(embedded.shape[0] - 1):
                    trajectory_length += np.linalg.norm(embedded[i+1] - embedded[i])
                
                if embedded.shape[1] > 0:
                    ranges = [np.max(embedded[:, dim]) - np.min(embedded[:, dim]) 
                             for dim in range(embedded.shape[1])]
                    bounding_box_perimeter = sum(ranges)
                    
                    if bounding_box_perimeter > 0:
                        complexity_ratio = trajectory_length / bounding_box_perimeter
                        chaos_indicators.append(complexity_ratio > 5.0)
            
            # 3. æ£€æŸ¥æ•æ„Ÿä¾èµ–æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if len(series) > 8:
                sensitivity = self._check_sensitive_dependence(series)
                chaos_indicators.append(sensitivity)
            
            # å¦‚æœå¤šæ•°æŒ‡æ ‡æ”¯æŒæ··æ²Œï¼Œåˆ™è®¤ä¸ºæ˜¯æ··æ²Œå¸å¼•å­
            return sum(chaos_indicators) >= 2
            
        except Exception as e:
            return False
    
    def _check_sensitive_dependence(self, series: np.ndarray) -> bool:
        """æ£€æŸ¥å¯¹åˆå§‹æ¡ä»¶çš„æ•æ„Ÿä¾èµ–æ€§"""
        try:
            # ç®€åŒ–çš„æ•æ„Ÿæ€§æ£€æµ‹
            if len(series) < 8:
                return False
            
            # æ¯”è¾ƒç›¸è¿‘åˆå§‹æ¡ä»¶çš„æ¼”åŒ–
            sensitivity_count = 0
            total_comparisons = 0
            
            for i in range(len(series) - 4):
                for j in range(i + 1, min(i + 3, len(series) - 4)):
                    # æ¯”è¾ƒåˆå§‹å·®å¼‚å’Œåç»­å·®å¼‚
                    initial_diff = abs(series[i] - series[j])
                    
                    if initial_diff > 0 and initial_diff < 0.1:  # ç›¸è¿‘çš„åˆå§‹æ¡ä»¶
                        # æ£€æŸ¥3æ­¥åçš„å·®å¼‚
                        final_diff = abs(series[i + 3] - series[j + 3])
                        
                        # å¦‚æœå·®å¼‚æ”¾å¤§äº†ï¼Œè¡¨æ˜æ•æ„Ÿä¾èµ–
                        if final_diff > initial_diff * 2:
                            sensitivity_count += 1
                        
                        total_comparisons += 1
            
            if total_comparisons > 0:
                sensitivity_ratio = sensitivity_count / total_comparisons
                return sensitivity_ratio > 0.3
            
            return False
            
        except Exception as e:
            return False
    
    def _detect_strange_attractor_properties(self, embedded: np.ndarray) -> bool:
        """æ£€æµ‹å¥‡å¼‚å¸å¼•å­çš„æ€§è´¨"""
        try:
            if embedded.shape[0] < 10:
                return False
            
            # å¥‡å¼‚å¸å¼•å­çš„ç‰¹å¾ï¼šåˆ†å½¢ç»´æ•°ã€æ•æ„Ÿä¾èµ–æ€§ã€æ‹“æ‰‘æ··åˆ
            strange_properties = []
            
            # 1. æ£€æŸ¥åˆ†å½¢ç»“æ„
            fractal_dimension = self._estimate_attractor_dimension(embedded)
            # å¥‡å¼‚å¸å¼•å­é€šå¸¸æœ‰éæ•´æ•°ç»´æ•°
            is_fractal = (fractal_dimension > 0 and 
                         abs(fractal_dimension - round(fractal_dimension)) > 0.1)
            strange_properties.append(is_fractal)
            
            # 2. æ£€æŸ¥è‡ªç›¸ä¼¼æ€§
            self_similarity = self._check_self_similarity(embedded)
            strange_properties.append(self_similarity)
            
            # 3. æ£€æŸ¥æœ‰ç•Œæ€§
            is_bounded = self._check_boundedness(embedded)
            strange_properties.append(is_bounded)
            
            # å¥‡å¼‚å¸å¼•å­éœ€è¦æ»¡è¶³å¤šä¸ªæ¡ä»¶
            return sum(strange_properties) >= 2
            
        except Exception as e:
            return False
    
    def _check_self_similarity(self, embedded: np.ndarray) -> bool:
        """æ£€æŸ¥è‡ªç›¸ä¼¼æ€§"""
        try:
            if embedded.shape[0] < 8:
                return False
            
            # ç®€åŒ–çš„è‡ªç›¸ä¼¼æ€§æ£€æµ‹ï¼šæ¯”è¾ƒä¸åŒå°ºåº¦çš„ç»“æ„
            scales = [2, 3, 4]
            similarities = []
            
            for scale in scales:
                if embedded.shape[0] > scale * 2:
                    # æ¯”è¾ƒåŸå§‹è½¨é“å’Œç¼©æ”¾è½¨é“çš„ç›¸ä¼¼æ€§
                    scaled_indices = list(range(0, embedded.shape[0], scale))
                    scaled_trajectory = embedded[scaled_indices]
                    
                    if len(scaled_trajectory) > 3:
                        # ä½¿ç”¨ç›¸å…³æ€§åº¦é‡ç›¸ä¼¼æ€§
                        original_dists = []
                        scaled_dists = []
                        
                        for i in range(min(len(scaled_trajectory) - 1, embedded.shape[0] - 1)):
                            if i < embedded.shape[0] - 1:
                                orig_dist = np.linalg.norm(embedded[i+1] - embedded[i])
                                original_dists.append(orig_dist)
                            
                            if i < len(scaled_trajectory) - 1:
                                scaled_dist = np.linalg.norm(scaled_trajectory[i+1] - scaled_trajectory[i])
                                scaled_dists.append(scaled_dist)
                        
                        if len(original_dists) >= 3 and len(scaled_dists) >= 3:
                            min_len = min(len(original_dists), len(scaled_dists))
                            correlation = np.corrcoef(
                                original_dists[:min_len], 
                                scaled_dists[:min_len]
                            )[0, 1]
                            
                            if not np.isnan(correlation):
                                similarities.append(abs(correlation))
            
            if similarities:
                avg_similarity = np.mean(similarities)
                return avg_similarity > 0.6  # é«˜ç›¸ä¼¼æ€§è¡¨æ˜è‡ªç›¸ä¼¼ç»“æ„
            
            return False
            
        except Exception as e:
            return False
    
    def _check_boundedness(self, embedded: np.ndarray) -> bool:
        """æ£€æŸ¥æœ‰ç•Œæ€§"""
        try:
            if embedded.shape[0] < 5:
                return False
            
            # æ£€æŸ¥è½¨é“æ˜¯å¦åœ¨æœ‰é™åŒºåŸŸå†…
            for dim in range(embedded.shape[1]):
                dim_range = np.max(embedded[:, dim]) - np.min(embedded[:, dim])
                dim_std = np.std(embedded[:, dim])
                
                # å¦‚æœæŸä¸ªç»´åº¦çš„èŒƒå›´è¿‡å¤§æˆ–æ–¹å·®è¿‡å¤§ï¼Œå¯èƒ½ä¸æ˜¯æœ‰ç•Œçš„
                if dim_range > 10 * dim_std:
                    return False
            
            return True
            
        except Exception as e:
            return False
    
    def _analyze_attractor_characteristics(self, embedded: np.ndarray) -> Dict:
        """åˆ†æå¸å¼•å­ç‰¹å¾"""
        try:
            characteristics = {
                'fractal_dimension': 0.0,
                'lyapunov_dimension': 0.0,
                'box_counting_dimension': 0.0,
                'correlation_dimension': 0.0,
                'information_dimension': 0.0
            }
            
            if embedded.shape[0] < 8:
                return characteristics
            
            # 1. å…³è”ç»´æ•°
            correlation_dim = self._estimate_attractor_dimension(embedded)
            characteristics['correlation_dimension'] = correlation_dim
            
            # 2. ç›’è®¡æ•°ç»´æ•°
            box_counting_dim = self._calculate_box_counting_dimension(embedded)
            characteristics['box_counting_dimension'] = box_counting_dim
            
            # 3. ä¿¡æ¯ç»´æ•°
            information_dim = self._calculate_information_dimension(embedded)
            characteristics['information_dimension'] = information_dim
            
            # 4. ç»¼åˆåˆ†å½¢ç»´æ•°
            available_dims = [d for d in [correlation_dim, box_counting_dim, information_dim] if d > 0]
            if available_dims:
                characteristics['fractal_dimension'] = np.mean(available_dims)
            
            return characteristics
            
        except Exception as e:
            return {}
    
    def _calculate_box_counting_dimension(self, embedded: np.ndarray) -> float:
        """è®¡ç®—ç›’è®¡æ•°ç»´æ•°"""
        try:
            if embedded.shape[0] < 8:
                return 0.0
            
            # ç¡®å®šè¾¹ç•Œæ¡†
            mins = np.min(embedded, axis=0)
            maxs = np.max(embedded, axis=0)
            ranges = maxs - mins
            
            # é¿å…é™¤é›¶
            ranges = np.maximum(ranges, 1e-10)
            
            # ä¸åŒçš„ç›’å­å°ºå¯¸
            box_sizes = []
            box_counts = []
            
            for scale_factor in [2, 4, 8, 16]:
                box_size = np.max(ranges) / scale_factor
                if box_size > 1e-10:
                    count = self._count_occupied_boxes(embedded, mins, box_size, ranges)
                    if count > 0:
                        box_sizes.append(box_size)
                        box_counts.append(count)
            
            # æ‹Ÿåˆå¯¹æ•°-å¯¹æ•°å…³ç³»
            if len(box_sizes) >= 3:
                log_sizes = np.log(box_sizes)
                log_counts = np.log(box_counts)
                
                try:
                    slope = -np.polyfit(log_sizes, log_counts, 1)[0]
                    return max(0.0, slope)
                except:
                    return 0.0
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _count_occupied_boxes(self, embedded: np.ndarray, mins: np.ndarray, 
                            box_size: float, ranges: np.ndarray) -> int:
        """è®¡ç®—è¢«å æ®çš„ç›’å­æ•°é‡"""
        try:
            occupied_boxes = set()
            
            for point in embedded:
                # è®¡ç®—ç‚¹æ‰€åœ¨çš„ç›’å­ç´¢å¼•
                box_indices = []
                for dim in range(len(point)):
                    if ranges[dim] > 0:
                        normalized_coord = (point[dim] - mins[dim]) / ranges[dim]
                        box_index = int(normalized_coord / (box_size / np.max(ranges)))
                        box_indices.append(box_index)
                    else:
                        box_indices.append(0)
                
                occupied_boxes.add(tuple(box_indices))
            
            return len(occupied_boxes)
            
        except Exception as e:
            return 0
    
    def _calculate_information_dimension(self, embedded: np.ndarray) -> float:
        """è®¡ç®—ä¿¡æ¯ç»´æ•°"""
        try:
            if embedded.shape[0] < 10:
                return 0.0
            
            # ä½¿ç”¨ç½‘æ ¼æ–¹æ³•è®¡ç®—ä¿¡æ¯ç»´æ•°
            grid_sizes = [4, 8, 16]
            dimensions = []
            
            for grid_size in grid_sizes:
                # åˆ›å»ºç½‘æ ¼å¹¶è®¡ç®—æ¯ä¸ªç½‘æ ¼çš„æ¦‚ç‡
                grid_probabilities = self._calculate_grid_probabilities(embedded, grid_size)
                
                if grid_probabilities:
                    # è®¡ç®—ä¿¡æ¯ç†µ
                    entropy = -np.sum([p * np.log2(p) for p in grid_probabilities if p > 0])
                    
                    # ä¿¡æ¯ç»´æ•°è¿‘ä¼¼
                    box_size = 1.0 / grid_size
                    if box_size > 0:
                        dimension = entropy / np.log2(1.0 / box_size)
                        dimensions.append(dimension)
            
            if dimensions:
                return np.mean(dimensions)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_grid_probabilities(self, embedded: np.ndarray, grid_size: int) -> List[float]:
        """è®¡ç®—ç½‘æ ¼æ¦‚ç‡åˆ†å¸ƒ"""
        try:
            # å½’ä¸€åŒ–æ•°æ®åˆ°[0,1]
            mins = np.min(embedded, axis=0)
            maxs = np.max(embedded, axis=0)
            ranges = maxs - mins
            ranges = np.maximum(ranges, 1e-10)
            
            normalized = (embedded - mins) / ranges
            
            # è®¡ç®—æ¯ä¸ªç‚¹çš„ç½‘æ ¼ç´¢å¼•
            grid_counts = {}
            
            for point in normalized:
                grid_indices = []
                for dim in range(len(point)):
                    grid_index = min(int(point[dim] * grid_size), grid_size - 1)
                    grid_indices.append(grid_index)
                
                grid_key = tuple(grid_indices)
                grid_counts[grid_key] = grid_counts.get(grid_key, 0) + 1
            
            # è½¬æ¢ä¸ºæ¦‚ç‡
            total_points = embedded.shape[0]
            probabilities = [count / total_points for count in grid_counts.values()]
            
            return probabilities
            
        except Exception as e:
            return []
    
    def _analyze_basin_of_attraction(self, embedded: np.ndarray) -> Dict:
        """åˆ†æå¸å¼•åŸŸ"""
        try:
            basin_analysis = {
                'basin_size_estimate': 0.0,
                'basin_boundary_complexity': 0.0,
                'multiple_attractors': False,
                'basin_stability': 0.0
            }
            
            if embedded.shape[0] < 10:
                return basin_analysis
            
            # 1. ä¼°ç®—å¸å¼•åŸŸå¤§å°
            basin_size = self._estimate_basin_size(embedded)
            basin_analysis['basin_size_estimate'] = basin_size
            
            # 2. åˆ†æè¾¹ç•Œå¤æ‚æ€§
            boundary_complexity = self._analyze_basin_boundary_complexity(embedded)
            basin_analysis['basin_boundary_complexity'] = boundary_complexity
            
            # 3. æ£€æµ‹å¤šé‡å¸å¼•å­
            multiple_attractors = self._detect_multiple_attractors(embedded)
            basin_analysis['multiple_attractors'] = multiple_attractors
            
            # 4. å¸å¼•åŸŸç¨³å®šæ€§
            stability = self._calculate_basin_stability(embedded)
            basin_analysis['basin_stability'] = stability
            
            return basin_analysis
            
        except Exception as e:
            return {}
    
    def _estimate_basin_size(self, embedded: np.ndarray) -> float:
        """ä¼°ç®—å¸å¼•åŸŸå¤§å°"""
        try:
            # ä½¿ç”¨è½¨é“å æ®çš„ç›¸ç©ºé—´ä½“ç§¯ä¼°ç®—å¸å¼•åŸŸ
            if embedded.shape[1] == 0:
                return 0.0
            
            # è®¡ç®—æ¯ä¸ªç»´åº¦çš„èŒƒå›´
            ranges = []
            for dim in range(embedded.shape[1]):
                dim_range = np.max(embedded[:, dim]) - np.min(embedded[:, dim])
                ranges.append(max(dim_range, 1e-10))
            
            # å¸å¼•åŸŸä½“ç§¯è¿‘ä¼¼
            basin_volume = np.prod(ranges)
            
            # å½’ä¸€åŒ–åˆ°[0,1]
            max_possible_volume = 1.0  # å‡è®¾å½’ä¸€åŒ–åçš„æœ€å¤§ä½“ç§¯
            normalized_size = min(1.0, basin_volume / max_possible_volume)
            
            return normalized_size
            
        except Exception as e:
            return 0.0
    
    def _analyze_basin_boundary_complexity(self, embedded: np.ndarray) -> float:
        """åˆ†æå¸å¼•åŸŸè¾¹ç•Œå¤æ‚æ€§"""
        try:
            if embedded.shape[0] < 8:
                return 0.0
            
            # åˆ†æè½¨é“åˆ°è¾¹ç•Œçš„è·ç¦»å˜åŒ–
            boundary_distances = []
            
            # è®¡ç®—æ¯ä¸ªç‚¹åˆ°è¾¹ç•Œçš„ä¼°ç®—è·ç¦»
            for point in embedded:
                # ç®€åŒ–çš„è¾¹ç•Œè·ç¦»ï¼šåˆ°æœ€å°åŒ…å›´ç›’è¾¹ç•Œçš„è·ç¦»
                distances_to_boundaries = []
                
                for dim in range(len(point)):
                    dim_min = np.min(embedded[:, dim])
                    dim_max = np.max(embedded[:, dim])
                    
                    dist_to_min = abs(point[dim] - dim_min)
                    dist_to_max = abs(point[dim] - dim_max)
                    min_boundary_dist = min(dist_to_min, dist_to_max)
                    
                    distances_to_boundaries.append(min_boundary_dist)
                
                boundary_distance = min(distances_to_boundaries)
                boundary_distances.append(boundary_distance)
            
            if boundary_distances:
                # è¾¹ç•Œå¤æ‚æ€§ï¼šè·ç¦»å˜åŒ–çš„å¤æ‚ç¨‹åº¦
                boundary_variance = np.var(boundary_distances)
                boundary_mean = np.mean(boundary_distances)
                
                if boundary_mean > 0:
                    complexity = boundary_variance / boundary_mean
                    return min(1.0, complexity)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _detect_multiple_attractors(self, embedded: np.ndarray) -> bool:
        """æ£€æµ‹å¤šé‡å¸å¼•å­"""
        try:
            if embedded.shape[0] < 12:
                return False
            
            # ä½¿ç”¨èšç±»æ–¹æ³•æ£€æµ‹å¤šä¸ªå¸å¼•åŒºåŸŸ
            # ç®€åŒ–çš„èšç±»ï¼šåŸºäºè·ç¦»çš„åˆ†ç»„
            
            # è®¡ç®—æ‰€æœ‰ç‚¹ä¹‹é—´çš„è·ç¦»
            distances = []
            for i in range(embedded.shape[0]):
                for j in range(i + 1, embedded.shape[0]):
                    distance = np.linalg.norm(embedded[i] - embedded[j])
                    distances.append(distance)
            
            if not distances:
                return False
            
            # ä½¿ç”¨è·ç¦»é˜ˆå€¼è¿›è¡Œç®€å•èšç±»
            threshold = np.median(distances)
            
            clusters = []
            unassigned = list(range(embedded.shape[0]))
            
            while unassigned:
                # å¼€å§‹æ–°çš„èšç±»
                current_cluster = [unassigned[0]]
                unassigned.remove(unassigned[0])
                
                # æ·»åŠ ç›¸è¿‘çš„ç‚¹
                cluster_changed = True
                while cluster_changed:
                    cluster_changed = False
                    for i in unassigned[:]:  # ä½¿ç”¨å‰¯æœ¬è¿›è¡Œè¿­ä»£
                        for j in current_cluster:
                            distance = np.linalg.norm(embedded[i] - embedded[j])
                            if distance < threshold:
                                current_cluster.append(i)
                                unassigned.remove(i)
                                cluster_changed = True
                                break
                        if cluster_changed:
                            break
                
                if len(current_cluster) >= 3:  # è‡³å°‘3ä¸ªç‚¹æ‰ç®—ä¸€ä¸ªæœ‰æ•ˆèšç±»
                    clusters.append(current_cluster)
            
            # å¦‚æœæœ‰å¤šä¸ªæœ‰æ•ˆèšç±»ï¼Œè¯´æ˜å¯èƒ½æœ‰å¤šä¸ªå¸å¼•å­
            return len(clusters) >= 2
            
        except Exception as e:
            return False
    
    def _calculate_basin_stability(self, embedded: np.ndarray) -> float:
        """è®¡ç®—å¸å¼•åŸŸç¨³å®šæ€§"""
        try:
            if embedded.shape[0] < 8:
                return 0.0
            
            # åˆ†æè½¨é“çš„ç¨³å®šæ€§ï¼šè½¨é“æ˜¯å¦ä¿æŒåœ¨ç›¸åŒåŒºåŸŸ
            stability_measures = []
            
            # åˆ†æè½¨é“çš„å±€éƒ¨ç¨³å®šæ€§
            window_size = min(4, embedded.shape[0] // 2)
            
            for start in range(embedded.shape[0] - window_size + 1):
                window = embedded[start:start + window_size]
                
                # è®¡ç®—çª—å£å†…çš„ç¨³å®šæ€§
                center = np.mean(window, axis=0)
                distances_from_center = [np.linalg.norm(point - center) for point in window]
                
                if distances_from_center:
                    stability = 1.0 / (1.0 + np.std(distances_from_center))
                    stability_measures.append(stability)
            
            if stability_measures:
                return np.mean(stability_measures)
            
            return 0.5
            
        except Exception as e:
            return 0.0
    
    def _analyze_attractor_stability(self, embedded: np.ndarray, series: np.ndarray) -> Dict:
        """åˆ†æå¸å¼•å­ç¨³å®šæ€§"""
        try:
            stability_analysis = {
                'orbital_stability': 0.0,
                'structural_stability': 0.0,
                'asymptotic_stability': 0.0,
                'stability_classification': 'unknown'
            }
            
            if embedded.shape[0] < 8 or len(series) < 8:
                return stability_analysis
            
            # 1. è½¨é“ç¨³å®šæ€§
            orbital_stability = self._calculate_orbital_stability(embedded)
            stability_analysis['orbital_stability'] = orbital_stability
            
            # 2. ç»“æ„ç¨³å®šæ€§
            structural_stability = self._calculate_structural_stability(embedded, series)
            stability_analysis['structural_stability'] = structural_stability
            
            # 3. æ¸è¿‘ç¨³å®šæ€§
            asymptotic_stability = self._calculate_asymptotic_stability(series)
            stability_analysis['asymptotic_stability'] = asymptotic_stability
            
            # 4. ç¨³å®šæ€§åˆ†ç±»
            classification = self._classify_stability(
                orbital_stability, structural_stability, asymptotic_stability
            )
            stability_analysis['stability_classification'] = classification
            
            return stability_analysis
            
        except Exception as e:
            return {}
    
    def _calculate_orbital_stability(self, embedded: np.ndarray) -> float:
        """è®¡ç®—è½¨é“ç¨³å®šæ€§"""
        try:
            if embedded.shape[0] < 5:
                return 0.0
            
            # åˆ†æç›¸é‚»è½¨é“ç‚¹çš„ç¨³å®šæ€§
            stability_scores = []
            
            for i in range(embedded.shape[0] - 1):
                current_point = embedded[i]
                next_point = embedded[i + 1]
                
                # è®¡ç®—è½¨é“å˜åŒ–
                step_size = np.linalg.norm(next_point - current_point)
                
                # å¯»æ‰¾ç›¸è¿‘çš„å†å²ç‚¹
                distances = np.linalg.norm(embedded - current_point, axis=1)
                similar_indices = np.where((distances > 0) & (distances < 0.1))[0]
                
                if len(similar_indices) > 0:
                    # æ¯”è¾ƒç›¸ä¼¼æ¡ä»¶ä¸‹çš„æ¼”åŒ–
                    similar_steps = []
                    for idx in similar_indices:
                        if idx + 1 < embedded.shape[0]:
                            similar_step = np.linalg.norm(embedded[idx + 1] - embedded[idx])
                            similar_steps.append(similar_step)
                    
                    if similar_steps:
                        # ç¨³å®šæ€§ï¼šç›¸ä¼¼æ¡ä»¶ä¸‹æ¼”åŒ–çš„ä¸€è‡´æ€§
                        step_consistency = 1.0 / (1.0 + np.std(similar_steps))
                        stability_scores.append(step_consistency)
            
            if stability_scores:
                return np.mean(stability_scores)
            
            return 0.5
            
        except Exception as e:
            return 0.0
    
    def _calculate_structural_stability(self, embedded: np.ndarray, series: np.ndarray) -> float:
        """è®¡ç®—ç»“æ„ç¨³å®šæ€§"""
        try:
            # åˆ†æå¸å¼•å­ç»“æ„å¯¹å°æ‰°åŠ¨çš„æ•æ„Ÿæ€§
            if len(series) < 10:
                return 0.0
            
            # æ·»åŠ å°æ‰°åŠ¨å¹¶æ¯”è¾ƒç»“æ„å˜åŒ–
            perturbation_magnitude = np.std(series) * 0.05  # 5%çš„æ‰°åŠ¨
            
            # åˆ›å»ºæ‰°åŠ¨åºåˆ—
            perturbed_series = series + np.random.normal(0, perturbation_magnitude, len(series))
            
            # é‡æ„æ‰°åŠ¨åçš„ç›¸ç©ºé—´
            perturbed_embedded = self._embed_time_series(perturbed_series, embedded.shape[1], 1)
            
            if perturbed_embedded.shape[0] < 5:
                return 0.5
            
            # æ¯”è¾ƒåŸå§‹å’Œæ‰°åŠ¨åçš„ç»“æ„
            original_features = self._analyze_phase_space_features(embedded)
            perturbed_features = self._analyze_phase_space_features(perturbed_embedded)
            
            # è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§
            structural_differences = []
            
            common_features = ['trajectory_length', 'trajectory_density', 'recurrence_rate']
            for feature in common_features:
                if (feature in original_features and feature in perturbed_features and
                    original_features[feature] > 0):
                    
                    relative_change = abs(
                        perturbed_features[feature] - original_features[feature]
                    ) / original_features[feature]
                    
                    structural_differences.append(relative_change)
            
            if structural_differences:
                avg_change = np.mean(structural_differences)
                # ç»“æ„ç¨³å®šæ€§ï¼šå˜åŒ–è¶Šå°è¶Šç¨³å®š
                structural_stability = 1.0 / (1.0 + avg_change)
                return structural_stability
            
            return 0.5
            
        except Exception as e:
            return 0.0
    
    def _calculate_asymptotic_stability(self, series: np.ndarray) -> float:
        """è®¡ç®—æ¸è¿‘ç¨³å®šæ€§"""
        try:
            if len(series) < 8:
                return 0.0
            
            # åˆ†ææ—¶é—´åºåˆ—çš„é•¿æœŸè¡Œä¸º
            # æ£€æŸ¥æ˜¯å¦æ”¶æ•›åˆ°ç¨³å®šçŠ¶æ€
            
            # åˆ†æè¶‹åŠ¿
            time_indices = np.arange(len(series))
            try:
                slope = np.polyfit(time_indices, series, 1)[0]
                
                # è®¡ç®—å›´ç»•è¶‹åŠ¿çš„æ–¹å·®å˜åŒ–
                detrended = series - (slope * time_indices + np.mean(series))
                
                # åˆ†ææ–¹å·®çš„æ—¶é—´æ¼”åŒ–
                window_size = min(4, len(series) // 2)
                variance_timeline = []
                
                for start in range(len(detrended) - window_size + 1):
                    window_variance = np.var(detrended[start:start + window_size])
                    variance_timeline.append(window_variance)
                
                if len(variance_timeline) >= 3:
                    # æ¸è¿‘ç¨³å®šæ€§ï¼šæ–¹å·®æ˜¯å¦éšæ—¶é—´å‡å°
                    early_variance = np.mean(variance_timeline[:len(variance_timeline)//2])
                    late_variance = np.mean(variance_timeline[len(variance_timeline)//2:])
                    
                    if early_variance > 0:
                        stability_improvement = max(0, early_variance - late_variance) / early_variance
                        return min(1.0, stability_improvement * 2)  # æ”¾å¤§æ•ˆæœ
                
            except:
                pass
            
            return 0.5
            
        except Exception as e:
            return 0.0
    
    def _classify_stability(self, orbital: float, structural: float, asymptotic: float) -> str:
        """åˆ†ç±»ç¨³å®šæ€§ç±»å‹"""
        try:
            avg_stability = (orbital + structural + asymptotic) / 3
            
            if avg_stability > 0.8:
                return 'highly_stable'
            elif avg_stability > 0.6:
                return 'stable'
            elif avg_stability > 0.4:
                return 'marginally_stable'
            elif avg_stability > 0.2:
                return 'unstable'
            else:
                return 'highly_unstable'
                
        except Exception as e:
            return 'unknown'

    def _perform_bifurcation_analysis(self, time_series: List[float]) -> Dict:
        """
        æ‰§è¡Œåˆ†å²”åˆ†æ - åŸºäºåˆ†å²”ç†è®ºå’ŒåŠ¨åŠ›å­¦ç³»ç»Ÿç†è®º
        å®ç°Hopfåˆ†å²”ã€å‘¨æœŸå€åŒ–åˆ†å²”å’Œéç»“åˆ†å²”çš„æ£€æµ‹
        """
        try:
            bifurcation_analysis = {
                'bifurcation_points': [],
                'bifurcation_types': [],
                'parameter_critical_values': {},
                'route_to_chaos': {},
                'period_doubling_cascade': {},
                'hopf_bifurcations': {},
                'saddle_node_bifurcations': {}
            }
            
            if len(time_series) < 15:
                return bifurcation_analysis
            
            series = np.array(time_series)
            
            # 1. æ£€æµ‹å‘¨æœŸå€åŒ–åˆ†å²”
            period_doubling = self._detect_period_doubling_bifurcations(series)
            bifurcation_analysis['period_doubling_cascade'] = period_doubling
            
            # 2. æ£€æµ‹Hopfåˆ†å²”
            hopf_bifurcations = self._detect_hopf_bifurcations(series)
            bifurcation_analysis['hopf_bifurcations'] = hopf_bifurcations
            
            # 3. æ£€æµ‹éç»“åˆ†å²”
            saddle_node = self._detect_saddle_node_bifurcations(series)
            bifurcation_analysis['saddle_node_bifurcations'] = saddle_node
            
            # 4. åˆ†æé€šå‘æ··æ²Œçš„è·¯å¾„
            chaos_route = self._analyze_route_to_chaos(series, period_doubling)
            bifurcation_analysis['route_to_chaos'] = chaos_route
            
            # 5. ç¡®å®šä¸´ç•Œå‚æ•°å€¼
            critical_values = self._determine_critical_parameter_values(series)
            bifurcation_analysis['parameter_critical_values'] = critical_values
            
            # 6. ç»¼åˆåˆ†å²”ç‚¹
            all_bifurcations = []
            if period_doubling.get('bifurcation_detected', False):
                all_bifurcations.extend(period_doubling.get('bifurcation_points', []))
            if hopf_bifurcations.get('bifurcation_detected', False):
                all_bifurcations.extend(hopf_bifurcations.get('bifurcation_points', []))
            if saddle_node.get('bifurcation_detected', False):
                all_bifurcations.extend(saddle_node.get('bifurcation_points', []))
            
            bifurcation_analysis['bifurcation_points'] = sorted(set(all_bifurcations))
            
            # 7. åˆ†å²”ç±»å‹ç»Ÿè®¡
            bifurcation_types = []
            if period_doubling.get('bifurcation_detected', False):
                bifurcation_types.append('period_doubling')
            if hopf_bifurcations.get('bifurcation_detected', False):
                bifurcation_types.append('hopf')
            if saddle_node.get('bifurcation_detected', False):
                bifurcation_types.append('saddle_node')
            
            bifurcation_analysis['bifurcation_types'] = bifurcation_types
            
            return bifurcation_analysis
            
        except Exception as e:
            print(f"      âŒ åˆ†å²”åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _detect_period_doubling_bifurcations(self, series: np.ndarray) -> Dict:
        """æ£€æµ‹å‘¨æœŸå€åŒ–åˆ†å²”"""
        try:
            period_analysis = {
                'bifurcation_detected': False,
                'bifurcation_points': [],
                'period_sequence': [],
                'doubling_ratio': 0.0,
                'feigenbaum_constant': 0.0
            }
            
            if len(series) < 12:
                return period_analysis
            
            # åˆ†æä¸åŒæ—¶é—´çª—å£çš„å‘¨æœŸæ€§
            window_size = min(8, len(series) // 2)
            periods = []
            bifurcation_candidates = []
            
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                period = self._detect_period_in_window(window)
                periods.append(period)
            
            # å¯»æ‰¾å‘¨æœŸå€åŒ–æ¨¡å¼
            for i in range(1, len(periods)):
                if periods[i] > 0 and periods[i-1] > 0:
                    ratio = periods[i] / periods[i-1]
                    
                    # å‘¨æœŸå€åŒ–çš„ç‰¹å¾ï¼šå‘¨æœŸå¤§çº¦ç¿»å€
                    if 1.8 <= ratio <= 2.2:
                        bifurcation_candidates.append(i)
                        period_analysis['bifurcation_detected'] = True
            
            if bifurcation_candidates:
                period_analysis['bifurcation_points'] = bifurcation_candidates
                period_analysis['period_sequence'] = [periods[i] for i in bifurcation_candidates]
                
                # è®¡ç®—å¹³å‡å€åŒ–æ¯”ç‡
                ratios = []
                for i in range(1, len(period_analysis['period_sequence'])):
                    if period_analysis['period_sequence'][i-1] > 0:
                        ratio = period_analysis['period_sequence'][i] / period_analysis['period_sequence'][i-1]
                        ratios.append(ratio)
                
                if ratios:
                    period_analysis['doubling_ratio'] = np.mean(ratios)
                
                # ä¼°ç®—Feigenbaumå¸¸æ•°
                if len(ratios) >= 2:
                    feigenbaum = self._estimate_feigenbaum_constant(ratios)
                    period_analysis['feigenbaum_constant'] = feigenbaum
            
            return period_analysis
            
        except Exception as e:
            return {'bifurcation_detected': False}
    
    def _detect_period_in_window(self, window: np.ndarray) -> int:
        """æ£€æµ‹çª—å£å†…çš„å‘¨æœŸ"""
        try:
            if len(window) < 4:
                return 0
            
            # ä½¿ç”¨è‡ªç›¸å…³æ£€æµ‹å‘¨æœŸ
            max_period = len(window) // 2
            best_period = 0
            best_correlation = 0.0
            
            for period in range(1, max_period + 1):
                if len(window) > period:
                    # è®¡ç®—å‘¨æœŸæ€§ç›¸å…³
                    correlation = 0.0
                    count = 0
                    
                    for i in range(len(window) - period):
                        correlation += abs(window[i] - window[i + period])
                        count += 1
                    
                    if count > 0:
                        avg_difference = correlation / count
                        # ç›¸å…³æ€§ï¼šå·®å¼‚è¶Šå°ï¼Œå‘¨æœŸæ€§è¶Šå¼º
                        period_correlation = 1.0 / (1.0 + avg_difference)
                        
                        if period_correlation > best_correlation:
                            best_correlation = period_correlation
                            best_period = period
            
            # åªæœ‰å½“ç›¸å…³æ€§è¶³å¤Ÿå¼ºæ—¶æ‰è®¤ä¸ºæ‰¾åˆ°äº†å‘¨æœŸ
            if best_correlation > 0.7:
                return best_period
            
            return 0
            
        except Exception as e:
            return 0
    
    def _estimate_feigenbaum_constant(self, ratios: List[float]) -> float:
        """ä¼°ç®—Feigenbaumå¸¸æ•°"""
        try:
            # Feigenbaumå¸¸æ•°Î´ â‰ˆ 4.669...
            # åœ¨å‘¨æœŸå€åŒ–çº§è”ä¸­ï¼ŒÎ´ = lim(nâ†’âˆ) (r_{n} - r_{n-1}) / (r_{n+1} - r_{n})
            
            if len(ratios) < 3:
                return 0.0
            
            # ç®€åŒ–ä¼°ç®—ï¼šä½¿ç”¨è¿ç»­æ¯”ç‡çš„æ¯”å€¼
            feigenbaum_estimates = []
            
            for i in range(len(ratios) - 2):
                if abs(ratios[i+2] - ratios[i+1]) > 1e-10:
                    delta = abs(ratios[i+1] - ratios[i]) / abs(ratios[i+2] - ratios[i+1])
                    feigenbaum_estimates.append(delta)
            
            if feigenbaum_estimates:
                estimated_delta = np.mean(feigenbaum_estimates)
                
                # æ£€æŸ¥æ˜¯å¦æ¥è¿‘ç†è®ºå€¼
                theoretical_delta = 4.669
                if abs(estimated_delta - theoretical_delta) < 2.0:
                    return estimated_delta
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _detect_hopf_bifurcations(self, series: np.ndarray) -> Dict:
        """æ£€æµ‹Hopfåˆ†å²”"""
        try:
            hopf_analysis = {
                'bifurcation_detected': False,
                'bifurcation_points': [],
                'oscillation_onset': [],
                'amplitude_changes': {},
                'frequency_changes': {}
            }
            
            if len(series) < 10:
                return hopf_analysis
            
            # Hopfåˆ†å²”çš„ç‰¹å¾ï¼šä»ç¨³å®šçŠ¶æ€åˆ°æŒ¯è¡çŠ¶æ€çš„è½¬å˜
            
            # 1. åˆ†ææŒ¯å¹…å˜åŒ–
            amplitude_timeline = []
            frequency_timeline = []
            window_size = min(6, len(series) // 3)
            
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                
                # è®¡ç®—æŒ¯å¹…ï¼ˆæ ‡å‡†å·®ï¼‰
                amplitude = np.std(window)
                amplitude_timeline.append(amplitude)
                
                # ä¼°ç®—ä¸»é¢‘ç‡
                frequency = self._estimate_dominant_frequency(window)
                frequency_timeline.append(frequency)
            
            # 2. æ£€æµ‹æŒ¯è¡çªç„¶å‡ºç°çš„ç‚¹
            bifurcation_candidates = []
            threshold_factor = 2.0
            
            for i in range(1, len(amplitude_timeline)):
                if amplitude_timeline[i-1] > 0:
                    amplitude_ratio = amplitude_timeline[i] / amplitude_timeline[i-1]
                    
                    # Hopfåˆ†å²”ï¼šæŒ¯å¹…çªç„¶å¢å¤§
                    if amplitude_ratio > threshold_factor:
                        bifurcation_candidates.append(i)
                        hopf_analysis['bifurcation_detected'] = True
            
            if bifurcation_candidates:
                hopf_analysis['bifurcation_points'] = bifurcation_candidates
                hopf_analysis['oscillation_onset'] = [amplitude_timeline[i] for i in bifurcation_candidates]
                
                # åˆ†ææŒ¯å¹…å’Œé¢‘ç‡å˜åŒ–
                hopf_analysis['amplitude_changes'] = {
                    'timeline': amplitude_timeline,
                    'sudden_increases': bifurcation_candidates
                }
                hopf_analysis['frequency_changes'] = {
                    'timeline': frequency_timeline,
                    'bifurcation_frequencies': [frequency_timeline[i] for i in bifurcation_candidates if i < len(frequency_timeline)]
                }
            
            return hopf_analysis
            
        except Exception as e:
            return {'bifurcation_detected': False}
    
    def _estimate_dominant_frequency(self, window: np.ndarray) -> float:
        """ä¼°ç®—ä¸»å¯¼é¢‘ç‡"""
        try:
            if len(window) < 4:
                return 0.0
            
            # ä½¿ç”¨FFTä¼°ç®—ä¸»å¯¼é¢‘ç‡
            try:
                fft = np.fft.fft(window - np.mean(window))
                power_spectrum = np.abs(fft) ** 2
                
                # æ‰¾åˆ°æœ€å¤§åŠŸç‡å¯¹åº”çš„é¢‘ç‡
                max_power_index = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1
                
                # å½’ä¸€åŒ–é¢‘ç‡
                normalized_frequency = max_power_index / len(window)
                return normalized_frequency
                
            except:
                # å¦‚æœFFTå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„å‘¨æœŸæ£€æµ‹
                period = self._detect_period_in_window(window)
                if period > 0:
                    return 1.0 / period
                return 0.0
                
        except Exception as e:
            return 0.0
    
    def _detect_saddle_node_bifurcations(self, series: np.ndarray) -> Dict:
        """æ£€æµ‹éç»“åˆ†å²”"""
        try:
            saddle_node_analysis = {
                'bifurcation_detected': False,
                'bifurcation_points': [],
                'equilibrium_changes': [],
                'hysteresis_detected': False
            }
            
            if len(series) < 10:
                return saddle_node_analysis
            
            # éç»“åˆ†å²”çš„ç‰¹å¾ï¼šå¹³è¡¡ç‚¹çš„çªç„¶å‡ºç°æˆ–æ¶ˆå¤±
            
            # 1. åˆ†æå±€éƒ¨å¹³è¡¡çŠ¶æ€
            equilibrium_timeline = []
            window_size = min(5, len(series) // 3)
            
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                
                # åˆ¤æ–­æ˜¯å¦æ¥è¿‘å¹³è¡¡ï¼ˆå˜åŒ–å¾ˆå°ï¼‰
                variation = np.std(window)
                is_equilibrium = variation < np.std(series) * 0.2
                equilibrium_timeline.append(is_equilibrium)
            
            # 2. æ£€æµ‹å¹³è¡¡çŠ¶æ€çš„çªç„¶å˜åŒ–
            bifurcation_candidates = []
            
            for i in range(1, len(equilibrium_timeline)):
                # ä»å¹³è¡¡åˆ°éå¹³è¡¡æˆ–ç›¸å
                if equilibrium_timeline[i] != equilibrium_timeline[i-1]:
                    bifurcation_candidates.append(i)
                    saddle_node_analysis['bifurcation_detected'] = True
            
            if bifurcation_candidates:
                saddle_node_analysis['bifurcation_points'] = bifurcation_candidates
                saddle_node_analysis['equilibrium_changes'] = [
                    'equilibrium_loss' if not equilibrium_timeline[i] else 'equilibrium_gain'
                    for i in bifurcation_candidates
                ]
                
                # 3. æ£€æµ‹æ»å›ç°è±¡
                hysteresis = self._detect_hysteresis_in_bifurcation(series, bifurcation_candidates)
                saddle_node_analysis['hysteresis_detected'] = hysteresis
            
            return saddle_node_analysis
            
        except Exception as e:
            return {'bifurcation_detected': False}
    
    def _detect_hysteresis_in_bifurcation(self, series: np.ndarray, bifurcation_points: List[int]) -> bool:
        """æ£€æµ‹åˆ†å²”ä¸­çš„æ»å›ç°è±¡"""
        try:
            if len(bifurcation_points) < 2:
                return False
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸åŒè·¯å¾„çš„ä¸åŒå“åº”
            for i in range(len(bifurcation_points) - 1):
                start_point = bifurcation_points[i]
                end_point = bifurcation_points[i + 1]
                
                if end_point - start_point > 3:
                    segment = series[start_point:end_point + 1]
                    
                    # åˆ†æä¸Šå‡å’Œä¸‹é™é˜¶æ®µ
                    gradients = np.diff(segment)
                    
                    # å¯»æ‰¾æ–¹å‘å˜åŒ–
                    direction_changes = 0
                    for j in range(len(gradients) - 1):
                        if gradients[j] * gradients[j + 1] < 0:  # ç¬¦å·å˜åŒ–
                            direction_changes += 1
                    
                    # å¦‚æœæœ‰å¤šæ¬¡æ–¹å‘å˜åŒ–ï¼Œå¯èƒ½å­˜åœ¨æ»å›
                    if direction_changes >= 2:
                        return True
            
            return False
            
        except Exception as e:
            return False
    
    def _analyze_route_to_chaos(self, series: np.ndarray, period_doubling: Dict) -> Dict:
        """åˆ†æé€šå‘æ··æ²Œçš„è·¯å¾„"""
        try:
            chaos_route = {
                'route_type': 'unknown',
                'chaos_onset': 0,
                'route_characteristics': {},
                'universality_class': 'unknown'
            }
            
            if len(series) < 15:
                return chaos_route
            
            # 1. æ£€æŸ¥å‘¨æœŸå€åŒ–è·¯å¾„
            if period_doubling.get('bifurcation_detected', False):
                chaos_route['route_type'] = 'period_doubling'
                
                # åˆ†æå‘¨æœŸå€åŒ–çº§è”
                doubling_points = period_doubling.get('bifurcation_points', [])
                if doubling_points:
                    chaos_route['chaos_onset'] = doubling_points[-1]
                    chaos_route['route_characteristics'] = {
                        'doubling_cascade_length': len(doubling_points),
                        'feigenbaum_constant': period_doubling.get('feigenbaum_constant', 0.0),
                        'scaling_behavior': 'geometric'
                    }
                    
                    # åˆ¤æ–­æ™®é€‚æ€§ç±»åˆ«
                    feigenbaum = period_doubling.get('feigenbaum_constant', 0.0)
                    if abs(feigenbaum - 4.669) < 0.5:
                        chaos_route['universality_class'] = 'feigenbaum'
            
            # 2. æ£€æŸ¥å‡†å‘¨æœŸè·¯å¾„
            elif self._detect_quasiperiodic_route(series):
                chaos_route['route_type'] = 'quasiperiodic'
                chaos_route['route_characteristics'] = {
                    'torus_breakdown': True,
                    'frequency_locking': self._analyze_frequency_locking(series)
                }
            
            # 3. æ£€æŸ¥é—´æ­‡æ€§è·¯å¾„
            elif self._detect_intermittency_route(series):
                chaos_route['route_type'] = 'intermittency'
                chaos_route['route_characteristics'] = {
                    'laminar_phases': self._analyze_laminar_phases(series),
                    'burst_statistics': self._analyze_burst_statistics(series)
                }
            
            # 4. æ£€æŸ¥å±æœºè·¯å¾„
            elif self._detect_crisis_route(series):
                chaos_route['route_type'] = 'crisis'
                chaos_route['route_characteristics'] = {
                    'attractor_collision': True,
                    'sudden_expansion': True
                }
            
            return chaos_route
            
        except Exception as e:
            return {'route_type': 'unknown'}
    
    def _detect_quasiperiodic_route(self, series: np.ndarray) -> bool:
        """æ£€æµ‹å‡†å‘¨æœŸè·¯å¾„"""
        try:
            if len(series) < 12:
                return False
            
            # åˆ†æé¢‘è°±ç‰¹å¾
            try:
                fft = np.fft.fft(series - np.mean(series))
                power_spectrum = np.abs(fft) ** 2
                
                # å¯»æ‰¾å¤šä¸ªæ˜¾è‘—é¢‘ç‡å³°
                threshold = np.max(power_spectrum) * 0.1
                peaks = []
                
                for i in range(1, len(power_spectrum) // 2):
                    if (power_spectrum[i] > threshold and 
                        power_spectrum[i] > power_spectrum[i-1] and 
                        power_spectrum[i] > power_spectrum[i+1]):
                        peaks.append(i)
                
                # å‡†å‘¨æœŸè¿åŠ¨é€šå¸¸æœ‰2ä¸ªæˆ–æ›´å¤šä¸ç›¸å…³çš„é¢‘ç‡
                if len(peaks) >= 2:
                    # æ£€æŸ¥é¢‘ç‡æ˜¯å¦ä¸ç›¸å…³ï¼ˆéæ•´æ•°æ¯”ï¼‰
                    for i in range(len(peaks)):
                        for j in range(i + 1, len(peaks)):
                            ratio = peaks[j] / peaks[i] if peaks[i] > 0 else 0
                            # å¦‚æœæ¯”å€¼æ¥è¿‘æ•´æ•°ï¼Œè¯´æ˜æ˜¯è°æ³¢å…³ç³»
                            if abs(ratio - round(ratio)) > 0.1:
                                return True
                
            except:
                pass
            
            return False
            
        except Exception as e:
            return False
    
    def _analyze_frequency_locking(self, series: np.ndarray) -> Dict:
        """åˆ†æé¢‘ç‡é”å®š"""
        try:
            # ç®€åŒ–çš„é¢‘ç‡é”å®šåˆ†æ
            if len(series) < 10:
                return {}
            
            # åˆ†æé¢‘ç‡çš„æ—¶é—´æ¼”åŒ–
            window_size = min(6, len(series) // 2)
            frequencies = []
            
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                freq = self._estimate_dominant_frequency(window)
                frequencies.append(freq)
            
            # æ£€æŸ¥é¢‘ç‡æ˜¯å¦é”å®šåœ¨ç‰¹å®šå€¼
            if frequencies:
                freq_variance = np.var(frequencies)
                freq_mean = np.mean(frequencies)
                
                # é¢‘ç‡é”å®šï¼šä½æ–¹å·®
                is_locked = freq_variance < (freq_mean * 0.1) ** 2 if freq_mean > 0 else False
                
                return {
                    'frequency_locked': is_locked,
                    'locked_frequency': freq_mean if is_locked else 0,
                    'frequency_variance': freq_variance
                }
            
            return {}
            
        except Exception as e:
            return {}
    
    def _detect_intermittency_route(self, series: np.ndarray) -> bool:
        """æ£€æµ‹é—´æ­‡æ€§è·¯å¾„"""
        try:
            if len(series) < 10:
                return False
            
            # é—´æ­‡æ€§çš„ç‰¹å¾ï¼šè§„å¾‹ç›¸å’Œæ··æ²Œç›¸çš„äº¤æ›¿
            
            # 1. æ£€æµ‹è§„å¾‹ç›¸ï¼ˆlaminar phasesï¼‰
            laminar_phases = []
            chaos_phases = []
            
            window_size = 4
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºè§„å¾‹ç›¸ï¼ˆå˜åŒ–å°ï¼‰
                variation = np.std(window)
                threshold = np.std(series) * 0.3
                
                is_laminar = variation < threshold
                
                if is_laminar:
                    laminar_phases.append(start)
                else:
                    chaos_phases.append(start)
            
            # 2. æ£€æŸ¥æ˜¯å¦æœ‰äº¤æ›¿æ¨¡å¼
            if len(laminar_phases) > 0 and len(chaos_phases) > 0:
                # ç®€å•æ£€æŸ¥ï¼šæ˜¯å¦å­˜åœ¨laminar-chaos-laminaræ¨¡å¼
                total_phases = len(laminar_phases) + len(chaos_phases)
                alternation_score = min(len(laminar_phases), len(chaos_phases)) / total_phases
                
                # å¦‚æœäº¤æ›¿ç¨‹åº¦è¾ƒé«˜ï¼Œè®¤ä¸ºæ˜¯é—´æ­‡æ€§
                return alternation_score > 0.3
            
            return False
            
        except Exception as e:
            return False
    
    def _analyze_laminar_phases(self, series: np.ndarray) -> Dict:
        """åˆ†æå±‚æµç›¸"""
        try:
            # è¯†åˆ«å’Œåˆ†æè§„å¾‹è¿åŠ¨çš„é˜¶æ®µ
            laminar_analysis = {
                'phase_count': 0,
                'average_duration': 0.0,
                'duration_distribution': []
            }
            
            if len(series) < 8:
                return laminar_analysis
            
            # æ£€æµ‹å±‚æµç›¸
            threshold = np.std(series) * 0.3
            current_phase_length = 0
            phase_durations = []
            
            for i in range(len(series) - 3):
                window = series[i:i+4]
                variation = np.std(window)
                
                if variation < threshold:
                    current_phase_length += 1
                else:
                    if current_phase_length >= 2:  # è‡³å°‘2ä¸ªç‚¹æ‰ç®—ä¸€ä¸ªç›¸
                        phase_durations.append(current_phase_length)
                    current_phase_length = 0
            
            # å¤„ç†æœ€åä¸€ä¸ªç›¸
            if current_phase_length >= 2:
                phase_durations.append(current_phase_length)
            
            if phase_durations:
                laminar_analysis['phase_count'] = len(phase_durations)
                laminar_analysis['average_duration'] = np.mean(phase_durations)
                laminar_analysis['duration_distribution'] = phase_durations
            
            return laminar_analysis
            
        except Exception as e:
            return {}
    
    def _analyze_burst_statistics(self, series: np.ndarray) -> Dict:
        """åˆ†æçªå‘ç»Ÿè®¡"""
        try:
            burst_analysis = {
                'burst_count': 0,
                'average_intensity': 0.0,
                'burst_intervals': []
            }
            
            if len(series) < 8:
                return burst_analysis
            
            # æ£€æµ‹çªå‘äº‹ä»¶ï¼ˆå¤§å¹…å˜åŒ–ï¼‰
            threshold = np.std(series) * 1.5
            burst_points = []
            
            for i in range(len(series) - 1):
                change = abs(series[i + 1] - series[i])
                if change > threshold:
                    burst_points.append(i)
            
            if burst_points:
                burst_analysis['burst_count'] = len(burst_points)
                
                # è®¡ç®—çªå‘å¼ºåº¦
                intensities = [abs(series[i + 1] - series[i]) for i in burst_points if i + 1 < len(series)]
                if intensities:
                    burst_analysis['average_intensity'] = np.mean(intensities)
                
                # è®¡ç®—çªå‘é—´éš”
                if len(burst_points) > 1:
                    intervals = [burst_points[i + 1] - burst_points[i] for i in range(len(burst_points) - 1)]
                    burst_analysis['burst_intervals'] = intervals
            
            return burst_analysis
            
        except Exception as e:
            return {}
    
    def _detect_crisis_route(self, series: np.ndarray) -> bool:
        """æ£€æµ‹å±æœºè·¯å¾„"""
        try:
            if len(series) < 10:
                return False
            
            # å±æœºçš„ç‰¹å¾ï¼šå¸å¼•å­çš„çªç„¶æ‰©å¼ æˆ–æ”¶ç¼©
            
            # åˆ†ææ—¶é—´åºåˆ—èŒƒå›´çš„çªç„¶å˜åŒ–
            window_size = min(5, len(series) // 3)
            range_timeline = []
            
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                window_range = np.max(window) - np.min(window)
                range_timeline.append(window_range)
            
            # æ£€æµ‹èŒƒå›´çš„çªç„¶å˜åŒ–
            for i in range(1, len(range_timeline)):
                if range_timeline[i - 1] > 0:
                    range_ratio = range_timeline[i] / range_timeline[i - 1]
                    
                    # å±æœºï¼šèŒƒå›´çªç„¶å¤§å¹…æ‰©å¼ 
                    if range_ratio > 3.0:
                        return True
            
            return False
            
        except Exception as e:
            return False
    
    def _determine_critical_parameter_values(self, series: np.ndarray) -> Dict:
        """ç¡®å®šä¸´ç•Œå‚æ•°å€¼"""
        try:
            critical_values = {
                'onset_of_chaos': 0.0,
                'period_doubling_threshold': 0.0,
                'intermittency_threshold': 0.0,
                'parameter_estimation_method': 'time_series_analysis'
            }
            
            if len(series) < 10:
                return critical_values
            
            # ä½¿ç”¨æ—¶é—´åºåˆ—çš„ç»Ÿè®¡ç‰¹æ€§ä¼°ç®—ä¸´ç•Œå‚æ•°
            
            # 1. æ··æ²Œå¼€å§‹çš„é˜ˆå€¼
            complexity_timeline = []
            window_size = min(6, len(series) // 2)
            
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                
                # ä½¿ç”¨ç†µåº¦é‡å¤æ‚æ€§
                hist, _ = np.histogram(window, bins=5, density=True)
                entropy = -np.sum([p * np.log2(p + 1e-10) for p in hist if p > 0])
                complexity_timeline.append(entropy)
            
            # æ‰¾åˆ°å¤æ‚æ€§æ€¥å‰§å¢åŠ çš„ç‚¹
            if len(complexity_timeline) > 3:
                for i in range(1, len(complexity_timeline)):
                    if complexity_timeline[i - 1] > 0:
                        complexity_ratio = complexity_timeline[i] / complexity_timeline[i - 1]
                        if complexity_ratio > 1.5:  # å¤æ‚æ€§æ˜¾è‘—å¢åŠ 
                            critical_values['onset_of_chaos'] = i / len(complexity_timeline)
                            break
            
            # 2. å‘¨æœŸå€åŒ–é˜ˆå€¼
            period_changes = []
            for start in range(len(series) - window_size + 1):
                window = series[start:start + window_size]
                period = self._detect_period_in_window(window)
                period_changes.append(period)
            
            # æ‰¾åˆ°å‘¨æœŸå¼€å§‹å˜åŒ–çš„ç‚¹
            for i in range(1, len(period_changes)):
                if period_changes[i] != period_changes[i - 1] and period_changes[i] > 0:
                    critical_values['period_doubling_threshold'] = i / len(period_changes)
                    break
            
            return critical_values
            
        except Exception as e:
            return {}
    
    def _calculate_comprehensive_chaos_indicators(self, time_series: List[float], phase_space: Dict) -> Dict:
        """
        è®¡ç®—ç»¼åˆæ··æ²ŒæŒ‡æ ‡ - åŸºäºå¤šç§æ··æ²Œæ£€æµ‹æ–¹æ³•
        å®ç°0-1æµ‹è¯•ã€è¿‘ä¼¼ç†µã€æ ·æœ¬ç†µç­‰ç°ä»£æ··æ²Œæ£€æµ‹æŠ€æœ¯
        """
        try:
            chaos_indicators = {
                'zero_one_test': 0.0,
                'approximate_entropy': 0.0,
                'sample_entropy': 0.0,
                'permutation_entropy': 0.0,
                'multiscale_entropy': {},
                'recurrence_quantification': {},
                'detrended_fluctuation_analysis': {},
                'chaos_classification': 'unknown',
                'chaos_confidence': 0.0
            }
            
            if len(time_series) < 10:
                return chaos_indicators
            
            series = np.array(time_series)
            
            # 1. 0-1æµ‹è¯•
            zero_one_result = self._calculate_zero_one_test(series)
            chaos_indicators['zero_one_test'] = zero_one_result
            
            # 2. è¿‘ä¼¼ç†µ
            approximate_entropy = self._calculate_approximate_entropy(series)
            chaos_indicators['approximate_entropy'] = approximate_entropy
            
            # 3. æ ·æœ¬ç†µ
            sample_entropy = self._calculate_sample_entropy(series)
            chaos_indicators['sample_entropy'] = sample_entropy
            
            # 4. æ’åˆ—ç†µ
            permutation_entropy = self._calculate_permutation_entropy(series)
            chaos_indicators['permutation_entropy'] = permutation_entropy
            
            # 5. å¤šå°ºåº¦ç†µ
            multiscale_entropy = self._calculate_multiscale_entropy(series)
            chaos_indicators['multiscale_entropy'] = multiscale_entropy
            
            # 6. é€’å½’å®šé‡åˆ†æ
            if len(series) >= 15:
                rqa_results = self._perform_recurrence_quantification_analysis(series)
                chaos_indicators['recurrence_quantification'] = rqa_results
            
            # 7. å»è¶‹åŠ¿æ³¢åŠ¨åˆ†æ
            if len(series) >= 12:
                dfa_results = self._perform_detrended_fluctuation_analysis(series)
                chaos_indicators['detrended_fluctuation_analysis'] = dfa_results
            
            # 8. ç»¼åˆæ··æ²Œåˆ†ç±»
            classification, confidence = self._classify_chaos_comprehensive(chaos_indicators)
            chaos_indicators['chaos_classification'] = classification
            chaos_indicators['chaos_confidence'] = confidence
            
            return chaos_indicators
            
        except Exception as e:
            print(f"      âŒ ç»¼åˆæ··æ²ŒæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_zero_one_test(self, series: np.ndarray) -> float:
        """è®¡ç®—0-1æµ‹è¯•æ··æ²Œæ£€æµ‹"""
        try:
            if len(series) < 10:
                return 0.5
            
            # 0-1æµ‹è¯•çš„ç®€åŒ–å®ç°
            n = len(series)
            
            # æ„é€ æµ‹è¯•ç»Ÿè®¡é‡
            c = np.random.random()  # éšæœºå¸¸æ•°
            
            # è®¡ç®—å¹³ç§»åæ ‡
            p_values = []
            q_values = []
            
            for j in range(n):
                p_j = 0.0
                q_j = 0.0
                
                for i in range(j + 1):
                    p_j += series[i] * np.cos(i * c)
                    q_j += series[i] * np.sin(i * c)
                
                p_values.append(p_j)
                q_values.append(q_j)
            
            # è®¡ç®—å‡æ–¹ä½ç§»
            if len(p_values) > 1 and len(q_values) > 1:
                p_array = np.array(p_values)
                q_array = np.array(q_values)
                
                # è®¡ç®—æ¸è¿‘å¢é•¿ç‡
                mean_square_displacement = np.mean(p_array ** 2 + q_array ** 2)
                
                # æ ‡å‡†åŒ–ç»“æœ
                if mean_square_displacement > 0:
                    K = np.log(mean_square_displacement) / np.log(n)
                    
                    # 0-1æµ‹è¯•ï¼šKæ¥è¿‘1è¡¨ç¤ºæ··æ²Œï¼Œæ¥è¿‘0è¡¨ç¤ºè§„å¾‹
                    return min(1.0, max(0.0, K))
            
            return 0.5
            
        except Exception as e:
            return 0.5
    
    def _calculate_approximate_entropy(self, series: np.ndarray, m: int = 2, r: float = None) -> float:
        """è®¡ç®—è¿‘ä¼¼ç†µ"""
        try:
            if len(series) < m + 1:
                return 0.0
            
            N = len(series)
            
            if r is None:
                r = 0.2 * np.std(series)
            
            def _maxdist(xi, xj):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])
            
            def _phi(m):
                patterns = np.array([series[i:i + m] for i in range(N - m + 1)])
                C = np.zeros(N - m + 1)
                
                for i in range(N - m + 1):
                    template_i = patterns[i]
                    for j in range(N - m + 1):
                        if _maxdist(template_i, patterns[j]) <= r:
                            C[i] += 1.0
                
                phi = np.mean(np.log(C / (N - m + 1.0)))
                return phi
            
            approximate_entropy = _phi(m) - _phi(m + 1)
            return max(0.0, approximate_entropy)
            
        except Exception as e:
            return 0.0
    
    def _calculate_permutation_entropy(self, series: np.ndarray, order: int = 3, delay: int = 1) -> float:
        """è®¡ç®—æ’åˆ—ç†µ"""
        try:
            if len(series) < order:
                return 0.0
            
            # ç”Ÿæˆæ’åˆ—æ¨¡å¼
            permutations = []
            
            for i in range(len(series) - delay * (order - 1)):
                # æå–åµŒå…¥å‘é‡
                vector = [series[i + j * delay] for j in range(order)]
                
                # è®¡ç®—æ’åˆ—
                sorted_indices = sorted(range(len(vector)), key=lambda k: vector[k])
                permutation = tuple(sorted_indices)
                permutations.append(permutation)
            
            # è®¡ç®—æ’åˆ—æ¦‚ç‡
            from collections import Counter
            perm_counts = Counter(permutations)
            total_perms = len(permutations)
            
            # è®¡ç®—ç†µ
            entropy = 0.0
            for count in perm_counts.values():
                probability = count / total_perms
                if probability > 0:
                    entropy -= probability * np.log2(probability)
            
            # å½’ä¸€åŒ–
            max_entropy = np.log2(np.math.factorial(order))
            if max_entropy > 0:
                return entropy / max_entropy
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_multiscale_entropy(self, series: np.ndarray, max_scale: int = 5) -> Dict:
        """è®¡ç®—å¤šå°ºåº¦ç†µ"""
        try:
            multiscale_results = {
                'scale_entropies': {},
                'complexity_index': 0.0,
                'scale_invariance': 0.0
            }
            
            if len(series) < 10:
                return multiscale_results
            
            entropies = []
            
            for scale in range(1, min(max_scale + 1, len(series) // 5)):
                # ç²—ç²’åŒ–
                coarse_grained = []
                for i in range(len(series) // scale):
                    segment_mean = np.mean(series[i * scale:(i + 1) * scale])
                    coarse_grained.append(segment_mean)
                
                if len(coarse_grained) >= 5:
                    # è®¡ç®—æ ·æœ¬ç†µ
                    entropy = self._calculate_sample_entropy(np.array(coarse_grained))
                    entropies.append(entropy)
                    multiscale_results['scale_entropies'][scale] = entropy
            
            if entropies:
                # å¤æ‚æ€§æŒ‡æ•°ï¼šå¤šå°ºåº¦ç†µçš„æ€»å’Œ
                multiscale_results['complexity_index'] = np.sum(entropies)
                
                # å°ºåº¦ä¸å˜æ€§ï¼šä¸åŒå°ºåº¦ç†µçš„å˜åŒ–ç¨‹åº¦
                if len(entropies) > 1:
                    entropy_variance = np.var(entropies)
                    entropy_mean = np.mean(entropies)
                    if entropy_mean > 0:
                        multiscale_results['scale_invariance'] = 1.0 - (entropy_variance / entropy_mean)
            
            return multiscale_results
            
        except Exception as e:
            return {}
    
    def _perform_recurrence_quantification_analysis(self, series: np.ndarray) -> Dict:
        """æ‰§è¡Œé€’å½’å®šé‡åˆ†æ"""
        try:
            rqa_results = {
                'recurrence_rate': 0.0,
                'determinism': 0.0,
                'average_diagonal_length': 0.0,
                'longest_diagonal_line': 0,
                'entropy': 0.0,
                'laminarity': 0.0
            }
            
            if len(series) < 10:
                return rqa_results
            
            # æ„å»ºé€’å½’çŸ©é˜µ
            threshold = 0.1 * np.std(series)
            n = len(series)
            recurrence_matrix = np.zeros((n, n))
            
            for i in range(n):
                for j in range(n):
                    if abs(series[i] - series[j]) <= threshold:
                        recurrence_matrix[i, j] = 1
            
            # 1. é€’å½’ç‡
            recurrence_rate = np.sum(recurrence_matrix) / (n * n)
            rqa_results['recurrence_rate'] = recurrence_rate
            
            # 2. ç¡®å®šæ€§åˆ†æï¼ˆå¯¹è§’çº¿åˆ†æï¼‰
            diagonal_lengths = []
            
            # åˆ†æä¸»å¯¹è§’çº¿é™„è¿‘çš„çº¿
            for offset in range(-n + 1, n):
                diagonal = np.diag(recurrence_matrix, k=offset)
                
                # æ‰¾åˆ°è¿ç»­çš„1
                current_length = 0
                for value in diagonal:
                    if value == 1:
                        current_length += 1
                    else:
                        if current_length >= 2:  # æœ€å°çº¿é•¿åº¦
                            diagonal_lengths.append(current_length)
                        current_length = 0
                
                # å¤„ç†æœ€åä¸€æ®µ
                if current_length >= 2:
                    diagonal_lengths.append(current_length)
            
            if diagonal_lengths:
                total_diagonal_points = sum(diagonal_lengths)
                total_recurrence_points = np.sum(recurrence_matrix)
                
                if total_recurrence_points > 0:
                    rqa_results['determinism'] = total_diagonal_points / total_recurrence_points
                
                rqa_results['average_diagonal_length'] = np.mean(diagonal_lengths)
                rqa_results['longest_diagonal_line'] = max(diagonal_lengths)
                
                # 3. ç†µ
                length_counts = {}
                for length in diagonal_lengths:
                    length_counts[length] = length_counts.get(length, 0) + 1
                
                total_lines = len(diagonal_lengths)
                entropy = 0.0
                for count in length_counts.values():
                    probability = count / total_lines
                    if probability > 0:
                        entropy -= probability * np.log2(probability)
                
                rqa_results['entropy'] = entropy
            
            # 4. å±‚æµæ€§åˆ†æï¼ˆå‚ç›´çº¿åˆ†æï¼‰
            vertical_lengths = []
            
            for j in range(n):
                column = recurrence_matrix[:, j]
                current_length = 0
                
                for value in column:
                    if value == 1:
                        current_length += 1
                    else:
                        if current_length >= 2:
                            vertical_lengths.append(current_length)
                        current_length = 0
                
                if current_length >= 2:
                    vertical_lengths.append(current_length)
            
            if vertical_lengths:
                total_vertical_points = sum(vertical_lengths)
                total_recurrence_points = np.sum(recurrence_matrix)
                
                if total_recurrence_points > 0:
                    rqa_results['laminarity'] = total_vertical_points / total_recurrence_points
            
            return rqa_results
            
        except Exception as e:
            return {}
    
    def _perform_detrended_fluctuation_analysis(self, series: np.ndarray) -> Dict:
        """æ‰§è¡Œå»è¶‹åŠ¿æ³¢åŠ¨åˆ†æ"""
        try:
            dfa_results = {
                'scaling_exponent': 0.0,
                'hurst_exponent': 0.0,
                'long_range_correlation': False,
                'fractal_dimension': 0.0
            }
            
            if len(series) < 10:
                return dfa_results
            
            # 1. è®¡ç®—ç´¯ç§¯åå·®
            mean_series = np.mean(series)
            cumulative_deviation = np.cumsum(series - mean_series)
            
            # 2. åˆ†æä¸åŒçª—å£å¤§å°çš„æ³¢åŠ¨
            window_sizes = [4, 6, 8, 10]
            fluctuations = []
            valid_sizes = []
            
            for window_size in window_sizes:
                if len(cumulative_deviation) >= window_size * 2:
                    n_windows = len(cumulative_deviation) // window_size
                    
                    window_fluctuations = []
                    for i in range(n_windows):
                        start = i * window_size
                        end = start + window_size
                        window_data = cumulative_deviation[start:end]
                        
                        # çº¿æ€§å»è¶‹åŠ¿
                        x = np.arange(len(window_data))
                        try:
                            slope, intercept = np.polyfit(x, window_data, 1)
                            trend = slope * x + intercept
                            detrended = window_data - trend
                            
                            # è®¡ç®—æ³¢åŠ¨
                            fluctuation = np.sqrt(np.mean(detrended ** 2))
                            window_fluctuations.append(fluctuation)
                        except:
                            continue
                    
                    if window_fluctuations:
                        avg_fluctuation = np.mean(window_fluctuations)
                        fluctuations.append(avg_fluctuation)
                        valid_sizes.append(window_size)
            
            # 3. è®¡ç®—æ ‡åº¦æŒ‡æ•°
            if len(fluctuations) >= 3 and len(valid_sizes) >= 3:
                try:
                    log_sizes = np.log(valid_sizes)
                    log_fluctuations = np.log([f + 1e-10 for f in fluctuations])
                    
                    slope = np.polyfit(log_sizes, log_fluctuations, 1)[0]
                    dfa_results['scaling_exponent'] = slope
                    
                    # HurstæŒ‡æ•°è¿‘ä¼¼
                    dfa_results['hurst_exponent'] = slope
                    
                    # é•¿ç¨‹ç›¸å…³æ€§åˆ¤æ–­
                    if slope > 0.5:
                        dfa_results['long_range_correlation'] = True
                    
                    # åˆ†å½¢ç»´æ•°
                    dfa_results['fractal_dimension'] = 2.0 - slope
                    
                except:
                    pass
            
            return dfa_results
            
        except Exception as e:
            return {}
    
    def _classify_chaos_comprehensive(self, chaos_indicators: Dict) -> tuple:
        """ç»¼åˆåˆ†ç±»æ··æ²Œæ€§è´¨"""
        try:
            # æ”¶é›†å„ç§æŒ‡æ ‡
            indicators = []
            
            # 0-1æµ‹è¯•
            zero_one = chaos_indicators.get('zero_one_test', 0.5)
            if zero_one > 0.7:
                indicators.append('chaotic')
            elif zero_one < 0.3:
                indicators.append('regular')
            else:
                indicators.append('transitional')
            
            # ç†µæŒ‡æ ‡
            sample_entropy = chaos_indicators.get('sample_entropy', 0.0)
            if sample_entropy > 0.5:
                indicators.append('chaotic')
            elif sample_entropy < 0.2:
                indicators.append('regular')
            
            # æ’åˆ—ç†µ
            perm_entropy = chaos_indicators.get('permutation_entropy', 0.0)
            if perm_entropy > 0.8:
                indicators.append('chaotic')
            elif perm_entropy < 0.4:
                indicators.append('regular')
            
            # DFAæ ‡åº¦æŒ‡æ•°
            dfa_results = chaos_indicators.get('detrended_fluctuation_analysis', {})
            scaling_exponent = dfa_results.get('scaling_exponent', 0.5)
            if scaling_exponent > 0.7:
                indicators.append('chaotic')
            elif scaling_exponent < 0.3:
                indicators.append('regular')
            
            # æŠ•ç¥¨åˆ†ç±»
            from collections import Counter
            votes = Counter(indicators)
            
            if votes['chaotic'] > votes.get('regular', 0):
                classification = 'chaotic'
                confidence = votes['chaotic'] / len(indicators)
            elif votes.get('regular', 0) > votes['chaotic']:
                classification = 'regular'
                confidence = votes['regular'] / len(indicators)
            else:
                classification = 'transitional'
                confidence = votes.get('transitional', 0) / len(indicators)
            
            return classification, min(1.0, confidence)
            
        except Exception as e:
            return 'unknown', 0.0
    
    def _analyze_nonlinear_prediction_capability(self, time_series: List[float]) -> Dict:
        """
        åˆ†æéçº¿æ€§é¢„æµ‹èƒ½åŠ› - åŸºäºæ··æ²Œç†è®ºçš„å¯é¢„æµ‹æ€§åˆ†æ
        """
        try:
            prediction_analysis = {
                'prediction_horizon': 0,
                'forecast_accuracy': 0.0,
                'lyapunov_time': 0.0,
                'predictability_decay': {},
                'nonlinear_forecasting_performance': {}
            }
            
            if len(time_series) < 15:
                return prediction_analysis
            
            series = np.array(time_series)
            
            # 1. è®¡ç®—é¢„æµ‹è§†é‡
            prediction_horizon = self._calculate_prediction_horizon(series)
            prediction_analysis['prediction_horizon'] = prediction_horizon
            
            # 2. çŸ­æœŸé¢„æµ‹å‡†ç¡®æ€§
            forecast_accuracy = self._evaluate_short_term_forecast(series)
            prediction_analysis['forecast_accuracy'] = forecast_accuracy
            
            # 3. Lyapunovæ—¶é—´
            lyapunov_time = self._estimate_lyapunov_time(series)
            prediction_analysis['lyapunov_time'] = lyapunov_time
            
            # 4. å¯é¢„æµ‹æ€§è¡°å‡åˆ†æ
            decay_analysis = self._analyze_predictability_decay(series)
            prediction_analysis['predictability_decay'] = decay_analysis
            
            # 5. éçº¿æ€§é¢„æµ‹æ€§èƒ½
            forecasting_performance = self._evaluate_nonlinear_forecasting(series)
            prediction_analysis['nonlinear_forecasting_performance'] = forecasting_performance
            
            return prediction_analysis
            
        except Exception as e:
            print(f"      âŒ éçº¿æ€§é¢„æµ‹åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_prediction_horizon(self, series: np.ndarray) -> int:
        """è®¡ç®—é¢„æµ‹è§†é‡"""
        try:
            if len(series) < 10:
                return 0
            
            # ä½¿ç”¨ç›¸ç©ºé—´æœ€è¿‘é‚»æ–¹æ³•ä¼°ç®—é¢„æµ‹è§†é‡
            embedding_dim = 3
            embedded = self._embed_time_series(series, embedding_dim, 1)
            
            if embedded.shape[0] < 8:
                return 0
            
            prediction_errors = []
            
            # æµ‹è¯•ä¸åŒé¢„æµ‹æ­¥é•¿çš„è¯¯å·®
            for step in range(1, min(6, len(series) // 3)):
                errors = []
                
                for i in range(embedded.shape[0] - step):
                    # å¯»æ‰¾æœ€è¿‘é‚»
                    distances = np.linalg.norm(embedded - embedded[i], axis=1)
                    distances[i] = np.inf
                    
                    if np.min(distances) < np.inf:
                        nearest_idx = np.argmin(distances)
                        
                        # é¢„æµ‹
                        if (i + step < len(series) and 
                            nearest_idx + step < len(series)):
                            
                            actual = series[i + step]
                            predicted = series[nearest_idx + step]
                            error = abs(actual - predicted)
                            errors.append(error)
                
                if errors:
                    avg_error = np.mean(errors)
                    prediction_errors.append(avg_error)
                else:
                    prediction_errors.append(float('inf'))
            
            # æ‰¾åˆ°è¯¯å·®æ€¥å‰§å¢åŠ çš„ç‚¹
            threshold = np.std(series) * 0.5
            
            for i, error in enumerate(prediction_errors):
                if error > threshold:
                    return i + 1
            
            return len(prediction_errors)
            
        except Exception as e:
            return 0
    
    def _evaluate_short_term_forecast(self, series: np.ndarray) -> float:
        """è¯„ä¼°çŸ­æœŸé¢„æµ‹å‡†ç¡®æ€§"""
        try:
            if len(series) < 8:
                return 0.0
            
            # ä½¿ç”¨ç®€å•çš„çº¿æ€§å’Œéçº¿æ€§æ–¹æ³•è¿›è¡Œ1æ­¥é¢„æµ‹
            predictions = []
            actuals = []
            
            for i in range(3, len(series) - 1):
                # çº¿æ€§é¢„æµ‹ï¼ˆåŸºäºè¶‹åŠ¿ï¼‰
                if i >= 2:
                    linear_trend = series[i] - series[i - 1]
                    linear_pred = series[i] + linear_trend
                else:
                    linear_pred = series[i]
                
                # éçº¿æ€§é¢„æµ‹ï¼ˆåŸºäºå±€éƒ¨å¹³å‡ï¼‰
                recent_window = series[max(0, i - 3):i + 1]
                nonlinear_pred = np.mean(recent_window)
                
                # ç»„åˆé¢„æµ‹
                combined_pred = 0.6 * linear_pred + 0.4 * nonlinear_pred
                
                predictions.append(combined_pred)
                actuals.append(series[i + 1])
            
            if len(predictions) > 0 and len(actuals) > 0:
                # è®¡ç®—é¢„æµ‹å‡†ç¡®æ€§
                errors = [abs(p - a) for p, a in zip(predictions, actuals)]
                mae = np.mean(errors)
                
                # ç›¸å¯¹äºæ•°æ®å˜åŒ–çš„å‡†ç¡®æ€§
                data_std = np.std(series)
                if data_std > 0:
                    relative_accuracy = 1.0 - (mae / data_std)
                    return max(0.0, relative_accuracy)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _estimate_lyapunov_time(self, series: np.ndarray) -> float:
        """ä¼°ç®—Lyapunovæ—¶é—´"""
        try:
            # Lyapunovæ—¶é—´ = 1 / |æœ€å¤§LyapunovæŒ‡æ•°|
            lyapunov_exp = self._calculate_lyapunov_rosenstein(series)
            
            if lyapunov_exp and lyapunov_exp > 0:
                lyapunov_time = 1.0 / lyapunov_exp
                return min(lyapunov_time, len(series))  # é™åˆ¶ä¸Šç•Œ
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _analyze_predictability_decay(self, series: np.ndarray) -> Dict:
        """åˆ†æå¯é¢„æµ‹æ€§è¡°å‡"""
        try:
            decay_analysis = {
                'decay_rate': 0.0,
                'half_life': 0.0,
                'exponential_decay': False,
                'decay_function_type': 'unknown'
            }
            
            if len(series) < 12:
                return decay_analysis
            
            # è®¡ç®—ä¸åŒæ—¶é—´æ­¥é•¿çš„é¢„æµ‹è¯¯å·®
            time_steps = list(range(1, min(8, len(series) // 2)))
            prediction_errors = []
            
            for step in time_steps:
                errors = []
                
                for i in range(len(series) - step):
                    if i >= 2:
                        # ç®€å•é¢„æµ‹ï¼šçº¿æ€§å¤–æ¨
                        trend = series[i] - series[i - 1]
                        predicted = series[i] + trend * step
                        actual = series[i + step]
                        error = abs(predicted - actual)
                        errors.append(error)
                
                if errors:
                    avg_error = np.mean(errors)
                    prediction_errors.append(avg_error)
                else:
                    prediction_errors.append(0.0)
            
            if len(prediction_errors) >= 3:
                # åˆ†æè¡°å‡æ¨¡å¼
                
                # æ£€æŸ¥æŒ‡æ•°è¡°å‡
                try:
                    log_errors = [np.log(e + 1e-10) for e in prediction_errors]
                    slope = np.polyfit(time_steps[:len(log_errors)], log_errors, 1)[0]
                    
                    if slope > 0:  # è¯¯å·®éšæ—¶é—´å¢é•¿
                        decay_analysis['decay_rate'] = slope
                        decay_analysis['exponential_decay'] = True
                        
                        # åŠè¡°æœŸï¼ˆè¯¯å·®ç¿»å€çš„æ—¶é—´ï¼‰
                        decay_analysis['half_life'] = np.log(2) / slope
                        decay_analysis['decay_function_type'] = 'exponential'
                    
                except:
                    pass
                
                # æ£€æŸ¥å¹‚å¾‹è¡°å‡
                try:
                    log_times = [np.log(t) for t in time_steps[:len(prediction_errors)]]
                    log_errors = [np.log(e + 1e-10) for e in prediction_errors]
                    
                    correlation = abs(np.corrcoef(log_times, log_errors)[0, 1])
                    if not np.isnan(correlation) and correlation > 0.8:
                        decay_analysis['decay_function_type'] = 'power_law'
                
                except:
                    pass
            
            return decay_analysis
            
        except Exception as e:
            return {}
    
    def _evaluate_nonlinear_forecasting(self, series: np.ndarray) -> Dict:
        """è¯„ä¼°éçº¿æ€§é¢„æµ‹æ€§èƒ½"""
        try:
            forecasting_performance = {
                'local_linear_prediction': 0.0,
                'global_nonlinear_prediction': 0.0,
                'neural_network_prediction': 0.0,
                'ensemble_prediction': 0.0,
                'best_method': 'unknown'
            }
            
            if len(series) < 10:
                return forecasting_performance
            
            # 1. å±€éƒ¨çº¿æ€§é¢„æµ‹
            local_accuracy = self._evaluate_local_linear_prediction(series)
            forecasting_performance['local_linear_prediction'] = local_accuracy
            
            # 2. å…¨å±€éçº¿æ€§é¢„æµ‹
            global_accuracy = self._evaluate_global_nonlinear_prediction(series)
            forecasting_performance['global_nonlinear_prediction'] = global_accuracy
            
            # 3. ç¥ç»ç½‘ç»œé¢„æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            nn_accuracy = self._evaluate_simple_neural_prediction(series)
            forecasting_performance['neural_network_prediction'] = nn_accuracy
            
            # 4. é›†æˆé¢„æµ‹
            ensemble_accuracy = (local_accuracy + global_accuracy + nn_accuracy) / 3
            forecasting_performance['ensemble_prediction'] = ensemble_accuracy
            
            # 5. æœ€ä½³æ–¹æ³•
            methods = {
                'local_linear': local_accuracy,
                'global_nonlinear': global_accuracy,
                'neural_network': nn_accuracy,
                'ensemble': ensemble_accuracy
            }
            
            best_method = max(methods.items(), key=lambda x: x[1])[0]
            forecasting_performance['best_method'] = best_method
            
            return forecasting_performance
            
        except Exception as e:
            return {}
    
    def _evaluate_local_linear_prediction(self, series: np.ndarray) -> float:
        """è¯„ä¼°å±€éƒ¨çº¿æ€§é¢„æµ‹"""
        try:
            if len(series) < 8:
                return 0.0
            
            errors = []
            
            for i in range(3, len(series) - 1):
                # ä½¿ç”¨æœ€è¿‘3ä¸ªç‚¹è¿›è¡Œçº¿æ€§æ‹Ÿåˆ
                recent_points = series[i - 2:i + 1]
                x = np.array([0, 1, 2])
                
                try:
                    slope, intercept = np.polyfit(x, recent_points, 1)
                    predicted = slope * 3 + intercept  # é¢„æµ‹ä¸‹ä¸€ä¸ªç‚¹
                    actual = series[i + 1]
                    error = abs(predicted - actual)
                    errors.append(error)
                except:
                    continue
            
            if errors:
                mae = np.mean(errors)
                data_std = np.std(series)
                if data_std > 0:
                    accuracy = 1.0 - (mae / data_std)
                    return max(0.0, accuracy)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _evaluate_global_nonlinear_prediction(self, series: np.ndarray) -> float:
        """è¯„ä¼°å…¨å±€éçº¿æ€§é¢„æµ‹"""
        try:
            if len(series) < 8:
                return 0.0
            
            errors = []
            
            for i in range(4, len(series) - 1):
                # ä½¿ç”¨å…¨å±€éçº¿æ€§æ¨¡å¼
                # ç®€åŒ–ç‰ˆï¼šåŸºäºæ•´ä¸ªå†å²çš„åŠ æƒå¹³å‡
                
                weights = []
                values = []
                
                for j in range(i):
                    # è®¡ç®—ä¸å½“å‰çŠ¶æ€çš„ç›¸ä¼¼æ€§
                    current_context = series[max(0, i - 2):i + 1]
                    historical_context = series[max(0, j - 2):j + 1]
                    
                    if len(current_context) == len(historical_context):
                        similarity = 1.0 / (1.0 + np.linalg.norm(
                            np.array(current_context) - np.array(historical_context)
                        ))
                        
                        if j + 1 < len(series):
                            weights.append(similarity)
                            values.append(series[j + 1])
                
                if weights and values:
                    # åŠ æƒé¢„æµ‹
                    total_weight = sum(weights)
                    if total_weight > 0:
                        predicted = sum(w * v for w, v in zip(weights, values)) / total_weight
                        actual = series[i + 1]
                        error = abs(predicted - actual)
                        errors.append(error)
            
            if errors:
                mae = np.mean(errors)
                data_std = np.std(series)
                if data_std > 0:
                    accuracy = 1.0 - (mae / data_std)
                    return max(0.0, accuracy)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _evaluate_simple_neural_prediction(self, series: np.ndarray) -> float:
        """è¯„ä¼°ç®€å•ç¥ç»ç½‘ç»œé¢„æµ‹"""
        try:
            if len(series) < 8:
                return 0.0
            
            # æç®€åŒ–çš„"ç¥ç»ç½‘ç»œ"ï¼šéçº¿æ€§åŠ æƒç»„åˆ
            errors = []
            
            for i in range(3, len(series) - 1):
                # è¾“å…¥ï¼šæœ€è¿‘3ä¸ªå€¼
                inputs = series[i - 2:i + 1]
                
                # ç®€åŒ–çš„éçº¿æ€§å˜æ¢
                transformed = []
                for x in inputs:
                    # ç®€å•çš„æ¿€æ´»å‡½æ•°
                    activated = np.tanh(x)
                    transformed.append(activated)
                
                # åŠ æƒç»„åˆï¼ˆæƒé‡æ ¹æ®ä½ç½®è¡°å‡ï¼‰
                weights = [0.5, 0.3, 0.2]
                predicted = sum(w * t for w, t in zip(weights, transformed))
                
                actual = series[i + 1]
                error = abs(predicted - actual)
                errors.append(error)
            
            if errors:
                mae = np.mean(errors)
                data_std = np.std(series)
                if data_std > 0:
                    accuracy = 1.0 - (mae / data_std)
                    return max(0.0, accuracy)
            
            return 0.0
            
        except Exception as e:
            return 0.0

    def _perform_comprehensive_fractal_analysis(self, processed_data: List[Dict]) -> Dict:
        """
        æ‰§è¡Œç»¼åˆåˆ†å½¢åˆ†æ - åŸºäºåˆ†å½¢å‡ ä½•ç†è®º
        å®ç°Mandelbrotåˆ†å½¢ç†è®ºã€Hausdorffç»´æ•°å’Œå¤šé‡åˆ†å½¢åˆ†æ
        """
        try:
            fractal_analysis = {
                'fractal_dimension': 0.0,
                'hausdorff_dimension': 0.0,
                'box_counting_dimension': 0.0,
                'correlation_dimension': 0.0,
                'multifractal_spectrum': {},
                'self_similarity_analysis': {},
                'scaling_properties': {},
                'fractal_classification': 'non_fractal'
            }
            
            if len(processed_data) < 15:
                return fractal_analysis
            
            # æ„å»ºåˆ†å½¢åˆ†æçš„æ—¶é—´åºåˆ—
            fractal_series = self._construct_fractal_time_series(processed_data)
            
            if len(fractal_series) < 10:
                return fractal_analysis
            
            # 1. Hausdorffç»´æ•°è®¡ç®—
            hausdorff_dim = self._calculate_hausdorff_dimension(fractal_series)
            fractal_analysis['hausdorff_dimension'] = hausdorff_dim
            
            # 2. ç›’è®¡æ•°ç»´æ•°
            box_counting_dim = self._calculate_box_counting_dimension_1d(fractal_series)
            fractal_analysis['box_counting_dimension'] = box_counting_dim
            
            # 3. å…³è”ç»´æ•°
            correlation_dim = self._calculate_correlation_dimension_1d(fractal_series)
            fractal_analysis['correlation_dimension'] = correlation_dim
            
            # 4. å¤šé‡åˆ†å½¢åˆ†æ
            multifractal_spectrum = self._perform_multifractal_analysis(fractal_series)
            fractal_analysis['multifractal_spectrum'] = multifractal_spectrum
            
            # 5. è‡ªç›¸ä¼¼æ€§åˆ†æ
            self_similarity = self._analyze_self_similarity_comprehensive(fractal_series)
            fractal_analysis['self_similarity_analysis'] = self_similarity
            
            # 6. æ ‡åº¦æ€§è´¨åˆ†æ
            scaling_properties = self._analyze_scaling_properties(fractal_series)
            fractal_analysis['scaling_properties'] = scaling_properties
            
            # 7. ç»¼åˆåˆ†å½¢ç»´æ•°
            valid_dimensions = [d for d in [hausdorff_dim, box_counting_dim, correlation_dim] if d > 0]
            if valid_dimensions:
                fractal_analysis['fractal_dimension'] = np.mean(valid_dimensions)
            
            # 8. åˆ†å½¢åˆ†ç±»
            classification = self._classify_fractal_type(fractal_analysis)
            fractal_analysis['fractal_classification'] = classification
            
            return fractal_analysis
            
        except Exception as e:
            print(f"      âŒ ç»¼åˆåˆ†å½¢åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _construct_fractal_time_series(self, processed_data: List[Dict]) -> List[float]:
        """æ„å»ºç”¨äºåˆ†å½¢åˆ†æçš„æ—¶é—´åºåˆ—"""
        try:
            # æ„å»ºå¤šç»´åˆ†å½¢ç‰¹å¾
            fractal_series = []
            
            for period in processed_data:
                tails = period.get('tails', [])
                
                if tails:
                    # æ„å»ºåˆ†å½¢ç‰¹å¾å‘é‡
                    features = []
                    
                    # 1. ç©ºé—´åˆ†å¸ƒç‰¹å¾
                    spatial_center = np.mean(tails)
                    spatial_spread = np.std(tails) if len(tails) > 1 else 0
                    features.extend([spatial_center, spatial_spread])
                    
                    # 2. æ‹“æ‰‘ç‰¹å¾
                    unique_count = len(set(tails))
                    density = unique_count / 10.0
                    features.extend([unique_count, density])
                    
                    # 3. å‡ ä½•ç‰¹å¾
                    if len(tails) > 1:
                        range_span = max(tails) - min(tails)
                        geometric_mean = np.exp(np.mean(np.log(np.array(tails) + 1)))
                        features.extend([range_span, geometric_mean])
                    else:
                        features.extend([0, tails[0] + 1])
                    
                    # ç»„åˆç‰¹å¾ä¸ºå•ä¸€åˆ†å½¢åºåˆ—å€¼
                    fractal_value = np.linalg.norm(features)
                    fractal_series.append(fractal_value)
                else:
                    fractal_series.append(0.0)
            
            return fractal_series
            
        except Exception as e:
            print(f"      âŒ åˆ†å½¢æ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {e}")
            return []
    
    def _calculate_hausdorff_dimension(self, series: List[float]) -> float:
        """è®¡ç®—Hausdorffç»´æ•°"""
        try:
            if len(series) < 8:
                return 0.0
            
            # ä½¿ç”¨å˜åˆ†æ–¹æ³•ä¼°ç®—Hausdorffç»´æ•°
            # æ„å»ºä¸åŒå°ºåº¦ä¸‹çš„è¦†ç›–
            
            series_array = np.array(series)
            min_val = np.min(series_array)
            max_val = np.max(series_array)
            
            if max_val == min_val:
                return 0.0
            
            # ä¸åŒçš„è¦†ç›–å°ºåº¦
            scales = []
            cover_counts = []
            
            for scale_factor in [2, 4, 8, 16]:
                scale = (max_val - min_val) / scale_factor
                if scale > 0:
                    # è®¡ç®—éœ€è¦çš„è¦†ç›–æ•°
                    covers_needed = 0
                    covered_points = set()
                    
                    for i, value in enumerate(series_array):
                        if i not in covered_points:
                            # åˆ›å»ºä»¥å½“å‰ç‚¹ä¸ºä¸­å¿ƒçš„è¦†ç›–
                            cover_min = value - scale / 2
                            cover_max = value + scale / 2
                            
                            # æ‰¾åˆ°è¢«è¿™ä¸ªè¦†ç›–åŒ…å«çš„æ‰€æœ‰ç‚¹
                            for j, other_value in enumerate(series_array):
                                if cover_min <= other_value <= cover_max:
                                    covered_points.add(j)
                            
                            covers_needed += 1
                    
                    scales.append(scale)
                    cover_counts.append(covers_needed)
            
            # è®¡ç®—Hausdorffç»´æ•°
            if len(scales) >= 3:
                log_scales = [-np.log(s) for s in scales]
                log_counts = [np.log(c) for c in cover_counts]
                
                try:
                    slope = np.polyfit(log_scales, log_counts, 1)[0]
                    return max(0.0, slope)
                except:
                    return 0.0
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_box_counting_dimension_1d(self, series: List[float]) -> float:
        """è®¡ç®—ä¸€ç»´åºåˆ—çš„ç›’è®¡æ•°ç»´æ•°"""
        try:
            if len(series) < 8:
                return 0.0
            
            series_array = np.array(series)
            min_val = np.min(series_array)
            max_val = np.max(series_array)
            
            if max_val == min_val:
                return 0.0
            
            # æ„å»ºäºŒç»´è½¨è¿¹ç”¨äºç›’è®¡æ•°
            trajectory_2d = []
            for i, value in enumerate(series_array):
                # å°†æ—¶é—´å’Œå€¼æ˜ å°„åˆ°äºŒç»´ç©ºé—´
                normalized_time = i / len(series_array)
                normalized_value = (value - min_val) / (max_val - min_val)
                trajectory_2d.append([normalized_time, normalized_value])
            
            trajectory_2d = np.array(trajectory_2d)
            
            # ä¸åŒçš„ç›’å­å¤§å°
            box_sizes = []
            box_counts = []
            
            for divisions in [4, 8, 16, 32]:
                box_size = 1.0 / divisions
                
                # è®¡ç®—è¢«å æ®çš„ç›’å­
                occupied_boxes = set()
                
                for point in trajectory_2d:
                    box_x = int(point[0] * divisions)
                    box_y = int(point[1] * divisions)
                    
                    # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    box_x = min(box_x, divisions - 1)
                    box_y = min(box_y, divisions - 1)
                    
                    occupied_boxes.add((box_x, box_y))
                
                box_sizes.append(box_size)
                box_counts.append(len(occupied_boxes))
            
            # è®¡ç®—ç›’è®¡æ•°ç»´æ•°
            if len(box_sizes) >= 3:
                log_sizes = [np.log(s) for s in box_sizes]
                log_counts = [np.log(c) for c in box_counts]
                
                try:
                    slope = -np.polyfit(log_sizes, log_counts, 1)[0]
                    return max(0.0, slope)
                except:
                    return 0.0
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_correlation_dimension_1d(self, series: List[float]) -> float:
        """è®¡ç®—ä¸€ç»´åºåˆ—çš„å…³è”ç»´æ•°"""
        try:
            if len(series) < 10:
                return 0.0
            
            # æ„å»ºåµŒå…¥å‘é‡
            embedding_dim = 3
            embedded = self._embed_time_series(np.array(series), embedding_dim, 1)
            
            if embedded.shape[0] < 5:
                return 0.0
            
            # è®¡ç®—æ‰€æœ‰ç‚¹å¯¹ä¹‹é—´çš„è·ç¦»
            distances = []
            for i in range(embedded.shape[0]):
                for j in range(i + 1, embedded.shape[0]):
                    distance = np.linalg.norm(embedded[i] - embedded[j])
                    distances.append(distance)
            
            distances = np.array(distances)
            distances = distances[distances > 1e-10]
            
            if len(distances) < 10:
                return 0.0
            
            # è®¡ç®—å…³è”ç§¯åˆ†
            min_distance = np.min(distances)
            max_distance = np.max(distances)
            
            radii = np.logspace(np.log10(min_distance), np.log10(max_distance), 15)
            correlation_integrals = []
            
            for radius in radii:
                count = np.sum(distances <= radius)
                correlation_integral = count / len(distances)
                correlation_integrals.append(correlation_integral + 1e-10)
            
            # æ‹Ÿåˆå…³è”ç»´æ•°
            valid_indices = np.where(np.array(correlation_integrals) > 1e-8)[0]
            if len(valid_indices) >= 5:
                log_radii = np.log(radii[valid_indices])
                log_corr = np.log(correlation_integrals)[valid_indices]
                
                try:
                    slope = np.polyfit(log_radii, log_corr, 1)[0]
                    return max(0.0, slope)
                except:
                    return 0.0
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _perform_multifractal_analysis(self, series: List[float]) -> Dict:
        """æ‰§è¡Œå¤šé‡åˆ†å½¢åˆ†æ"""
        try:
            multifractal_results = {
                'spectrum_width': 0.0,
                'multifractality_detected': False,
                'singularity_spectrum': {},
                'generalized_dimensions': {},
                'scaling_exponents': {}
            }
            
            if len(series) < 12:
                return multifractal_results
            
            series_array = np.array(series)
            
            # 1. è®¡ç®—å¹¿ä¹‰ç»´æ•°
            q_values = [-2, -1, 0, 1, 2, 3]
            generalized_dims = {}
            
            for q in q_values:
                dim = self._calculate_generalized_dimension(series_array, q)
                if dim > 0:
                    generalized_dims[q] = dim
            
            multifractal_results['generalized_dimensions'] = generalized_dims
            
            # 2. è®¡ç®—æ ‡åº¦æŒ‡æ•°
            if len(generalized_dims) >= 3:
                scaling_exponents = {}
                for q, dim in generalized_dims.items():
                    tau_q = (q - 1) * dim
                    scaling_exponents[q] = tau_q
                
                multifractal_results['scaling_exponents'] = scaling_exponents
                
                # 3. å¥‡å¼‚è°±åˆ†æ
                singularity_spectrum = self._calculate_singularity_spectrum(scaling_exponents)
                multifractal_results['singularity_spectrum'] = singularity_spectrum
                
                # 4. å¤šé‡åˆ†å½¢æ€§æ£€æµ‹
                if singularity_spectrum:
                    spectrum_width = singularity_spectrum.get('spectrum_width', 0.0)
                    multifractal_results['spectrum_width'] = spectrum_width
                    
                    # è°±å®½åº¦å¤§äº0.1è¡¨ç¤ºå¤šé‡åˆ†å½¢æ€§
                    if spectrum_width > 0.1:
                        multifractal_results['multifractality_detected'] = True
            
            return multifractal_results
            
        except Exception as e:
            return {}
    
    def _calculate_generalized_dimension(self, series: np.ndarray, q: int) -> float:
        """è®¡ç®—å¹¿ä¹‰ç»´æ•°Dq"""
        try:
            if len(series) < 8:
                return 0.0
            
            # ä½¿ç”¨ç›’è®¡æ•°æ–¹æ³•è®¡ç®—å¹¿ä¹‰ç»´æ•°
            min_val = np.min(series)
            max_val = np.max(series)
            
            if max_val == min_val:
                return 0.0
            
            box_sizes = []
            info_measures = []
            
            for divisions in [4, 8, 16]:
                box_size = 1.0 / divisions
                
                # è®¡ç®—æ¯ä¸ªç›’å­çš„æ¦‚ç‡
                box_probs = {}
                total_points = len(series)
                
                for value in series:
                    normalized_value = (value - min_val) / (max_val - min_val)
                    box_index = min(int(normalized_value * divisions), divisions - 1)
                    box_probs[box_index] = box_probs.get(box_index, 0) + 1
                
                # å½’ä¸€åŒ–æ¦‚ç‡
                for box_idx in box_probs:
                    box_probs[box_idx] /= total_points
                
                # è®¡ç®—ä¿¡æ¯æµ‹åº¦
                if q == 1:
                    # ä¿¡æ¯ç»´æ•°
                    info_measure = -sum(p * np.log(p) for p in box_probs.values() if p > 0)
                elif q == 0:
                    # å®¹é‡ç»´æ•°
                    info_measure = np.log(len(box_probs))
                else:
                    # å¹¿ä¹‰ä¿¡æ¯æµ‹åº¦
                    sum_pq = sum(p**q for p in box_probs.values() if p > 0)
                    if sum_pq > 0:
                        info_measure = np.log(sum_pq) / (q - 1)
                    else:
                        continue
                
                box_sizes.append(box_size)
                info_measures.append(info_measure)
            
            # è®¡ç®—ç»´æ•°
            if len(box_sizes) >= 2:
                log_sizes = [np.log(s) for s in box_sizes]
                
                try:
                    if q == 1:
                        slope = np.polyfit(log_sizes, info_measures, 1)[0]
                        return -slope
                    else:
                        slope = np.polyfit(log_sizes, info_measures, 1)[0]
                        return slope
                except:
                    return 0.0
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_singularity_spectrum(self, scaling_exponents: Dict) -> Dict:
        """è®¡ç®—å¥‡å¼‚è°±"""
        try:
            spectrum_results = {
                'alpha_values': [],
                'f_alpha_values': [],
                'spectrum_width': 0.0,
                'spectrum_asymmetry': 0.0
            }
            
            if len(scaling_exponents) < 3:
                return spectrum_results
            
            # ä»æ ‡åº¦æŒ‡æ•°è®¡ç®—å¥‡å¼‚è°±
            q_values = sorted(scaling_exponents.keys())
            tau_values = [scaling_exponents[q] for q in q_values]
            
            alpha_values = []
            f_alpha_values = []
            
            # è®¡ç®—å¥‡å¼‚å¼ºåº¦Î±å’Œå¥‡å¼‚è°±f(Î±)
            for i in range(len(q_values)):
                q = q_values[i]
                tau_q = tau_values[i]
                
                # Î±(q) = dÏ„(q)/dq
                if i > 0 and i < len(q_values) - 1:
                    # ä½¿ç”¨ä¸­å¿ƒå·®åˆ†
                    dq = q_values[i+1] - q_values[i-1]
                    dtau = tau_values[i+1] - tau_values[i-1]
                    if dq != 0:
                        alpha = dtau / dq
                    else:
                        continue
                elif i == 0:
                    # å‘å‰å·®åˆ†
                    dq = q_values[i+1] - q_values[i]
                    dtau = tau_values[i+1] - tau_values[i]
                    if dq != 0:
                        alpha = dtau / dq
                    else:
                        continue
                else:
                    # å‘åå·®åˆ†
                    dq = q_values[i] - q_values[i-1]
                    dtau = tau_values[i] - tau_values[i-1]
                    if dq != 0:
                        alpha = dtau / dq
                    else:
                        continue
                
                # f(Î±) = qÎ± - Ï„(q)
                f_alpha = q * alpha - tau_q
                
                alpha_values.append(alpha)
                f_alpha_values.append(f_alpha)
            
            if alpha_values and f_alpha_values:
                spectrum_results['alpha_values'] = alpha_values
                spectrum_results['f_alpha_values'] = f_alpha_values
                
                # è°±å®½åº¦ï¼šÎ±çš„èŒƒå›´
                spectrum_width = max(alpha_values) - min(alpha_values)
                spectrum_results['spectrum_width'] = spectrum_width
                
                # è°±ä¸å¯¹ç§°æ€§
                if len(alpha_values) >= 3:
                    alpha_center = np.mean(alpha_values)
                    left_width = alpha_center - min(alpha_values)
                    right_width = max(alpha_values) - alpha_center
                    
                    if left_width + right_width > 0:
                        asymmetry = (right_width - left_width) / (right_width + left_width)
                        spectrum_results['spectrum_asymmetry'] = asymmetry
            
            return spectrum_results
            
        except Exception as e:
            return {}
    
    def _analyze_self_similarity_comprehensive(self, series: List[float]) -> Dict:
        """ç»¼åˆè‡ªç›¸ä¼¼æ€§åˆ†æ"""
        try:
            self_similarity_results = {
                'global_self_similarity': 0.0,
                'local_self_similarity': {},
                'scale_invariance': 0.0,
                'self_affinity': 0.0,
                'hurst_exponent': 0.0
            }
            
            if len(series) < 10:
                return self_similarity_results
            
            series_array = np.array(series)
            
            # 1. å…¨å±€è‡ªç›¸ä¼¼æ€§
            global_similarity = self._calculate_global_self_similarity(series_array)
            self_similarity_results['global_self_similarity'] = global_similarity
            
            # 2. å±€éƒ¨è‡ªç›¸ä¼¼æ€§
            local_similarities = self._calculate_local_self_similarities(series_array)
            self_similarity_results['local_self_similarity'] = local_similarities
            
            # 3. å°ºåº¦ä¸å˜æ€§
            scale_invariance = self._calculate_scale_invariance(series_array)
            self_similarity_results['scale_invariance'] = scale_invariance
            
            # 4. è‡ªä»¿å°„æ€§
            self_affinity = self._calculate_self_affinity(series_array)
            self_similarity_results['self_affinity'] = self_affinity
            
            # 5. HurstæŒ‡æ•°
            hurst_exponent = self._calculate_hurst_exponent(series_array)
            self_similarity_results['hurst_exponent'] = hurst_exponent
            
            return self_similarity_results
            
        except Exception as e:
            return {}
    
    def _calculate_global_self_similarity(self, series: np.ndarray) -> float:
        """è®¡ç®—å…¨å±€è‡ªç›¸ä¼¼æ€§"""
        try:
            if len(series) < 8:
                return 0.0
            
            # æ¯”è¾ƒä¸åŒå°ºåº¦çš„ç›¸ä¼¼æ€§
            scales = [2, 3, 4]
            similarities = []
            
            for scale in scales:
                if len(series) >= scale * 3:
                    # æ„å»ºä¸‹é‡‡æ ·åºåˆ—
                    downsampled = series[::scale]
                    
                    # æ¯”è¾ƒåŸå§‹åºåˆ—çš„å¼€å§‹éƒ¨åˆ†ä¸ä¸‹é‡‡æ ·åºåˆ—
                    min_length = min(len(downsampled), len(series) // scale)
                    
                    if min_length >= 3:
                        original_segment = series[:min_length]
                        downsampled_segment = downsampled[:min_length]
                        
                        # æ ‡å‡†åŒ–
                        if np.std(original_segment) > 0 and np.std(downsampled_segment) > 0:
                            orig_norm = (original_segment - np.mean(original_segment)) / np.std(original_segment)
                            down_norm = (downsampled_segment - np.mean(downsampled_segment)) / np.std(downsampled_segment)
                            
                            # è®¡ç®—ç›¸å…³æ€§
                            correlation = np.corrcoef(orig_norm, down_norm)[0, 1]
                            if not np.isnan(correlation):
                                similarities.append(abs(correlation))
            
            if similarities:
                return np.mean(similarities)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_local_self_similarities(self, series: np.ndarray) -> Dict:
        """è®¡ç®—å±€éƒ¨è‡ªç›¸ä¼¼æ€§"""
        try:
            local_results = {
                'window_similarities': [],
                'average_local_similarity': 0.0,
                'similarity_variance': 0.0
            }
            
            if len(series) < 12:
                return local_results
            
            window_size = min(6, len(series) // 3)
            similarities = []
            
            for start in range(len(series) - window_size * 2 + 1):
                window1 = series[start:start + window_size]
                window2 = series[start + window_size:start + 2 * window_size]
                
                # è®¡ç®—çª—å£é—´çš„ç›¸ä¼¼æ€§
                if np.std(window1) > 0 and np.std(window2) > 0:
                    # æ ‡å‡†åŒ–
                    w1_norm = (window1 - np.mean(window1)) / np.std(window1)
                    w2_norm = (window2 - np.mean(window2)) / np.std(window2)
                    
                    # ç›¸å…³æ€§
                    correlation = np.corrcoef(w1_norm, w2_norm)[0, 1]
                    if not np.isnan(correlation):
                        similarities.append(abs(correlation))
            
            if similarities:
                local_results['window_similarities'] = similarities
                local_results['average_local_similarity'] = np.mean(similarities)
                local_results['similarity_variance'] = np.var(similarities)
            
            return local_results
            
        except Exception as e:
            return {}
    
    def _calculate_scale_invariance(self, series: np.ndarray) -> float:
        """è®¡ç®—å°ºåº¦ä¸å˜æ€§"""
        try:
            if len(series) < 10:
                return 0.0
            
            # åˆ†æåŠŸç‡è°±çš„å°ºåº¦ä¸å˜æ€§
            try:
                # è®¡ç®—åŠŸç‡è°±
                fft = np.fft.fft(series - np.mean(series))
                power_spectrum = np.abs(fft) ** 2
                frequencies = np.fft.fftfreq(len(series))
                
                # åªè€ƒè™‘æ­£é¢‘ç‡
                positive_freqs = frequencies[1:len(frequencies)//2]
                positive_power = power_spectrum[1:len(power_spectrum)//2]
                
                if len(positive_freqs) >= 5:
                    # æ£€æŸ¥æ˜¯å¦ç¬¦åˆå¹‚å¾‹ï¼šP(f) ~ f^(-Î²)
                    log_freqs = np.log(positive_freqs)
                    log_power = np.log(positive_power + 1e-10)
                    
                    # æ‹Ÿåˆå¹‚å¾‹
                    slope = np.polyfit(log_freqs, log_power, 1)[0]
                    
                    # è®¡ç®—æ‹Ÿåˆè´¨é‡
                    predicted = slope * log_freqs + np.mean(log_power)
                    r_squared = 1 - np.sum((log_power - predicted)**2) / np.sum((log_power - np.mean(log_power))**2)
                    
                    # å°ºåº¦ä¸å˜æ€§ï¼šå¥½çš„å¹‚å¾‹æ‹Ÿåˆ
                    if r_squared > 0.7:
                        return r_squared
                
            except:
                pass
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_self_affinity(self, series: np.ndarray) -> float:
        """è®¡ç®—è‡ªä»¿å°„æ€§"""
        try:
            if len(series) < 8:
                return 0.0
            
            # è‡ªä»¿å°„æ€§ï¼šä¸åŒæ–¹å‘çš„ä¸åŒæ ‡åº¦è¡Œä¸º
            # åˆ†ææ—¶é—´å’Œå¹…åº¦æ–¹å‘çš„æ ‡åº¦å…³ç³»
            
            time_scales = [2, 3, 4]
            amplitude_scalings = []
            
            for time_scale in time_scales:
                if len(series) >= time_scale * 4:
                    # æ—¶é—´æ–¹å‘çš„é‡æ ‡åº¦
                    time_rescaled = series[::time_scale]
                    
                    if len(time_rescaled) >= 3:
                        # è®¡ç®—å¹…åº¦çš„æ ‡åº¦å› å­
                        original_std = np.std(series)
                        rescaled_std = np.std(time_rescaled)
                        
                        if original_std > 0 and rescaled_std > 0:
                            amplitude_scaling = rescaled_std / original_std
                            amplitude_scalings.append(amplitude_scaling)
            
            if len(amplitude_scalings) >= 2:
                # è‡ªä»¿å°„æ€§ï¼šæ ‡åº¦å…³ç³»çš„ä¸€è‡´æ€§
                scaling_consistency = 1.0 - np.std(amplitude_scalings) / (np.mean(amplitude_scalings) + 1e-10)
                return max(0.0, scaling_consistency)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_hurst_exponent(self, series: np.ndarray) -> float:
        """è®¡ç®—HurstæŒ‡æ•°"""
        try:
            if len(series) < 8:
                return 0.5
            
            # ä½¿ç”¨R/Såˆ†æè®¡ç®—HurstæŒ‡æ•°
            n = len(series)
            
            # è®¡ç®—ç´¯ç§¯åå·®
            mean_series = np.mean(series)
            deviations = series - mean_series
            cumulative_deviations = np.cumsum(deviations)
            
            # ä¸åŒçš„æ—¶é—´å°ºåº¦
            scales = []
            rs_values = []
            
            for scale in [4, 6, 8]:
                if n >= scale * 2:
                    # åˆ†å‰²æˆå­åºåˆ—
                    n_segments = n // scale
                    rs_segment_values = []
                    
                    for i in range(n_segments):
                        start = i * scale
                        end = start + scale
                        
                        segment = cumulative_deviations[start:end]
                        
                        # è®¡ç®—èŒƒå›´
                        segment_range = np.max(segment) - np.min(segment)
                        
                        # è®¡ç®—æ ‡å‡†å·®
                        segment_data = series[start:end]
                        segment_std = np.std(segment_data)
                        
                        # R/Sæ¯”å€¼
                        if segment_std > 0:
                            rs_value = segment_range / segment_std
                            rs_segment_values.append(rs_value)
                    
                    if rs_segment_values:
                        avg_rs = np.mean(rs_segment_values)
                        scales.append(scale)
                        rs_values.append(avg_rs)
            
            # è®¡ç®—HurstæŒ‡æ•°
            if len(scales) >= 2:
                log_scales = [np.log(s) for s in scales]
                log_rs = [np.log(rs) for rs in rs_values]
                
                try:
                    hurst = np.polyfit(log_scales, log_rs, 1)[0]
                    return max(0.0, min(1.0, hurst))
                except:
                    return 0.5
            
            return 0.5
            
        except Exception as e:
            return 0.5
    
    def _analyze_scaling_properties(self, series: List[float]) -> Dict:
        """åˆ†ææ ‡åº¦æ€§è´¨"""
        try:
            scaling_results = {
                'power_law_exponent': 0.0,
                'scaling_regime_identified': False,
                'crossover_scales': [],
                'finite_size_effects': 0.0,
                'scaling_quality': 0.0
            }
            
            if len(series) < 10:
                return scaling_results
            
            series_array = np.array(series)
            
            # 1. åŠŸç‡è°±åˆ†æ
            power_law_analysis = self._analyze_power_law_scaling(series_array)
            scaling_results.update(power_law_analysis)
            
            # 2. ç»“æ„å‡½æ•°åˆ†æ
            structure_function_analysis = self._analyze_structure_functions(series_array)
            scaling_results.update(structure_function_analysis)
            
            # 3. æœ‰é™å°ºå¯¸æ•ˆåº”
            finite_size_effects = self._analyze_finite_size_scaling_effects(series_array)
            scaling_results['finite_size_effects'] = finite_size_effects
            
            return scaling_results
            
        except Exception as e:
            return {}
    
    def _analyze_power_law_scaling(self, series: np.ndarray) -> Dict:
        """åˆ†æå¹‚å¾‹æ ‡åº¦"""
        try:
            power_law_results = {
                'power_law_exponent': 0.0,
                'scaling_quality': 0.0,
                'scaling_regime_identified': False
            }
            
            if len(series) < 8:
                return power_law_results
            
            # è®¡ç®—åŠŸç‡è°±
            try:
                fft = np.fft.fft(series - np.mean(series))
                power_spectrum = np.abs(fft) ** 2
                frequencies = np.fft.fftfreq(len(series))
                
                # æ­£é¢‘ç‡
                positive_indices = np.where(frequencies > 0)[0]
                positive_freqs = frequencies[positive_indices]
                positive_power = power_spectrum[positive_indices]
                
                if len(positive_freqs) >= 5:
                    # å¹‚å¾‹æ‹Ÿåˆï¼šP(f) ~ f^(-Î²)
                    log_freqs = np.log(positive_freqs)
                    log_power = np.log(positive_power + 1e-10)
                    
                    # çº¿æ€§æ‹Ÿåˆ
                    slope, intercept = np.polyfit(log_freqs, log_power, 1)
                    
                    # æ‹Ÿåˆè´¨é‡
                    predicted = slope * log_freqs + intercept
                    r_squared = 1 - np.sum((log_power - predicted)**2) / np.sum((log_power - np.mean(log_power))**2)
                    
                    power_law_results['power_law_exponent'] = -slope
                    power_law_results['scaling_quality'] = max(0.0, r_squared)
                    
                    if r_squared > 0.7:
                        power_law_results['scaling_regime_identified'] = True
                
            except:
                pass
            
            return power_law_results
            
        except Exception as e:
            return {}
    
    def _analyze_structure_functions(self, series: np.ndarray) -> Dict:
        """åˆ†æç»“æ„å‡½æ•°"""
        try:
            structure_results = {
                'structure_function_exponents': {},
                'intermittency_detected': False,
                'multiscaling_detected': False
            }
            
            if len(series) < 10:
                return structure_results
            
            # è®¡ç®—ä¸åŒé˜¶çš„ç»“æ„å‡½æ•°
            orders = [1, 2, 3, 4]
            scales = [2, 3, 4, 5]
            
            exponents = {}
            
            for order in orders:
                scale_values = []
                structure_values = []
                
                for scale in scales:
                    if len(series) >= scale + 1:
                        # è®¡ç®—å¢é‡
                        increments = []
                        for i in range(len(series) - scale):
                            increment = abs(series[i + scale] - series[i])
                            increments.append(increment)
                        
                        if increments:
                            # ç»“æ„å‡½æ•°ï¼š<|Î”x(Ï„)|^p>
                            structure_function = np.mean([inc**order for inc in increments])
                            
                            scale_values.append(scale)
                            structure_values.append(structure_function + 1e-10)
                
                # æ‹Ÿåˆæ ‡åº¦æŒ‡æ•°
                if len(scale_values) >= 3:
                    log_scales = [np.log(s) for s in scale_values]
                    log_structure = [np.log(s) for s in structure_values]
                    
                    try:
                        exponent = np.polyfit(log_scales, log_structure, 1)[0]
                        exponents[order] = exponent
                    except:
                        continue
            
            structure_results['structure_function_exponents'] = exponents
            
            # æ£€æµ‹é—´æ­‡æ€§å’Œå¤šæ ‡åº¦æ€§
            if len(exponents) >= 3:
                # é—´æ­‡æ€§ï¼šÎ¶(p) â‰  p/3
                linear_exponents = [order / 3.0 for order in exponents.keys()]
                actual_exponents = list(exponents.values())
                
                if len(linear_exponents) == len(actual_exponents):
                    deviations = [abs(a - l) for a, l in zip(actual_exponents, linear_exponents)]
                    avg_deviation = np.mean(deviations)
                    
                    if avg_deviation > 0.1:
                        structure_results['intermittency_detected'] = True
                
                # å¤šæ ‡åº¦æ€§ï¼šéçº¿æ€§çš„Î¶(p)
                orders_list = list(exponents.keys())
                exponents_list = list(exponents.values())
                
                if len(orders_list) >= 3:
                    try:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¿æ€§å…³ç³»
                        slope = np.polyfit(orders_list, exponents_list, 1)[0]
                        predicted = [slope * order for order in orders_list]
                        
                        nonlinearity = np.sum([(e - p)**2 for e, p in zip(exponents_list, predicted)])
                        
                        if nonlinearity > 0.01:
                            structure_results['multiscaling_detected'] = True
                    except:
                        pass
            
            return structure_results
            
        except Exception as e:
            return {}
    
    def _analyze_finite_size_scaling_effects(self, series: np.ndarray) -> float:
        """åˆ†ææœ‰é™å°ºå¯¸æ ‡åº¦æ•ˆåº”"""
        try:
            if len(series) < 8:
                return 0.0
            
            # åˆ†æè¾¹ç•Œæ•ˆåº”å’Œæœ‰é™å°ºå¯¸ä¿®æ­£
            series_length = len(series)
            
            # è®¡ç®—ä¸åŒå­åºåˆ—çš„æ ‡åº¦è¡Œä¸º
            subseries_exponents = []
            
            for subseries_length in [series_length // 2, series_length // 3, series_length // 4]:
                if subseries_length >= 4:
                    subseries = series[:subseries_length]
                    
                    # è®¡ç®—è¯¥å­åºåˆ—çš„æ ‡åº¦æŒ‡æ•°
                    try:
                        fft = np.fft.fft(subseries - np.mean(subseries))
                        power_spectrum = np.abs(fft) ** 2
                        frequencies = np.fft.fftfreq(len(subseries))
                        
                        positive_indices = np.where(frequencies > 0)[0]
                        positive_freqs = frequencies[positive_indices]
                        positive_power = power_spectrum[positive_indices]
                        
                        if len(positive_freqs) >= 3:
                            log_freqs = np.log(positive_freqs)
                            log_power = np.log(positive_power + 1e-10)
                            
                            slope = np.polyfit(log_freqs, log_power, 1)[0]
                            subseries_exponents.append(-slope)
                    except:
                        continue
            
            # æœ‰é™å°ºå¯¸æ•ˆåº”ï¼šä¸åŒé•¿åº¦çš„æ ‡åº¦æŒ‡æ•°å·®å¼‚
            if len(subseries_exponents) >= 2:
                exponent_variance = np.var(subseries_exponents)
                exponent_mean = np.mean(subseries_exponents)
                
                if exponent_mean > 0:
                    finite_size_effect = exponent_variance / exponent_mean
                    return min(1.0, finite_size_effect)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _classify_fractal_type(self, fractal_analysis: Dict) -> str:
        """åˆ†ç±»åˆ†å½¢ç±»å‹"""
        try:
            # è·å–å…³é”®æŒ‡æ ‡
            fractal_dim = fractal_analysis.get('fractal_dimension', 0.0)
            multifractal_detected = fractal_analysis.get('multifractal_spectrum', {}).get('multifractality_detected', False)
            self_similarity = fractal_analysis.get('self_similarity_analysis', {}).get('global_self_similarity', 0.0)
            scaling_quality = fractal_analysis.get('scaling_properties', {}).get('scaling_quality', 0.0)
            
            # åˆ†ç±»è§„åˆ™
            if fractal_dim > 0.1 and scaling_quality > 0.7:
                if multifractal_detected:
                    return 'multifractal'
                elif self_similarity > 0.7:
                    return 'self_similar_fractal'
                elif 1.0 < fractal_dim < 2.0:
                    return 'monofractal'
                else:
                    return 'fractal_like'
            elif fractal_dim > 0.05:
                return 'weak_fractal'
            else:
                return 'non_fractal'
                
        except Exception as e:
            return 'unknown'
    
    def _calculate_gini_coefficient(self, values: np.ndarray) -> float:
        """è®¡ç®—åŸºå°¼ç³»æ•°"""
        try:
            if len(values) == 0:
                return 0.0
            
            # æ’åº
            sorted_values = np.sort(values)
            n = len(sorted_values)
            
            # è®¡ç®—åŸºå°¼ç³»æ•°
            cumsum = np.cumsum(sorted_values)
            total = cumsum[-1]
            
            if total == 0:
                return 0.0
            
            # åŸºå°¼ç³»æ•°å…¬å¼
            gini = (2 * np.sum((np.arange(1, n + 1) * sorted_values))) / (n * total) - (n + 1) / n
            
            return max(0.0, gini)
            
        except Exception as e:
            return 0.0
    
    def _analyze_emotional_dynamics(self, processed_data: List[Dict]) -> Dict:
        """
        åˆ†ææƒ…ç»ªåŠ¨åŠ›å­¦ - åŸºäºæƒ…ç»ªæ„ŸæŸ“ç†è®ºå’Œé›†ä½“æƒ…ç»ªæ¨¡å‹
        å®ç°Hatfieldæƒ…ç»ªæ„ŸæŸ“ç†è®ºå’ŒBarsadeç¾¤ä½“æƒ…ç»ªåˆ†æ
        """
        try:
            emotional_analysis = {
                'emotional_contagion': {},
                'collective_mood_states': {},
                'emotional_volatility': 0.0,
                'sentiment_propagation': {},
                'emotional_synchrony': 0.0,
                'mood_transitions': {},
                'emotional_resilience': 0.0
            }
            
            if len(processed_data) < 10:
                return emotional_analysis
            
            # æ„å»ºæƒ…ç»ªæ—¶é—´åºåˆ—
            emotion_series = self._construct_emotion_time_series(processed_data)
            
            if len(emotion_series) < 5:
                return emotional_analysis
            
            # 1. æƒ…ç»ªæ„ŸæŸ“åˆ†æ
            contagion_analysis = self._analyze_emotion_contagion(emotion_series, processed_data)
            emotional_analysis['emotional_contagion'] = contagion_analysis
            
            # 2. é›†ä½“æƒ…ç»ªçŠ¶æ€è¯†åˆ«
            mood_states = self._identify_collective_mood_states(emotion_series)
            emotional_analysis['collective_mood_states'] = mood_states
            
            # 3. æƒ…ç»ªæ³¢åŠ¨æ€§
            volatility = self._calculate_emotional_volatility(emotion_series)
            emotional_analysis['emotional_volatility'] = volatility
            
            # 4. æƒ…æ„Ÿä¼ æ’­åˆ†æ
            sentiment_propagation = self._analyze_sentiment_propagation(emotion_series, processed_data)
            emotional_analysis['sentiment_propagation'] = sentiment_propagation
            
            # 5. æƒ…ç»ªåŒæ­¥æ€§
            synchrony = self._calculate_emotional_synchrony(emotion_series)
            emotional_analysis['emotional_synchrony'] = synchrony
            
            # 6. æƒ…ç»ªè½¬æ¢åˆ†æ
            mood_transitions = self._analyze_mood_transitions(emotion_series)
            emotional_analysis['mood_transitions'] = mood_transitions
            
            # 7. æƒ…ç»ªéŸ§æ€§
            resilience = self._calculate_emotional_resilience(emotion_series)
            emotional_analysis['emotional_resilience'] = resilience
            
            return emotional_analysis
            
        except Exception as e:
            print(f"      âŒ æƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _construct_emotion_time_series(self, processed_data: List[Dict]) -> List[Dict]:
        """æ„å»ºæƒ…ç»ªæ—¶é—´åºåˆ—"""
        try:
            emotion_series = []
            
            for i, period in enumerate(processed_data):
                tails = period.get('tails', [])
                
                # æ„å»ºå¤šç»´æƒ…ç»ªç‰¹å¾
                emotion_features = {
                    'optimism': 0.0,      # ä¹è§‚åº¦
                    'anxiety': 0.0,       # ç„¦è™‘åº¦
                    'confidence': 0.0,    # ä¿¡å¿ƒåº¦
                    'excitement': 0.0,    # å…´å¥‹åº¦
                    'uncertainty': 0.0,   # ä¸ç¡®å®šæ€§
                    'stability': 0.0      # ç¨³å®šæ€§
                }
                
                if tails:
                    # åŸºäºé€‰æ‹©æ¨¡å¼æ¨æ–­æƒ…ç»ªçŠ¶æ€
                    
                    # ä¹è§‚åº¦ï¼šé€‰æ‹©è¾ƒå¤§æ•°å­—çš„å€¾å‘
                    avg_tail = np.mean(tails)
                    emotion_features['optimism'] = avg_tail / 9.0
                    
                    # ç„¦è™‘åº¦ï¼šé€‰æ‹©åˆ†æ•£ç¨‹åº¦
                    if len(tails) > 1:
                        spread = np.std(tails)
                        emotion_features['anxiety'] = spread / 3.0
                    
                    # ä¿¡å¿ƒåº¦ï¼šé€‰æ‹©æ•°é‡ï¼ˆæ›´å¤šé€‰æ‹©è¡¨ç¤ºæ›´æœ‰ä¿¡å¿ƒï¼‰
                    emotion_features['confidence'] = len(tails) / 10.0
                    
                    # å…´å¥‹åº¦ï¼šé€‰æ‹©çš„æç«¯ç¨‹åº¦
                    if tails:
                        extremity = max(abs(t - 4.5) for t in tails) / 4.5
                        emotion_features['excitement'] = extremity
                    
                    # ä¸ç¡®å®šæ€§ï¼šé€‰æ‹©çš„éšæœºæ€§
                    if len(tails) > 1:
                        sorted_tails = sorted(tails)
                        gaps = [sorted_tails[j+1] - sorted_tails[j] for j in range(len(sorted_tails)-1)]
                        avg_gap = np.mean(gaps) if gaps else 0
                        emotion_features['uncertainty'] = avg_gap / 9.0
                    
                    # ç¨³å®šæ€§ï¼šä¸å‰æœŸçš„ç›¸ä¼¼æ€§
                    if i > 0:
                        prev_tails = processed_data[i-1].get('tails', [])
                        if prev_tails:
                            current_set = set(tails)
                            prev_set = set(prev_tails)
                            
                            if current_set or prev_set:
                                overlap = len(current_set.intersection(prev_set))
                                union = len(current_set.union(prev_set))
                                similarity = overlap / union if union > 0 else 0
                                emotion_features['stability'] = similarity
                
                emotion_series.append(emotion_features)
            
            return emotion_series
            
        except Exception as e:
            print(f"      âŒ æƒ…ç»ªæ—¶é—´åºåˆ—æ„å»ºå¤±è´¥: {e}")
            return []
    
    def _analyze_emotion_contagion(self, emotion_series: List[Dict], processed_data: List[Dict]) -> Dict:
        """åˆ†ææƒ…ç»ªæ„ŸæŸ“"""
        try:
            contagion_analysis = {
                'contagion_strength': 0.0,
                'contagion_speed': 0.0,
                'dominant_emotions': [],
                'contagion_patterns': {}
            }
            
            if len(emotion_series) < 3:
                return contagion_analysis
            
            # åˆ†æå„ç§æƒ…ç»ªçš„ä¼ æ’­
            emotion_types = ['optimism', 'anxiety', 'confidence', 'excitement', 'uncertainty', 'stability']
            contagion_strengths = {}
            
            for emotion_type in emotion_types:
                # æå–è¯¥æƒ…ç»ªçš„æ—¶é—´åºåˆ—
                emotion_timeline = [period.get(emotion_type, 0.0) for period in emotion_series]
                
                # è®¡ç®—ä¼ æ’­å¼ºåº¦
                contagion_strength = self._calculate_emotion_contagion_strength(emotion_timeline)
                contagion_strengths[emotion_type] = contagion_strength
            
            # æ•´ä½“æ„ŸæŸ“å¼ºåº¦
            if contagion_strengths:
                contagion_analysis['contagion_strength'] = np.mean(list(contagion_strengths.values()))
                
                # è¯†åˆ«ä¸»å¯¼æƒ…ç»ª
                sorted_emotions = sorted(contagion_strengths.items(), key=lambda x: x[1], reverse=True)
                contagion_analysis['dominant_emotions'] = [emotion for emotion, strength in sorted_emotions[:3]]
                
                # æ„ŸæŸ“æ¨¡å¼
                contagion_analysis['contagion_patterns'] = contagion_strengths
            
            # è®¡ç®—æ„ŸæŸ“é€Ÿåº¦
            contagion_speed = self._calculate_emotion_contagion_speed(emotion_series)
            contagion_analysis['contagion_speed'] = contagion_speed
            
            return contagion_analysis
            
        except Exception as e:
            return {}
    
    def _calculate_emotion_contagion_strength(self, emotion_timeline: List[float]) -> float:
        """è®¡ç®—æƒ…ç»ªæ„ŸæŸ“å¼ºåº¦"""
        try:
            if len(emotion_timeline) < 3:
                return 0.0
            
            # è®¡ç®—æƒ…ç»ªå˜åŒ–çš„ç›¸å…³æ€§ï¼ˆæ„ŸæŸ“æŒ‡æ ‡ï¼‰
            correlations = []
            
            for lag in range(1, min(4, len(emotion_timeline) // 2)):
                if len(emotion_timeline) > lag:
                    # è®¡ç®—æ»åç›¸å…³æ€§
                    current = emotion_timeline[:-lag]
                    lagged = emotion_timeline[lag:]
                    
                    if len(current) == len(lagged) and len(current) > 2:
                        correlation = np.corrcoef(current, lagged)[0, 1]
                        if not np.isnan(correlation):
                            correlations.append(abs(correlation))
            
            if correlations:
                # æ„ŸæŸ“å¼ºåº¦ï¼šæ»åç›¸å…³æ€§çš„å¹³å‡å€¼
                return np.mean(correlations)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_emotion_contagion_speed(self, emotion_series: List[Dict]) -> float:
        """è®¡ç®—æƒ…ç»ªæ„ŸæŸ“é€Ÿåº¦"""
        try:
            if len(emotion_series) < 4:
                return 0.0
            
            # åˆ†ææƒ…ç»ªå˜åŒ–çš„ä¼ æ’­é€Ÿåº¦
            speed_indicators = []
            
            emotion_types = ['optimism', 'anxiety', 'confidence', 'excitement']
            
            for emotion_type in emotion_types:
                timeline = [period.get(emotion_type, 0.0) for period in emotion_series]
                
                # æ£€æµ‹æƒ…ç»ªå³°å€¼çš„ä¼ æ’­
                peaks = self._detect_emotion_peaks(timeline)
                
                if len(peaks) >= 2:
                    # è®¡ç®—å³°å€¼é—´çš„æ—¶é—´é—´éš”
                    intervals = [peaks[i+1] - peaks[i] for i in range(len(peaks)-1)]
                    if intervals:
                        avg_interval = np.mean(intervals)
                        # é€Ÿåº¦ï¼šé—´éš”çš„å€’æ•°
                        speed = 1.0 / (avg_interval + 1)
                        speed_indicators.append(speed)
            
            if speed_indicators:
                return np.mean(speed_indicators)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _detect_emotion_peaks(self, timeline: List[float]) -> List[int]:
        """æ£€æµ‹æƒ…ç»ªå³°å€¼"""
        try:
            if len(timeline) < 3:
                return []
            
            peaks = []
            threshold = np.mean(timeline) + 0.5 * np.std(timeline)
            
            for i in range(1, len(timeline) - 1):
                if (timeline[i] > timeline[i-1] and 
                    timeline[i] > timeline[i+1] and 
                    timeline[i] > threshold):
                    peaks.append(i)
            
            return peaks
            
        except Exception as e:
            return []
    
    def _identify_collective_mood_states(self, emotion_series: List[Dict]) -> Dict:
        """è¯†åˆ«é›†ä½“æƒ…ç»ªçŠ¶æ€"""
        try:
            mood_states = {
                'dominant_moods': [],
                'mood_stability': 0.0,
                'mood_intensity': 0.0,
                'emotional_balance': 0.0
            }
            
            if len(emotion_series) < 3:
                return mood_states
            
            # è®¡ç®—æ¯ä¸ªæ—¶æœŸçš„ä¸»å¯¼æƒ…ç»ª
            period_moods = []
            
            for period in emotion_series:
                if period:
                    # æ‰¾åˆ°æœ€å¼ºçš„æƒ…ç»ª
                    max_emotion = max(period.items(), key=lambda x: x[1])
                    period_moods.append(max_emotion[0])
            
            # ç»Ÿè®¡ä¸»å¯¼æƒ…ç»ª
            if period_moods:
                from collections import Counter
                mood_counts = Counter(period_moods)
                
                # æœ€å¸¸è§çš„æƒ…ç»ª
                dominant_moods = [mood for mood, count in mood_counts.most_common(3)]
                mood_states['dominant_moods'] = dominant_moods
                
                # æƒ…ç»ªç¨³å®šæ€§ï¼šä¸»å¯¼æƒ…ç»ªçš„ä¸€è‡´æ€§
                most_common_count = mood_counts.most_common(1)[0][1]
                mood_stability = most_common_count / len(period_moods)
                mood_states['mood_stability'] = mood_stability
            
            # è®¡ç®—æƒ…ç»ªå¼ºåº¦
            all_intensities = []
            for period in emotion_series:
                period_intensity = np.mean(list(period.values())) if period else 0
                all_intensities.append(period_intensity)
            
            if all_intensities:
                mood_states['mood_intensity'] = np.mean(all_intensities)
            
            # æƒ…ç»ªå¹³è¡¡åº¦
            if emotion_series:
                # è®¡ç®—æ­£è´Ÿæƒ…ç»ªçš„å¹³è¡¡
                positive_emotions = ['optimism', 'confidence', 'excitement', 'stability']
                negative_emotions = ['anxiety', 'uncertainty']
                
                positive_scores = []
                negative_scores = []
                
                for period in emotion_series:
                    pos_score = np.mean([period.get(emotion, 0) for emotion in positive_emotions])
                    neg_score = np.mean([period.get(emotion, 0) for emotion in negative_emotions])
                    
                    positive_scores.append(pos_score)
                    negative_scores.append(neg_score)
                
                if positive_scores and negative_scores:
                    avg_positive = np.mean(positive_scores)
                    avg_negative = np.mean(negative_scores)
                    
                    # å¹³è¡¡åº¦ï¼šæ­£è´Ÿæƒ…ç»ªçš„æ¯”ä¾‹
                    total_emotion = avg_positive + avg_negative
                    if total_emotion > 0:
                        balance = 1.0 - abs(avg_positive - avg_negative) / total_emotion
                        mood_states['emotional_balance'] = balance
            
            return mood_states
            
        except Exception as e:
            return {}
    
    def _calculate_emotional_volatility(self, emotion_series: List[Dict]) -> float:
        """è®¡ç®—æƒ…ç»ªæ³¢åŠ¨æ€§"""
        try:
            if len(emotion_series) < 3:
                return 0.0
            
            # è®¡ç®—å„ç§æƒ…ç»ªçš„æ³¢åŠ¨æ€§
            volatilities = []
            
            emotion_types = ['optimism', 'anxiety', 'confidence', 'excitement', 'uncertainty', 'stability']
            
            for emotion_type in emotion_types:
                timeline = [period.get(emotion_type, 0.0) for period in emotion_series]
                
                if len(timeline) > 1:
                    # è®¡ç®—æ ‡å‡†å·®ä½œä¸ºæ³¢åŠ¨æ€§æŒ‡æ ‡
                    volatility = np.std(timeline)
                    volatilities.append(volatility)
            
            if volatilities:
                return np.mean(volatilities)
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _analyze_sentiment_propagation(self, emotion_series: List[Dict], processed_data: List[Dict]) -> Dict:
        """åˆ†ææƒ…æ„Ÿä¼ æ’­"""
        try:
            propagation_analysis = {
                'propagation_speed': 0.0,
                'propagation_reach': 0.0,
                'cascade_events': [],
                'amplification_factor': 0.0
            }
            
            if len(emotion_series) < 5:
                return propagation_analysis
            
            # æ£€æµ‹æƒ…æ„Ÿçº§è”äº‹ä»¶
            cascades = []
            
            for emotion_type in ['optimism', 'anxiety', 'excitement']:
                timeline = [period.get(emotion_type, 0.0) for period in emotion_series]
                
                # æ£€æµ‹æƒ…æ„Ÿçªå˜å’Œä¼ æ’­
                for i in range(len(timeline) - 3):
                    window = timeline[i:i+4]
                    
                    # æ£€æµ‹æ˜¯å¦æœ‰æ˜¾è‘—å¢é•¿
                    if window[0] < 0.3 and window[-1] > 0.7:  # ä»ä½åˆ°é«˜çš„è½¬å˜
                        cascade_event = {
                            'emotion_type': emotion_type,
                            'start_period': i,
                            'end_period': i + 3,
                            'initial_level': window[0],
                            'final_level': window[-1],
                            'amplification': window[-1] / (window[0] + 1e-10)
                        }
                        cascades.append(cascade_event)
            
            propagation_analysis['cascade_events'] = cascades
            
            if cascades:
                # ä¼ æ’­é€Ÿåº¦ï¼šçº§è”äº‹ä»¶çš„å¹³å‡æŒç»­æ—¶é—´
                durations = [event['end_period'] - event['start_period'] for event in cascades]
                avg_duration = np.mean(durations)
                propagation_analysis['propagation_speed'] = 1.0 / (avg_duration + 1)
                
                # ä¼ æ’­è¦†ç›–é¢
                affected_periods = set()
                for event in cascades:
                    for period in range(event['start_period'], event['end_period'] + 1):
                        affected_periods.add(period)
                
                propagation_reach = len(affected_periods) / len(emotion_series)
                propagation_analysis['propagation_reach'] = propagation_reach
                
                # æ”¾å¤§å› å­
                amplifications = [event['amplification'] for event in cascades]
                propagation_analysis['amplification_factor'] = np.mean(amplifications)
            
            return propagation_analysis
            
        except Exception as e:
            return {}
    
    def _calculate_emotional_synchrony(self, emotion_series: List[Dict]) -> float:
        """è®¡ç®—æƒ…ç»ªåŒæ­¥æ€§"""
        try:
            if len(emotion_series) < 3:
                return 0.0
            
            # è®¡ç®—ä¸åŒæƒ…ç»ªé—´çš„åŒæ­¥æ€§
            emotion_types = ['optimism', 'anxiety', 'confidence', 'excitement', 'uncertainty', 'stability']
            
            # æ„å»ºæƒ…ç»ªçŸ©é˜µ
            emotion_matrix = []
            for emotion_type in emotion_types:
                timeline = [period.get(emotion_type, 0.0) for period in emotion_series]
                emotion_matrix.append(timeline)
            
            emotion_matrix = np.array(emotion_matrix)
            
            # è®¡ç®—ç›¸å…³çŸ©é˜µ
            try:
                correlation_matrix = np.corrcoef(emotion_matrix)
                
                # è®¡ç®—å¹³å‡ç›¸å…³æ€§ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
                correlations = []
                for i in range(len(emotion_types)):
                    for j in range(i + 1, len(emotion_types)):
                        correlation = correlation_matrix[i, j]
                        if not np.isnan(correlation):
                            correlations.append(abs(correlation))
                
                if correlations:
                    return np.mean(correlations)
                
            except:
                pass
            
            return 0.0
            
        except Exception as e:
            return 0.0
    
    def _analyze_mood_transitions(self, emotion_series: List[Dict]) -> Dict:
        """åˆ†ææƒ…ç»ªè½¬æ¢"""
        try:
            transition_analysis = {
                'transition_frequency': 0.0,
                'transition_patterns': {},
                'dominant_transitions': [],
                'transition_stability': 0.0
            }
            
            if len(emotion_series) < 4:
                return transition_analysis
            
            # è¯†åˆ«ä¸»å¯¼æƒ…ç»ªè½¬æ¢
            dominant_emotions = []
            for period in emotion_series:
                if period:
                    max_emotion = max(period.items(), key=lambda x: x[1])[0]
                    dominant_emotions.append(max_emotion)
            
            # åˆ†æè½¬æ¢æ¨¡å¼
            transitions = []
            for i in range(len(dominant_emotions) - 1):
                transition = (dominant_emotions[i], dominant_emotions[i + 1])
                transitions.append(transition)
            
            if transitions:
                # è½¬æ¢é¢‘ç‡
                unique_transitions = len(set(transitions))
                transition_frequency = unique_transitions / len(transitions)
                transition_analysis['transition_frequency'] = transition_frequency
                
                # è½¬æ¢æ¨¡å¼ç»Ÿè®¡
                from collections import Counter
                transition_counts = Counter(transitions)
                
                # æœ€å¸¸è§çš„è½¬æ¢
                common_transitions = transition_counts.most_common(3)
                transition_analysis['dominant_transitions'] = [
                    f"{t[0]} -> {t[1]}" for t, count in common_transitions
                ]
                
                # è½¬æ¢æ¨¡å¼
                transition_analysis['transition_patterns'] = dict(transition_counts)
                
                # è½¬æ¢ç¨³å®šæ€§ï¼šè‡ªè½¬æ¢çš„æ¯”ä¾‹
                self_transitions = sum(1 for from_emotion, to_emotion in transitions 
                                     if from_emotion == to_emotion)
                transition_stability = self_transitions / len(transitions)
                transition_analysis['transition_stability'] = transition_stability
            
            return transition_analysis
            
        except Exception as e:
            return {}
    
    def _calculate_emotional_resilience(self, emotion_series: List[Dict]) -> float:
        """è®¡ç®—æƒ…ç»ªéŸ§æ€§"""
        try:
            if len(emotion_series) < 5:
                return 0.0
            
            # åˆ†ææƒ…ç»ªä»è´Ÿé¢çŠ¶æ€æ¢å¤çš„èƒ½åŠ›
            resilience_indicators = []
            
            negative_emotions = ['anxiety', 'uncertainty']
            positive_emotions = ['optimism', 'confidence', 'stability']
            
            for i in range(len(emotion_series) - 3):
                # æ£€æµ‹è´Ÿé¢æƒ…ç»ªé«˜å³°
                current_negative = np.mean([emotion_series[i].get(emotion, 0) for emotion in negative_emotions])
                
                if current_negative > 0.6:  # é«˜è´Ÿé¢æƒ…ç»ª
                    # æ£€æŸ¥åç»­æ¢å¤
                    recovery_scores = []
                    for j in range(i + 1, min(i + 4, len(emotion_series))):
                        period_positive = np.mean([emotion_series[j].get(emotion, 0) for emotion in positive_emotions])
                        period_negative = np.mean([emotion_series[j].get(emotion, 0) for emotion in negative_emotions])
                        
                        # æ¢å¤å¾—åˆ†ï¼šæ­£é¢æƒ…ç»ªå¢åŠ  + è´Ÿé¢æƒ…ç»ªå‡å°‘
                        recovery_score = period_positive - period_negative
                        recovery_scores.append(recovery_score)
                    
                    if recovery_scores:
                        # éŸ§æ€§ï¼šæ¢å¤çš„é€Ÿåº¦å’Œç¨‹åº¦
                        avg_recovery = np.mean(recovery_scores)
                        resilience_indicators.append(max(0.0, avg_recovery))
            
            if resilience_indicators:
                return np.mean(resilience_indicators)
            
            return 0.5  # é»˜è®¤ä¸­ç­‰éŸ§æ€§
            
        except Exception as e:
            return 0.0

    def _perform_multidimensional_bias_analysis(self, processed_data: List[Dict]) -> Dict:
        """
        å¤šç»´è®¤çŸ¥åå·®åˆ†æ - ç»¼åˆåå·®æ£€æµ‹ä¸é‡åŒ–
        åŸºäºè¡Œä¸ºç»æµå­¦å’Œè®¤çŸ¥å¿ƒç†å­¦ç†è®º
        """
        print("   ğŸ§© æ‰§è¡Œå¤šç»´è®¤çŸ¥åå·®åˆ†æ...")
        
        try:
            bias_analysis = {
                'detected_biases': {},
                'bias_strengths': {},
                'bias_interactions': {},
                'temporal_bias_patterns': {},
                'bias_clustering': {},
                'dominant_bias_type': None
            }
            
            # 1. é€ä¸€æ£€æµ‹å„ç±»è®¤çŸ¥åå·®
            for bias_type in CognitiveBias:
                detector_func = self.bias_detection_system.bias_detectors[bias_type.name]['detector_function']
                try:
                    bias_strength = detector_func(processed_data)
                    bias_analysis['bias_strengths'][bias_type.name] = bias_strength
                    
                    # å¦‚æœåå·®å¼ºåº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ ‡è®°ä¸ºæ£€æµ‹åˆ°çš„åå·®
                    threshold = self.bias_detection_system.bias_detectors[bias_type.name]['threshold']
                    if bias_strength > threshold:
                        bias_analysis['detected_biases'][bias_type.name] = {
                            'strength': bias_strength,
                            'confidence': min(1.0, bias_strength * 1.2),
                            'bias_type': bias_type.value
                        }
                except Exception as e:
                    print(f"      âš ï¸ åå·®æ£€æµ‹å¤±è´¥ {bias_type.name}: {e}")
                    bias_analysis['bias_strengths'][bias_type.name] = 0.0
            
            # 2. åå·®äº¤äº’åˆ†æ
            bias_interactions = self._analyze_bias_interactions(bias_analysis['bias_strengths'])
            bias_analysis['bias_interactions'] = bias_interactions
            
            # 3. æ—¶é—´æ¨¡å¼åˆ†æ
            temporal_patterns = self._analyze_temporal_bias_patterns(processed_data, bias_analysis['bias_strengths'])
            bias_analysis['temporal_bias_patterns'] = temporal_patterns
            
            # 4. åå·®èšç±»åˆ†æ
            if SKLEARN_AVAILABLE and len(bias_analysis['bias_strengths']) > 5:
                bias_clustering = self._perform_bias_clustering_analysis(bias_analysis['bias_strengths'])
                bias_analysis['bias_clustering'] = bias_clustering
            
            # 5. ç¡®å®šä¸»å¯¼åå·®ç±»å‹
            if bias_analysis['bias_strengths']:
                dominant_bias = max(bias_analysis['bias_strengths'].items(), key=lambda x: x[1])
                bias_analysis['dominant_bias_type'] = dominant_bias[0]
                bias_analysis['dominant_bias_strength'] = dominant_bias[1]
            
            detected_count = len(bias_analysis['detected_biases'])
            print(f"      âœ“ åå·®åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ° {detected_count} ç§æ˜¾è‘—åå·®")
            
            return bias_analysis
            
        except Exception as e:
            print(f"      âŒ å¤šç»´åå·®åˆ†æå¤±è´¥: {e}")
            return {'error': str(e), 'detected_biases': {}}
    
    def _analyze_bias_interaction_network(self, bias_analysis: Dict) -> Dict:
        """
        åå·®äº¤äº’ç½‘ç»œåˆ†æ - åˆ†æè®¤çŸ¥åå·®ä¹‹é—´çš„ç›¸äº’ä½œç”¨
        åŸºäºç½‘ç»œç†è®ºå’Œç³»ç»ŸåŠ¨åŠ›å­¦
        """
        print("   ğŸ•¸ï¸ æ‰§è¡Œåå·®äº¤äº’ç½‘ç»œåˆ†æ...")
        
        try:
            network_analysis = {
                'interaction_matrix': np.zeros((len(CognitiveBias), len(CognitiveBias))),
                'network_centrality': 0.0,
                'hub_biases': [],
                'bias_clusters': [],
                'network_efficiency': 0.0,
                'cascade_potential': 0.0
            }
            
            bias_strengths = bias_analysis.get('bias_strengths', {})
            if not bias_strengths:
                return network_analysis
            
            # 1. æ„å»ºåå·®äº¤äº’çŸ©é˜µ
            bias_names = list(bias_strengths.keys())
            n_biases = len(bias_names)
            interaction_matrix = np.zeros((n_biases, n_biases))
            
            for i, bias1 in enumerate(bias_names):
                for j, bias2 in enumerate(bias_names):
                    if i != j:
                        # è®¡ç®—åå·®é—´çš„ç›¸å…³æ€§
                        strength1 = bias_strengths[bias1]
                        strength2 = bias_strengths[bias2]
                        
                        # åŸºäºç†è®ºçš„åå·®äº¤äº’å¼ºåº¦
                        interaction_strength = self._calculate_theoretical_bias_interaction(bias1, bias2)
                        
                        # ç»“åˆå®é™…å¼ºåº¦
                        actual_interaction = interaction_strength * min(strength1, strength2)
                        interaction_matrix[i, j] = actual_interaction
            
            network_analysis['interaction_matrix'] = interaction_matrix
            
            # 2. è®¡ç®—ç½‘ç»œä¸­å¿ƒæ€§
            if n_biases > 1:
                # åº¦ä¸­å¿ƒæ€§
                degree_centralities = np.sum(interaction_matrix > 0.1, axis=1)
                max_degree = np.max(degree_centralities)
                network_centrality = max_degree / (n_biases - 1) if n_biases > 1 else 0
                network_analysis['network_centrality'] = network_centrality
                
                # è¯†åˆ«æ¢çº½åå·®
                hub_threshold = np.mean(degree_centralities) + np.std(degree_centralities)
                hub_indices = np.where(degree_centralities >= hub_threshold)[0]
                hub_biases = [bias_names[i] for i in hub_indices]
                network_analysis['hub_biases'] = hub_biases
            
            # 3. åå·®èšç±»
            if SKLEARN_AVAILABLE and n_biases >= 3:
                try:
                    # ä½¿ç”¨å±‚æ¬¡èšç±»
                    condensed_matrix = spatial.distance.squareform(1 - interaction_matrix)
                    linkage_matrix = linkage(condensed_matrix, method='ward')
                    cluster_labels = fcluster(linkage_matrix, t=3, criterion='maxclust')
                    
                    bias_clusters = {}
                    for i, label in enumerate(cluster_labels):
                        if label not in bias_clusters:
                            bias_clusters[label] = []
                        bias_clusters[label].append(bias_names[i])
                    
                    network_analysis['bias_clusters'] = list(bias_clusters.values())
                except Exception as clustering_e:
                    print(f"      âš ï¸ åå·®èšç±»å¤±è´¥: {clustering_e}")
            
            # 4. ç½‘ç»œæ•ˆç‡
            if n_biases > 1:
                efficiency = np.mean(interaction_matrix[interaction_matrix > 0]) if np.any(interaction_matrix > 0) else 0
                network_analysis['network_efficiency'] = efficiency
            
            # 5. çº§è”æ½œåŠ›
            cascade_potential = np.sum(interaction_matrix) / (n_biases * (n_biases - 1)) if n_biases > 1 else 0
            network_analysis['cascade_potential'] = cascade_potential
            
            print(f"      âœ“ ç½‘ç»œåˆ†æå®Œæˆï¼Œç½‘ç»œä¸­å¿ƒæ€§: {network_analysis['network_centrality']:.4f}")
            return network_analysis
            
        except Exception as e:
            print(f"      âŒ åå·®äº¤äº’ç½‘ç»œåˆ†æå¤±è´¥: {e}")
            return {'error': str(e), 'network_centrality': 0.0}
    
    def _analyze_bias_temporal_evolution(self, bias_analysis: Dict, processed_data: List[Dict]) -> Dict:
        """
        åå·®æ—¶é—´æ¼”åŒ–åˆ†æ - åˆ†æè®¤çŸ¥åå·®éšæ—¶é—´çš„å˜åŒ–æ¨¡å¼
        åŸºäºæ—¶é—´åºåˆ—åˆ†æå’ŒåŠ¨æ€ç³»ç»Ÿç†è®º
        """
        print("   ğŸ“Š æ‰§è¡Œåå·®æ—¶é—´æ¼”åŒ–åˆ†æ...")
        
        try:
            evolution_analysis = {
                'evolution_stability': 0.0,
                'trend_analysis': {},
                'cyclical_patterns': {},
                'regime_changes': [],
                'adaptation_rates': {},
                'prediction_horizon': 0
            }
            
            bias_strengths = bias_analysis.get('bias_strengths', {})
            if not bias_strengths or len(processed_data) < 10:
                return evolution_analysis
            
            # 1. æ„å»ºæ—¶é—´åºåˆ—
            window_size = 5
            bias_time_series = {}
            
            for bias_name in bias_strengths.keys():
                time_series = []
                detector_func = self.bias_detection_system.bias_detectors[bias_name]['detector_function']
                
                # æ»‘åŠ¨çª—å£è®¡ç®—åå·®å¼ºåº¦
                for i in range(window_size, len(processed_data)):
                    window_data = processed_data[i-window_size:i]
                    try:
                        bias_strength = detector_func(window_data)
                        time_series.append(bias_strength)
                    except:
                        time_series.append(0.0)
                
                bias_time_series[bias_name] = np.array(time_series)
            
            # 2. æ¼”åŒ–ç¨³å®šæ€§åˆ†æ
            stability_scores = []
            for bias_name, series in bias_time_series.items():
                if len(series) > 1:
                    # è®¡ç®—å˜å¼‚ç³»æ•°
                    cv = np.std(series) / (np.mean(series) + 1e-10)
                    stability = 1.0 / (1.0 + cv)
                    stability_scores.append(stability)
                    evolution_analysis['adaptation_rates'][bias_name] = cv
            
            evolution_analysis['evolution_stability'] = np.mean(stability_scores) if stability_scores else 0.0
            
            # 3. è¶‹åŠ¿åˆ†æ
            for bias_name, series in bias_time_series.items():
                if len(series) >= 3:
                    # çº¿æ€§è¶‹åŠ¿
                    x = np.arange(len(series))
                    trend_slope = np.polyfit(x, series, 1)[0] if len(series) > 1 else 0
                    
                    # è¶‹åŠ¿æ˜¾è‘—æ€§æ£€éªŒ
                    if len(series) > 3:
                        correlation = np.corrcoef(x, series)[0, 1] if not np.isnan(np.corrcoef(x, series)[0, 1]) else 0
                        trend_significance = abs(correlation)
                    else:
                        trend_significance = 0
                    
                    evolution_analysis['trend_analysis'][bias_name] = {
                        'slope': trend_slope,
                        'significance': trend_significance,
                        'direction': 'increasing' if trend_slope > 0.01 else ('decreasing' if trend_slope < -0.01 else 'stable')
                    }
            
            # 4. å‘¨æœŸæ€§æ¨¡å¼æ£€æµ‹
            for bias_name, series in bias_time_series.items():
                if len(series) >= 8:
                    # ç®€åŒ–çš„å‘¨æœŸæ€§æ£€æµ‹
                    autocorrelations = []
                    for lag in range(1, min(len(series)//2, 5)):
                        if len(series) > lag:
                            autocorr = np.corrcoef(series[:-lag], series[lag:])[0, 1]
                            if not np.isnan(autocorr):
                                autocorrelations.append(abs(autocorr))
                    
                    if autocorrelations:
                        max_autocorr = max(autocorrelations)
                        period_strength = max_autocorr
                        dominant_period = autocorrelations.index(max_autocorr) + 1
                        
                        evolution_analysis['cyclical_patterns'][bias_name] = {
                            'period_length': dominant_period,
                            'strength': period_strength,
                            'is_periodic': period_strength > 0.3
                        }
            
            # 5. åˆ¶åº¦å˜åŒ–æ£€æµ‹
            regime_changes = []
            for bias_name, series in bias_time_series.items():
                if len(series) >= 6:
                    # ç®€åŒ–çš„ç»“æ„æ–­ç‚¹æ£€æµ‹
                    changes = self._detect_structural_breaks(series)
                    if changes:
                        regime_changes.extend([{
                            'bias_name': bias_name,
                            'change_point': change,
                            'change_magnitude': abs(series[change] - series[max(0, change-1)])
                        } for change in changes])
            
            evolution_analysis['regime_changes'] = regime_changes
            
            # 6. é¢„æµ‹åœ°å¹³çº¿ä¼°ç®—
            prediction_horizons = []
            for bias_name, series in bias_time_series.items():
                if len(series) >= 5:
                    # åŸºäºè‡ªç›¸å…³è¡°å‡ä¼°ç®—é¢„æµ‹åœ°å¹³çº¿
                    autocorr_decay = self._estimate_autocorrelation_decay(series)
                    horizon = int(1 / max(autocorr_decay, 0.01))
                    prediction_horizons.append(horizon)
            
            evolution_analysis['prediction_horizon'] = int(np.mean(prediction_horizons)) if prediction_horizons else 3
            
            print(f"      âœ“ æ¼”åŒ–åˆ†æå®Œæˆï¼Œæ¼”åŒ–ç¨³å®šæ€§: {evolution_analysis['evolution_stability']:.4f}")
            return evolution_analysis
            
        except Exception as e:
            print(f"      âŒ åå·®æ—¶é—´æ¼”åŒ–åˆ†æå¤±è´¥: {e}")
            return {'error': str(e), 'evolution_stability': 0.0}
    
    # ============ è¾…åŠ©æ–¹æ³• ============
    
    def _identify_individual_decision_patterns(self, processed_data: List[Dict]) -> Dict:
        """è¯†åˆ«ä¸ªä½“å†³ç­–æ¨¡å¼"""
        patterns = {
            'consistency_pattern': 'mixed',
            'risk_preference': 'neutral',
            'choice_diversity': 0.0,
            'decision_speed': 'moderate'
        }
        
        if len(processed_data) < 3:
            return patterns
        
        # åˆ†æé€‰æ‹©ä¸€è‡´æ€§
        tail_counts = [len(period.get('tails', [])) for period in processed_data]
        count_variance = np.var(tail_counts)
        
        if count_variance < 1.0:
            patterns['consistency_pattern'] = 'highly_consistent'
        elif count_variance < 4.0:
            patterns['consistency_pattern'] = 'moderately_consistent'
        else:
            patterns['consistency_pattern'] = 'inconsistent'
        
        # åˆ†æé£é™©åå¥½
        avg_choice_count = np.mean(tail_counts)
        if avg_choice_count <= 3:
            patterns['risk_preference'] = 'risk_averse'
        elif avg_choice_count >= 7:
            patterns['risk_preference'] = 'risk_seeking'
        else:
            patterns['risk_preference'] = 'risk_neutral'
        
        # é€‰æ‹©å¤šæ ·æ€§
        all_chosen_tails = set()
        for period in processed_data:
            all_chosen_tails.update(period.get('tails', []))
        patterns['choice_diversity'] = len(all_chosen_tails) / 10.0
        
        return patterns
    
    def _calculate_choice_consistency_metrics(self, processed_data: List[Dict]) -> Dict:
        """è®¡ç®—é€‰æ‹©ä¸€è‡´æ€§åº¦é‡"""
        metrics = {
            'temporal_consistency': 0.0,
            'choice_stability': 0.0,
            'preference_coherence': 0.0
        }
        
        if len(processed_data) < 3:
            return metrics
        
        # æ—¶é—´ä¸€è‡´æ€§
        similarities = []
        for i in range(len(processed_data) - 1):
            current_tails = set(processed_data[i].get('tails', []))
            next_tails = set(processed_data[i + 1].get('tails', []))
            
            if current_tails or next_tails:
                jaccard = len(current_tails.intersection(next_tails)) / len(current_tails.union(next_tails))
                similarities.append(jaccard)
        
        metrics['temporal_consistency'] = np.mean(similarities) if similarities else 0.0
        
        # é€‰æ‹©ç¨³å®šæ€§
        tail_frequencies = defaultdict(int)
        total_periods = len(processed_data)
        
        for period in processed_data:
            for tail in period.get('tails', []):
                tail_frequencies[tail] += 1
        
        if tail_frequencies:
            freq_values = list(tail_frequencies.values())
            stability = 1.0 - (np.std(freq_values) / (np.mean(freq_values) + 1e-10))
            metrics['choice_stability'] = max(0.0, stability)
        
        return metrics
    
    def _analyze_decision_tree_complexity(self, processed_data: List[Dict], feature_matrix: np.ndarray) -> float:
        """åˆ†æå†³ç­–æ ‘å¤æ‚åº¦"""
        try:
            from sklearn.tree import DecisionTreeClassifier
            
            # æ„å»ºæ ‡ç­¾ï¼ˆä¸‹ä¸€æœŸæ˜¯å¦åŒ…å«ç‰¹å®šå°¾æ•°ï¼‰
            if len(processed_data) < 10:
                return 0.0
            
            X = feature_matrix[:-1]  # å‰n-1æœŸçš„ç‰¹å¾
            
            complexities = []
            for target_tail in range(10):
                # æ„å»ºäºŒåˆ†ç±»æ ‡ç­¾
                y = [1 if target_tail in processed_data[i].get('tails', []) else 0 
                     for i in range(1, len(processed_data))]
                
                if len(set(y)) > 1:  # ç¡®ä¿æœ‰ä¸¤ä¸ªç±»åˆ«
                    # è®­ç»ƒå†³ç­–æ ‘
                    dt = DecisionTreeClassifier(max_depth=5, random_state=42)
                    dt.fit(X, y)
                    
                    # è®¡ç®—æ ‘å¤æ‚åº¦
                    n_nodes = dt.tree_.node_count
                    max_depth = dt.tree_.max_depth
                    complexity = (n_nodes * max_depth) / (len(X) + 1)
                    complexities.append(complexity)
            
            return np.mean(complexities) if complexities else 0.0
            
        except Exception as e:
            return 0.0
    
    def _calculate_cognitive_load_indicators(self, processed_data: List[Dict]) -> Dict:
        """è®¡ç®—è®¤çŸ¥è´Ÿè·æŒ‡æ ‡"""
        indicators = {
            'choice_complexity': 0.0,
            'information_overload': 0.0,
            'decision_fatigue': 0.0,
            'cognitive_strain': 0.0
        }
        
        if len(processed_data) < 5:
            return indicators
        
        # é€‰æ‹©å¤æ‚åº¦
        choice_counts = [len(period.get('tails', [])) for period in processed_data]
        choice_complexity = np.mean(choice_counts) / 10.0  # å½’ä¸€åŒ–
        indicators['choice_complexity'] = choice_complexity
        
        # ä¿¡æ¯è¿‡è½½
        choice_variance = np.var(choice_counts)
        information_overload = choice_variance / (np.mean(choice_counts) + 1e-10)
        indicators['information_overload'] = min(1.0, information_overload)
        
        # å†³ç­–ç–²åŠ³ï¼ˆéšæ—¶é—´é€’å‡çš„é€‰æ‹©è´¨é‡ï¼‰
        if len(processed_data) >= 10:
            early_choices = choice_counts[:5]
            late_choices = choice_counts[-5:]
            fatigue = (np.mean(early_choices) - np.mean(late_choices)) / np.mean(early_choices)
            indicators['decision_fatigue'] = max(0.0, fatigue)
        
        # è®¤çŸ¥ç´§å¼ 
        cognitive_strain = choice_complexity * information_overload
        indicators['cognitive_strain'] = cognitive_strain
        
        return indicators
    
    def _detect_micro_adaptation_signals(self, processed_data: List[Dict]) -> Dict:
        """æ£€æµ‹å¾®è§‚é€‚åº”ä¿¡å·"""
        signals = {
            'learning_signals': [],
            'adaptation_rate': 0.0,
            'flexibility_index': 0.0,
            'innovation_tendency': 0.0
        }
        
        if len(processed_data) < 8:
            return signals
        
        # å­¦ä¹ ä¿¡å·æ£€æµ‹
        for i in range(3, len(processed_data)):
            current_choice = set(processed_data[i].get('tails', []))
            recent_choices = [set(processed_data[j].get('tails', [])) for j in range(i-3, i)]
            
            # æ£€æµ‹æ˜¯å¦æœ‰åŸºäºæœ€è¿‘ç»éªŒçš„è°ƒæ•´
            if recent_choices:
                avg_similarity = np.mean([
                    len(current_choice.intersection(prev_choice)) / max(1, len(current_choice.union(prev_choice)))
                    for prev_choice in recent_choices
                ])
                
                if 0.3 < avg_similarity < 0.7:  # é€‚åº¦å˜åŒ–è¡¨ç¤ºå­¦ä¹ 
                    signals['learning_signals'].append(i)
        
        # é€‚åº”ç‡
        signals['adaptation_rate'] = len(signals['learning_signals']) / len(processed_data)
        
        # çµæ´»æ€§æŒ‡æ•°
        unique_choices = len(set(tuple(sorted(period.get('tails', []))) for period in processed_data))
        max_possible_unique = min(2**10, len(processed_data))  # æœ€å¤§å¯èƒ½çš„ç‹¬ç‰¹é€‰æ‹©æ•°
        signals['flexibility_index'] = unique_choices / max_possible_unique
        
        # åˆ›æ–°å€¾å‘
        innovation_count = 0
        for i in range(1, len(processed_data)):
            current_tails = set(processed_data[i].get('tails', []))
            prev_tails = set(processed_data[i-1].get('tails', []))
            
            new_tails = current_tails - prev_tails
            if len(new_tails) >= 2:  # å¼•å…¥å¤šä¸ªæ–°é€‰æ‹©
                innovation_count += 1
        
        signals['innovation_tendency'] = innovation_count / len(processed_data)
        
        return signals
    
    def _analyze_individual_differences(self, processed_data: List[Dict]) -> Dict:
        """åˆ†æä¸ªä½“å·®å¼‚"""
        differences = {
            'choice_signature': [],
            'preference_stability': 0.0,
            'behavioral_uniqueness': 0.0,
            'decision_style': 'balanced'
        }
        
        if len(processed_data) < 5:
            return differences
        
        # é€‰æ‹©ç­¾åï¼ˆæœ€å¸¸é€‰æ‹©çš„å°¾æ•°ç»„åˆï¼‰
        choice_patterns = defaultdict(int)
        for period in processed_data:
            tails = tuple(sorted(period.get('tails', [])))
            choice_patterns[tails] += 1
        
        if choice_patterns:
            most_common = max(choice_patterns.items(), key=lambda x: x[1])
            differences['choice_signature'] = list(most_common[0])
            
            # åå¥½ç¨³å®šæ€§
            max_frequency = most_common[1]
            differences['preference_stability'] = max_frequency / len(processed_data)
        
        # è¡Œä¸ºç‹¬ç‰¹æ€§
        all_tail_combinations = set()
        for period in processed_data:
            all_tail_combinations.add(tuple(sorted(period.get('tails', []))))
        
        differences['behavioral_uniqueness'] = len(all_tail_combinations) / len(processed_data)
        
        # å†³ç­–é£æ ¼
        avg_choices = np.mean([len(period.get('tails', [])) for period in processed_data])
        choice_variance = np.var([len(period.get('tails', [])) for period in processed_data])
        
        if avg_choices <= 3 and choice_variance < 1:
            differences['decision_style'] = 'conservative'
        elif avg_choices >= 7 and choice_variance < 1:
            differences['decision_style'] = 'aggressive'
        elif choice_variance >= 3:
            differences['decision_style'] = 'variable'
        else:
            differences['decision_style'] = 'balanced'
        
        return differences
    
    def _analyze_crowd_behavior_patterns(self, data_list: List[Dict]) -> Dict:
        """åˆ†æç¾¤ä½“è¡Œä¸ºæ¨¡å¼"""
        analysis_depth = min(20, len(data_list))
        if analysis_depth < 8:
            return {'dominant_behavior': 'insufficient_data', 'behavior_strength': 0.0}
        
        recent_data = data_list[:analysis_depth]
        
        # 1. è®¡ç®—å°¾æ•°é€‰æ‹©é›†ä¸­åº¦
        tail_concentrations = []
        for period in recent_data:
            tails = period.get('tails', [])
            if len(tails) > 0:
                # ä½¿ç”¨åŸºå°¼ç³»æ•°è®¡ç®—é›†ä¸­åº¦
                tail_counts = [0] * 10
                for tail in tails:
                    tail_counts[tail] += 1
                concentration = self._calculate_gini_coefficient(tail_counts)
                tail_concentrations.append(concentration)
        
        avg_concentration = np.mean(tail_concentrations) if tail_concentrations else 0.5
        
        # 2. åˆ†æç¾¤ä½“è·Ÿé£è¡Œä¸º
        herding_signals = self._detect_herding_signals(recent_data)
        
        # 3. åˆ†æé€‰æ‹©å¤šæ ·æ€§
        choice_diversity = self._calculate_choice_diversity(recent_data)
        
        # 4. åˆ†æç¾¤ä½“ä¸€è‡´æ€§
        group_consistency = self._calculate_group_consistency(recent_data)
        
        # 5. è¯†åˆ«ä¸»å¯¼è¡Œä¸ºæ¨¡å¼
        if avg_concentration > 0.7 and herding_signals > 3:
            dominant_behavior = 'extreme_herding'      # æåº¦ä»ä¼—
            behavior_strength = 0.9
        elif avg_concentration > 0.6:
            dominant_behavior = 'strong_consensus'      # å¼ºå…±è¯†
            behavior_strength = 0.75
        elif choice_diversity > 0.7:
            dominant_behavior = 'diverse_selection'     # å¤šæ ·åŒ–é€‰æ‹©
            behavior_strength = 0.6
        elif group_consistency < 0.3:
            dominant_behavior = 'chaotic_behavior'      # æ··ä¹±è¡Œä¸º
            behavior_strength = 0.4
        else:
            dominant_behavior = 'moderate_herding'      # é€‚åº¦ä»ä¼—
            behavior_strength = 0.5
        
        return {
            'dominant_behavior': dominant_behavior,
            'behavior_strength': behavior_strength,
            'concentration_level': avg_concentration,
            'herding_signals': herding_signals,
            'choice_diversity': choice_diversity,
            'group_consistency': group_consistency,
            'analysis_periods': analysis_depth
        }
    
    def _detect_herd_mentality(self, data_list: List[Dict], candidate_tails: List[int]) -> Dict:
        """æ£€æµ‹ä»ä¼—æ•ˆåº”"""
        analysis_depth = min(15, len(data_list))
        recent_data = data_list[:analysis_depth]
        
        herd_indicators = {
            'momentum_herding': 0.0,     # åŠ¨é‡ä»ä¼—
            'popularity_herding': 0.0,   # æµè¡Œåº¦ä»ä¼—  
            'recency_herding': 0.0,      # è¿‘æœŸä»ä¼—
            'frequency_herding': 0.0     # é¢‘ç‡ä»ä¼—
        }
        
        if len(recent_data) < 5:
            return {
                'herd_intensity': 0.0,
                'herd_targets': [],
                'herd_indicators': herd_indicators
            }
        
        # 1. åŠ¨é‡ä»ä¼—æ£€æµ‹
        momentum_scores = {}
        for tail in range(10):
            recent_appearances = [1 if tail in period.get('tails', []) else 0 
                                for period in recent_data[:5]]
            momentum = sum(recent_appearances) / 5.0
            momentum_scores[tail] = momentum
        
        # æ‰¾åˆ°é«˜åŠ¨é‡å°¾æ•°
        high_momentum_tails = [tail for tail, score in momentum_scores.items() 
                              if score > self.crowd_analysis_params['herd_threshold']]
        
        if high_momentum_tails:
            herd_indicators['momentum_herding'] = max(momentum_scores.values())
        
        # 2. æµè¡Œåº¦ä»ä¼—æ£€æµ‹  
        popularity_scores = {}
        for tail in range(10):
            total_appearances = sum(1 for period in recent_data 
                                  if tail in period.get('tails', []))
            popularity = total_appearances / len(recent_data)
            popularity_scores[tail] = popularity
        
        popular_tails = [tail for tail, score in popularity_scores.items() 
                        if score > 0.6]
        
        if popular_tails:
            herd_indicators['popularity_herding'] = max(popularity_scores.values())
        
        # 3. è¿‘æœŸä»ä¼—æ£€æµ‹ï¼ˆæœ€è¿‘3æœŸçš„é›†ä¸­åº¦ï¼‰
        recent_3_data = recent_data[:3]
        recent_tail_counts = defaultdict(int)
        for period in recent_3_data:
            for tail in period.get('tails', []):
                recent_tail_counts[tail] += 1
        
        if recent_tail_counts:
            max_recent_count = max(recent_tail_counts.values())
            herd_indicators['recency_herding'] = max_recent_count / 3.0
        
        # 4. é¢‘ç‡ä»ä¼—æ£€æµ‹ï¼ˆæ£€æµ‹æ˜¯å¦æœ‰å°¾æ•°é¢‘ç¹å‡ºç°ï¼‰
        frequency_analysis = self._analyze_frequency_clustering(recent_data)
        herd_indicators['frequency_herding'] = frequency_analysis['max_cluster_strength']
        
        # 5. è®¡ç®—ç»¼åˆä»ä¼—å¼ºåº¦
        herd_intensity = np.mean(list(herd_indicators.values()))
        
        # 6. ç¡®å®šä»ä¼—ç›®æ ‡
        herd_targets = []
        threshold = self.crowd_analysis_params['herd_threshold']
        
        for tail in range(10):
            tail_herd_score = (
                momentum_scores.get(tail, 0) * 0.3 +
                popularity_scores.get(tail, 0) * 0.3 +
                (recent_tail_counts.get(tail, 0) / 3.0) * 0.2 +
                frequency_analysis['tail_strengths'].get(tail, 0) * 0.2
            )
            
            if tail_herd_score > threshold:
                herd_targets.append({
                    'tail': tail,
                    'herd_score': tail_herd_score,
                    'risk_level': 'high' if tail_herd_score > 0.8 else 'moderate'
                })
        
        # æŒ‰ä»ä¼—ç¨‹åº¦æ’åº
        herd_targets.sort(key=lambda x: x['herd_score'], reverse=True)
        
        return {
            'herd_intensity': herd_intensity,
            'herd_targets': herd_targets,
            'herd_indicators': herd_indicators,
            'high_momentum_tails': high_momentum_tails,
            'popular_tails': popular_tails
        }
    
    def _analyze_psychological_biases(self, data_list: List[Dict], candidate_tails: List[int]) -> Dict:
        """åˆ†æå¿ƒç†åå·®"""
        bias_analysis = {
            'anchoring_bias': 0.0,      # é”šå®šåå·®
            'availability_bias': 0.0,   # å¯å¾—æ€§åå·®
            'confirmation_bias': 0.0,   # ç¡®è®¤åå·®
            'loss_aversion': 0.0,       # æŸå¤±åŒæ¶
            'overconfidence': 0.0,      # è¿‡åº¦è‡ªä¿¡
            'recency_bias': 0.0,        # è¿‘æœŸåå·®
            'hot_hand_fallacy': 0.0     # çƒ­æ‰‹è°¬è¯¯
        }
        
        if len(data_list) < 10:
            return {
                'dominant_bias': 'insufficient_data',
                'bias_strength': 0.0,
                'bias_analysis': bias_analysis,
                'exploitable_biases': []
            }
        
        analysis_data = data_list[:15]  # ä½¿ç”¨æœ€è¿‘15æœŸæ•°æ®
        
        # 1. é”šå®šåå·®æ£€æµ‹
        anchoring_score = self._detect_anchoring_bias(analysis_data)
        bias_analysis['anchoring_bias'] = anchoring_score
        
        # 2. å¯å¾—æ€§åå·®æ£€æµ‹
        availability_score = self._detect_availability_bias(analysis_data)
        bias_analysis['availability_bias'] = availability_score
        
        # 3. ç¡®è®¤åå·®æ£€æµ‹
        confirmation_score = self._detect_confirmation_bias(analysis_data)
        bias_analysis['confirmation_bias'] = confirmation_score
        
        # 4. æŸå¤±åŒæ¶æ£€æµ‹
        loss_aversion_score = self._detect_loss_aversion(analysis_data)
        bias_analysis['loss_aversion'] = loss_aversion_score
        
        # 5. è¿‡åº¦è‡ªä¿¡æ£€æµ‹
        overconfidence_score = self._detect_overconfidence(analysis_data)
        bias_analysis['overconfidence'] = overconfidence_score
        
        # 6. è¿‘æœŸåå·®æ£€æµ‹
        recency_score = self._detect_recency_bias(analysis_data)
        bias_analysis['recency_bias'] = recency_score
        
        # 7. çƒ­æ‰‹è°¬è¯¯æ£€æµ‹
        hot_hand_score = self._detect_hot_hand_fallacy(analysis_data)
        bias_analysis['hot_hand_fallacy'] = hot_hand_score
        
        # 8. æ‰¾åˆ°ä¸»å¯¼åå·®
        dominant_bias_name = max(bias_analysis.keys(), key=lambda k: bias_analysis[k])
        dominant_bias_strength = bias_analysis[dominant_bias_name]
        
        # 9. è¯†åˆ«å¯åˆ©ç”¨çš„åå·®
        exploitable_biases = []
        threshold = self.bias_detection_params['confirmation_bias_threshold']
        
        for bias_name, strength in bias_analysis.items():
            if strength > threshold:
                exploitable_biases.append({
                    'bias_type': bias_name,
                    'strength': strength,
                    'exploitation_potential': min(1.0, strength * 1.2)
                })
        
        # æŒ‰åˆ©ç”¨æ½œåŠ›æ’åº
        exploitable_biases.sort(key=lambda x: x['exploitation_potential'], reverse=True)
        
        return {
            'dominant_bias': dominant_bias_name,
            'bias_strength': dominant_bias_strength,
            'bias_analysis': bias_analysis,
            'exploitable_biases': exploitable_biases,
            'total_bias_score': sum(bias_analysis.values()) / len(bias_analysis)
        }
    
    def _analyze_crowd_emotions(self, data_list: List[Dict]) -> Dict:
        """åˆ†æç¾¤ä½“æƒ…ç»ªçŠ¶æ€"""
        if len(data_list) < 6:
            return {
                'current_state': PsychologyState.NEUTRAL,
                'emotion_intensity': 0.5,
                'emotion_trend': 'stable',
                'fear_greed_index': 0.5
            }
        
        analysis_data = data_list[:12]  # æœ€è¿‘12æœŸ
        
        # 1. è®¡ç®—ææƒ§-è´ªå©ªæŒ‡æ•°
        fear_greed_index = self._calculate_fear_greed_index(analysis_data)
        
        # 2. æ£€æµ‹æƒ…ç»ªæ³¢åŠ¨æ€§
        emotion_volatility = self._calculate_emotion_volatility(analysis_data)
        
        # 3. åˆ†ææƒ…ç»ªè¶‹åŠ¿
        emotion_trend = self._analyze_emotion_trend(analysis_data)
        
        # 4. ç¡®å®šå½“å‰æƒ…ç»ªçŠ¶æ€
        current_state = self._determine_psychology_state(fear_greed_index, emotion_volatility)
        
        # 5. è®¡ç®—æƒ…ç»ªå¼ºåº¦
        emotion_intensity = self._calculate_emotion_intensity(fear_greed_index, emotion_volatility)
        
        # 6. ç¾¤ä½“æƒ…ç»ªæŒç»­æ€§åˆ†æ
        emotion_persistence = self._analyze_emotion_persistence(analysis_data)
        
        return {
            'current_state': current_state,
            'emotion_intensity': emotion_intensity,
            'emotion_trend': emotion_trend,
            'fear_greed_index': fear_greed_index,
            'emotion_volatility': emotion_volatility,
            'emotion_persistence': emotion_persistence,
            'contrarian_signal_strength': self._calculate_contrarian_signal_strength(
                current_state, emotion_intensity, fear_greed_index
            )
        }
    
    def _analyze_market_consensus(self, data_list: List[Dict], candidate_tails: List[int]) -> Dict:
        """åˆ†æå¸‚åœºå…±è¯†åº¦"""
        if len(data_list) < 8:
            return {'consensus_level': 0.5, 'consensus_targets': [], 'danger_level': 'low'}
        
        analysis_data = data_list[:12]
        
        # 1. è®¡ç®—æ•´ä½“å…±è¯†åº¦
        consensus_scores = []
        for period in analysis_data:
            tails = period.get('tails', [])
            if tails:
                # ä½¿ç”¨ç†µæ¥æµ‹é‡å…±è¯†åº¦ï¼ˆç†µè¶Šä½ï¼Œå…±è¯†åº¦è¶Šé«˜ï¼‰
                tail_counts = [0] * 10
                for tail in tails:
                    tail_counts[tail] += 1
                
                # å½’ä¸€åŒ–
                total_count = sum(tail_counts)
                if total_count > 0:
                    probs = [count / total_count for count in tail_counts]
                    entropy = -sum(p * math.log2(p) for p in probs if p > 0)
                    # è½¬æ¢ä¸ºå…±è¯†åº¦ï¼ˆentropyèŒƒå›´0-3.32ï¼Œå…±è¯†åº¦èŒƒå›´1-0ï¼‰
                    consensus = 1.0 - (entropy / 3.32)
                    consensus_scores.append(consensus)
        
        overall_consensus = np.mean(consensus_scores) if consensus_scores else 0.5
        
        # 2. è¯†åˆ«å…±è¯†ç›®æ ‡
        consensus_targets = []
        tail_consensus_scores = {}
        
        for tail in range(10):
            appearances = sum(1 for period in analysis_data 
                            if tail in period.get('tails', []))
            consensus_score = appearances / len(analysis_data)
            tail_consensus_scores[tail] = consensus_score
            
            if consensus_score > self.crowd_analysis_params['consensus_danger_level']:
                consensus_targets.append({
                    'tail': tail,
                    'consensus_score': consensus_score,
                    'danger_level': 'extreme' if consensus_score > 0.9 else 'high'
                })
        
        # 3. è¯„ä¼°å…±è¯†å±é™©ç¨‹åº¦
        if overall_consensus > 0.8:
            danger_level = 'extreme'
        elif overall_consensus > 0.6:
            danger_level = 'high'  
        elif overall_consensus > 0.4:
            danger_level = 'moderate'
        else:
            danger_level = 'low'
        
        return {
            'consensus_level': overall_consensus,
            'consensus_targets': sorted(consensus_targets, key=lambda x: x['consensus_score'], reverse=True),
            'danger_level': danger_level,
            'tail_consensus_scores': tail_consensus_scores,
            'consensus_volatility': np.std(consensus_scores) if consensus_scores else 0.0
        }
    
    def _comprehensive_tail_psychology_analysis(self, tail: int, data_list: List[Dict], 
                                               crowd_behavior: Dict, herd_analysis: Dict,
                                               bias_analysis: Dict, emotion_analysis: Dict,
                                               consensus_analysis: Dict) -> Dict:
        """å¯¹å•ä¸ªå°¾æ•°è¿›è¡Œå…¨é¢å¿ƒç†åˆ†æ"""
        analysis_data = data_list[:15]
        
        # 1. è®¡ç®—ç¾¤ä½“åå¥½ç¨‹åº¦
        crowd_preference = self._calculate_crowd_preference(tail, analysis_data, herd_analysis)
        
        # 2. è¯„ä¼°åå‘æœºä¼š
        contrarian_opportunity = self._evaluate_contrarian_opportunity(
            tail, crowd_preference, emotion_analysis, consensus_analysis
        )
        
        # 3. å¿ƒç†é˜»åŠ›åˆ†æ
        psychological_resistance = self._analyze_psychological_resistance(
            tail, analysis_data, bias_analysis
        )
        
        # 4. ç¾¤ä½“ç–²åŠ³åº¦
        crowd_fatigue = self._calculate_crowd_fatigue(tail, analysis_data, herd_analysis)
        
        # 5. åå‘ç­–ç•¥é€‚åˆåº¦
        contrarian_suitability = self._calculate_contrarian_suitability(
            tail, crowd_preference, crowd_fatigue, emotion_analysis
        )
        
        # 6. å¿ƒç†æ”¯æ’‘/é˜»åŠ›ä½
        psychological_levels = self._identify_psychological_levels(tail, analysis_data)
        
        # 7. ç¾¤ä½“é¢„æœŸåˆ†æ
        crowd_expectation = self._analyze_crowd_expectation(tail, analysis_data, bias_analysis)
        
        return {
            'tail': tail,
            'crowd_preference': crowd_preference,
            'contrarian_opportunity': contrarian_opportunity,
            'psychological_resistance': psychological_resistance,
            'crowd_fatigue': crowd_fatigue,
            'contrarian_suitability': contrarian_suitability,
            'psychological_levels': psychological_levels,
            'crowd_expectation': crowd_expectation,
            'overall_psychology_score': self._calculate_overall_psychology_score(
                crowd_preference, contrarian_opportunity, psychological_resistance, crowd_fatigue
            )
        }
    
    def _generate_contrarian_strategies(self, tail_psychology: Dict, 
                                      crowd_behavior: Dict, emotion_analysis: Dict) -> List[Dict]:
        """ç”Ÿæˆåå‘ç­–ç•¥"""
        strategies = []
        
        for tail, analysis in tail_psychology.items():
            # ç­–ç•¥1ï¼šåä»ä¼—ç­–ç•¥
            if analysis['crowd_preference'] > 0.7 and analysis['crowd_fatigue'] > 0.6:
                confidence = self._calculate_anti_herd_confidence(analysis, emotion_analysis)
                if confidence > 0.5:
                    strategies.append({
                        'tail': tail,
                        'strategy_type': 'anti_herd_contrarian',
                        'confidence': min(0.95, confidence),
                        'reasoning': f'æ£€æµ‹åˆ°å°¾æ•°{tail}å­˜åœ¨ä¸¥é‡ä»ä¼—æ•ˆåº”(åå¥½åº¦:{analysis["crowd_preference"]:.3f})å’Œç¾¤ä½“ç–²åŠ³(ç–²åŠ³åº¦:{analysis["crowd_fatigue"]:.3f})ï¼Œæ‰§è¡Œåä»ä¼—ç­–ç•¥'
                    })
            
            # ç­–ç•¥2ï¼šæƒ…ç»ªåå‘ç­–ç•¥
            emotion_state = emotion_analysis['current_state']
            if emotion_state in [PsychologyState.EUPHORIA, PsychologyState.PANIC]:
                if analysis['contrarian_opportunity'] > 0.6:
                    confidence = self._calculate_emotion_contrarian_confidence(analysis, emotion_analysis)
                    if confidence > 0.4:
                        strategies.append({
                            'tail': tail,
                            'strategy_type': 'emotion_contrarian',
                            'confidence': min(0.9, confidence),
                            'reasoning': f'ç¾¤ä½“æƒ…ç»ªå¤„äºæç«¯çŠ¶æ€({emotion_state.value})ï¼Œå°¾æ•°{tail}å‘ˆç°åå‘æœºä¼š(æœºä¼šåº¦:{analysis["contrarian_opportunity"]:.3f})'
                        })
            
            # ç­–ç•¥3ï¼šå…±è¯†æ¶ˆé€€ç­–ç•¥
            if analysis['psychological_resistance'] > 0.7 and analysis['contrarian_suitability'] > 0.6:
                confidence = self._calculate_consensus_fade_confidence(analysis, crowd_behavior)
                if confidence > 0.5:
                    strategies.append({
                        'tail': tail,
                        'strategy_type': 'consensus_fade',
                        'confidence': min(0.88, confidence),
                        'reasoning': f'å°¾æ•°{tail}é¢ä¸´å¼ºå¿ƒç†é˜»åŠ›(é˜»åŠ›:{analysis["psychological_resistance"]:.3f})ï¼Œé¢„æœŸå…±è¯†æ¶ˆé€€ï¼Œé€‚åˆåå‘æ“ä½œ(é€‚åˆåº¦:{analysis["contrarian_suitability"]:.3f})'
                    })
            
            # ç­–ç•¥4ï¼šåå·®åˆ©ç”¨ç­–ç•¥
            if analysis['crowd_expectation']['bias_influenced'] and analysis['contrarian_opportunity'] > 0.5:
                confidence = self._calculate_bias_exploitation_confidence(analysis)
                if confidence > 0.45:
                    strategies.append({
                        'tail': tail,
                        'strategy_type': 'bias_exploitation',
                        'confidence': min(0.85, confidence),
                        'reasoning': f'æ£€æµ‹åˆ°å°¾æ•°{tail}å—ç¾¤ä½“è®¤çŸ¥åå·®å½±å“ä¸¥é‡ï¼Œå­˜åœ¨åå‘åˆ©ç”¨æœºä¼š(æœŸæœ›åå·®:{analysis["crowd_expectation"]["expectation_bias"]:.3f})'
                    })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        strategies.sort(key=lambda x: x['confidence'], reverse=True)
        
        return strategies[:3]  # è¿”å›æœ€å¤š3ä¸ªæœ€ä½³ç­–ç•¥
    
    # ç¡®è®¤åå·®ç›¸å…³è¾…åŠ©æ–¹æ³•
    def _analyze_evidence_selectivity(self, decision_data, prior_beliefs=None):
        """åˆ†æè¯æ®é€‰æ‹©æ€§"""
        try:
            import numpy as np
            
            # ç®€åŒ–çš„è¯æ®é€‰æ‹©æ€§åˆ†æ
            if not decision_data:
                return {'selectivity_score': 0.5}
            
            # æ¨¡æ‹Ÿè¯æ®é€‰æ‹©æ¨¡å¼åˆ†æ
            supporting_evidence_ratio = 0.7  # ç®€åŒ–è®¡ç®—
            opposing_evidence_ratio = 0.3
            
            selectivity_score = supporting_evidence_ratio / (supporting_evidence_ratio + opposing_evidence_ratio)
            
            return {
                'selectivity_score': float(selectivity_score),
                'supporting_evidence_ratio': float(supporting_evidence_ratio),
                'opposing_evidence_ratio': float(opposing_evidence_ratio),
                'evidence_balance': 'biased' if selectivity_score > 0.7 else 'balanced'
            }
        except:
            return {'selectivity_score': 0.5}

    def _analyze_information_processing_bias(self, decision_data):
        """åˆ†æä¿¡æ¯å¤„ç†åå·®"""
        try:
            import numpy as np
            
            # åŸºäºå†³ç­–æ•°æ®çš„ä¿¡æ¯å¤„ç†åˆ†æ
            data_variance = np.var([len(str(d)) for d in decision_data]) if decision_data else 1
            processing_complexity = min(1.0, data_variance / 10.0)
            
            bias_score = 0.6 + processing_complexity * 0.3  # ç®€åŒ–è®¡ç®—
            
            return {
                'bias_score': float(bias_score),
                'processing_complexity': float(processing_complexity),
                'attention_focus': 'narrow' if bias_score > 0.7 else 'broad',
                'information_filtering': 'selective' if bias_score > 0.6 else 'comprehensive'
            }
        except:
            return {'bias_score': 0.5}

    def _analyze_belief_persistence(self, decision_data, prior_beliefs=None):
        """åˆ†æä¿¡å¿µæŒç»­æ€§"""
        try:
            # ä¿¡å¿µå˜åŒ–åˆ†æ
            if prior_beliefs is None:
                # å¦‚æœæ²¡æœ‰å…ˆéªŒä¿¡å¿µï¼ŒåŸºäºå†³ç­–æ¨¡å¼åˆ†æ
                persistence_score = 0.6
            else:
                # è®¡ç®—ä¿¡å¿µå˜åŒ–ç¨‹åº¦
                belief_changes = len([d for d in decision_data if 'change' in str(d).lower()])
                total_decisions = len(decision_data)
                change_rate = belief_changes / total_decisions if total_decisions > 0 else 0
                persistence_score = 1.0 - change_rate
        
            return {
                'persistence_score': float(persistence_score),
                'belief_flexibility': 'low' if persistence_score > 0.7 else 'high',
                'change_resistance': 'strong' if persistence_score > 0.8 else 'moderate'
            }
        except:
            return {'persistence_score': 0.6}

    def _analyze_contradictory_evidence_neglect(self, decision_data, evidence_weights=None):
        """åˆ†æåé©³è¯æ®å¿½è§†"""
        try:
            import numpy as np
            
            # åé©³è¯æ®å¤„ç†åˆ†æ
            if evidence_weights:
                negative_evidence_weight = np.mean([w for w in evidence_weights if w < 0])
                positive_evidence_weight = np.mean([w for w in evidence_weights if w > 0])
                
                if positive_evidence_weight > 0:
                    neglect_score = abs(negative_evidence_weight) / positive_evidence_weight
                    neglect_score = min(1.0, neglect_score)
                else:
                    neglect_score = 0.5
            else:
                # åŸºäºå†³ç­–æ•°æ®æ¨æ–­
                neglect_score = 0.4  # ç®€åŒ–å€¼
            
            return {
                'neglect_score': float(neglect_score),
                'contradictory_processing': 'poor' if neglect_score > 0.6 else 'good',
                'evidence_integration': 'biased' if neglect_score > 0.5 else 'balanced'
            }
        except:
            return {'neglect_score': 0.5}

    def _determine_bias_strength(self, bias_score):
        """ç¡®å®šåå·®å¼ºåº¦"""
        if bias_score >= 0.8:
            return "æå¼ºåå·®"
        elif bias_score >= 0.7:
            return "å¼ºåå·®"
        elif bias_score >= 0.6:
            return "ä¸­ç­‰åå·®"
        elif bias_score >= 0.4:
            return "è½»å¾®åå·®"
        else:
            return "æ— æ˜æ˜¾åå·®"

    def _analyze_belief_updating_quality(self, decision_data, prior_beliefs=None):
        """åˆ†æä¿¡å¿µæ›´æ–°è´¨é‡"""
        try:
            # ä¿¡å¿µæ›´æ–°åˆ†æ
            update_frequency = len([d for d in decision_data if 'update' in str(d).lower()])
            total_opportunities = len(decision_data)
            update_rate = update_frequency / total_opportunities if total_opportunities > 0 else 0
            
            return {
                'update_rate': float(update_rate),
                'update_quality': 'good' if update_rate > 0.3 else 'poor',
                'bayesian_updating': 'approximate' if update_rate > 0.2 else 'minimal',
                'information_integration': 'adaptive' if update_rate > 0.4 else 'rigid'
            }
        except:
            return {'update_rate': 0.2}

    def _test_bias_significance(self, bias_indicators):
        """æµ‹è¯•åå·®æ˜¾è‘—æ€§"""
        try:
            import numpy as np
            
            # ç®€åŒ–çš„æ˜¾è‘—æ€§æ£€éªŒ
            bias_values = list(bias_indicators.values())
            mean_bias = np.mean(bias_values)
            std_bias = np.std(bias_values)
            
            # å•æ ·æœ¬tæ£€éªŒè¿‘ä¼¼
            t_statistic = (mean_bias - 0.5) / (std_bias / np.sqrt(len(bias_values))) if std_bias > 0 else 0
            p_value = 0.05 if abs(t_statistic) > 1.96 else 0.1  # ç®€åŒ–è®¡ç®—
            
            return {
                'mean_bias': float(mean_bias),
                'bias_variability': float(std_bias),
                't_statistic': float(t_statistic),
                'p_value': float(p_value),
                'is_significant': p_value < 0.05
            }
        except:
            return {'is_significant': False}

    def _suggest_bias_mitigation_strategies(self, bias_score):
        """å»ºè®®åå·®ç¼“è§£ç­–ç•¥"""
        strategies = []
        
        if bias_score > 0.7:
            strategies.extend([
                "å®æ–½ç»“æ„åŒ–å†³ç­–æ¡†æ¶",
                "å¯»æ±‚åå¯¹æ„è§å’Œè´¨ç–‘",
                "ä½¿ç”¨é­”é¬¼ä»£è¨€äººç­–ç•¥",
                "è¿›è¡Œé¢„é˜²æ€§æ€è€ƒç»ƒä¹ "
            ])
        elif bias_score > 0.5:
            strategies.extend([
                "å¢åŠ ä¿¡æ¯æ¥æºå¤šæ ·æ€§",
                "å®šæœŸå®¡æŸ¥å†³ç­–è¿‡ç¨‹",
                "åŸ¹å…»æ‰¹åˆ¤æ€§æ€ç»´æŠ€èƒ½"
            ])
        else:
            strategies.append("ç»´æŒå½“å‰å†³ç­–è´¨é‡")
        
        return strategies

    def _generate_confirmation_bias_recommendations(self, bias_score, bias_indicators, evidence_selectivity, belief_updating):
        """ç”Ÿæˆç¡®è®¤åå·®å»ºè®®"""
        recommendations = []
        
        if bias_score > 0.7:
            recommendations.append("ç¡®è®¤åå·®ä¸¥é‡ï¼Œéœ€è¦ç³»ç»Ÿæ€§æ”¹è¿›å†³ç­–è¿‡ç¨‹")
        
        if evidence_selectivity.get('selectivity_score', 0) > 0.7:
            recommendations.append("è¯æ®é€‰æ‹©è¿‡äºåå‘ï¼Œåº”ä¸»åŠ¨å¯»æ±‚åé©³è¯æ®")
        
        if belief_updating.get('update_rate', 0) < 0.3:
            recommendations.append("ä¿¡å¿µæ›´æ–°ä¸è¶³ï¼Œåº”æé«˜å¯¹æ–°ä¿¡æ¯çš„å¼€æ”¾æ€§")
        
        if not recommendations:
            recommendations.append("ç¡®è®¤åå·®æ§åˆ¶è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå®¢è§‚å†³ç­–")
        
        return recommendations

    # é”šå®šåå·®ç›¸å…³è¾…åŠ©æ–¹æ³•
    def _identify_potential_anchors(self, decision_sequence, initial_anchors=None):
        """è¯†åˆ«æ½œåœ¨é”šç‚¹"""
        try:
            import numpy as np
            
            anchors = []
            
            if initial_anchors:
                anchors.extend(initial_anchors)
            
            # ä»å†³ç­–åºåˆ—ä¸­è¯†åˆ«å¯èƒ½çš„é”šç‚¹
            if decision_sequence:
                first_value = decision_sequence[0] if hasattr(decision_sequence[0], '__iter__') else decision_sequence[0]
                anchors.append(first_value)
                
                # æ·»åŠ æå€¼ä½œä¸ºæ½œåœ¨é”šç‚¹
                if len(decision_sequence) > 2:
                    numeric_values = [float(str(d)[:3]) if str(d) else 0 for d in decision_sequence]
                    anchors.extend([max(numeric_values), min(numeric_values)])
            
            return anchors[:5]  # é™åˆ¶é”šç‚¹æ•°é‡
        except:
            return [0.5]  # é»˜è®¤é”šç‚¹

    def _calculate_anchoring_effect(self, decision_sequence, anchor):
        """è®¡ç®—é”šå®šæ•ˆåº”å¼ºåº¦"""
        try:
            import numpy as np
            
            # ç®€åŒ–çš„é”šå®šæ•ˆåº”è®¡ç®—
            numeric_decisions = []
            for d in decision_sequence:
                try:
                    num_val = float(str(d)[:3]) if str(d) else 0.5
                    numeric_decisions.append(num_val)
                except:
                    numeric_decisions.append(0.5)
            
            if not numeric_decisions:
                return 0.5
            
            # è®¡ç®—ä¸é”šç‚¹çš„ç›¸å…³æ€§
            anchor_val = float(str(anchor)[:3]) if str(anchor) else 0.5
            correlations = [abs(decision - anchor_val) for decision in numeric_decisions]
            effect_strength = 1.0 - (np.mean(correlations) / max(numeric_decisions) if max(numeric_decisions) > 0 else 0.5)
            
            return max(0, min(1, effect_strength))
        except:
            return 0.5

    def _analyze_insufficient_adjustment(self, decision_sequence, potential_anchors):
        """åˆ†æè°ƒæ•´ä¸å……åˆ†"""
        try:
            import numpy as np
            
            if not decision_sequence or not potential_anchors:
                return {'adjustment_adequacy': 0.5}
            
            # è®¡ç®—ä»é”šç‚¹çš„è°ƒæ•´ç¨‹åº¦
            adjustments = []
            for anchor in potential_anchors:
                anchor_val = float(str(anchor)[:3]) if str(anchor) else 0.5
                for decision in decision_sequence:
                    decision_val = float(str(decision)[:3]) if str(decision) else 0.5
                    adjustment = abs(decision_val - anchor_val)
                    adjustments.append(adjustment)
            
            avg_adjustment = np.mean(adjustments) if adjustments else 0.5
            adjustment_adequacy = min(1.0, avg_adjustment / 0.5)  # å½’ä¸€åŒ–
            
            return {
                'adjustment_adequacy': float(adjustment_adequacy),
                'average_adjustment': float(avg_adjustment),
                'insufficient_adjustment': adjustment_adequacy < 0.3
            }
        except:
            return {'adjustment_adequacy': 0.5}

    def _analyze_temporal_anchoring_pattern(self, decision_sequence):
        """åˆ†ææ—¶é—´é”šå®šæ¨¡å¼"""
        try:
            # æ—¶é—´æ¨¡å¼åˆ†æ
            return {
                'early_anchoring_influence': 0.7,  # ç®€åŒ–å€¼
                'persistence_over_time': 0.6,
                'decay_pattern': 'slow' if len(decision_sequence) > 5 else 'fast'
            }
        except:
            return {'early_anchoring_influence': 0.5}

    def _interpret_anchoring_strength(self, anchoring_score):
        """è§£é‡Šé”šå®šå¼ºåº¦"""
        if anchoring_score >= 0.8:
            return "æå¼ºé”šå®šæ•ˆåº”"
        elif anchoring_score >= 0.6:
            return "å¼ºé”šå®šæ•ˆåº”"
        elif anchoring_score >= 0.4:
            return "ä¸­ç­‰é”šå®šæ•ˆåº”"
        else:
            return "è½»å¾®é”šå®šæ•ˆåº”"

    def _generate_anchoring_bias_recommendations(self, anchoring_score):
        """ç”Ÿæˆé”šå®šåå·®å»ºè®®"""
        recommendations = []
        
        if anchoring_score > 0.7:
            recommendations.extend([
                "ä½¿ç”¨å¤šä¸ªå‚è€ƒç‚¹è¿›è¡Œæ¯”è¾ƒ",
                "å»¶è¿Ÿåˆå§‹åˆ¤æ–­çš„å½¢æˆ",
                "é‡‡ç”¨ç³»ç»Ÿæ€§è¯„ä¼°æ–¹æ³•"
            ])
        elif anchoring_score > 0.5:
            recommendations.extend([
                "å¢åŠ ä¿¡æ¯æ”¶é›†çš„å¹¿åº¦",
                "è€ƒè™‘æç«¯æƒ…å†µå’Œæ›¿ä»£æ–¹æ¡ˆ"
            ])
        else:
            recommendations.append("é”šå®šæ•ˆåº”æ§åˆ¶è‰¯å¥½")
        
        return recommendations

    # å¯å¾—æ€§å¯å‘å¼ç›¸å…³è¾…åŠ©æ–¹æ³•
    def _analyze_memory_influence_on_decisions(self, memory_data, decision_data):
        """åˆ†æè®°å¿†å¯¹å†³ç­–çš„å½±å“"""
        try:
            # è®°å¿†å½±å“å¼ºåº¦åˆ†æ
            influence_score = 0.6  # ç®€åŒ–è®¡ç®—
            
            return {
                'influence_score': float(influence_score),
                'memory_accessibility': 'high' if influence_score > 0.6 else 'moderate',
                'decision_bias': 'present' if influence_score > 0.5 else 'minimal'
            }
        except:
            return {'influence_score': 0.5}

    def _detect_recency_effect(self, memory_data, decision_data, recency_weights=None):
        """æ£€æµ‹è¿‘æœŸæ€§æ•ˆåº”"""
        try:
            import numpy as np
            
            if recency_weights:
                effect_strength = np.mean(recency_weights[-3:]) / np.mean(recency_weights[:-3]) if len(recency_weights) > 3 else 1.0
            else:
                # åŸºäºå†³ç­–æ•°æ®æ¨æ–­è¿‘æœŸæ€§æ•ˆåº”
                effect_strength = 0.7  # ç®€åŒ–å€¼
            
            return {
                'effect_strength': float(effect_strength),
                'recency_bias': 'strong' if effect_strength > 1.2 else 'moderate',
                'temporal_weighting': 'biased' if effect_strength > 1.1 else 'balanced'
            }
        except:
            return {'effect_strength': 1.0}

    def _analyze_vividness_bias(self, memory_data, decision_data):
        """åˆ†æç”ŸåŠ¨æ€§åå·®"""
        try:
            # ç”ŸåŠ¨æ€§åå·®åˆ†æ
            bias_score = 0.5  # ç®€åŒ–å€¼
            
            return {
                'bias_score': float(bias_score),
                'emotional_influence': 'high' if bias_score > 0.6 else 'moderate',
                'memory_distortion': 'present' if bias_score > 0.5 else 'minimal'
            }
        except:
            return {'bias_score': 0.5}

    def _analyze_frequency_probability_bias(self, memory_data, decision_data):
        """åˆ†æé¢‘ç‡æ¦‚ç‡åå·®"""
        try:
            # é¢‘ç‡ä¸æ¦‚ç‡ä¼°è®¡åå·®
            bias_score = 0.4  # ç®€åŒ–å€¼
            
            return {
                'bias_score': float(bias_score),
                'frequency_overestimation': bias_score > 0.5,
                'probability_miscalibration': 'present' if bias_score > 0.4 else 'minimal'
            }
        except:
            return {'bias_score': 0.4}

    def _interpret_availability_bias_strength(self, availability_score):
        """è§£é‡Šå¯å¾—æ€§åå·®å¼ºåº¦"""
        if availability_score >= 0.8:
            return "æå¼ºå¯å¾—æ€§åå·®"
        elif availability_score >= 0.6:
            return "å¼ºå¯å¾—æ€§åå·®"
        elif availability_score >= 0.4:
            return "ä¸­ç­‰å¯å¾—æ€§åå·®"
        else:
            return "è½»å¾®å¯å¾—æ€§åå·®"

    def _identify_cognitive_shortcuts(self, decision_data):
        """è¯†åˆ«è®¤çŸ¥æ·å¾„"""
        shortcuts = []
        
        if len(decision_data) < 5:
            shortcuts.append("å¿«é€Ÿå†³ç­–æ¨¡å¼")
        
        # åŸºäºå†³ç­–æ¨¡å¼è¯†åˆ«
        shortcuts.extend(["å¯å‘å¼å¤„ç†", "æ¨¡å¼è¯†åˆ«", "ç»éªŒä¾èµ–"])
        
        return shortcuts

    def _generate_availability_bias_recommendations(self, availability_score):
        """ç”Ÿæˆå¯å¾—æ€§åå·®å»ºè®®"""
        recommendations = []
        
        if availability_score > 0.7:
            recommendations.extend([
                "ä¸»åŠ¨å¯»æ±‚ç»Ÿè®¡æ•°æ®æ”¯æŒ",
                "ä½¿ç”¨ç»“æ„åŒ–ä¿¡æ¯æ”¶é›†æ–¹æ³•",
                "å‡å°‘å¯¹è®°å¿†çš„ä¾èµ–"
            ])
        elif availability_score > 0.5:
            recommendations.extend([
                "å¢åŠ ä¿¡æ¯æ¥æºå¤šæ ·æ€§",
                "éªŒè¯ç›´è§‰åˆ¤æ–­çš„å‡†ç¡®æ€§"
            ])
        else:
            recommendations.append("å¯å¾—æ€§åå·®æ§åˆ¶è‰¯å¥½")
        
        return recommendations

    # å…¶ä»–åå·®ç±»å‹çš„åŸºç¡€è¾…åŠ©æ–¹æ³•
    def _analyze_base_rate_neglect(self, categorization_data, base_rate_data=None):
        """åˆ†æåŸºç¡€æ¦‚ç‡å¿½è§†"""
        return {'neglect_score': 0.5}

    def _analyze_sample_size_neglect(self, categorization_data):
        """åˆ†ææ ·æœ¬å¤§å°å¿½è§†"""
        return {'neglect_score': 0.4}

    def _analyze_regression_to_mean_neglect(self, categorization_data):
        """åˆ†æå›å½’å¹³å‡å¿½è§†"""
        return {'neglect_score': 0.3}

    def _analyze_conjunction_fallacy(self, categorization_data):
        """åˆ†æè”åˆäº‹ä»¶æ¦‚ç‡è¯¯åˆ¤"""
        return {'fallacy_score': 0.3}

    def _interpret_representativeness_bias_strength(self, score):
        """è§£é‡Šä»£è¡¨æ€§åå·®å¼ºåº¦"""
        return "ä¸­ç­‰ä»£è¡¨æ€§åå·®" if score > 0.5 else "è½»å¾®ä»£è¡¨æ€§åå·®"

    def _analyze_stereotype_reliance(self, categorization_data):
        """åˆ†æåˆ»æ¿å°è±¡ä¾èµ–"""
        return {'reliance_level': 'moderate'}

    def _generate_representativeness_bias_recommendations(self, score):
        """ç”Ÿæˆä»£è¡¨æ€§åå·®å»ºè®®"""
        return ["è€ƒè™‘åŸºç¡€æ¦‚ç‡", "æ³¨æ„æ ·æœ¬å¤§å°", "é¿å…è¿‡åº¦æ¦‚æ‹¬"]

    # è¿‡åº¦è‡ªä¿¡ç›¸å…³æ–¹æ³•
    def _analyze_confidence_calibration(self, confidence_ratings, actual_performance):
        """åˆ†æç½®ä¿¡åº¦æ ¡å‡†"""
        return {'calibration_score': 0.6}

    def _analyze_overconfidence_levels(self, confidence_ratings, actual_performance):
        """åˆ†æè¿‡åº¦è‡ªä¿¡æ°´å¹³"""
        return {'overconfidence_level': 0.5}

    def _analyze_difficulty_effect_on_confidence(self, confidence_ratings, actual_performance):
        """åˆ†æå›°éš¾åº¦å¯¹ç½®ä¿¡åº¦çš„å½±å“"""
        return {'difficulty_effect': 'moderate'}

    def _analyze_metacognitive_accuracy(self, confidence_ratings, actual_performance):
        """åˆ†æå…ƒè®¤çŸ¥å‡†ç¡®æ€§"""
        return {'accuracy_score': 0.6}

    def _calculate_overconfidence_score(self, calibration_analysis, overconfidence_analysis):
        """è®¡ç®—è¿‡åº¦è‡ªä¿¡å¾—åˆ†"""
        return 0.5

    def _interpret_overconfidence_strength(self, score):
        """è§£é‡Šè¿‡åº¦è‡ªä¿¡å¼ºåº¦"""
        return "ä¸­ç­‰è¿‡åº¦è‡ªä¿¡" if score > 0.5 else "è½»å¾®è¿‡åº¦è‡ªä¿¡"

    def _calculate_confidence_intervals(self, confidence_ratings):
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        return {'lower': 0.3, 'upper': 0.7}

    def _detect_better_than_average_effect(self, confidence_ratings):
        """æ£€æµ‹ä¼˜äºå¹³å‡æ•ˆåº”"""
        return {'effect_present': True, 'strength': 'moderate'}

    def _generate_overconfidence_bias_recommendations(self, score):
        """ç”Ÿæˆè¿‡åº¦è‡ªä¿¡åå·®å»ºè®®"""
        return ["æ ¡å‡†ç½®ä¿¡åº¦åˆ¤æ–­", "å¯»æ±‚åé¦ˆ", "ä½¿ç”¨ç»“æ„åŒ–è¯„ä¼°"]

    # å…¶ä»–æ–¹æ³•çš„ç®€åŒ–å®ç°
    def _calculate_loss_aversion_coefficient(self, choice_data, gains_losses_data):
        """è®¡ç®—æŸå¤±åŒæ¶ç³»æ•°"""
        return {'coefficient': 2.0}

    def _analyze_reference_point_dependence(self, choice_data, reference_points):
        """åˆ†æå‚è€ƒç‚¹ä¾èµ–"""
        return {'dependence_level': 'high'}

    def _analyze_endowment_effect(self, choice_data):
        """åˆ†æç¦€èµ‹æ•ˆåº”"""
        return {'effect_strength': 'moderate'}

    def _analyze_framing_effect(self, choice_data, gains_losses_data):
        """åˆ†ææ¡†æ¶æ•ˆåº”"""
        return {'effect_present': True}

    def _analyze_risk_preference_inconsistency(self, choice_data, gains_losses_data):
        """åˆ†æé£é™©åå¥½ä¸ä¸€è‡´æ€§"""
        return {'inconsistency_level': 'moderate'}

    def _calculate_comprehensive_loss_aversion_score(self, coefficient, reference_dependence, endowment):
        """è®¡ç®—ç»¼åˆæŸå¤±åŒæ¶å¾—åˆ†"""
        return 0.6

    def _interpret_loss_aversion_strength(self, score):
        """è§£é‡ŠæŸå¤±åŒæ¶å¼ºåº¦"""
        return "ä¸­ç­‰æŸå¤±åŒæ¶" if score > 0.5 else "è½»å¾®æŸå¤±åŒæ¶"

    def _assess_prospect_theory_fit(self, choice_data, gains_losses_data):
        """è¯„ä¼°å‰æ™¯ç†è®ºæ‹Ÿåˆåº¦"""
        return {'fit_quality': 'good'}

    def _generate_loss_aversion_bias_recommendations(self, score):
        """ç”ŸæˆæŸå¤±åŒæ¶åå·®å»ºè®®"""
        return ["ç†æ€§è¯„ä¼°å¾—å¤±", "æ‰©å¤§å‚è€ƒæ¡†æ¶", "è€ƒè™‘é•¿æœŸåæœ"]

    # æ²‰æ²¡æˆæœ¬ç›¸å…³æ–¹æ³•
    def _analyze_sunk_cost_influence(self, investment_data, decision_points, cost_information):
        """åˆ†ææ²‰æ²¡æˆæœ¬å½±å“"""
        return {'influence_score': 0.5}

    def _analyze_escalation_of_commitment(self, investment_data, decision_points):
        """åˆ†ææ‰¿è¯ºå‡çº§"""
        return {'escalation_level': 'moderate'}

    def _analyze_rational_decision_deviation(self, investment_data, decision_points, cost_information):
        """åˆ†æç†æ€§å†³ç­–åç¦»"""
        return {'deviation_score': 0.4}

    def _analyze_time_investment_effect(self, investment_data, decision_points):
        """åˆ†ææ—¶é—´æŠ•å…¥æ•ˆåº”"""
        return {'effect_strength': 'moderate'}

    def _calculate_sunk_cost_fallacy_score(self, influence, escalation, deviation):
        """è®¡ç®—æ²‰æ²¡æˆæœ¬è°¬è¯¯å¾—åˆ†"""
        return 0.5

    def _interpret_sunk_cost_fallacy_strength(self, score):
        """è§£é‡Šæ²‰æ²¡æˆæœ¬è°¬è¯¯å¼ºåº¦"""
        return "ä¸­ç­‰æ²‰æ²¡æˆæœ¬è°¬è¯¯" if score > 0.5 else "è½»å¾®æ²‰æ²¡æˆæœ¬è°¬è¯¯"

    def _analyze_cost_sensitivity(self, cost_information):
        """åˆ†ææˆæœ¬æ•æ„Ÿæ€§"""
        return {'sensitivity_level': 'moderate'}

    def _generate_sunk_cost_fallacy_recommendations(self, score):
        """ç”Ÿæˆæ²‰æ²¡æˆæœ¬è°¬è¯¯å»ºè®®"""
        return ["å…³æ³¨æœªæ¥æˆæœ¬æ•ˆç›Š", "å¿½ç•¥å·²å‘ç”Ÿæˆæœ¬", "è®¾ç½®æ˜ç¡®é€€å‡ºæ ‡å‡†"]

    # ç¾¤ä½“æ€ç»´ç›¸å…³æ–¹æ³•
    def _analyze_conformity_pressure(self, group_decision_data, individual_preferences):
        """åˆ†æä¸€è‡´æ€§å‹åŠ›"""
        return {'pressure_level': 'moderate'}

    def _analyze_dissent_suppression(self, group_decision_data, dissent_data):
        """åˆ†æå¼‚è®®æŠ‘åˆ¶"""
        return {'suppression_level': 'low'}

    def _analyze_information_diversity(self, group_decision_data):
        """åˆ†æä¿¡æ¯å¤šæ ·æ€§"""
        return {'diversity_score': 0.6}

    def _analyze_group_polarization(self, group_decision_data, individual_preferences):
        """åˆ†æç¾¤ä½“æåŒ–"""
        return {'polarization_level': 'moderate'}

    def _analyze_leader_influence(self, group_decision_data):
        """åˆ†æé¢†å¯¼è€…å½±å“"""
        return {'influence_strength': 'high'}

    def _calculate_groupthink_score(self, conformity, dissent, diversity):
        """è®¡ç®—ç¾¤ä½“æ€ç»´å¾—åˆ†"""
        return 0.5

    def _interpret_groupthink_strength(self, score):
        """è§£é‡Šç¾¤ä½“æ€ç»´å¼ºåº¦"""
        return "ä¸­ç­‰ç¾¤ä½“æ€ç»´å€¾å‘" if score > 0.5 else "è½»å¾®ç¾¤ä½“æ€ç»´å€¾å‘"

    def _assess_group_decision_quality(self, group_decision_data):
        """è¯„ä¼°ç¾¤ä½“å†³ç­–è´¨é‡"""
        return {'quality_score': 0.6}

    def _generate_groupthink_bias_recommendations(self, score):
        """ç”Ÿæˆç¾¤ä½“æ€ç»´åå·®å»ºè®®"""
        return ["é¼“åŠ±å¼‚è®®è¡¨è¾¾", "å¢åŠ ä¿¡æ¯æ¥æº", "ä½¿ç”¨ç»“æ„åŒ–å†³ç­–è¿‡ç¨‹", "è®¾ç½®é­”é¬¼ä»£è¨€äºº"]

    # ============ æ ¸å¿ƒè®¡ç®—æ–¹æ³• ============
    
    def _calculate_gini_coefficient(self, values: List[int]) -> float:
        """è®¡ç®—åŸºå°¼ç³»æ•°ï¼ˆè¡¡é‡åˆ†å¸ƒä¸å‡ç¨‹åº¦ï¼‰"""
        if not values or sum(values) == 0:
            return 0.0
        
        sorted_values = sorted([x for x in values if x >= 0])
        n = len(sorted_values)
        total = sum(sorted_values)
        
        if total == 0:
            return 0.0
        
        numerator = sum((i + 1) * value for i, value in enumerate(sorted_values))
        return (2 * numerator) / (n * total) - (n + 1) / n
    
    def _detect_herding_signals(self, data_list: List[Dict]) -> int:
        """æ£€æµ‹ä»ä¼—ä¿¡å·æ•°é‡"""
        signals = 0
        
        if len(data_list) < 4:
            return signals
        
        # æ£€æµ‹è¿ç»­çš„çƒ­é—¨å°¾æ•°é‡å¤
        for i in range(len(data_list) - 1):
            current_tails = set(data_list[i].get('tails', []))
            next_tails = set(data_list[i + 1].get('tails', []))
            
            # å¦‚æœæœ‰3ä¸ªæˆ–ä»¥ä¸Šç›¸åŒå°¾æ•°ï¼Œè®¤ä¸ºæ˜¯ä»ä¼—ä¿¡å·
            overlap = len(current_tails.intersection(next_tails))
            if overlap >= 3:
                signals += 1
        
        return signals
    
    def _calculate_choice_diversity(self, data_list: List[Dict]) -> float:
        """è®¡ç®—é€‰æ‹©å¤šæ ·æ€§"""
        if len(data_list) < 3:
            return 0.5
        
        all_tails = set()
        period_diversities = []
        
        for period in data_list:
            tails = set(period.get('tails', []))
            all_tails.update(tails)
            
            # è®¡ç®—è¯¥æœŸçš„å¤šæ ·æ€§ï¼ˆä¸åŒå°¾æ•°çš„æ•°é‡ / æ€»å¯èƒ½å°¾æ•°ï¼‰
            if tails:
                diversity = len(tails) / 10.0
                period_diversities.append(diversity)
        
        return np.mean(period_diversities) if period_diversities else 0.5
    
    def _calculate_group_consistency(self, data_list: List[Dict]) -> float:
        """è®¡ç®—ç¾¤ä½“ä¸€è‡´æ€§"""
        if len(data_list) < 3:
            return 0.5
        
        # è®¡ç®—ç›¸é‚»æœŸä¹‹é—´çš„ç›¸ä¼¼åº¦
        similarities = []
        for i in range(len(data_list) - 1):
            current_tails = set(data_list[i].get('tails', []))
            next_tails = set(data_list[i + 1].get('tails', []))
            
            if current_tails and next_tails:
                intersection = len(current_tails.intersection(next_tails))
                union = len(current_tails.union(next_tails))
                similarity = intersection / union if union > 0 else 0.0
                similarities.append(similarity)
        
        return np.mean(similarities) if similarities else 0.5
    
    def _analyze_frequency_clustering(self, data_list: List[Dict]) -> Dict:
        """åˆ†æé¢‘ç‡èšç±»"""
        tail_frequencies = defaultdict(int)
        
        for period in data_list:
            for tail in period.get('tails', []):
                tail_frequencies[tail] += 1
        
        if not tail_frequencies:
            return {'max_cluster_strength': 0.0, 'tail_strengths': {}}
        
        max_freq = max(tail_frequencies.values())
        total_periods = len(data_list)
        
        tail_strengths = {}
        for tail, freq in tail_frequencies.items():
            strength = freq / total_periods
            tail_strengths[tail] = strength
        
        max_cluster_strength = max_freq / total_periods
        
        return {
            'max_cluster_strength': max_cluster_strength,
            'tail_strengths': tail_strengths
        }
    
    def _detect_anchoring_bias(self, data_list: List[Dict]) -> float:
        """æ£€æµ‹é”šå®šåå·®"""
        if len(data_list) < 6:
            return 0.0
        
        # å¯»æ‰¾å¯èƒ½çš„é”šå®šç‚¹ï¼ˆæ¯”å¦‚ç¬¬ä¸€æœŸçš„å°¾æ•°ï¼‰
        anchor_tails = set(data_list[-1].get('tails', []))  # ä½¿ç”¨æœ€æ—©æœŸä½œä¸ºé”šç‚¹
        
        anchoring_effects = 0
        for period in data_list[:-1]:
            current_tails = set(period.get('tails', []))
            overlap = len(current_tails.intersection(anchor_tails))
            if overlap >= 2:  # å¦‚æœæœ‰2ä¸ªä»¥ä¸Šç›¸åŒï¼Œè®¤ä¸ºæœ‰é”šå®šæ•ˆåº”
                anchoring_effects += 1
        
        return min(1.0, anchoring_effects / len(data_list[:-1]))
    
    def _detect_availability_bias(self, data_list: List[Dict]) -> float:
        """æ£€æµ‹å¯å¾—æ€§åå·®ï¼ˆå®¹æ˜“å›å¿†çš„äº‹ä»¶è¢«è®¤ä¸ºæ›´å¯èƒ½å‘ç”Ÿï¼‰"""
        if len(data_list) < 5:
            return 0.0
        
        # æœ€è¿‘æœŸçš„å°¾æ•°æ›´å®¹æ˜“è¢«è®°ä½å’Œé€‰æ‹©
        recent_tails = set()
        for period in data_list[:3]:  # æœ€è¿‘3æœŸ
            recent_tails.update(period.get('tails', []))
        
        availability_score = 0
        for period in data_list[3:]:  # åç»­æœŸæ•°
            current_tails = set(period.get('tails', []))
            overlap = len(current_tails.intersection(recent_tails))
            if overlap >= 2:
                availability_score += 1
        
        return min(1.0, availability_score / max(1, len(data_list) - 3))
    
    def _detect_confirmation_bias(self, data_list: List[Dict]) -> float:
        """æ£€æµ‹ç¡®è®¤åå·®ï¼ˆå€¾å‘äºå¯»æ‰¾æ”¯æŒå·²æœ‰è§‚ç‚¹çš„ä¿¡æ¯ï¼‰"""
        if len(data_list) < 8:
            return 0.0
        
        # æ£€æµ‹æ˜¯å¦æœ‰æŒç»­çš„æ¨¡å¼å¼ºåŒ–
        pattern_confirmations = 0
        
        # å¯»æ‰¾æ—©æœŸæ¨¡å¼
        early_pattern = set(data_list[-3:][0].get('tails', []))  # æ—©æœŸæ¨¡å¼
        
        for i in range(len(data_list) - 3):
            current_tails = set(data_list[i].get('tails', []))
            overlap = len(current_tails.intersection(early_pattern))
            if overlap >= 2:
                pattern_confirmations += 1
        
        return min(1.0, pattern_confirmations / max(1, len(data_list) - 3))
    
    def _detect_loss_aversion(self, data_list: List[Dict]) -> float:
        """æ£€æµ‹æŸå¤±åŒæ¶ï¼ˆå¯¹æŸå¤±æ¯”å¯¹æ”¶ç›Šæ›´æ•æ„Ÿï¼‰"""
        if len(data_list) < 6:
            return 0.0
        
        # åˆ†æè¿ç»­å¤±è´¥åçš„ä¿å®ˆè¡Œä¸º
        conservative_behaviors = 0
        
        for i in range(len(data_list) - 2):
            current_tails = set(data_list[i].get('tails', []))
            next_tails = set(data_list[i + 1].get('tails', []))
            
            # å¦‚æœä¸‹ä¸€æœŸé€‰æ‹©æ›´å°‘çš„å°¾æ•°ï¼ˆä¿å®ˆè¡Œä¸ºï¼‰
            if len(next_tails) < len(current_tails) - 1:
                conservative_behaviors += 1
        
        return min(1.0, conservative_behaviors / max(1, len(data_list) - 2))
    
    def _detect_overconfidence(self, data_list: List[Dict]) -> float:
        """æ£€æµ‹è¿‡åº¦è‡ªä¿¡"""
        if len(data_list) < 5:
            return 0.0
        
        # æ£€æµ‹æ˜¯å¦æœ‰è¿‡åº¦é›†ä¸­çš„é€‰æ‹©ï¼ˆè¿‡åº¦è‡ªä¿¡çš„è¡¨ç°ï¼‰
        overconfident_periods = 0
        
        for period in data_list:
            tails = period.get('tails', [])
            # å¦‚æœé€‰æ‹©çš„å°¾æ•°è¿‡å°‘ï¼ˆè¿‡åº¦è‡ªä¿¡ï¼‰æˆ–è¿‡å¤šï¼ˆè¿‡åº¦ä¿å®ˆä¹Ÿæ˜¯è¿‡åº¦è‡ªä¿¡çš„åé¢ï¼‰
            if len(tails) <= 3 or len(tails) >= 8:
                overconfident_periods += 1
        
        return min(1.0, overconfident_periods / len(data_list))
    
    def _detect_recency_bias(self, data_list: List[Dict]) -> float:
        """æ£€æµ‹è¿‘æœŸåå·®ï¼ˆè¿‡åº¦é‡è§†æœ€è¿‘çš„ä¿¡æ¯ï¼‰"""
        if len(data_list) < 5:
            return 0.0
        
        # æœ€è¿‘2æœŸçš„å°¾æ•°
        recent_tails = set()
        for period in data_list[:2]:
            recent_tails.update(period.get('tails', []))
        
        recency_influences = 0
        for period in data_list[2:5]:  # æ£€æŸ¥æ¥ä¸‹æ¥çš„3æœŸ
            current_tails = set(period.get('tails', []))
            overlap = len(current_tails.intersection(recent_tails))
            if overlap >= 3:  # å¦‚æœæœ‰3ä¸ªä»¥ä¸Šç›¸åŒ
                recency_influences += 1
        
        return min(1.0, recency_influences / 3.0)
    
    def _detect_hot_hand_fallacy(self, data_list: List[Dict]) -> float:
        """æ£€æµ‹çƒ­æ‰‹è°¬è¯¯ï¼ˆè®¤ä¸ºè¿ç»­æˆåŠŸä¼šç»§ç»­æˆåŠŸï¼‰"""
        if len(data_list) < 6:
            return 0.0
        
        hot_hand_behaviors = 0
        
        # å¯»æ‰¾è¿ç»­å‡ºç°çš„å°¾æ•°ï¼Œç„¶åæ£€æŸ¥æ˜¯å¦ç»§ç»­è¢«é€‰æ‹©
        for tail in range(10):
            consecutive_appearances = []
            current_streak = 0
            
            for period in reversed(data_list):  # ä»æœ€æ—©å¼€å§‹
                if tail in period.get('tails', []):
                    current_streak += 1
                else:
                    if current_streak >= 2:  # è¿ç»­å‡ºç°2æ¬¡ä»¥ä¸Š
                        consecutive_appearances.append(current_streak)
                    current_streak = 0
            
            if current_streak >= 2:
                consecutive_appearances.append(current_streak)
            
            # å¦‚æœæœ‰è¿ç»­å‡ºç°ï¼Œæ£€æŸ¥åç»­æ˜¯å¦ç»§ç»­è¢«é€‰æ‹©
            if consecutive_appearances:
                hot_hand_behaviors += len(consecutive_appearances)
        
        return min(1.0, hot_hand_behaviors / 10.0)  # å½’ä¸€åŒ–
    
    def _calculate_fear_greed_index(self, data_list: List[Dict]) -> float:
        """è®¡ç®—ææƒ§-è´ªå©ªæŒ‡æ•°"""
        if len(data_list) < 4:
            return 0.5  # ä¸­æ€§
        
        indicators = []
        
        # 1. é€‰æ‹©é›†ä¸­åº¦ï¼ˆè´ªå©ªæŒ‡æ ‡ï¼‰
        concentration_scores = []
        for period in data_list:
            tails = period.get('tails', [])
            if tails:
                # é›†ä¸­åº¦è¶Šé«˜ï¼Œè´ªå©ªç¨‹åº¦è¶Šé«˜
                unique_count = len(set(tails))
                concentration = 1.0 - (unique_count / 10.0)
                concentration_scores.append(concentration)
        
        if concentration_scores:
            avg_concentration = np.mean(concentration_scores)
            indicators.append(avg_concentration)
        
        # 2. é€‰æ‹©æ•°é‡å˜åŒ–ï¼ˆææƒ§æŒ‡æ ‡ï¼‰
        choice_counts = [len(period.get('tails', [])) for period in data_list]
        if len(choice_counts) >= 2:
            count_volatility = np.std(choice_counts) / max(1, np.mean(choice_counts))
            # æ³¢åŠ¨æ€§è¶Šå¤§ï¼Œææƒ§ç¨‹åº¦è¶Šé«˜
            indicators.append(min(1.0, count_volatility))
        
        # 3. çƒ­é—¨è¿½é€å€¾å‘ï¼ˆè´ªå©ªæŒ‡æ ‡ï¼‰
        hot_chasing = self._calculate_hot_chasing_tendency(data_list)
        indicators.append(hot_chasing)
        
        # 4. ä¿å®ˆç¨‹åº¦ï¼ˆææƒ§æŒ‡æ ‡ï¼‰
        conservatism = self._calculate_conservatism_level(data_list)
        indicators.append(1.0 - conservatism)  # è½¬æ¢ä¸ºè´ªå©ªæŒ‡æ ‡
        
        # ç»¼åˆæŒ‡æ•°ï¼š0=æåº¦ææƒ§ï¼Œ1=æåº¦è´ªå©ª
        fear_greed_index = np.mean(indicators) if indicators else 0.5
        return min(1.0, max(0.0, fear_greed_index))
    
    def _determine_psychology_state(self, fear_greed_index: float, volatility: float) -> PsychologyState:
        """æ ¹æ®ææƒ§è´ªå©ªæŒ‡æ•°å’Œæ³¢åŠ¨æ€§ç¡®å®šå¿ƒç†çŠ¶æ€"""
        # ç»“åˆææƒ§è´ªå©ªæŒ‡æ•°å’Œæ³¢åŠ¨æ€§
        if fear_greed_index > 0.9 and volatility > 0.7:
            return PsychologyState.EUPHORIA
        elif fear_greed_index > 0.8:
            return PsychologyState.GREED
        elif fear_greed_index > 0.6:
            return PsychologyState.OPTIMISM
        elif fear_greed_index > 0.5:
            return PsychologyState.HOPE
        elif fear_greed_index > 0.4:
            return PsychologyState.NEUTRAL
        elif fear_greed_index > 0.3:
            return PsychologyState.ANXIETY
        elif fear_greed_index > 0.2:
            return PsychologyState.FEAR
        elif fear_greed_index > 0.1 and volatility > 0.6:
            return PsychologyState.PANIC
        else:
            return PsychologyState.DESPAIR
    
    def _calculate_crowd_preference(self, tail: int, data_list: List[Dict], herd_analysis: Dict) -> float:
        """è®¡ç®—ç¾¤ä½“å¯¹ç‰¹å®šå°¾æ•°çš„åå¥½ç¨‹åº¦"""
        if not data_list:
            return 0.0
        
        # 1. åŸºç¡€å‡ºç°é¢‘ç‡
        appearances = sum(1 for period in data_list if tail in period.get('tails', []))
        base_preference = appearances / len(data_list)
        
        # 2. ä»ä¼—æ•ˆåº”åŠ æƒ
        herd_targets = herd_analysis.get('herd_targets', [])
        herd_multiplier = 1.0
        for target in herd_targets:
            if target['tail'] == tail:
                herd_multiplier = 1.0 + target['herd_score']
                break
        
        # 3. è¿‘æœŸåå¥½åŠ æƒ
        recent_appearances = sum(1 for period in data_list[:5] 
                               if tail in period.get('tails', []))
        recent_preference = recent_appearances / min(5, len(data_list))
        
        # 4. ç»¼åˆåå¥½è®¡ç®—
        crowd_preference = (base_preference * 0.4 + recent_preference * 0.6) * herd_multiplier
        
        return min(1.0, crowd_preference)
    
    def _evaluate_contrarian_opportunity(self, tail: int, crowd_preference: float, 
                                       emotion_analysis: Dict, consensus_analysis: Dict) -> float:
        """è¯„ä¼°åå‘æœºä¼š"""
        opportunity_factors = []
        
        # 1. åŸºäºç¾¤ä½“åå¥½çš„åå‘æœºä¼š
        if crowd_preference > 0.7:
            # é«˜åå¥½çš„åå‘æœºä¼š
            opportunity_factors.append(crowd_preference * 0.8)
        elif crowd_preference < 0.3:
            # ä½åå¥½çš„åå‘æœºä¼šï¼ˆå¯èƒ½è¢«å¿½è§†ï¼‰
            opportunity_factors.append((1.0 - crowd_preference) * 0.6)
        else:
            # ä¸­ç­‰åå¥½ï¼Œåå‘æœºä¼šè¾ƒå°
            opportunity_factors.append(0.3)
        
        # 2. åŸºäºæƒ…ç»ªæç«¯çŠ¶æ€çš„åå‘æœºä¼š
        emotion_state = emotion_analysis['current_state']
        if emotion_state in [PsychologyState.EUPHORIA, PsychologyState.PANIC]:
            opportunity_factors.append(0.8)
        elif emotion_state in [PsychologyState.GREED, PsychologyState.FEAR]:
            opportunity_factors.append(0.6)
        else:
            opportunity_factors.append(0.3)
        
        # 3. åŸºäºå…±è¯†åº¦çš„åå‘æœºä¼š
        consensus_level = consensus_analysis.get('consensus_level', 0.5)
        if consensus_level > 0.7:
            opportunity_factors.append(consensus_level * 0.9)
        else:
            opportunity_factors.append(0.2)
        
        return np.mean(opportunity_factors)
    
    def _record_prediction(self, recommended_tail: int, confidence: float, strategy_type: str,
                         tail_psychology: Dict, emotion_analysis: Dict):
        """è®°å½•é¢„æµ‹è¯¦æƒ…"""
        prediction_record = {
            'timestamp': datetime.now(),
            'recommended_tail': recommended_tail,
            'confidence': confidence,
            'strategy_type': strategy_type,
            'crowd_emotion': emotion_analysis['current_state'].value,
            'fear_greed_index': emotion_analysis['fear_greed_index'],
            'psychology_snapshot': tail_psychology.get(recommended_tail, {})
        }
        
        self.prediction_history.append(prediction_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        if len(self.prediction_history) > 100:
            self.prediction_history = self.prediction_history[-50:]
    
    # ============ å…¶ä»–è¾…åŠ©æ–¹æ³• ============
    
    def _calculate_emotion_volatility(self, data_list: List[Dict]) -> float:
        """è®¡ç®—æƒ…ç»ªæ³¢åŠ¨æ€§"""
        # åŸºäºé€‰æ‹©è¡Œä¸ºçš„å˜åŒ–æ¥æ¨æ–­æƒ…ç»ªæ³¢åŠ¨
        choice_changes = []
        
        for i in range(len(data_list) - 1):
            current_tails = set(data_list[i].get('tails', []))
            next_tails = set(data_list[i + 1].get('tails', []))
            
            # è®¡ç®—é€‰æ‹©å˜åŒ–ç¨‹åº¦
            total_choices = len(current_tails.union(next_tails))
            common_choices = len(current_tails.intersection(next_tails))
            
            if total_choices > 0:
                change_rate = 1.0 - (common_choices / total_choices)
                choice_changes.append(change_rate)
        
        return np.std(choice_changes) if choice_changes else 0.0
    
    def _analyze_emotion_trend(self, data_list: List[Dict]) -> str:
        """åˆ†ææƒ…ç»ªè¶‹åŠ¿"""
        if len(data_list) < 6:
            return 'stable'
        
        # è®¡ç®—æœ€è¿‘æœŸå’Œå†å²æœŸçš„æƒ…ç»ªæŒ‡æ ‡å·®å¼‚
        recent_fear_greed = self._calculate_fear_greed_index(data_list[:3])
        historical_fear_greed = self._calculate_fear_greed_index(data_list[3:6])
        
        diff = recent_fear_greed - historical_fear_greed
        
        if diff > 0.1:
            return 'increasingly_greedy'
        elif diff < -0.1:
            return 'increasingly_fearful'
        else:
            return 'stable'
    
    def _calculate_hot_chasing_tendency(self, data_list: List[Dict]) -> float:
        """è®¡ç®—çƒ­é—¨è¿½é€å€¾å‘"""
        if len(data_list) < 4:
            return 0.0
        
        hot_chasing_behaviors = 0
        
        for i in range(len(data_list) - 1):
            current_tails = data_list[i].get('tails', [])
            prev_tails = data_list[i + 1].get('tails', [])
            
            # å¦‚æœå½“å‰æœŸé€‰æ‹©äº†ä¸ŠæœŸçš„çƒ­é—¨å°¾æ•°
            overlap = len(set(current_tails).intersection(set(prev_tails)))
            if overlap >= 3:  # æœ‰3ä¸ªä»¥ä¸Šç›¸åŒè®¤ä¸ºæ˜¯è¿½é€è¡Œä¸º
                hot_chasing_behaviors += 1
        
        return hot_chasing_behaviors / max(1, len(data_list) - 1)
    
    def _calculate_conservatism_level(self, data_list: List[Dict]) -> float:
        """è®¡ç®—ä¿å®ˆç¨‹åº¦"""
        if not data_list:
            return 0.5
        
        choice_counts = [len(period.get('tails', [])) for period in data_list]
        avg_choices = np.mean(choice_counts)
        
        # é€‰æ‹©æ•°é‡ç¨³å®šä¸”é€‚ä¸­è®¤ä¸ºæ˜¯ä¿å®ˆ
        choice_stability = 1.0 - (np.std(choice_counts) / max(1, avg_choices))
        moderate_choice = 1.0 - abs(avg_choices - 5.0) / 5.0  # 5æ˜¯ä¸­ç­‰é€‰æ‹©æ•°é‡
        
        return (choice_stability + moderate_choice) / 2.0
    
    # ============ æ›´å¤šè¾…åŠ©è®¡ç®—æ–¹æ³• ============
    
    def _calculate_emotion_intensity(self, fear_greed_index: float, volatility: float) -> float:
        """è®¡ç®—æƒ…ç»ªå¼ºåº¦"""
        # è·ç¦»ä¸­æ€§ç‚¹(0.5)è¶Šè¿œï¼Œå¼ºåº¦è¶Šé«˜
        deviation_from_neutral = abs(fear_greed_index - 0.5)
        intensity = (deviation_from_neutral * 2.0 + volatility) / 2.0
        return min(1.0, intensity)
    
    def _analyze_emotion_persistence(self, data_list: List[Dict]) -> float:
        """åˆ†ææƒ…ç»ªæŒç»­æ€§"""
        if len(data_list) < 6:
            return 0.5
        
        # è®¡ç®—æƒ…ç»ªæŒ‡æ ‡çš„è‡ªç›¸å…³æ€§
        emotion_indicators = []
        for i in range(0, len(data_list), 2):  # æ¯2æœŸè®¡ç®—ä¸€æ¬¡
            if i + 1 < len(data_list):
                period_data = data_list[i:i+2]
                emotion_score = self._calculate_fear_greed_index(period_data)
                emotion_indicators.append(emotion_score)
        
        if len(emotion_indicators) < 3:
            return 0.5
        
        # è®¡ç®—è¿ç»­æ€§ï¼ˆç›¸é‚»æƒ…ç»ªæŒ‡æ ‡çš„ç›¸ä¼¼æ€§ï¼‰
        similarities = []
        for i in range(len(emotion_indicators) - 1):
            similarity = 1.0 - abs(emotion_indicators[i] - emotion_indicators[i+1])
            similarities.append(similarity)
        
        return np.mean(similarities) if similarities else 0.5
    
    def _calculate_contrarian_signal_strength(self, emotion_state: PsychologyState, 
                                            emotion_intensity: float, fear_greed_index: float) -> float:
        """è®¡ç®—åå‘ä¿¡å·å¼ºåº¦"""
        # æç«¯æƒ…ç»ªçŠ¶æ€ä¸‹åå‘ä¿¡å·æ›´å¼º
        extreme_states = [PsychologyState.EUPHORIA, PsychologyState.PANIC, 
                         PsychologyState.GREED, PsychologyState.DESPAIR]
        
        if emotion_state in extreme_states:
            base_strength = 0.8
        elif emotion_state in [PsychologyState.FEAR, PsychologyState.OPTIMISM]:
            base_strength = 0.6
        else:
            base_strength = 0.3
        
        # ç»“åˆæƒ…ç»ªå¼ºåº¦å’Œæç«¯ç¨‹åº¦
        extremeness = abs(fear_greed_index - 0.5) * 2.0
        signal_strength = base_strength * emotion_intensity * (1 + extremeness)
        
        return min(1.0, signal_strength)
    
    def _analyze_psychological_resistance(self, tail: int, data_list: List[Dict], bias_analysis: Dict) -> float:
        """åˆ†æå¿ƒç†é˜»åŠ›"""
        resistance_factors = []
        
        # 1. åŸºäºå†å²å¤±è´¥çš„é˜»åŠ›
        recent_failures = 0
        for period in data_list[:5]:
            if tail not in period.get('tails', []):
                recent_failures += 1
        
        failure_resistance = recent_failures / 5.0
        resistance_factors.append(failure_resistance)
        
        # 2. åŸºäºè®¤çŸ¥åå·®çš„é˜»åŠ›
        bias_resistance = bias_analysis.get('total_bias_score', 0.5)
        if tail in [0, 5]:  # ç‰¹æ®Šæ•°å­—çš„å¿ƒç†é˜»åŠ›
            bias_resistance *= 1.2
        resistance_factors.append(bias_resistance)
        
        # 3. åŸºäºç¾¤ä½“è®°å¿†çš„é˜»åŠ›
        long_term_appearances = sum(1 for period in data_list 
                                  if tail in period.get('tails', []))
        memory_resistance = 1.0 - (long_term_appearances / len(data_list))
        resistance_factors.append(memory_resistance)
        
        return np.mean(resistance_factors)
    
    def _calculate_crowd_fatigue(self, tail: int, data_list: List[Dict], herd_analysis: Dict) -> float:
        """è®¡ç®—ç¾¤ä½“ç–²åŠ³åº¦"""
        if len(data_list) < 5:
            return 0.0
        
        # 1. åŸºäºè¿ç»­å…³æ³¨çš„ç–²åŠ³
        consecutive_attention = 0
        for period in data_list:
            if tail in period.get('tails', []):
                consecutive_attention += 1
            else:
                break
        
        attention_fatigue = min(1.0, consecutive_attention / 8.0)
        
        # 2. åŸºäºæ€»ä½“å…³æ³¨åº¦çš„ç–²åŠ³
        total_attention = sum(1 for period in data_list if tail in period.get('tails', []))
        attention_rate = total_attention / len(data_list)
        
        if attention_rate > 0.6:
            rate_fatigue = attention_rate
        else:
            rate_fatigue = 0.0
        
        # 3. åŸºäºä»ä¼—æ•ˆåº”çš„ç–²åŠ³
        herd_fatigue = 0.0
        for target in herd_analysis.get('herd_targets', []):
            if target['tail'] == tail:
                herd_fatigue = target['herd_score'] * 0.8
                break
        
        return max(attention_fatigue, rate_fatigue, herd_fatigue)
    
    def _calculate_contrarian_suitability(self, tail: int, crowd_preference: float, 
                                        crowd_fatigue: float, emotion_analysis: Dict) -> float:
        """è®¡ç®—åå‘ç­–ç•¥é€‚åˆåº¦"""
        suitability_factors = []
        
        # 1. åŸºäºç¾¤ä½“åå¥½çš„é€‚åˆåº¦
        if crowd_preference > 0.7:  # é«˜åå¥½ï¼Œé€‚åˆåå‘
            suitability_factors.append(crowd_preference)
        elif crowd_preference < 0.3:  # ä½åå¥½ï¼Œå¯èƒ½è¢«ä½ä¼°
            suitability_factors.append(0.7)
        else:
            suitability_factors.append(0.3)
        
        # 2. åŸºäºç¾¤ä½“ç–²åŠ³çš„é€‚åˆåº¦
        suitability_factors.append(crowd_fatigue)
        
        # 3. åŸºäºæƒ…ç»ªçŠ¶æ€çš„é€‚åˆåº¦
        contrarian_signal = emotion_analysis.get('contrarian_signal_strength', 0.5)
        suitability_factors.append(contrarian_signal)
        
        # 4. åŸºäºå¿ƒç†å‘¨æœŸçš„é€‚åˆåº¦
        cycle_suitability = self._calculate_psychological_cycle_suitability(tail)
        suitability_factors.append(cycle_suitability)
        
        return np.mean(suitability_factors)
    
    def _identify_psychological_levels(self, tail: int, data_list: List[Dict]) -> Dict:
        """è¯†åˆ«å¿ƒç†æ”¯æ’‘/é˜»åŠ›ä½"""
        if len(data_list) < 10:
            return {'support_level': 0.3, 'resistance_level': 0.7}
        
        # è®¡ç®—å†å²å‡ºç°é¢‘ç‡åˆ†å¸ƒ
        appearances = [1 if tail in period.get('tails', []) else 0 for period in data_list]
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—æ”¯æ’‘é˜»åŠ›ä½
        window_size = 5
        frequency_windows = []
        
        for i in range(len(appearances) - window_size + 1):
            window_freq = sum(appearances[i:i + window_size]) / window_size
            frequency_windows.append(window_freq)
        
        if frequency_windows:
            support_level = min(frequency_windows) + 0.1  # å†å²æœ€ä½ç‚¹ä¸Šæ–¹
            resistance_level = max(frequency_windows) - 0.1  # å†å²æœ€é«˜ç‚¹ä¸‹æ–¹
        else:
            support_level = 0.3
            resistance_level = 0.7
        
        return {
            'support_level': max(0.0, support_level),
            'resistance_level': min(1.0, resistance_level),
            'current_level': sum(appearances[:5]) / 5.0  # å½“å‰5æœŸé¢‘ç‡
        }
    
    def _analyze_crowd_expectation(self, tail: int, data_list: List[Dict], bias_analysis: Dict) -> Dict:
        """åˆ†æç¾¤ä½“é¢„æœŸ"""
        if len(data_list) < 5:
            return {
                'expectation_level': 0.5,
                'expectation_bias': 0.0,
                'bias_influenced': False
            }
        
        # 1. åŸºäºæœ€è¿‘è¡¨ç°çš„é¢„æœŸ
        recent_performance = sum(1 for period in data_list[:3] 
                               if tail in period.get('tails', []))
        recent_expectation = recent_performance / 3.0
        
        # 2. åŸºäºé•¿æœŸè¡¨ç°çš„é¢„æœŸ
        long_term_performance = sum(1 for period in data_list 
                                  if tail in period.get('tails', []))
        long_term_expectation = long_term_performance / len(data_list)
        
        # 3. é¢„æœŸåå·®è®¡ç®—
        expectation_bias = recent_expectation - long_term_expectation
        
        # 4. åˆ¤æ–­æ˜¯å¦å—è®¤çŸ¥åå·®å½±å“
        bias_influenced = False
        dominant_bias = bias_analysis.get('dominant_bias', '')
        
        if abs(expectation_bias) > 0.2:
            bias_influenced = True
            
            if dominant_bias in ['recency_bias', 'availability_bias']:
                expectation_bias *= 1.5  # æ”¾å¤§è¿‘æœŸåå·®å½±å“
            elif dominant_bias in ['anchoring_bias', 'confirmation_bias']:
                expectation_bias *= 1.2  # é€‚åº¦æ”¾å¤§é”šå®š/ç¡®è®¤åå·®å½±å“
        
        expectation_level = (recent_expectation + long_term_expectation) / 2.0
        
        return {
            'expectation_level': expectation_level,
            'expectation_bias': expectation_bias,
            'bias_influenced': bias_influenced,
            'recent_expectation': recent_expectation,
            'long_term_expectation': long_term_expectation
        }
    
    def _calculate_overall_psychology_score(self, crowd_preference: float, contrarian_opportunity: float,
                                          psychological_resistance: float, crowd_fatigue: float) -> float:
        """è®¡ç®—ç»¼åˆå¿ƒç†åˆ†æ•°"""
        # æƒé‡åˆ†é…
        weights = {
            'crowd_preference': 0.3,
            'contrarian_opportunity': 0.3, 
            'psychological_resistance': 0.2,
            'crowd_fatigue': 0.2
        }
        
        # è®¡ç®—åŠ æƒå¹³å‡
        score = (crowd_preference * weights['crowd_preference'] +
                contrarian_opportunity * weights['contrarian_opportunity'] +
                psychological_resistance * weights['psychological_resistance'] +
                crowd_fatigue * weights['crowd_fatigue'])
        
        return min(1.0, max(0.0, score))
    
    # ============ åå‘ç­–ç•¥ç½®ä¿¡åº¦è®¡ç®— ============
    
    def _calculate_anti_herd_confidence(self, analysis: Dict, emotion_analysis: Dict) -> float:
        """è®¡ç®—åä»ä¼—ç½®ä¿¡åº¦"""
        base_confidence = self.contrarian_strategy_params['contrarian_confidence_base']
        
        # ä»ä¼—ç¨‹åº¦è¶Šé«˜ï¼Œåå‘ä¿¡å¿ƒè¶Šå¼º
        herd_factor = analysis['crowd_preference'] * self.contrarian_strategy_params['herd_penalty_factor']
        
        # ç¾¤ä½“ç–²åŠ³åº¦è¶Šé«˜ï¼Œåå‘ä¿¡å¿ƒè¶Šå¼º
        fatigue_factor = analysis['crowd_fatigue'] * 0.8
        
        # æƒ…ç»ªæç«¯ç¨‹åº¦å¢å¼ºä¿¡å¿ƒ
        emotion_factor = emotion_analysis['contrarian_signal_strength'] * 0.6
        
        confidence = base_confidence + (herd_factor + fatigue_factor + emotion_factor) / 3.0
        
        return min(0.95, confidence)
    
    def _calculate_emotion_contrarian_confidence(self, analysis: Dict, emotion_analysis: Dict) -> float:
        """è®¡ç®—æƒ…ç»ªåå‘ç½®ä¿¡åº¦"""
        base_confidence = 0.5
        
        # æƒ…ç»ªæç«¯ç¨‹åº¦
        emotion_intensity = emotion_analysis['emotion_intensity']
        
        # åå‘æœºä¼šè¯„åˆ†
        opportunity_score = analysis['contrarian_opportunity']
        
        # æƒ…ç»ªåå‘ä¿¡å·å¼ºåº¦
        contrarian_signal = emotion_analysis['contrarian_signal_strength']
        
        confidence = base_confidence + (emotion_intensity + opportunity_score + contrarian_signal) / 3.0 * 0.4
        
        return min(0.9, confidence)
    
    def _calculate_consensus_fade_confidence(self, analysis: Dict, crowd_behavior: Dict) -> float:
        """è®¡ç®—å…±è¯†æ¶ˆé€€ç½®ä¿¡åº¦"""
        base_confidence = 0.4
        
        # å¿ƒç†é˜»åŠ›è¶Šå¼ºï¼Œå…±è¯†æ¶ˆé€€å¯èƒ½æ€§è¶Šå¤§
        resistance_factor = analysis['psychological_resistance'] * 0.7
        
        # ç¾¤ä½“è¡Œä¸ºå¼ºåº¦
        behavior_strength = crowd_behavior.get('behavior_strength', 0.5)
        behavior_factor = behavior_strength * self.contrarian_strategy_params['consensus_fade_factor']
        
        # åå‘é€‚åˆåº¦
        suitability_factor = analysis['contrarian_suitability'] * 0.6
        
        confidence = base_confidence + (resistance_factor + behavior_factor + suitability_factor) / 3.0
        
        return min(0.88, confidence)
    
    def _calculate_bias_exploitation_confidence(self, analysis: Dict) -> float:
        """è®¡ç®—åå·®åˆ©ç”¨ç½®ä¿¡åº¦"""
        base_confidence = 0.35
        
        # ç¾¤ä½“é¢„æœŸåå·®ç¨‹åº¦
        expectation_bias = abs(analysis['crowd_expectation']['expectation_bias'])
        bias_factor = min(0.4, expectation_bias * 2.0)
        
        # åå‘æœºä¼šè¯„åˆ†
        opportunity_factor = analysis['contrarian_opportunity'] * 0.3
        
        # å¿ƒç†é˜»åŠ›ï¼ˆåå·®å¯èƒ½é€ æˆçš„è¯¯åˆ¤ï¼‰
        resistance_factor = analysis['psychological_resistance'] * 0.2
        
        confidence = base_confidence + bias_factor + opportunity_factor + resistance_factor
        
        return min(0.85, confidence)
    
    def _calculate_psychological_cycle_suitability(self, tail: int) -> float:
        """è®¡ç®—å¿ƒç†å‘¨æœŸé€‚åˆåº¦ï¼ˆåŸºäºæ•°å­—å¿ƒç†å­¦ï¼‰"""
        # åŸºäºæ•°å­—å¿ƒç†å­¦çš„é€‚åˆåº¦è¯„ä¼°
        cycle_scores = {
            0: 0.8,  # åœ†æ»¡æ•°å­—ï¼Œå¿ƒç†å‘¨æœŸæ€§å¼º
            1: 0.6,  # èµ·å§‹æ•°å­—
            2: 0.5,  # å¹³è¡¡æ•°å­—
            3: 0.7,  # ç¨³å®šæ•°å­—
            4: 0.4,  # ä¸å‰åˆ©æ•°å­—ï¼ˆéƒ¨åˆ†æ–‡åŒ–ï¼‰
            5: 0.8,  # ä¸­é—´æ•°å­—ï¼Œå¹³è¡¡æ„Ÿå¼º
            6: 0.7,  # å’Œè°æ•°å­—
            7: 0.9,  # å¹¸è¿æ•°å­—ï¼Œå¿ƒç†åå¥½å¼º
            8: 0.8,  # å‘è´¢æ•°å­—ï¼Œå¿ƒç†æœŸå¾…é«˜
            9: 0.6   # æé™æ•°å­—
        }
        
        return cycle_scores.get(tail, 0.5)
    
    # ============ å­¦ä¹ å’Œå‚æ•°è°ƒæ•´ ============
    
    def _update_psychology_model_based_on_outcome(self, detailed_analysis: Dict, 
                                                actual_tails: List[int], prediction_correct: bool):
        """åŸºäºç»“æœæ›´æ–°å¿ƒç†æ¨¡å‹"""
        # æ›´æ–°ç¾¤ä½“å¿ƒç†çŠ¶æ€å†å²
        emotion_analysis = detailed_analysis.get('emotion_analysis', {})
        if emotion_analysis:
            psychology_state = {
                'timestamp': datetime.now(),
                'emotion_state': emotion_analysis.get('current_state', PsychologyState.NEUTRAL),
                'fear_greed_index': emotion_analysis.get('fear_greed_index', 0.5),
                'prediction_outcome': prediction_correct
            }
            self.psychology_history.append(psychology_state)
        
        # æ›´æ–°åå·®æ£€æµ‹å†å²
        bias_analysis = detailed_analysis.get('bias_analysis', {})
        if bias_analysis:
            bias_record = {
                'timestamp': datetime.now(),
                'dominant_bias': bias_analysis.get('dominant_bias', 'unknown'),
                'bias_strength': bias_analysis.get('bias_strength', 0.0),
                'prediction_success': prediction_correct
            }
            self.bias_detection_history.append(bias_record)
            
            # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
            if len(self.bias_detection_history) > 100:
                self.bias_detection_history = self.bias_detection_history[-50:]
    
    def _adjust_psychology_parameters(self, prediction_result: Dict, actual_tails: List[int], prediction_correct: bool):
        """åŸºäºç»“æœåŠ¨æ€è°ƒæ•´å¿ƒç†åˆ†æå‚æ•°"""
        strategy_type = prediction_result.get('strategy_type', 'unknown')
        confidence = prediction_result.get('confidence', 0.0)
        
        adjustment_rate = self.adaptive_params['psychology_learning_rate']
        
        if prediction_correct:
            # é¢„æµ‹æ­£ç¡®ï¼Œå¼ºåŒ–ç›¸å…³å‚æ•°
            if 'contrarian' in strategy_type:
                self.contrarian_strategy_params['contrarian_confidence_base'] *= (1 + adjustment_rate * 0.1)
                
                if 'emotion' in strategy_type:
                    self.emotion_analysis_params['fear_greed_sensitivity'] *= (1 + adjustment_rate * 0.05)
                elif 'herd' in strategy_type:
                    self.crowd_analysis_params['herd_threshold'] *= (1 - adjustment_rate * 0.03)
        else:
            # é¢„æµ‹é”™è¯¯ï¼Œè°ƒæ•´å‚æ•°é™ä½ç±»ä¼¼é¢„æµ‹çš„æ¦‚ç‡
            if 'contrarian' in strategy_type:
                self.contrarian_strategy_params['contrarian_confidence_base'] *= (1 - adjustment_rate * 0.08)
                
                if confidence > 0.7:  # é«˜ç½®ä¿¡åº¦é”™è¯¯ï¼Œæ›´å¤§è°ƒæ•´
                    self.crowd_analysis_params['concentration_sensitivity'] *= (1 + adjustment_rate * 0.1)
        
        # åº”ç”¨å‚æ•°ç¨³å®šæ€§é™åˆ¶
        self._apply_psychology_parameter_limits()
    
    def _apply_psychology_parameter_limits(self):
        """åº”ç”¨å¿ƒç†å‚æ•°ç¨³å®šæ€§é™åˆ¶"""
        # é™åˆ¶å…³é”®å‚æ•°çš„å˜åŒ–èŒƒå›´
        self.contrarian_strategy_params['contrarian_confidence_base'] = max(0.3, min(0.8,
            self.contrarian_strategy_params['contrarian_confidence_base']))
        
        self.crowd_analysis_params['herd_threshold'] = max(0.5, min(0.9,
            self.crowd_analysis_params['herd_threshold']))
        
        self.emotion_analysis_params['fear_greed_sensitivity'] = max(0.6, min(1.2,
            self.emotion_analysis_params['fear_greed_sensitivity']))

# ============ é«˜çº§åˆ†æç»„ä»¶ç±» ============

class HerdBehaviorDetector:
    """
    ç§‘ç ”çº§ä»ä¼—è¡Œä¸ºæ£€æµ‹å™¨ - åŸºäºå¤æ‚ç½‘ç»œç†è®ºå’Œç¾¤ä½“åŠ¨åŠ›å­¦
    
    ç†è®ºåŸºç¡€ï¼š
    - Aschä»ä¼—å®éªŒç†è®ºæ‰©å±•
    - ä¿¡æ¯çº§è”æ¨¡å‹ï¼ˆBikhchandaniç­‰ï¼‰
    - ç¤¾ä¼šç½‘ç»œä¼ æ’­åŠ¨åŠ›å­¦
    - ä¸´ç•Œç›¸å˜ç†è®º
    - é›†ä½“è¡Œä¸ºæ¶Œç°ç†è®º
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç§‘ç ”çº§ä»ä¼—è¡Œä¸ºæ£€æµ‹å™¨"""
        print("ğŸ‘ å¯åŠ¨ç§‘ç ”çº§ä»ä¼—è¡Œä¸ºæ£€æµ‹å™¨...")
        
        # æ£€æµ‹å†å²å’Œæ¨¡å¼å­˜å‚¨
        self.detection_history = deque(maxlen=1000)
        self.herd_patterns = {
            'temporal_patterns': {},      # æ—¶é—´æ¨¡å¼
            'spatial_patterns': {},       # ç©ºé—´æ¨¡å¼
            'cascade_patterns': {},       # çº§è”æ¨¡å¼
            'threshold_patterns': {},     # é˜ˆå€¼æ¨¡å¼
            'network_patterns': {},       # ç½‘ç»œæ¨¡å¼
            'emergence_patterns': {}      # æ¶Œç°æ¨¡å¼
        }
        
        # é«˜çº§æ£€æµ‹æ¨¡å‹
        self.detection_models = {
            'percolation_model': self._init_percolation_model(),
            'ising_model': self._init_ising_model(),
            'voter_model': self._init_voter_model(),
            'threshold_model': self._init_threshold_model(),
            'cascade_model': self._init_cascade_model(),
            'network_diffusion_model': self._init_network_diffusion_model()
        }
        
        # ä»ä¼—è¡Œä¸ºé‡åŒ–æŒ‡æ ‡
        self.herd_metrics = {
            'conformity_index': 0.0,         # ä»ä¼—æŒ‡æ•°
            'social_proof_strength': 0.0,    # ç¤¾ä¼šè¯æ˜å¼ºåº¦
            'information_cascade_intensity': 0.0,  # ä¿¡æ¯çº§è”å¼ºåº¦
            'herd_persistence': 0.0,         # ä»ä¼—æŒç»­æ€§
            'collective_momentum': 0.0,      # é›†ä½“åŠ¨é‡
            'group_polarization': 0.0,       # ç¾¤ä½“æåŒ–åº¦
            'bandwagon_effect': 0.0,         # ä»ä¼—æ•ˆåº”
            'peer_pressure_intensity': 0.0   # åŒä¼´å‹åŠ›å¼ºåº¦
        }
        
        # åŠ¨æ€å‚æ•°ç³»ç»Ÿ
        self.dynamic_parameters = {
            'detection_sensitivity': 0.75,    # æ£€æµ‹æ•æ„Ÿåº¦
            'temporal_window': 10,            # æ—¶é—´çª—å£
            'spatial_threshold': 0.6,         # ç©ºé—´é˜ˆå€¼
            'cascade_threshold': 0.7,         # çº§è”é˜ˆå€¼
            'network_influence_decay': 0.8,   # ç½‘ç»œå½±å“è¡°å‡
            'adaptation_rate': 0.1            # é€‚åº”é€Ÿç‡
        }
        
        # èµ”ç‡æ•æ„Ÿæ€§å‚æ•°
        self.odds_sensitivity = {
            'zero_tail_herding_factor': 1.2,  # 0å°¾(2å€èµ”ç‡)ä»ä¼—å› å­
            'regular_tail_herding_factor': 1.0,  # 1-9å°¾(1.8å€èµ”ç‡)ä»ä¼—å› å­
            'odds_differential_impact': 0.15,    # èµ”ç‡å·®å¼‚å½±å“
            'risk_aversion_clustering': 0.8      # é£é™©åŒæ¶èšé›†æ•ˆåº”
        }
        
        # å­¦ä¹ å’Œé€‚åº”ç³»ç»Ÿ
        self.learning_system = {
            'detection_accuracy_history': [],
            'false_positive_rate': 0.0,
            'false_negative_rate': 0.0,
            'model_confidence': 0.5,
            'adaptation_triggers': set(),
            'learning_rate': 0.05
        }
        
        # é«˜çº§åˆ†æå·¥å…·
        self.analysis_tools = {
            'fractal_analyzer': self._init_fractal_analyzer(),
            'entropy_calculator': self._init_entropy_calculator(),
            'correlation_detector': self._init_correlation_detector(),
            'anomaly_detector': self._init_anomaly_detector(),
            'trend_analyzer': self._init_trend_analyzer()
        }
        
        print("âœ… ç§‘ç ”çº§ä»ä¼—è¡Œä¸ºæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def detect_herd_behavior(self, data_sequence: List[Dict], candidates: List[int] = None) -> Dict:
        """
        æ£€æµ‹ä»ä¼—è¡Œä¸º - å¤šæ¨¡å‹é›†æˆåˆ†æ
        
        Args:
            data_sequence: å†å²æ•°æ®åºåˆ—
            candidates: å€™é€‰å°¾æ•°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: ä»ä¼—è¡Œä¸ºæ£€æµ‹ç»“æœ
        """
        try:
            print(f"ğŸ‘ å¼€å§‹ä»ä¼—è¡Œä¸ºæ£€æµ‹åˆ†æ...")
            
            if len(data_sequence) < 3:
                return self._generate_insufficient_data_result()
            
            # === 1. å¤šç»´åº¦ä»ä¼—è¡Œä¸ºæ£€æµ‹ ===
            
            # æ—¶é—´ç»´åº¦ä»ä¼—æ£€æµ‹
            temporal_herding = self._detect_temporal_herding(data_sequence)
            
            # ç©ºé—´ç»´åº¦ä»ä¼—æ£€æµ‹
            spatial_herding = self._detect_spatial_herding(data_sequence)
            
            # é¢‘ç‡åŸŸä»ä¼—æ£€æµ‹
            frequency_herding = self._detect_frequency_domain_herding(data_sequence)
            
            # ç½‘ç»œä¼ æ’­ä»ä¼—æ£€æµ‹
            network_herding = self._detect_network_propagation_herding(data_sequence)
            
            # ä¿¡æ¯çº§è”ä»ä¼—æ£€æµ‹
            cascade_herding = self._detect_information_cascade_herding(data_sequence)
            
            # ç¤¾ä¼šè¯æ˜ä»ä¼—æ£€æµ‹
            social_proof_herding = self._detect_social_proof_herding(data_sequence)
            
            # === 2. é«˜çº§ç‰©ç†æ¨¡å‹åˆ†æ ===
            
            # Isingæ¨¡å‹åˆ†æ
            ising_analysis = self._apply_ising_model_analysis(data_sequence)
            
            # æ¸—é€æ¨¡å‹åˆ†æ
            percolation_analysis = self._apply_percolation_model_analysis(data_sequence)
            
            # æŠ•ç¥¨è€…æ¨¡å‹åˆ†æ
            voter_model_analysis = self._apply_voter_model_analysis(data_sequence)
            
            # é˜ˆå€¼æ¨¡å‹åˆ†æ
            threshold_analysis = self._apply_threshold_model_analysis(data_sequence)
            
            # === 3. èµ”ç‡æ•æ„Ÿæ€§ä»ä¼—åˆ†æ ===
            odds_herding_analysis = self._analyze_odds_sensitive_herding(
                data_sequence, candidates
            )
            
            # === 4. åŠ¨æ€ä»ä¼—å¼ºåº¦è®¡ç®— ===
            dynamic_herd_intensity = self._calculate_dynamic_herd_intensity({
                'temporal': temporal_herding,
                'spatial': spatial_herding,
                'frequency': frequency_herding,
                'network': network_herding,
                'cascade': cascade_herding,
                'social_proof': social_proof_herding,
                'ising': ising_analysis,
                'percolation': percolation_analysis,
                'voter': voter_model_analysis,
                'threshold': threshold_analysis,
                'odds_sensitive': odds_herding_analysis
            })
            
            # === 5. ä»ä¼—ç±»å‹è¯†åˆ« ===
            herd_type_classification = self._classify_herd_behavior_type({
                'temporal': temporal_herding,
                'spatial': spatial_herding,
                'cascade': cascade_herding,
                'network': network_herding
            })
            
            # === 6. ä»ä¼—ç¨³å®šæ€§åˆ†æ ===
            herd_stability = self._analyze_herd_stability(
                data_sequence, dynamic_herd_intensity
            )
            
            # === 7. ä»ä¼—é¢„æµ‹æ¨¡å‹ ===
            herd_prediction = self._predict_future_herd_behavior(
                data_sequence, dynamic_herd_intensity, herd_stability
            )
            
            # === 8. ä»ä¼—é£é™©è¯„ä¼° ===
            herd_risk_assessment = self._assess_herd_behavior_risks(
                dynamic_herd_intensity, herd_type_classification, herd_stability
            )
            
            # === 9. åä»ä¼—æœºä¼šè¯†åˆ« ===
            anti_herd_opportunities = self._identify_anti_herd_opportunities(
                dynamic_herd_intensity, herd_prediction, candidates
            )
            
            # === 10. ä»ä¼—å­¦ä¹ æ•ˆåº”åˆ†æ ===
            herd_learning_effects = self._analyze_herd_learning_effects(data_sequence)
            
            # === æ›´æ–°æ£€æµ‹å†å² ===
            detection_record = {
                'timestamp': self._get_timestamp(),
                'data_length': len(data_sequence),
                'herd_intensity': dynamic_herd_intensity,
                'herd_type': herd_type_classification,
                'detection_confidence': self._calculate_detection_confidence(dynamic_herd_intensity),
                'model_consensus': self._calculate_model_consensus({
                    'temporal': temporal_herding,
                    'spatial': spatial_herding,
                    'network': network_herding,
                    'cascade': cascade_herding
                })
            }
            
            self.detection_history.append(detection_record)
            
            # === æ„å»ºå®Œæ•´æ£€æµ‹ç»“æœ ===
            comprehensive_detection_result = {
                'timestamp': self._get_timestamp(),
                
                # æ ¸å¿ƒæ£€æµ‹ç»“æœ
                'herd_detected': dynamic_herd_intensity > self.dynamic_parameters['detection_sensitivity'],
                'herd_intensity': float(dynamic_herd_intensity),
                'herd_type': herd_type_classification,
                'detection_confidence': float(detection_record['detection_confidence']),
                
                # è¯¦ç»†åˆ†æç»„ä»¶
                'temporal_herding': temporal_herding,
                'spatial_herding': spatial_herding,
                'frequency_herding': frequency_herding,
                'network_herding': network_herding,
                'cascade_herding': cascade_herding,
                'social_proof_herding': social_proof_herding,
                
                # ç‰©ç†æ¨¡å‹åˆ†æ
                'ising_analysis': ising_analysis,
                'percolation_analysis': percolation_analysis,
                'voter_model_analysis': voter_model_analysis,
                'threshold_analysis': threshold_analysis,
                
                # èµ”ç‡æ•æ„Ÿæ€§åˆ†æ
                'odds_herding_analysis': odds_herding_analysis,
                
                # é«˜çº§æŒ‡æ ‡
                'herd_stability': herd_stability,
                'herd_prediction': herd_prediction,
                'herd_risk_assessment': herd_risk_assessment,
                'anti_herd_opportunities': anti_herd_opportunities,
                'herd_learning_effects': herd_learning_effects,
                
                # è´¨é‡ä¿è¯
                'model_consensus': detection_record['model_consensus'],
                'reliability_score': self._calculate_reliability_score(detection_record),
                'analysis_completeness': self._assess_analysis_completeness(data_sequence),
                
                # å…ƒæ•°æ®
                'detection_method': 'multi_model_ensemble',
                'models_used': list(self.detection_models.keys()),
                'analysis_depth': len(data_sequence),
                'computational_complexity': self._estimate_computational_complexity()
            }
            
            # === è‡ªé€‚åº”å­¦ä¹ æ›´æ–° ===
            self._update_learning_system(comprehensive_detection_result)
            
            print(f"âœ… ä»ä¼—è¡Œä¸ºæ£€æµ‹å®Œæˆ - å¼ºåº¦: {dynamic_herd_intensity:.3f}, ç±»å‹: {herd_type_classification}")
            
            return comprehensive_detection_result
            
        except Exception as e:
            print(f"âŒ ä»ä¼—è¡Œä¸ºæ£€æµ‹å¤±è´¥: {e}")
            return self._generate_error_result(str(e))
    
    def _detect_temporal_herding(self, data_sequence: List[Dict]) -> Dict:
        """æ—¶é—´ç»´åº¦ä»ä¼—æ£€æµ‹"""
        try:
            import numpy as np
            from scipy import stats, signal
            
            # æå–æ—¶é—´åºåˆ—ç‰¹å¾
            tail_sequences = []
            for i, period in enumerate(data_sequence):
                tails = period.get('tails', [])
                tail_vector = self._tails_to_vector(tails)
                tail_sequences.append(tail_vector)
            
            if len(tail_sequences) < 3:
                return {'insufficient_data': True}
            
            tail_matrix = np.array(tail_sequences)
            
            # æ—¶é—´ç›¸å…³æ€§åˆ†æ
            temporal_correlations = []
            for lag in range(1, min(6, len(tail_sequences))):
                if lag < len(tail_sequences):
                    corr_matrix = np.corrcoef(tail_matrix[:-lag], tail_matrix[lag:])
                    avg_correlation = np.mean(np.diag(corr_matrix, k=tail_matrix.shape[1]))
                    temporal_correlations.append(avg_correlation if not np.isnan(avg_correlation) else 0)
            
            # è‡ªç›¸å…³å‡½æ•°åˆ†æ
            autocorr_scores = []
            for tail_idx in range(10):
                tail_series = tail_matrix[:, tail_idx]
                if np.std(tail_series) > 0:
                    autocorr = self._calculate_autocorrelation(tail_series, max_lag=5)
                    autocorr_scores.append(np.mean(autocorr))
                else:
                    autocorr_scores.append(0)
            
            # è¶‹åŠ¿æŒç»­æ€§åˆ†æ
            trend_persistence = self._analyze_trend_persistence(tail_sequences)
            
            # æ—¶é—´èšé›†æ€§æ£€æµ‹
            temporal_clustering = self._detect_temporal_clustering(data_sequence)
            
            # è®°å¿†æ•ˆåº”åˆ†æ
            memory_effects = self._analyze_memory_effects(tail_sequences)
            
            # å‘¨æœŸæ€§ä»ä¼—æ£€æµ‹
            cyclical_herding = self._detect_cyclical_herding_patterns(tail_sequences)
            
            # è®¡ç®—ç»¼åˆæ—¶é—´ä»ä¼—æŒ‡æ•°
            temporal_herd_score = (
                np.mean(temporal_correlations) * 0.3 +
                np.mean(autocorr_scores) * 0.25 +
                trend_persistence * 0.2 +
                temporal_clustering * 0.15 +
                memory_effects * 0.1
            )
            
            return {
                'temporal_herd_score': float(temporal_herd_score),
                'temporal_correlations': temporal_correlations,
                'autocorr_scores': autocorr_scores,
                'trend_persistence': float(trend_persistence),
                'temporal_clustering': float(temporal_clustering),
                'memory_effects': float(memory_effects),
                'cyclical_herding': cyclical_herding,
                'analysis_quality': min(1.0, len(data_sequence) / 20.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'temporal_herd_score': 0.0}
    
    def _detect_spatial_herding(self, data_sequence: List[Dict]) -> Dict:
        """ç©ºé—´ç»´åº¦ä»ä¼—æ£€æµ‹"""
        try:
            import numpy as np
            from scipy.spatial.distance import pdist, squareform
            from scipy.cluster.hierarchy import linkage, fcluster
            
            # æ„å»ºå°¾æ•°ç©ºé—´åˆ†å¸ƒ
            spatial_distributions = []
            for period in data_sequence:
                tails = period.get('tails', [])
                if tails:
                    # è®¡ç®—å°¾æ•°çš„ç©ºé—´ç‰¹å¾
                    spatial_features = {
                        'mean_position': np.mean(tails),
                        'std_position': np.std(tails) if len(tails) > 1 else 0,
                        'range_span': max(tails) - min(tails) if len(tails) > 1 else 0,
                        'density_pattern': self._calculate_density_pattern(tails),
                        'symmetry_measure': self._calculate_symmetry_measure(tails),
                        'clustering_coefficient': self._calculate_clustering_coefficient(tails)
                    }
                    spatial_distributions.append(spatial_features)
            
            if len(spatial_distributions) < 3:
                return {'insufficient_data': True}
            
            # ç©ºé—´ç›¸ä¼¼æ€§åˆ†æ
            similarity_matrix = self._calculate_spatial_similarity_matrix(spatial_distributions)
            
            # å±‚æ¬¡èšç±»åˆ†æ
            feature_vectors = []
            for dist in spatial_distributions:
                vector = [
                    dist['mean_position'],
                    dist['std_position'],
                    dist['range_span'],
                    dist['density_pattern'],
                    dist['symmetry_measure'],
                    dist['clustering_coefficient']
                ]
                feature_vectors.append(vector)
            
            feature_matrix = np.array(feature_vectors)
            if feature_matrix.shape[0] > 1:
                distances = pdist(feature_matrix, metric='euclidean')
                linkage_matrix = linkage(distances, method='ward')
                clusters = fcluster(linkage_matrix, t=3, criterion='maxclust')
                
                # è®¡ç®—èšç±»è´¨é‡
                cluster_quality = self._assess_cluster_quality(feature_matrix, clusters)
            else:
                cluster_quality = 0.0
            
            # ç©ºé—´è‡ªç›¸å…³åˆ†æ
            spatial_autocorr = self._calculate_spatial_autocorrelation(spatial_distributions)
            
            # ç©ºé—´æ‰©æ•£æ¨¡å¼åˆ†æ
            diffusion_patterns = self._analyze_spatial_diffusion_patterns(data_sequence)
            
            # é‚»è¿‘æ•ˆåº”åˆ†æ
            proximity_effects = self._analyze_proximity_effects(spatial_distributions)
            
            # è®¡ç®—ç©ºé—´ä»ä¼—æŒ‡æ•°
            spatial_herd_score = (
                np.mean(similarity_matrix) * 0.3 +
                cluster_quality * 0.25 +
                spatial_autocorr * 0.2 +
                diffusion_patterns * 0.15 +
                proximity_effects * 0.1
            )
            
            return {
                'spatial_herd_score': float(spatial_herd_score),
                'similarity_matrix': similarity_matrix.tolist(),
                'cluster_quality': float(cluster_quality),
                'spatial_autocorr': float(spatial_autocorr),
                'diffusion_patterns': float(diffusion_patterns),
                'proximity_effects': float(proximity_effects),
                'spatial_distributions': spatial_distributions,
                'analysis_confidence': min(1.0, len(spatial_distributions) / 15.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'spatial_herd_score': 0.0}
    
    def _apply_ising_model_analysis(self, data_sequence: List[Dict]) -> Dict:
        """åº”ç”¨Isingæ¨¡å‹è¿›è¡Œä»ä¼—è¡Œä¸ºåˆ†æ"""
        try:
            import numpy as np
            import math
            
            # æ„å»ºIsingè‡ªæ—‹ç³»ç»Ÿ
            spin_configurations = []
            for period in data_sequence:
                tails = period.get('tails', [])
                # å°†å°¾æ•°é€‰æ‹©è½¬æ¢ä¸ºè‡ªæ—‹é…ç½®
                spins = np.zeros(10)
                for tail in tails:
                    if 0 <= tail <= 9:
                        spins[tail] = 1  # é€‰ä¸­ä¸º+1è‡ªæ—‹
                
                # å½’ä¸€åŒ–ä¸º[-1, +1]
                spins = 2 * spins - 1
                spin_configurations.append(spins)
            
            if len(spin_configurations) < 3:
                return {'insufficient_data': True}
            
            spin_matrix = np.array(spin_configurations)
            
            # è®¡ç®—Isingæ¨¡å‹å‚æ•°
            
            # 1. ç£åŒ–å¼ºåº¦ï¼ˆæ•´ä½“å€¾å‘æ€§ï¼‰
            magnetization = np.mean(np.abs(np.mean(spin_matrix, axis=1)))
            
            # 2. ç›¸å…³é•¿åº¦ï¼ˆç©ºé—´ç›¸å…³æ€§ï¼‰
            correlation_length = self._calculate_ising_correlation_length(spin_matrix)
            
            # 3. ç£åŒ–ç‡ï¼ˆå¯¹å¤–éƒ¨å½±å“çš„æ•æ„Ÿæ€§ï¼‰
            susceptibility = self._calculate_ising_susceptibility(spin_matrix)
            
            # 4. ç›¸å˜æ£€æµ‹ï¼ˆä¸´ç•Œè¡Œä¸ºï¼‰
            phase_transition_indicator = self._detect_ising_phase_transition(spin_matrix)
            
            # 5. èƒ½é‡å‡½æ•°åˆ†æ
            energy_analysis = self._analyze_ising_energy_landscape(spin_matrix)
            
            # 6. ä¸´ç•Œæ¸©åº¦ä¼°è®¡
            critical_temperature = self._estimate_ising_critical_temperature(spin_matrix)
            
            # 7. æœ‰åºå‚æ•°
            order_parameter = self._calculate_ising_order_parameter(spin_matrix)
            
            # 8. é›†ä½“è¡Œä¸ºå¼ºåº¦
            collective_behavior_strength = (
                magnetization * 0.3 +
                correlation_length * 0.25 +
                susceptibility * 0.2 +
                order_parameter * 0.15 +
                phase_transition_indicator * 0.1
            )
            
            return {
                'ising_herd_score': float(collective_behavior_strength),
                'magnetization': float(magnetization),
                'correlation_length': float(correlation_length),
                'susceptibility': float(susceptibility),
                'phase_transition_indicator': float(phase_transition_indicator),
                'energy_analysis': energy_analysis,
                'critical_temperature': float(critical_temperature),
                'order_parameter': float(order_parameter),
                'model_validity': self._assess_ising_model_validity(spin_matrix),
                'confidence': min(1.0, len(spin_configurations) / 25.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'ising_herd_score': 0.0}
    
    def _analyze_odds_sensitive_herding(self, data_sequence: List[Dict], candidates: List[int] = None) -> Dict:
        """åˆ†æèµ”ç‡æ•æ„Ÿçš„ä»ä¼—è¡Œä¸º"""
        try:
            import numpy as np
            
            # åˆ†ç¦»0å°¾å’Œå…¶ä»–å°¾æ•°çš„ä»ä¼—è¡Œä¸º
            zero_tail_herding = self._analyze_tail_specific_herding(data_sequence, [0], 2.0)
            other_tail_herding = self._analyze_tail_specific_herding(
                data_sequence, list(range(1, 10)), 1.8
            )
            
            # èµ”ç‡å·®å¼‚å¯¹ä»ä¼—è¡Œä¸ºçš„å½±å“
            odds_differential_impact = abs(
                zero_tail_herding.get('herding_intensity', 0) - 
                other_tail_herding.get('herding_intensity', 0)
            )
            
            # é£é™©åå¥½èšé›†åˆ†æ
            risk_preference_clustering = self._analyze_risk_preference_clustering(
                data_sequence, candidates
            )
            
            # é«˜èµ”ç‡å°¾æ•°çš„ç¾¤ä½“å¸å¼•åŠ›
            high_odds_attraction = self._analyze_high_odds_attraction(data_sequence)
            
            # èµ”ç‡æ•æ„Ÿçš„ç¤¾ä¼šè¯æ˜æ•ˆåº”
            odds_social_proof = self._analyze_odds_social_proof_effects(data_sequence)
            
            # é£é™©åŒæ¶ä»ä¼—æ¨¡å¼
            risk_aversion_herding = self._analyze_risk_aversion_herding_patterns(data_sequence)
            
            # èµ”ç‡å¥—åˆ©ä»ä¼—è¡Œä¸º
            arbitrage_herding = self._analyze_arbitrage_herding_behavior(data_sequence)
            
            # è®¡ç®—èµ”ç‡æ•æ„Ÿä»ä¼—ç»¼åˆæŒ‡æ•°
            odds_sensitive_score = (
                zero_tail_herding.get('herding_intensity', 0) * 0.25 +
                other_tail_herding.get('herding_intensity', 0) * 0.2 +
                odds_differential_impact * 0.2 +
                risk_preference_clustering * 0.15 +
                high_odds_attraction * 0.1 +
                odds_social_proof * 0.1
            )
            
            return {
                'odds_sensitive_herd_score': float(odds_sensitive_score),
                'zero_tail_herding': zero_tail_herding,
                'other_tail_herding': other_tail_herding,
                'odds_differential_impact': float(odds_differential_impact),
                'risk_preference_clustering': float(risk_preference_clustering),
                'high_odds_attraction': float(high_odds_attraction),
                'odds_social_proof': float(odds_social_proof),
                'risk_aversion_herding': risk_aversion_herding,
                'arbitrage_herding': arbitrage_herding,
                'odds_sensitivity_factor': self._calculate_odds_sensitivity_factor(
                    zero_tail_herding, other_tail_herding
                )
            }
            
        except Exception as e:
            return {'error': str(e), 'odds_sensitive_herd_score': 0.0}
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _tails_to_vector(self, tails: List[int]) -> np.ndarray:
        """å°†å°¾æ•°åˆ—è¡¨è½¬æ¢ä¸ºå‘é‡"""
        try:
            import numpy as np
            vector = np.zeros(10)
            for tail in tails:
                if 0 <= tail <= 9:
                    vector[tail] = 1
            return vector
        except:
            import numpy as np
            return np.zeros(10)
    
    def _init_percolation_model(self) -> Dict:
        """åˆå§‹åŒ–æ¸—é€æ¨¡å‹"""
        return {
            'lattice_size': (10, 10),
            'percolation_threshold': 0.593,  # 2Dæ–¹æ ¼æ™¶æ ¼ç†è®ºå€¼
            'bond_probability': 0.5,
            'cluster_analysis': True
        }
    
    def _init_ising_model(self) -> Dict:
        """åˆå§‹åŒ–Isingæ¨¡å‹"""
        return {
            'lattice_dimension': 1,
            'interaction_strength': 1.0,
            'external_field': 0.0,
            'temperature': 1.0,
            'boundary_conditions': 'periodic'
        }
    
    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        import datetime
        return datetime.datetime.now().isoformat()

class CrowdEmotionTracker:
    """
    ç§‘ç ”çº§ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªå™¨ - åŸºäºæƒ…æ„Ÿè®¡ç®—å’Œå¤æ‚ç³»ç»Ÿç†è®º
    
    ç†è®ºåŸºç¡€ï¼š
    - æƒ…æ„Ÿè½®æ¨¡å‹ï¼ˆPlutchik's Wheel of Emotionsï¼‰
    - æƒ…ç»ªä¼ æŸ“ç†è®ºï¼ˆHatfieldç­‰ï¼‰
    - é›†ä½“æƒ…ç»ªåŠ¨åŠ›å­¦
    - å¤æ‚é€‚åº”ç³»ç»Ÿä¸­çš„æƒ…ç»ªæ¶Œç°
    - ç¤¾ä¼šæƒ…ç»ªç½‘ç»œç†è®º
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç§‘ç ”çº§ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªå™¨"""
        print("â¤ï¸ å¯åŠ¨ç§‘ç ”çº§ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªå™¨...")
        
        # æƒ…ç»ªå†å²å’Œè½¬æ¢æ¨¡å¼
        self.emotion_history = deque(maxlen=1000)
        self.emotion_transitions = {
            'micro_transitions': {},    # å¾®è§‚æƒ…ç»ªè½¬æ¢ï¼ˆç¬æ—¶ï¼‰
            'meso_transitions': {},     # ä¸­è§‚æƒ…ç»ªè½¬æ¢ï¼ˆçŸ­æœŸï¼‰
            'macro_transitions': {},    # å®è§‚æƒ…ç»ªè½¬æ¢ï¼ˆé•¿æœŸï¼‰
            'cyclical_transitions': {}, # å‘¨æœŸæ€§è½¬æ¢
            'cascade_transitions': {}   # çº§è”è½¬æ¢
        }
        
        # å¤šç»´æƒ…ç»ªçŠ¶æ€ç©ºé—´
        self.emotion_dimensions = {
            'valence': deque(maxlen=500),      # æƒ…æ„Ÿæ•ˆä»·ï¼ˆæ­£é¢-è´Ÿé¢ï¼‰
            'arousal': deque(maxlen=500),      # æƒ…æ„Ÿå”¤é†’åº¦ï¼ˆæ¿€æ´»-å¹³é™ï¼‰
            'dominance': deque(maxlen=500),    # æƒ…æ„Ÿæ”¯é…æ€§ï¼ˆæ§åˆ¶-è¢«æ§åˆ¶ï¼‰
            'certainty': deque(maxlen=500),    # ç¡®å®šæ€§ï¼ˆç¡®å®š-ä¸ç¡®å®šï¼‰
            'intensity': deque(maxlen=500),    # å¼ºåº¦ï¼ˆå¼º-å¼±ï¼‰
            'coherence': deque(maxlen=500),    # ä¸€è‡´æ€§ï¼ˆç»Ÿä¸€-åˆ†åŒ–ï¼‰
            'persistence': deque(maxlen=500),  # æŒç»­æ€§ï¼ˆç¨³å®š-æ˜“å˜ï¼‰
            'contagion': deque(maxlen=500)     # ä¼ æŸ“æ€§ï¼ˆæ‰©æ•£-å±€é™ï¼‰
        }
        
        # é«˜çº§æƒ…ç»ªåˆ†ææ¨¡å‹
        self.emotion_models = {
            'circumplex_model': self._init_circumplex_model(),
            'appraisal_model': self._init_appraisal_model(),
            'dimensional_model': self._init_dimensional_model(),
            'discrete_emotion_model': self._init_discrete_emotion_model(),
            'social_emotion_model': self._init_social_emotion_model(),
            'dynamic_emotion_model': self._init_dynamic_emotion_model()
        }
        
        # æƒ…ç»ªæ£€æµ‹å’Œé‡åŒ–ç³»ç»Ÿ
        self.emotion_metrics = {
            'emotional_entropy': 0.0,         # æƒ…ç»ªç†µ
            'emotional_volatility': 0.0,      # æƒ…ç»ªæ³¢åŠ¨æ€§
            'emotional_momentum': 0.0,        # æƒ…ç»ªåŠ¨é‡
            'emotional_coherence': 0.0,       # æƒ…ç»ªä¸€è‡´æ€§
            'emotional_contagion_rate': 0.0,  # æƒ…ç»ªä¼ æŸ“ç‡
            'emotional_stability': 0.0,       # æƒ…ç»ªç¨³å®šæ€§
            'emotional_complexity': 0.0,      # æƒ…ç»ªå¤æ‚æ€§
            'emotional_resonance': 0.0        # æƒ…ç»ªå…±æŒ¯åº¦
        }
        
        # åŠ¨æ€å‚æ•°å’Œé˜ˆå€¼
        self.tracking_parameters = {
            'emotion_detection_sensitivity': 0.7,  # æƒ…ç»ªæ£€æµ‹æ•æ„Ÿåº¦
            'transition_threshold': 0.6,           # è½¬æ¢é˜ˆå€¼
            'contagion_threshold': 0.65,          # ä¼ æŸ“é˜ˆå€¼
            'stability_threshold': 0.8,           # ç¨³å®šæ€§é˜ˆå€¼
            'resonance_threshold': 0.75,          # å…±æŒ¯é˜ˆå€¼
            'temporal_decay_rate': 0.9,           # æ—¶é—´è¡°å‡ç‡
            'spatial_influence_radius': 3,        # ç©ºé—´å½±å“åŠå¾„
            'adaptation_learning_rate': 0.12      # é€‚åº”å­¦ä¹ ç‡
        }
        
        # èµ”ç‡æƒ…ç»ªå…³è”åˆ†æ
        self.odds_emotion_correlations = {
            'zero_tail_emotion_factor': 1.4,      # 0å°¾æƒ…ç»ªå› å­
            'high_odds_excitement_multiplier': 1.3, # é«˜èµ”ç‡å…´å¥‹å€æ•°
            'loss_aversion_emotion_weight': 2.0,   # æŸå¤±åŒæ¶æƒ…ç»ªæƒé‡
            'risk_seeking_emotion_threshold': 0.7  # é£é™©å¯»æ±‚æƒ…ç»ªé˜ˆå€¼
        }
        
        # æƒ…ç»ªå­¦ä¹ å’Œé¢„æµ‹ç³»ç»Ÿ
        self.learning_system = {
            'emotion_prediction_accuracy': 0.0,
            'transition_prediction_accuracy': 0.0,
            'contagion_prediction_accuracy': 0.0,
            'model_performance_history': [],
            'feature_importance_rankings': {},
            'emotion_forecasting_models': {},
            'cross_validation_results': []
        }
        
        # é«˜çº§åˆ†æå·¥å…·
        self.analysis_tools = {
            'wavelet_analyzer': self._init_wavelet_analyzer(),
            'network_analyzer': self._init_network_analyzer(),
            'chaos_analyzer': self._init_chaos_analyzer(),
            'fractal_analyzer': self._init_fractal_analyzer(),
            'information_analyzer': self._init_information_analyzer(),
            'spectral_analyzer': self._init_spectral_analyzer()
        }
        
        print("âœ… ç§‘ç ”çº§ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def track_crowd_emotions(self, data_sequence: List[Dict], 
                           context_data: Dict = None) -> Dict:
        """
        è·Ÿè¸ªç¾¤ä½“æƒ…ç»ª - å¤šç»´åº¦å®æ—¶åˆ†æ
        
        Args:
            data_sequence: å†å²æ•°æ®åºåˆ—
            context_data: ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªç»“æœ
        """
        try:
            print(f"â¤ï¸ å¼€å§‹ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªåˆ†æ...")
            
            if len(data_sequence) < 3:
                return self._generate_insufficient_data_result()
            
            # === 1. å¤šç»´åº¦æƒ…ç»ªçŠ¶æ€è¯†åˆ« ===
            
            # ç¬æ—¶æƒ…ç»ªçŠ¶æ€æ£€æµ‹
            instantaneous_emotions = self._detect_instantaneous_emotions(data_sequence)
            
            # æŒç»­æƒ…ç»ªæ¨¡å¼è¯†åˆ«
            persistent_emotion_patterns = self._identify_persistent_emotion_patterns(
                data_sequence
            )
            
            # æƒ…ç»ªå¼ºåº¦æ—¶é—´åºåˆ—åˆ†æ
            emotion_intensity_analysis = self._analyze_emotion_intensity_time_series(
                data_sequence
            )
            
            # æƒ…ç»ªç»´åº¦åˆ†è§£åˆ†æ
            dimensional_emotion_analysis = self._analyze_emotion_dimensions(data_sequence)
            
            # === 2. æƒ…ç»ªä¼ æŸ“åŠ¨åŠ›å­¦åˆ†æ ===
            
            # æƒ…ç»ªä¼ æŸ“ç½‘ç»œåˆ†æ
            contagion_network_analysis = self._analyze_emotion_contagion_network(
                data_sequence
            )
            
            # æƒ…ç»ªæ‰©æ•£æ¨¡å‹
            emotion_diffusion_analysis = self._model_emotion_diffusion_dynamics(
                data_sequence
            )
            
            # æƒ…ç»ªçº§è”æ£€æµ‹
            emotion_cascade_detection = self._detect_emotion_cascade_events(data_sequence)
            
            # === 3. æƒ…ç»ªè½¬æ¢åˆ†æ ===
            
            # æƒ…ç»ªçŠ¶æ€è½¬æ¢çŸ©é˜µ
            emotion_transition_matrix = self._build_emotion_transition_matrix(data_sequence)
            
            # æƒ…ç»ªè½¬æ¢é¢„æµ‹æ¨¡å‹
            transition_prediction_model = self._build_emotion_transition_predictor(
                data_sequence
            )
            
            # æƒ…ç»ªç¨³å®šæ€§åˆ†æ
            emotion_stability_analysis = self._analyze_emotion_stability(data_sequence)
            
            # === 4. é«˜çº§æƒ…ç»ªæ¨¡å‹åˆ†æ ===
            
            # ç¯å½¢æƒ…ç»ªæ¨¡å‹åˆ†æ
            circumplex_analysis = self._apply_circumplex_emotion_model(data_sequence)
            
            # è¯„ä»·ç†è®ºåˆ†æ
            appraisal_analysis = self._apply_appraisal_theory_analysis(
                data_sequence, context_data
            )
            
            # ç¤¾ä¼šæƒ…ç»ªç½‘ç»œåˆ†æ
            social_emotion_analysis = self._analyze_social_emotion_networks(data_sequence)
            
            # === 5. èµ”ç‡ç›¸å…³æƒ…ç»ªåˆ†æ ===
            odds_emotion_analysis = self._analyze_odds_related_emotions(
                data_sequence, context_data
            )
            
            # === 6. æƒ…ç»ªé¢„æµ‹å’Œé¢„è­¦ ===
            
            # æƒ…ç»ªè¶‹åŠ¿é¢„æµ‹
            emotion_trend_prediction = self._predict_emotion_trends(
                data_sequence, emotion_intensity_analysis
            )
            
            # æƒ…ç»ªé£é™©é¢„è­¦
            emotion_risk_warnings = self._generate_emotion_risk_warnings(
                instantaneous_emotions, emotion_stability_analysis
            )
            
            # æƒ…ç»ªæœºä¼šè¯†åˆ«
            emotion_opportunities = self._identify_emotion_based_opportunities(
                emotion_trend_prediction, odds_emotion_analysis
            )
            
            # === 7. å¤æ‚ç³»ç»Ÿæƒ…ç»ªåˆ†æ ===
            
            # æƒ…ç»ªæ¶Œç°æ£€æµ‹
            emotion_emergence_analysis = self._detect_emotion_emergence_phenomena(
                data_sequence
            )
            
            # æƒ…ç»ªè‡ªç»„ç»‡åˆ†æ
            self_organization_analysis = self._analyze_emotion_self_organization(
                data_sequence
            )
            
            # æƒ…ç»ªä¸´ç•Œç°è±¡æ£€æµ‹
            critical_phenomena_analysis = self._detect_emotion_critical_phenomena(
                data_sequence
            )
            
            # === 8. å½“å‰æƒ…ç»ªçŠ¶æ€ç»¼åˆè¯„ä¼° ===
            current_emotion_state = self._assess_current_emotion_state({
                'instantaneous': instantaneous_emotions,
                'persistent': persistent_emotion_patterns,
                'dimensional': dimensional_emotion_analysis,
                'circumplex': circumplex_analysis,
                'social': social_emotion_analysis
            })
            
            # === 9. æƒ…ç»ªè´¨é‡å’Œå¯é æ€§è¯„ä¼° ===
            emotion_quality_assessment = self._assess_emotion_tracking_quality({
                'contagion_network': contagion_network_analysis,
                'stability': emotion_stability_analysis,
                'transition_matrix': emotion_transition_matrix,
                'prediction_model': transition_prediction_model
            })
            
            # === æ›´æ–°æƒ…ç»ªå†å² ===
            emotion_record = {
                'timestamp': self._get_timestamp(),
                'current_emotion': current_emotion_state,
                'emotion_intensity': emotion_intensity_analysis.get('current_intensity', 0.5),
                'emotion_stability': emotion_stability_analysis.get('stability_score', 0.5),
                'contagion_level': contagion_network_analysis.get('contagion_strength', 0.0),
                'transition_probability': emotion_transition_matrix.get('dominant_transition_prob', 0.0),
                'tracking_confidence': emotion_quality_assessment.get('overall_confidence', 0.5)
            }
            
            self.emotion_history.append(emotion_record)
            
            # === æ›´æ–°æƒ…ç»ªç»´åº¦å†å² ===
            self._update_emotion_dimensions(dimensional_emotion_analysis)
            
            # === æ„å»ºå®Œæ•´è·Ÿè¸ªç»“æœ ===
            comprehensive_emotion_tracking = {
                'timestamp': self._get_timestamp(),
                
                # æ ¸å¿ƒæƒ…ç»ªçŠ¶æ€
                'current_emotion_state': current_emotion_state,
                'emotion_intensity': emotion_record['emotion_intensity'],
                'emotion_stability': emotion_record['emotion_stability'],
                'emotion_trend': emotion_trend_prediction.get('primary_trend', 'stable'),
                
                # è¯¦ç»†åˆ†æç»„ä»¶
                'instantaneous_emotions': instantaneous_emotions,
                'persistent_emotion_patterns': persistent_emotion_patterns,
                'emotion_intensity_analysis': emotion_intensity_analysis,
                'dimensional_emotion_analysis': dimensional_emotion_analysis,
                'contagion_network_analysis': contagion_network_analysis,
                'emotion_diffusion_analysis': emotion_diffusion_analysis,
                'emotion_cascade_detection': emotion_cascade_detection,
                'emotion_transition_matrix': emotion_transition_matrix,
                'transition_prediction_model': transition_prediction_model,
                'emotion_stability_analysis': emotion_stability_analysis,
                
                # é«˜çº§æ¨¡å‹åˆ†æ
                'circumplex_analysis': circumplex_analysis,
                'appraisal_analysis': appraisal_analysis,
                'social_emotion_analysis': social_emotion_analysis,
                'odds_emotion_analysis': odds_emotion_analysis,
                
                # é¢„æµ‹å’Œé¢„è­¦
                'emotion_trend_prediction': emotion_trend_prediction,
                'emotion_risk_warnings': emotion_risk_warnings,
                'emotion_opportunities': emotion_opportunities,
                
                # å¤æ‚ç³»ç»Ÿåˆ†æ
                'emotion_emergence_analysis': emotion_emergence_analysis,
                'self_organization_analysis': self_organization_analysis,
                'critical_phenomena_analysis': critical_phenomena_analysis,
                
                # è´¨é‡å’Œå¯é æ€§
                'emotion_quality_assessment': emotion_quality_assessment,
                'tracking_confidence': emotion_record['tracking_confidence'],
                'model_consensus': self._calculate_emotion_model_consensus({
                    'circumplex': circumplex_analysis,
                    'appraisal': appraisal_analysis,
                    'social': social_emotion_analysis
                }),
                
                # å…ƒæ•°æ®
                'tracking_method': 'multi_dimensional_emotion_tracking',
                'models_used': list(self.emotion_models.keys()),
                'analysis_depth': len(data_sequence),
                'context_integration': context_data is not None
            }
            
            # === è‡ªé€‚åº”å­¦ä¹ æ›´æ–° ===
            self._update_emotion_learning_system(comprehensive_emotion_tracking)
            
            print(f"âœ… ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªå®Œæˆ - å½“å‰æƒ…ç»ª: {current_emotion_state['dominant_emotion']}, "
                  f"å¼ºåº¦: {emotion_record['emotion_intensity']:.3f}")
            
            return comprehensive_emotion_tracking
            
        except Exception as e:
            print(f"âŒ ç¾¤ä½“æƒ…ç»ªè·Ÿè¸ªå¤±è´¥: {e}")
            return self._generate_error_result(str(e))
    
    def _detect_instantaneous_emotions(self, data_sequence: List[Dict]) -> Dict:
        """æ£€æµ‹ç¬æ—¶æƒ…ç»ªçŠ¶æ€"""
        try:
            import numpy as np
            
            if not data_sequence:
                return {'dominant_emotion': 'neutral', 'confidence': 0.0}
            
            # åˆ†ææœ€æ–°æœŸçš„æ•°æ®
            latest_period = data_sequence[0]
            tails = latest_period.get('tails', [])
            
            if not tails:
                return {'dominant_emotion': 'neutral', 'confidence': 0.0}
            
            # å¤šç»´åº¦æƒ…ç»ªç‰¹å¾æå–
            
            # 1. é€‰æ‹©æ•°é‡ç‰¹å¾ï¼ˆå†³ç­–å¤æ‚æ€§ï¼‰
            selection_count = len(tails)
            if selection_count <= 2:
                decisiveness_emotion = 'confident'
                decisiveness_intensity = 0.8
            elif selection_count >= 8:
                decisiveness_emotion = 'confused'
                decisiveness_intensity = 0.7
            else:
                decisiveness_emotion = 'cautious'
                decisiveness_intensity = 0.5
            
            # 2. å°¾æ•°åˆ†å¸ƒç‰¹å¾ï¼ˆé£é™©åå¥½ï¼‰
            tail_mean = np.mean(tails)
            tail_std = np.std(tails) if len(tails) > 1 else 0
            
            # é£é™©æƒ…ç»ªåˆ†æ
            if tail_std > 3:
                risk_emotion = 'adventurous'
                risk_intensity = 0.8
            elif tail_std < 1:
                risk_emotion = 'conservative'
                risk_intensity = 0.7
            else:
                risk_emotion = 'balanced'
                risk_intensity = 0.5
            
            # 3. èµ”ç‡æ•æ„Ÿæ€§æƒ…ç»ªï¼ˆ0å°¾vså…¶ä»–ï¼‰
            has_zero_tail = 0 in tails
            zero_tail_ratio = (1 if has_zero_tail else 0) / len(tails)
            
            if has_zero_tail and len(tails) <= 3:
                odds_emotion = 'greedy'
                odds_intensity = 0.9
            elif not has_zero_tail and len(tails) >= 6:
                odds_emotion = 'risk_averse'
                odds_intensity = 0.6
            else:
                odds_emotion = 'pragmatic'
                odds_intensity = 0.5
            
            # 4. å†å²å¯¹æ¯”æƒ…ç»ªï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
            if len(data_sequence) > 1:
                prev_tails = set(data_sequence[1].get('tails', []))
                curr_tails = set(tails)
                
                overlap = len(curr_tails.intersection(prev_tails))
                change_magnitude = len(curr_tails.symmetric_difference(prev_tails)) / 10.0
                
                if change_magnitude > 0.7:
                    change_emotion = 'volatile'
                    change_intensity = 0.8
                elif change_magnitude < 0.2:
                    change_emotion = 'stable'
                    change_intensity = 0.6
                else:
                    change_emotion = 'adaptive'
                    change_intensity = 0.5
            else:
                change_emotion = 'neutral'
                change_intensity = 0.5
            
            # ç»¼åˆæƒ…ç»ªçŠ¶æ€è®¡ç®—
            emotion_components = {
                'decisiveness': (decisiveness_emotion, decisiveness_intensity),
                'risk_attitude': (risk_emotion, risk_intensity),
                'odds_sensitivity': (odds_emotion, odds_intensity),
                'change_pattern': (change_emotion, change_intensity)
            }
            
            # æƒ…ç»ªèåˆç®—æ³•
            dominant_emotion, overall_intensity = self._fuse_emotion_components(
                emotion_components
            )
            
            # æƒ…ç»ªç»´åº¦æ˜ å°„
            emotion_dimensions = self._map_to_emotion_dimensions(
                dominant_emotion, overall_intensity, tails
            )
            
            return {
                'dominant_emotion': dominant_emotion,
                'emotion_intensity': float(overall_intensity),
                'emotion_components': emotion_components,
                'emotion_dimensions': emotion_dimensions,
                'confidence': min(1.0, len(tails) / 5.0),
                'data_quality': self._assess_emotion_data_quality(tails)
            }
            
        except Exception as e:
            return {'error': str(e), 'dominant_emotion': 'neutral', 'confidence': 0.0}

class MarketSentimentAnalyzer:
    """
    ç§‘ç ”çº§å¸‚åœºæƒ…ç»ªåˆ†æå™¨ - åŸºäºæƒ…æ„Ÿé‡‘èå­¦å’Œè¡Œä¸ºç»æµå­¦
    
    ç†è®ºåŸºç¡€ï¼š
    - æƒ…æ„Ÿé‡‘èå­¦ç†è®º
    - å¸‚åœºæƒ…ç»ªæŒ‡æ ‡ç†è®º
    - æŠ•èµ„è€…æƒ…ç»ªä¸å¸‚åœºå¼‚è±¡
    - è¡Œä¸ºé‡‘èæƒ…ç»ªæ¨¡å‹
    - é›†ä½“æƒ…ç»ªä¸ä»·æ ¼å‘ç°
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç§‘ç ”çº§å¸‚åœºæƒ…ç»ªåˆ†æå™¨"""
        print("ğŸ“Š å¯åŠ¨ç§‘ç ”çº§å¸‚åœºæƒ…ç»ªåˆ†æå™¨...")
        
        # æƒ…ç»ªæŒ‡æ ‡å’Œå‘¨æœŸ
        self.sentiment_indicators = {
            'fear_greed_index': deque(maxlen=500),      # ææƒ§è´ªå©ªæŒ‡æ•°
            'volatility_sentiment': deque(maxlen=500),  # æ³¢åŠ¨æ€§æƒ…ç»ª
            'momentum_sentiment': deque(maxlen=500),    # åŠ¨é‡æƒ…ç»ª
            'contrarian_sentiment': deque(maxlen=500),  # åå‘æƒ…ç»ª
            'risk_appetite': deque(maxlen=500),         # é£é™©åå¥½
            'confidence_index': deque(maxlen=500),      # ä¿¡å¿ƒæŒ‡æ•°
            'uncertainty_index': deque(maxlen=500),     # ä¸ç¡®å®šæ€§æŒ‡æ•°
            'euphoria_index': deque(maxlen=500),        # ç‹‚æ¬¢æŒ‡æ•°
            'panic_index': deque(maxlen=500),           # ææ…ŒæŒ‡æ•°
            'complacency_index': deque(maxlen=500)      # è‡ªæ»¡æŒ‡æ•°
        }
        
        self.sentiment_cycles = deque(maxlen=200)
        
        # é«˜çº§æƒ…ç»ªåˆ†ææ¨¡å‹
        self.sentiment_models = {
            'behavioral_sentiment_model': self._init_behavioral_sentiment_model(),
            'technical_sentiment_model': self._init_technical_sentiment_model(),
            'fundamental_sentiment_model': self._init_fundamental_sentiment_model(),
            'social_sentiment_model': self._init_social_sentiment_model(),
            'cognitive_sentiment_model': self._init_cognitive_sentiment_model(),
            'network_sentiment_model': self._init_network_sentiment_model(),
            'adaptive_sentiment_model': self._init_adaptive_sentiment_model(),
            'ensemble_sentiment_model': self._init_ensemble_sentiment_model()
        }
        
        # æƒ…ç»ªé‡åŒ–æŒ‡æ ‡ç³»ç»Ÿ
        self.sentiment_metrics = {
            'overall_sentiment_score': 0.0,       # æ€»ä½“æƒ…ç»ªå¾—åˆ†
            'sentiment_momentum': 0.0,            # æƒ…ç»ªåŠ¨é‡
            'sentiment_volatility': 0.0,          # æƒ…ç»ªæ³¢åŠ¨æ€§
            'sentiment_persistence': 0.0,         # æƒ…ç»ªæŒç»­æ€§
            'sentiment_extremity': 0.0,           # æƒ…ç»ªæç«¯æ€§
            'sentiment_coherence': 0.0,           # æƒ…ç»ªä¸€è‡´æ€§
            'sentiment_predictability': 0.0,      # æƒ…ç»ªå¯é¢„æµ‹æ€§
            'sentiment_regime_stability': 0.0,    # æƒ…ç»ªåˆ¶åº¦ç¨³å®šæ€§
            'sentiment_mean_reversion': 0.0,      # æƒ…ç»ªå‡å€¼å›å½’
            'sentiment_trend_strength': 0.0       # æƒ…ç»ªè¶‹åŠ¿å¼ºåº¦
        }
        
        # åŠ¨æ€åˆ†æå‚æ•°
        self.analysis_parameters = {
            'sentiment_detection_threshold': 0.6,    # æƒ…ç»ªæ£€æµ‹é˜ˆå€¼
            'extreme_sentiment_threshold': 0.85,     # æç«¯æƒ…ç»ªé˜ˆå€¼
            'sentiment_change_sensitivity': 0.75,    # æƒ…ç»ªå˜åŒ–æ•æ„Ÿåº¦
            'temporal_smoothing_factor': 0.8,        # æ—¶é—´å¹³æ»‘å› å­
            'sentiment_memory_decay': 0.9,           # æƒ…ç»ªè®°å¿†è¡°å‡
            'regime_change_threshold': 0.7,          # åˆ¶åº¦å˜åŒ–é˜ˆå€¼
            'contrarian_signal_threshold': 0.8,      # åå‘ä¿¡å·é˜ˆå€¼
            'adaptive_learning_rate': 0.1            # è‡ªé€‚åº”å­¦ä¹ ç‡
        }
        
        # èµ”ç‡ç›¸å…³æƒ…ç»ªå‚æ•°
        self.odds_sentiment_parameters = {
            'zero_tail_greed_factor': 1.5,           # 0å°¾è´ªå©ªå› å­
            'high_odds_excitement_amplifier': 1.3,   # é«˜èµ”ç‡å…´å¥‹æ”¾å¤§å™¨
            'loss_aversion_sentiment_weight': 2.2,   # æŸå¤±åŒæ¶æƒ…ç»ªæƒé‡
            'risk_seeking_threshold': 0.7,           # é£é™©å¯»æ±‚é˜ˆå€¼
            'odds_differential_sentiment_impact': 0.2 # èµ”ç‡å·®å¼‚æƒ…ç»ªå½±å“
        }
        
        # æƒ…ç»ªå­¦ä¹ å’Œé¢„æµ‹ç³»ç»Ÿ
        self.learning_system = {
            'sentiment_prediction_accuracy': 0.0,
            'sentiment_classification_accuracy': 0.0,
            'regime_prediction_accuracy': 0.0,
            'model_performance_tracking': [],
            'feature_importance_analysis': {},
            'sentiment_forecasting_models': {},
            'ensemble_optimization_history': [],
            'adaptive_parameter_history': []
        }
        
        # é«˜çº§åˆ†æå·¥å…·
        self.analysis_tools = {
            'spectral_analyzer': self._init_spectral_analyzer(),
            'wavelet_analyzer': self._init_wavelet_analyzer(),
            'fractal_analyzer': self._init_fractal_analyzer(),
            'entropy_analyzer': self._init_entropy_analyzer(),
            'network_analyzer': self._init_network_analyzer(),
            'regime_analyzer': self._init_regime_analyzer(),
            'causality_analyzer': self._init_causality_analyzer(),
            'prediction_analyzer': self._init_prediction_analyzer()
        }
        
        print("âœ… ç§‘ç ”çº§å¸‚åœºæƒ…ç»ªåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_market_sentiment(self, market_data: List[Dict], 
                                external_factors: Dict = None) -> Dict:
        """
        åˆ†æå¸‚åœºæƒ…ç»ª - å¤šç»´åº¦ç»¼åˆåˆ†æ
        
        Args:
            market_data: å¸‚åœºæ•°æ®åºåˆ—
            external_factors: å¤–éƒ¨å› ç´ ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: å¸‚åœºæƒ…ç»ªåˆ†æç»“æœ
        """
        try:
            print(f"ğŸ“Š å¼€å§‹å¸‚åœºæƒ…ç»ªåˆ†æ...")
            
            if len(market_data) < 5:
                return self._generate_insufficient_data_result()
            
            # === 1. å¤šç»´åº¦æƒ…ç»ªæŒ‡æ ‡è®¡ç®— ===
            
            # ææƒ§è´ªå©ªæŒ‡æ•°è®¡ç®—
            fear_greed_analysis = self._calculate_fear_greed_index(market_data)
            
            # æ³¢åŠ¨æ€§æƒ…ç»ªåˆ†æ
            volatility_sentiment_analysis = self._analyze_volatility_sentiment(market_data)
            
            # åŠ¨é‡æƒ…ç»ªåˆ†æ
            momentum_sentiment_analysis = self._analyze_momentum_sentiment(market_data)
            
            # é£é™©åå¥½åˆ†æ
            risk_appetite_analysis = self._analyze_risk_appetite(market_data)
            
            # ä¿¡å¿ƒæŒ‡æ•°åˆ†æ
            confidence_analysis = self._analyze_market_confidence(market_data)
            
            # ä¸ç¡®å®šæ€§æŒ‡æ•°åˆ†æ
            uncertainty_analysis = self._analyze_market_uncertainty(market_data)
            
            # === 2. é«˜çº§æƒ…ç»ªæ¨¡å‹åˆ†æ ===
            
            # è¡Œä¸ºæƒ…ç»ªæ¨¡å‹
            behavioral_sentiment_analysis = self._apply_behavioral_sentiment_model(
                market_data
            )
            
            # æŠ€æœ¯æƒ…ç»ªæ¨¡å‹
            technical_sentiment_analysis = self._apply_technical_sentiment_model(
                market_data
            )
            
            # è®¤çŸ¥æƒ…ç»ªæ¨¡å‹
            cognitive_sentiment_analysis = self._apply_cognitive_sentiment_model(
                market_data, external_factors
            )
            
            # ç¤¾ä¼šæƒ…ç»ªæ¨¡å‹
            social_sentiment_analysis = self._apply_social_sentiment_model(market_data)
            
            # ç½‘ç»œæƒ…ç»ªæ¨¡å‹
            network_sentiment_analysis = self._apply_network_sentiment_model(market_data)
            
            # === 3. æƒ…ç»ªåˆ¶åº¦å’Œå‘¨æœŸåˆ†æ ===
            
            # æƒ…ç»ªåˆ¶åº¦è¯†åˆ«
            sentiment_regime_analysis = self._identify_sentiment_regimes(market_data)
            
            # æƒ…ç»ªå‘¨æœŸåˆ†æ
            sentiment_cycle_analysis = self._analyze_sentiment_cycles(market_data)
            
            # æƒ…ç»ªè½¬æ¢åˆ†æ
            sentiment_transition_analysis = self._analyze_sentiment_transitions(
                market_data
            )
            
            # === 4. èµ”ç‡ç›¸å…³æƒ…ç»ªåˆ†æ ===
            odds_sentiment_analysis = self._analyze_odds_related_sentiment(
                market_data, external_factors
            )
            
            # === 5. æç«¯æƒ…ç»ªå’Œå¼‚å¸¸æ£€æµ‹ ===
            
            # æç«¯æƒ…ç»ªæ£€æµ‹
            extreme_sentiment_detection = self._detect_extreme_sentiment_events(
                market_data
            )
            
            # æƒ…ç»ªå¼‚å¸¸æ£€æµ‹
            sentiment_anomaly_detection = self._detect_sentiment_anomalies(market_data)
            
            # æƒ…ç»ªæ³¡æ²«æ£€æµ‹
            sentiment_bubble_detection = self._detect_sentiment_bubbles(market_data)
            
            # === 6. æƒ…ç»ªé¢„æµ‹å’Œé¢„è­¦ ===
            
            # æƒ…ç»ªè¶‹åŠ¿é¢„æµ‹
            sentiment_trend_prediction = self._predict_sentiment_trends(market_data)
            
            # æƒ…ç»ªåˆ¶åº¦å˜åŒ–é¢„è­¦
            regime_change_warnings = self._generate_sentiment_regime_warnings(
                sentiment_regime_analysis, sentiment_transition_analysis
            )
            
            # åå‘æƒ…ç»ªä¿¡å·
            contrarian_sentiment_signals = self._generate_contrarian_sentiment_signals(
                fear_greed_analysis, extreme_sentiment_detection
            )
            
            # === 7. æƒ…ç»ªå½±å“åˆ†æ ===
            
            # æƒ…ç»ªå¯¹å†³ç­–çš„å½±å“
            sentiment_decision_impact = self._analyze_sentiment_decision_impact(
                market_data, behavioral_sentiment_analysis
            )
            
            # æƒ…ç»ªä¼ æŸ“åˆ†æ
            sentiment_contagion_analysis = self._analyze_sentiment_contagion(
                network_sentiment_analysis, social_sentiment_analysis
            )
            
            # æƒ…ç»ªåé¦ˆå¾ªç¯
            sentiment_feedback_loops = self._analyze_sentiment_feedback_loops(
                market_data
            )
            
            # === 8. ç»¼åˆæƒ…ç»ªè¯„ä¼° ===
            
            # ç»¼åˆæƒ…ç»ªå¾—åˆ†è®¡ç®—
            comprehensive_sentiment_score = self._calculate_comprehensive_sentiment_score({
                'fear_greed': fear_greed_analysis,
                'volatility': volatility_sentiment_analysis,
                'momentum': momentum_sentiment_analysis,
                'risk_appetite': risk_appetite_analysis,
                'confidence': confidence_analysis,
                'uncertainty': uncertainty_analysis,
                'behavioral': behavioral_sentiment_analysis,
                'technical': technical_sentiment_analysis,
                'cognitive': cognitive_sentiment_analysis,
                'social': social_sentiment_analysis,
                'network': network_sentiment_analysis
            })
            
            # æƒ…ç»ªè´¨é‡è¯„ä¼°
            sentiment_quality_assessment = self._assess_sentiment_analysis_quality({
                'regime_analysis': sentiment_regime_analysis,
                'cycle_analysis': sentiment_cycle_analysis,
                'prediction': sentiment_trend_prediction,
                'anomaly_detection': sentiment_anomaly_detection
            })
            
            # === æ›´æ–°æƒ…ç»ªå†å² ===
            sentiment_record = {
                'timestamp': self._get_timestamp(),
                'overall_sentiment': comprehensive_sentiment_score.get('overall_sentiment', 'neutral'),
                'sentiment_strength': comprehensive_sentiment_score.get('sentiment_strength', 0.5),
                'sentiment_confidence': sentiment_quality_assessment.get('overall_confidence', 0.5),
                'regime': sentiment_regime_analysis.get('current_regime', 'normal'),
                'cycle_phase': sentiment_cycle_analysis.get('current_phase', 'neutral'),
                'extreme_level': extreme_sentiment_detection.get('extremity_score', 0.0)
            }
            
            # æ›´æ–°å„ç§æƒ…ç»ªæŒ‡æ ‡
            self._update_sentiment_indicators({
                'fear_greed': fear_greed_analysis,
                'volatility': volatility_sentiment_analysis,
                'momentum': momentum_sentiment_analysis,
                'risk_appetite': risk_appetite_analysis,
                'confidence': confidence_analysis,
                'uncertainty': uncertainty_analysis
            })
            
            # æ›´æ–°æƒ…ç»ªå‘¨æœŸ
            self.sentiment_cycles.append(sentiment_record)
            
            # === æ„å»ºå®Œæ•´åˆ†æç»“æœ ===
            comprehensive_sentiment_analysis = {
                'timestamp': self._get_timestamp(),
                
                # æ ¸å¿ƒæƒ…ç»ªçŠ¶æ€
                'overall_sentiment': sentiment_record['overall_sentiment'],
                'sentiment_strength': sentiment_record['sentiment_strength'],
                'sentiment_confidence': sentiment_record['sentiment_confidence'],
                'current_regime': sentiment_record['regime'],
                'cycle_phase': sentiment_record['cycle_phase'],
                
                # è¯¦ç»†æŒ‡æ ‡åˆ†æ
                'fear_greed_analysis': fear_greed_analysis,
                'volatility_sentiment_analysis': volatility_sentiment_analysis,
                'momentum_sentiment_analysis': momentum_sentiment_analysis,
                'risk_appetite_analysis': risk_appetite_analysis,
                'confidence_analysis': confidence_analysis,
                'uncertainty_analysis': uncertainty_analysis,
                
                # é«˜çº§æ¨¡å‹åˆ†æ
                'behavioral_sentiment_analysis': behavioral_sentiment_analysis,
                'technical_sentiment_analysis': technical_sentiment_analysis,
                'cognitive_sentiment_analysis': cognitive_sentiment_analysis,
                'social_sentiment_analysis': social_sentiment_analysis,
                'network_sentiment_analysis': network_sentiment_analysis,
                
                # åˆ¶åº¦å’Œå‘¨æœŸåˆ†æ
                'sentiment_regime_analysis': sentiment_regime_analysis,
                'sentiment_cycle_analysis': sentiment_cycle_analysis,
                'sentiment_transition_analysis': sentiment_transition_analysis,
                
                # èµ”ç‡ç›¸å…³åˆ†æ
                'odds_sentiment_analysis': odds_sentiment_analysis,
                
                # æç«¯å’Œå¼‚å¸¸æ£€æµ‹
                'extreme_sentiment_detection': extreme_sentiment_detection,
                'sentiment_anomaly_detection': sentiment_anomaly_detection,
                'sentiment_bubble_detection': sentiment_bubble_detection,
                
                # é¢„æµ‹å’Œé¢„è­¦
                'sentiment_trend_prediction': sentiment_trend_prediction,
                'regime_change_warnings': regime_change_warnings,
                'contrarian_sentiment_signals': contrarian_sentiment_signals,
                
                # å½±å“åˆ†æ
                'sentiment_decision_impact': sentiment_decision_impact,
                'sentiment_contagion_analysis': sentiment_contagion_analysis,
                'sentiment_feedback_loops': sentiment_feedback_loops,
                
                # ç»¼åˆè¯„ä¼°
                'comprehensive_sentiment_score': comprehensive_sentiment_score,
                'sentiment_quality_assessment': sentiment_quality_assessment,
                
                # è´¨é‡ä¿è¯
                'model_consensus': self._calculate_sentiment_model_consensus({
                    'behavioral': behavioral_sentiment_analysis,
                    'technical': technical_sentiment_analysis,
                    'cognitive': cognitive_sentiment_analysis
                }),
                'analysis_reliability': self._assess_sentiment_analysis_reliability(
                    sentiment_quality_assessment
                ),
                
                # å…ƒæ•°æ®
                'analysis_method': 'multi_model_sentiment_analysis',
                'models_used': list(self.sentiment_models.keys()),
                'analysis_depth': len(market_data),
                'external_factors_considered': external_factors is not None
            }
            
            # === è‡ªé€‚åº”å­¦ä¹ æ›´æ–° ===
            self._update_sentiment_learning_system(comprehensive_sentiment_analysis)
            
            print(f"âœ… å¸‚åœºæƒ…ç»ªåˆ†æå®Œæˆ - æ€»ä½“æƒ…ç»ª: {sentiment_record['overall_sentiment']}, "
                  f"å¼ºåº¦: {sentiment_record['sentiment_strength']:.3f}")
            
            return comprehensive_sentiment_analysis
            
        except Exception as e:
            print(f"âŒ å¸‚åœºæƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            return self._generate_error_result(str(e))
    
    # ==================== è¾…åŠ©æ–¹æ³•å®ç° ====================
    
    def _calculate_fear_greed_index(self, market_data: List[Dict]) -> Dict:
        """è®¡ç®—ææƒ§è´ªå©ªæŒ‡æ•°"""
        try:
            import numpy as np
            
            # å¤šå› å­ææƒ§è´ªå©ªæŒ‡æ•°è®¡ç®—
            factors = {
                'volatility_factor': 0.0,
                'momentum_factor': 0.0,
                'volume_factor': 0.0,
                'diversity_factor': 0.0,
                'trend_factor': 0.0
            }
            
            # æ³¢åŠ¨æ€§å› å­
            recent_volatility = self._calculate_recent_volatility(market_data)
            factors['volatility_factor'] = max(0, min(100, (1 - recent_volatility) * 100))
            
            # åŠ¨é‡å› å­
            momentum = self._calculate_momentum(market_data)
            factors['momentum_factor'] = max(0, min(100, (momentum + 1) * 50))
            
            # å¤šæ ·æ€§å› å­
            diversity = self._calculate_selection_diversity(market_data)
            factors['diversity_factor'] = max(0, min(100, diversity * 100))
            
            # è¶‹åŠ¿å› å­
            trend_strength = self._calculate_trend_strength(market_data)
            factors['trend_factor'] = max(0, min(100, (trend_strength + 1) * 50))
            
            # ç»¼åˆæŒ‡æ•°è®¡ç®—
            fear_greed_index = (
                factors['volatility_factor'] * 0.3 +
                factors['momentum_factor'] * 0.25 +
                factors['diversity_factor'] * 0.2 +
                factors['trend_factor'] * 0.25
            )
            
            # æƒ…ç»ªåˆ†ç±»
            if fear_greed_index >= 75:
                sentiment = 'extreme_greed'
            elif fear_greed_index >= 55:
                sentiment = 'greed'
            elif fear_greed_index >= 45:
                sentiment = 'neutral'
            elif fear_greed_index >= 25:
                sentiment = 'fear'
            else:
                sentiment = 'extreme_fear'
            
            return {
                'fear_greed_index': float(fear_greed_index),
                'sentiment_classification': sentiment,
                'component_factors': factors,
                'confidence': min(1.0, len(market_data) / 10.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'fear_greed_index': 50.0}
    
    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        import datetime
        return datetime.datetime.now().isoformat()
    
    def _generate_insufficient_data_result(self) -> Dict:
        """ç”Ÿæˆæ•°æ®ä¸è¶³ç»“æœ"""
        return {
            'error': 'insufficient_data',
            'message': 'æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ',
            'min_required_data': 5,
            'analysis_confidence': 0.0
        }
    
    def _generate_error_result(self, error_message: str) -> Dict:
        """ç”Ÿæˆé”™è¯¯ç»“æœ"""
        return {
            'error': 'analysis_failed',
            'message': f'åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error_message}',
            'analysis_confidence': 0.0,
            'timestamp': self._get_timestamp()
        }