#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åè¶‹åŠ¿çŒæ‰‹ (AntiTrendHunter) - ç§‘ç ”çº§å®Œæ•´å®ç°
ä¸“é—¨è¯†åˆ«å’Œæ•æ‰è¶‹åŠ¿ç»ˆç»“ç‚¹ï¼Œè¿›è¡Œåè¶‹åŠ¿é¢„æµ‹
åŸºäºåŠ¨é‡ç†è®ºã€å‡å€¼å›å½’å’Œè¶‹åŠ¿åè½¬ä¿¡å·è¯†åˆ«
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åè¶‹åŠ¿çŒæ‰‹ (AntiTrendHunter) - ç§‘ç ”çº§å®Œæ•´å®ç°
ä¸“é—¨è¯†åˆ«å’Œæ•æ‰è¶‹åŠ¿ç»ˆç»“ç‚¹ï¼Œè¿›è¡Œåè¶‹åŠ¿é¢„æµ‹
åŸºäºåŠ¨é‡ç†è®ºã€å‡å€¼å›å½’å’Œè¶‹åŠ¿åè½¬ä¿¡å·è¯†åˆ«
ç§‘ç ”çº§å®ç°ï¼šé›†æˆé«˜çº§æ•°å­¦æ¨¡å‹ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€ä¿¡å·å¤„ç†
"""

# æ ¸å¿ƒç§‘å­¦è®¡ç®—åº“
import numpy as np
import scipy as sp
from scipy import signal, stats, optimize, interpolate, fft
from scipy.signal import hilbert, find_peaks, peak_widths, savgol_filter
from scipy.stats import jarque_bera, normaltest, anderson, kstest
from scipy.optimize import minimize, differential_evolution
import pandas as pd

# é«˜çº§æ•°å­¦ä¸ç»Ÿè®¡
import math
import cmath
from math import log, exp, sqrt, pi, e
from statistics import harmonic_mean, geometric_mean
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller, kpss, coint
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.regression.rolling import RollingOLS
import pykalman
from pykalman import KalmanFilter

# æœºå™¨å­¦ä¹ æ¡†æ¶
import sklearn
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, 
                             ExtraTreesRegressor, IsolationForest)
from sklearn.linear_model import (Ridge, Lasso, ElasticNet, BayesianRidge,
                                 HuberRegressor, TheilSenRegressor)
from sklearn.svm import SVR, NuSVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.decomposition import PCA, FastICA, NMF
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# æ·±åº¦å­¦ä¹  (å¦‚æœå¯ç”¨)
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Conv1D
    from tensorflow.keras.optimizers import Adam, RMSprop
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False

# ä¿¡å·å¤„ç†ä¸å°æ³¢åˆ†æ
import pywt
from pywt import wavedec, waverec, threshold

# æ—¶é—´åºåˆ—åˆ†æ
try:
    import arch
    from arch import arch_model
    ARCH_AVAILABLE = True
except ImportError:
    ARCH_AVAILABLE = False

# ä¼˜åŒ–ä¸æ•°å€¼è®¡ç®—
from scipy.linalg import svd, inv, pinv, norm
from scipy.sparse import csr_matrix
from numba import jit, njit, prange
import joblib
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# å¯è§†åŒ–ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle

# åŸºç¡€æ•°æ®ç»“æ„
from collections import defaultdict, deque, Counter, OrderedDict
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Set, Union, Callable
from dataclasses import dataclass, field
from enum import Enum, IntEnum
import itertools
import functools
import operator
import heapq
import bisect

# ç³»ç»Ÿä¸æ€§èƒ½
import warnings
import logging
import time
import gc
import psutil
import threading
import multiprocessing as mp

# éšæœºæ•°ä¸æ¦‚ç‡
import random
from numpy.random import RandomState
import secrets

# é…ç½®è­¦å‘Šè¿‡æ»¤
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)

# è®¾ç½®è®¡ç®—ç²¾åº¦
np.seterr(divide='ignore', invalid='ignore')
np.random.seed(42)  # å¯é‡ç°æ€§

@dataclass
class TrendSignal:
    """è¶‹åŠ¿ä¿¡å·æ•°æ®ç»“æ„"""
    tail_number: int
    trend_type: str  # 'uptrend', 'downtrend', 'sideways'
    trend_strength: float  # 0-1
    momentum: float  # åŠ¨é‡å€¼
    duration: int  # è¶‹åŠ¿æŒç»­æœŸæ•°
    reversal_probability: float  # åè½¬æ¦‚ç‡
    breakout_level: float  # çªç ´æ°´å¹³
    volume_divergence: float  # æˆäº¤é‡èƒŒç¦»æŒ‡æ ‡
    rsi_value: float  # ç›¸å¯¹å¼ºå¼±æŒ‡æ•°
    exhaustion_level: float  # è¶‹åŠ¿è€—å°½ç¨‹åº¦

@dataclass
class ReversalPattern:
    """åè½¬æ¨¡å¼æ•°æ®ç»“æ„"""
    pattern_name: str  # æ¨¡å¼åç§°
    confidence: float  # ç½®ä¿¡åº¦
    target_tails: List[int]  # ç›®æ ‡å°¾æ•°
    reversal_timing: str  # 'immediate', 'delayed', 'gradual'
    risk_level: float  # é£é™©æ°´å¹³
    expected_magnitude: float  # é¢„æœŸåè½¬å¹…åº¦

@dataclass
class WaveletCoefficients:
    """å°æ³¢åˆ†è§£ç³»æ•°"""
    approximation: np.ndarray  # è¿‘ä¼¼ç³»æ•°
    details: List[np.ndarray]  # ç»†èŠ‚ç³»æ•°
    levels: int  # åˆ†è§£å±‚æ•°
    wavelet: str  # å°æ³¢ç±»å‹
    reconstruction_error: float  # é‡æ„è¯¯å·®

@dataclass
class FourierComponents:
    """å‚…é‡Œå¶åˆ†æç»„ä»¶"""
    frequencies: np.ndarray  # é¢‘ç‡
    amplitudes: np.ndarray  # æŒ¯å¹…
    phases: np.ndarray  # ç›¸ä½
    power_spectrum: np.ndarray  # åŠŸç‡è°±
    dominant_frequencies: List[Tuple[float, float]]  # ä¸»å¯¼é¢‘ç‡åŠå…¶å¼ºåº¦

@dataclass
class NonlinearDynamics:
    """éçº¿æ€§åŠ¨åŠ›å­¦æŒ‡æ ‡"""
    lyapunov_exponent: float  # æé›…æ™®è¯ºå¤«æŒ‡æ•°
    correlation_dimension: float  # å…³è”ç»´æ•°
    hurst_exponent: float  # èµ«æ–¯ç‰¹æŒ‡æ•°
    entropy: float  # ç†µå€¼
    embedding_dimension: int  # åµŒå…¥ç»´æ•°
    time_delay: int  # æ—¶é—´å»¶è¿Ÿ

@dataclass
class MarketMicrostructure:
    """å¸‚åœºå¾®è§‚ç»“æ„"""
    bid_ask_spread: float  # ä¹°å–ä»·å·®
    market_depth: float  # å¸‚åœºæ·±åº¦
    order_flow_imbalance: float  # è®¢å•æµä¸å¹³è¡¡
    trade_intensity: float  # äº¤æ˜“å¼ºåº¦
    volatility_clustering: float  # æ³¢åŠ¨æ€§èšé›†
    jump_detection: bool  # è·³è·ƒæ£€æµ‹

@dataclass
class QuantumIndicators:
    """é‡å­åŒ–æŒ‡æ ‡"""
    coherence_measure: float  # ç›¸å¹²æ€§åº¦é‡
    entanglement_entropy: float  # çº ç¼ ç†µ
    quantum_fidelity: float  # é‡å­ä¿çœŸåº¦
    superposition_state: complex  # å åŠ æ€
    measurement_probability: np.ndarray  # æµ‹é‡æ¦‚ç‡

@dataclass
class MachineLearningMetrics:
    """æœºå™¨å­¦ä¹ è¯„ä¼°æŒ‡æ ‡"""
    prediction_accuracy: float  # é¢„æµ‹å‡†ç¡®ç‡
    feature_importance: Dict[str, float]  # ç‰¹å¾é‡è¦æ€§
    model_confidence: float  # æ¨¡å‹ç½®ä¿¡åº¦
    cross_validation_scores: List[float]  # äº¤å‰éªŒè¯å¾—åˆ†
    learning_curve: List[Tuple[float, float]]  # å­¦ä¹ æ›²çº¿
    overfitting_score: float  # è¿‡æ‹Ÿåˆè¯„åˆ†

@dataclass
class NeuralNetworkState:
    """ç¥ç»ç½‘ç»œçŠ¶æ€"""
    hidden_states: List[np.ndarray]  # éšè—çŠ¶æ€
    attention_weights: np.ndarray  # æ³¨æ„åŠ›æƒé‡
    gradient_norms: List[float]  # æ¢¯åº¦èŒƒæ•°
    loss_history: List[float]  # æŸå¤±å†å²
    activation_patterns: Dict[str, np.ndarray]  # æ¿€æ´»æ¨¡å¼
    network_topology: Dict[str, Any]  # ç½‘ç»œæ‹“æ‰‘

@dataclass
class AdaptiveThresholds:
    """è‡ªé€‚åº”é˜ˆå€¼ç³»ç»Ÿ"""
    static_thresholds: Dict[str, float]  # é™æ€é˜ˆå€¼
    dynamic_thresholds: Dict[str, float]  # åŠ¨æ€é˜ˆå€¼
    adaptive_rates: Dict[str, float]  # è‡ªé€‚åº”é€Ÿç‡
    threshold_history: Dict[str, List[float]]  # é˜ˆå€¼å†å²
    optimization_targets: Dict[str, float]  # ä¼˜åŒ–ç›®æ ‡
    convergence_status: Dict[str, bool]  # æ”¶æ•›çŠ¶æ€

@dataclass
class RiskMetrics:
    """é£é™©åº¦é‡æŒ‡æ ‡"""
    value_at_risk: float  # é£é™©ä»·å€¼
    conditional_var: float  # æ¡ä»¶é£é™©ä»·å€¼
    expected_shortfall: float  # æœŸæœ›æŸå¤±
    maximum_drawdown: float  # æœ€å¤§å›æ’¤
    sharpe_ratio: float  # å¤æ™®æ¯”ç‡
    sortino_ratio: float  # ç´¢æè¯ºæ¯”ç‡
    calmar_ratio: float  # å¡å°”é©¬æ¯”ç‡
    omega_ratio: float  # æ¬§ç±³èŒ„æ¯”ç‡

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½è¯„ä¼°æŒ‡æ ‡"""
    total_return: float  # æ€»æ”¶ç›Šç‡
    annualized_return: float  # å¹´åŒ–æ”¶ç›Šç‡
    volatility: float  # æ³¢åŠ¨ç‡
    information_ratio: float  # ä¿¡æ¯æ¯”ç‡
    tracking_error: float  # è·Ÿè¸ªè¯¯å·®
    beta: float  # è´å¡”ç³»æ•°
    alpha: float  # é˜¿å°”æ³•ç³»æ•°
    win_rate: float  # èƒœç‡

class SignalQuality(IntEnum):
    """ä¿¡å·è´¨é‡ç­‰çº§"""
    NOISE = 0
    WEAK = 1
    MODERATE = 2
    STRONG = 3
    VERY_STRONG = 4
    EXTREME = 5

class MarketRegime(Enum):
    """å¸‚åœºçŠ¶æ€"""
    BULL_MARKET = "bull"
    BEAR_MARKET = "bear"
    SIDEWAYS = "sideways"
    HIGH_VOLATILITY = "high_vol"
    LOW_VOLATILITY = "low_vol"
    CRISIS = "crisis"
    RECOVERY = "recovery"

class PredictionHorizon(Enum):
    """é¢„æµ‹æ—¶é—´èŒƒå›´"""
    ULTRA_SHORT = 1  # 1æœŸ
    SHORT = 3        # 3æœŸ
    MEDIUM = 7       # 7æœŸ
    LONG = 15        # 15æœŸ
    ULTRA_LONG = 30  # 30æœŸ

class TrendState(Enum):
    """è¶‹åŠ¿çŠ¶æ€æšä¸¾"""
    STRONG_UPTREND = 4
    MODERATE_UPTREND = 3
    WEAK_UPTREND = 2
    SIDEWAYS = 1
    WEAK_DOWNTREND = -2
    MODERATE_DOWNTREND = -3
    STRONG_DOWNTREND = -4

class AntiTrendHunter:
    """
    åè¶‹åŠ¿çŒæ‰‹ - ç§‘ç ”çº§å®Œæ•´å®ç°
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¤šç»´åº¦è¶‹åŠ¿å¼ºåº¦é‡åŒ–
    2. è¶‹åŠ¿è€—å°½ç‚¹ç²¾ç¡®è¯†åˆ«
    3. åè½¬ä¿¡å·ç»¼åˆè¯„åˆ†
    4. çªç ´å£åŠ¨æ€å‘ç°
    5. è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–ç§‘ç ”çº§åè¶‹åŠ¿çŒæ‰‹"""
        # åŸºç¡€é…ç½®
        self.config = config or self._get_default_config()
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        self._set_random_seeds(42)
        
        # ========== æ ¸å¿ƒæ•°æ®å­˜å‚¨ ==========
        self.trend_history = defaultdict(lambda: deque(maxlen=self.config['history_window']))
        self.reversal_patterns = {}
        self.trend_states = {}
        self.momentum_indicators = defaultdict(dict)
        
        # ========== é«˜çº§æŠ€æœ¯æŒ‡æ ‡ç¼“å­˜ ==========
        self.technical_indicators = {
            # ç»å…¸æŠ€æœ¯æŒ‡æ ‡
            'rsi': defaultdict(lambda: deque(maxlen=100)),
            'macd': defaultdict(lambda: deque(maxlen=100)),
            'macd_histogram': defaultdict(lambda: deque(maxlen=100)),
            'macd_signal': defaultdict(lambda: deque(maxlen=100)),
            'stochastic_k': defaultdict(lambda: deque(maxlen=100)),
            'stochastic_d': defaultdict(lambda: deque(maxlen=100)),
            'williams_r': defaultdict(lambda: deque(maxlen=100)),
            'cci': defaultdict(lambda: deque(maxlen=100)),
            'adx': defaultdict(lambda: deque(maxlen=100)),
            'atr': defaultdict(lambda: deque(maxlen=100)),
            'bollinger_upper': defaultdict(lambda: deque(maxlen=100)),
            'bollinger_lower': defaultdict(lambda: deque(maxlen=100)),
            'bollinger_middle': defaultdict(lambda: deque(maxlen=100)),
            'bollinger_width': defaultdict(lambda: deque(maxlen=100)),
            'bollinger_position': defaultdict(lambda: deque(maxlen=100)),
            
            # é«˜çº§æŠ€æœ¯æŒ‡æ ‡
            'ichimoku_tenkan': defaultdict(lambda: deque(maxlen=100)),
            'ichimoku_kijun': defaultdict(lambda: deque(maxlen=100)),
            'ichimoku_senkou_a': defaultdict(lambda: deque(maxlen=100)),
            'ichimoku_senkou_b': defaultdict(lambda: deque(maxlen=100)),
            'parabolic_sar': defaultdict(lambda: deque(maxlen=100)),
            'pivot_points': defaultdict(lambda: deque(maxlen=100)),
            'fibonacci_levels': defaultdict(lambda: deque(maxlen=100)),
            'gann_angles': defaultdict(lambda: deque(maxlen=100)),
            
            # æˆäº¤é‡æŒ‡æ ‡
            'obv': defaultdict(lambda: deque(maxlen=100)),
            'ad_line': defaultdict(lambda: deque(maxlen=100)),
            'mfi': defaultdict(lambda: deque(maxlen=100)),
            'vwap': defaultdict(lambda: deque(maxlen=100)),
            'volume_profile': defaultdict(lambda: deque(maxlen=100)),
            'chaikin_oscillator': defaultdict(lambda: deque(maxlen=100)),
            
            # æ³¢åŠ¨ç‡æŒ‡æ ‡
            'historical_volatility': defaultdict(lambda: deque(maxlen=100)),
            'implied_volatility': defaultdict(lambda: deque(maxlen=100)),
            'volatility_smile': defaultdict(lambda: deque(maxlen=100)),
            'volatility_surface': defaultdict(lambda: deque(maxlen=100)),
        }
        
        # ========== å°æ³¢åˆ†æç»„ä»¶ ==========
        self.wavelet_analyzer = {
            'coefficients': defaultdict(lambda: deque(maxlen=50)),
            'reconstruction_errors': defaultdict(lambda: deque(maxlen=50)),
            'energy_distribution': defaultdict(lambda: deque(maxlen=50)),
            'dominant_scales': defaultdict(lambda: deque(maxlen=50)),
            'singularity_spectrum': defaultdict(lambda: deque(maxlen=50)),
        }
        
        # ========== å‚…é‡Œå¶åˆ†æç»„ä»¶ ==========
        self.fourier_analyzer = {
            'frequency_components': defaultdict(lambda: deque(maxlen=50)),
            'phase_spectrum': defaultdict(lambda: deque(maxlen=50)),
            'power_spectrum': defaultdict(lambda: deque(maxlen=50)),
            'spectral_centroid': defaultdict(lambda: deque(maxlen=50)),
            'spectral_rolloff': defaultdict(lambda: deque(maxlen=50)),
            'spectral_flux': defaultdict(lambda: deque(maxlen=50)),
        }
        
        # ========== éçº¿æ€§åŠ¨åŠ›å­¦åˆ†æå™¨ ==========
        self.nonlinear_analyzer = {
            'lyapunov_exponents': defaultdict(lambda: deque(maxlen=30)),
            'correlation_dimensions': defaultdict(lambda: deque(maxlen=30)),
            'hurst_exponents': defaultdict(lambda: deque(maxlen=30)),
            'fractal_dimensions': defaultdict(lambda: deque(maxlen=30)),
            'entropy_measures': defaultdict(lambda: deque(maxlen=30)),
            'recurrence_plots': defaultdict(lambda: deque(maxlen=30)),
        }
        
        # ========== æœºå™¨å­¦ä¹ ç»„ä»¶ ==========
        self.ml_models = self._initialize_ml_models()
        self.feature_extractors = self._initialize_feature_extractors()
        self.model_ensemble = None
        self.feature_importance_tracker = defaultdict(lambda: deque(maxlen=100))
        self.prediction_cache = {}
        
        # ========== æ·±åº¦å­¦ä¹ ç»„ä»¶ ==========
        if TORCH_AVAILABLE:
            self.pytorch_models = self._initialize_pytorch_models()
        else:
            self.pytorch_models = {}
            
        if TF_AVAILABLE:
            self.tensorflow_models = self._initialize_tensorflow_models()
        else:
            self.tensorflow_models = {}
        
        # ========== å¡å°”æ›¼æ»¤æ³¢å™¨ ==========
        self.kalman_filters = {}
        for tail in range(10):
            self.kalman_filters[tail] = self._create_kalman_filter()
        
        # ========== é‡å­åŒ–æŒ‡æ ‡ ==========
        self.quantum_indicators = {
            'coherence_measures': defaultdict(lambda: deque(maxlen=50)),
            'entanglement_entropies': defaultdict(lambda: deque(maxlen=50)),
            'quantum_fidelities': defaultdict(lambda: deque(maxlen=50)),
            'superposition_states': defaultdict(lambda: deque(maxlen=50)),
            'measurement_probabilities': defaultdict(lambda: deque(maxlen=50)),
        }
        
        # ========== å¸‚åœºå¾®è§‚ç»“æ„åˆ†æå™¨ ==========
        self.microstructure_analyzer = {
            'bid_ask_spreads': defaultdict(lambda: deque(maxlen=100)),
            'market_depths': defaultdict(lambda: deque(maxlen=100)),
            'order_flow_imbalances': defaultdict(lambda: deque(maxlen=100)),
            'trade_intensities': defaultdict(lambda: deque(maxlen=100)),
            'volatility_clustering': defaultdict(lambda: deque(maxlen=100)),
            'jump_detections': defaultdict(lambda: deque(maxlen=100)),
        }
        
        # ========== é£é™©ç®¡ç†ç³»ç»Ÿ ==========
        self.risk_manager = {
            'var_calculations': defaultdict(lambda: deque(maxlen=100)),
            'expected_shortfalls': defaultdict(lambda: deque(maxlen=100)),
            'maximum_drawdowns': defaultdict(lambda: deque(maxlen=100)),
            'volatility_forecasts': defaultdict(lambda: deque(maxlen=100)),
            'correlation_matrices': deque(maxlen=50),
            'stress_test_results': defaultdict(lambda: deque(maxlen=20)),
        }
        
        # ========== æ€§èƒ½ç›‘æ§ç³»ç»Ÿ ==========
        self.performance_monitor = {
            'prediction_accuracies': deque(maxlen=1000),
            'execution_times': defaultdict(lambda: deque(maxlen=100)),
            'memory_usage': deque(maxlen=100),
            'model_performances': defaultdict(lambda: deque(maxlen=100)),
            'signal_qualities': defaultdict(lambda: deque(maxlen=100)),
        }
        
        # ========== è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿ ==========
        self.adaptive_system = {
            'threshold_history': defaultdict(lambda: deque(maxlen=200)),
            'adaptation_rates': defaultdict(float),
            'learning_curves': defaultdict(lambda: deque(maxlen=100)),
            'model_selection_history': deque(maxlen=50),
            'hyperparameter_optimization': {},
        }
        
        # ========== ä¿¡å·èåˆç³»ç»Ÿ ==========
        self.signal_fusion = {
            'signal_weights': self.config['signal_weights'].copy(),
            'weight_history': defaultdict(lambda: deque(maxlen=100)),
            'fusion_results': defaultdict(lambda: deque(maxlen=100)),
            'consensus_scores': defaultdict(lambda: deque(maxlen=100)),
        }
        
        # ========== æ¨¡å¼è¯†åˆ«åº“ ==========
        self.pattern_library = self._initialize_advanced_pattern_library()
        self.pattern_evolution_tracker = defaultdict(list)
        self.geometric_patterns = {}
        self.temporal_patterns = {}
        
        # ========== åˆ†æçª—å£ ==========
        self.analysis_windows = {
            'nano': 1,          # çº³ç§’çº§
            'micro': 3,         # å¾®ç§’çº§
            'ultra_short': 5,   # è¶…çŸ­æœŸ
            'short': 10,        # çŸ­æœŸ
            'medium': 20,       # ä¸­æœŸ
            'long': 50,         # é•¿æœŸ
            'ultra_long': 100,  # è¶…é•¿æœŸ
            'macro': 200,       # å®è§‚
            'epoch': 500,       # æ—¶ä»£çº§
        }
        
        # ========== å¹¶è¡Œå¤„ç† ==========
        if self.config['parallel_processing']:
            self.thread_pool = ThreadPoolExecutor(max_workers=self.config['max_workers'])
            self.process_pool = ProcessPoolExecutor(max_workers=self.config['max_workers'])
        else:
            self.thread_pool = None
            self.process_pool = None
        
        # ========== ç¼“å­˜ç³»ç»Ÿ ==========
        if self.config['cache_enabled']:
            self.cache = OrderedDict()
            self.cache_hits = 0
            self.cache_misses = 0
        
        # ========== ç»Ÿè®¡è·Ÿè¸ª ==========
        self.total_predictions = 0
        self.successful_reversals = 0
        self.false_signals = 0
        self.prediction_accuracy = 0.0
        self.model_confidence_history = deque(maxlen=1000)
        
        # ========== åŠ¨æ€é˜ˆå€¼ç®¡ç† ==========
        self.dynamic_thresholds = AdaptiveThresholds(
            static_thresholds={
                'reversal_confidence': 0.75,
                'trend_exhaustion': 0.8,
                'momentum_divergence': 0.65,
                'volume_anomaly': 0.7,
                'pattern_matching': 0.8,
                'signal_quality': 0.6,
            },
            dynamic_thresholds={
                'reversal_confidence': 0.75,
                'trend_exhaustion': 0.8,
                'momentum_divergence': 0.65,
                'volume_anomaly': 0.7,
                'pattern_matching': 0.8,
                'signal_quality': 0.6,
            },
            adaptive_rates={
                'reversal_confidence': 0.05,
                'trend_exhaustion': 0.03,
                'momentum_divergence': 0.04,
                'volume_anomaly': 0.06,
                'pattern_matching': 0.02,
                'signal_quality': 0.05,
            },
            threshold_history=defaultdict(list),
            optimization_targets={
                'prediction_accuracy': 0.85,
                'false_positive_rate': 0.1,
                'signal_noise_ratio': 3.0,
            },
            convergence_status=defaultdict(bool)
        )
        
        # ========== æ—¥å¿—ç³»ç»Ÿ ==========
        self._setup_logging()
        
        print(f"ğŸ§¬ ç§‘ç ”çº§åè¶‹åŠ¿çŒæ‰‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“Š æŠ€æœ¯æŒ‡æ ‡: {len(self.technical_indicators)}ç§")
        print(f"   ğŸŒŠ å°æ³¢åˆ†æ: {len(self.wavelet_analyzer)}ä¸ªç»„ä»¶")
        print(f"   ğŸ“¡ å‚…é‡Œå¶åˆ†æ: {len(self.fourier_analyzer)}ä¸ªç»„ä»¶")
        print(f"   ğŸ”¬ éçº¿æ€§åŠ¨åŠ›å­¦: {len(self.nonlinear_analyzer)}ä¸ªåˆ†æå™¨")
        print(f"   ğŸ¤– æœºå™¨å­¦ä¹ æ¨¡å‹: {len(self.ml_models)}ä¸ª")
        print(f"   ğŸ§  æ·±åº¦å­¦ä¹ : PyTorch({TORCH_AVAILABLE}), TensorFlow({TF_AVAILABLE})")
        print(f"   âš¡ é‡å­æŒ‡æ ‡: {len(self.quantum_indicators)}ç§")
        print(f"   ğŸ’¹ å¾®è§‚ç»“æ„: {len(self.microstructure_analyzer)}ä¸ªåˆ†æå™¨")
        print(f"   ğŸ”„ å¹¶è¡Œå¤„ç†: {self.config['parallel_processing']}")
        print(f"   ğŸ’¾ ç¼“å­˜ç³»ç»Ÿ: {self.config['cache_enabled']}")
        print(f"   ğŸ¯ åˆ†æçª—å£: {len(self.analysis_windows)}ä¸ª")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–ç§‘ç ”çº§é»˜è®¤é…ç½®"""
        return {
            # ========== åŸºç¡€åˆ†æå‚æ•° ==========
            'history_window': 500,  # å†å²æ•°æ®çª—å£
            'min_trend_duration': 5,  # æœ€å°è¶‹åŠ¿æŒç»­æœŸ
            'max_analysis_depth': 100,  # æœ€å¤§åˆ†ææ·±åº¦
            'data_smoothing_window': 7,  # æ•°æ®å¹³æ»‘çª—å£
            
            # ========== è¶‹åŠ¿åˆ†æå‚æ•° ==========
            'trend_detection_methods': ['linear', 'polynomial', 'exponential', 'logarithmic'],
            'trend_strength_threshold': 0.7,
            'trend_consistency_weight': 0.35,
            'trend_momentum_weight': 0.4,
            'trend_duration_weight': 0.25,
            
            # ========== æŠ€æœ¯æŒ‡æ ‡å‚æ•° ==========
            'rsi_period': 21,  # RSIå‘¨æœŸ
            'rsi_overbought': 75,  # RSIè¶…ä¹°çº¿
            'rsi_oversold': 25,  # RSIè¶…å–çº¿
            'macd_fast': 12,  # MACDå¿«çº¿
            'macd_slow': 26,  # MACDæ…¢çº¿
            'macd_signal': 9,  # MACDä¿¡å·çº¿
            'bollinger_period': 20,  # å¸ƒæ—å¸¦å‘¨æœŸ
            'bollinger_std': 2.5,  # å¸ƒæ—å¸¦æ ‡å‡†å·®å€æ•°
            'stochastic_k': 14,  # éšæœºæŒ‡æ ‡Kå€¼å‘¨æœŸ
            'stochastic_d': 3,  # éšæœºæŒ‡æ ‡Då€¼å‘¨æœŸ
            'williams_r_period': 14,  # Williams %Rå‘¨æœŸ
            'cci_period': 20,  # CCIå‘¨æœŸ
            'adx_period': 14,  # ADXå‘¨æœŸ
            'atr_period': 14,  # ATRå‘¨æœŸ
            
            # ========== å°æ³¢åˆ†æå‚æ•° ==========
            'wavelet_type': 'daubechies',  # å°æ³¢ç±»å‹
            'wavelet_order': 8,  # å°æ³¢é˜¶æ•°
            'decomposition_levels': 6,  # åˆ†è§£å±‚æ•°
            'wavelet_threshold_mode': 'soft',  # é˜ˆå€¼æ¨¡å¼
            'wavelet_threshold_method': 'sure',  # é˜ˆå€¼æ–¹æ³•
            'wavelet_boundary_mode': 'symmetric',  # è¾¹ç•Œå¤„ç†æ¨¡å¼
            
            # ========== å‚…é‡Œå¶åˆ†æå‚æ•° ==========
            'fft_window_type': 'hann',  # FFTçª—å£ç±»å‹
            'spectral_density_method': 'welch',  # åŠŸç‡è°±å¯†åº¦æ–¹æ³•
            'frequency_resolution': 0.01,  # é¢‘ç‡åˆ†è¾¨ç‡
            'dominant_frequency_threshold': 0.1,  # ä¸»å¯¼é¢‘ç‡é˜ˆå€¼
            'phase_coherence_threshold': 0.7,  # ç›¸ä½ç›¸å¹²æ€§é˜ˆå€¼
            
            # ========== éçº¿æ€§åŠ¨åŠ›å­¦å‚æ•° ==========
            'embedding_dimension_range': [3, 15],  # åµŒå…¥ç»´æ•°èŒƒå›´
            'time_delay_range': [1, 10],  # æ—¶é—´å»¶è¿ŸèŒƒå›´
            'lyapunov_min_data_points': 100,  # æé›…æ™®è¯ºå¤«æŒ‡æ•°æœ€å°æ•°æ®ç‚¹
            'correlation_dimension_max_radius': 0.5,  # å…³è”ç»´æ•°æœ€å¤§åŠå¾„
            'entropy_bin_count': 50,  # ç†µè®¡ç®—çš„binæ•°é‡
            
            # ========== æœºå™¨å­¦ä¹ å‚æ•° ==========
            'ml_ensemble_size': 7,  # é›†æˆæ¨¡å‹æ•°é‡
            'ml_cross_validation_folds': 5,  # äº¤å‰éªŒè¯æŠ˜æ•°
            'ml_test_size': 0.2,  # æµ‹è¯•é›†æ¯”ä¾‹
            'ml_validation_size': 0.15,  # éªŒè¯é›†æ¯”ä¾‹
            'ml_feature_selection_threshold': 0.01,  # ç‰¹å¾é€‰æ‹©é˜ˆå€¼
            'ml_max_features': 50,  # æœ€å¤§ç‰¹å¾æ•°
            'ml_regularization_strength': [0.001, 0.01, 0.1, 1.0],  # æ­£åˆ™åŒ–å¼ºåº¦
            'ml_learning_rates': [0.001, 0.01, 0.1],  # å­¦ä¹ ç‡
            'ml_max_iterations': 1000,  # æœ€å¤§è¿­ä»£æ¬¡æ•°
            'ml_convergence_tolerance': 1e-6,  # æ”¶æ•›å®¹å¿åº¦
            
            # ========== æ·±åº¦å­¦ä¹ å‚æ•° ==========
            'dl_hidden_layers': [128, 64, 32],  # éšè—å±‚èŠ‚ç‚¹æ•°
            'dl_dropout_rates': [0.2, 0.3, 0.4],  # Dropoutæ¯”ç‡
            'dl_activation_functions': ['relu', 'tanh', 'leaky_relu'],  # æ¿€æ´»å‡½æ•°
            'dl_batch_size': 64,  # æ‰¹æ¬¡å¤§å°
            'dl_epochs': 200,  # è®­ç»ƒè½®æ¬¡
            'dl_early_stopping_patience': 20,  # æ—©åœè€å¿ƒ
            'dl_learning_rate_schedule': 'cosine_annealing',  # å­¦ä¹ ç‡è°ƒåº¦
            'dl_weight_decay': 1e-4,  # æƒé‡è¡°å‡
            'dl_gradient_clipping': 1.0,  # æ¢¯åº¦è£å‰ª
            
            # ========== LSTM/GRUå‚æ•° ==========
            'lstm_units': [64, 32],  # LSTMå•å…ƒæ•°
            'lstm_sequence_length': 30,  # åºåˆ—é•¿åº¦
            'lstm_return_sequences': True,  # è¿”å›åºåˆ—
            'gru_units': [64, 32],  # GRUå•å…ƒæ•°
            'attention_heads': 8,  # æ³¨æ„åŠ›å¤´æ•°
            'attention_key_dim': 64,  # æ³¨æ„åŠ›é”®ç»´åº¦
            
            # ========== ä¼˜åŒ–ç®—æ³•å‚æ•° ==========
            'optimization_algorithm': 'differential_evolution',  # ä¼˜åŒ–ç®—æ³•
            'population_size': 50,  # ç§ç¾¤å¤§å°
            'mutation_rate': 0.8,  # å˜å¼‚ç‡
            'crossover_rate': 0.7,  # äº¤å‰ç‡
            'max_generations': 100,  # æœ€å¤§ä»£æ•°
            'tolerance': 1e-6,  # å®¹å¿åº¦
            'constraint_penalty': 1000,  # çº¦æŸæƒ©ç½š
            
            # ========== å¡å°”æ›¼æ»¤æ³¢å‚æ•° ==========
            'kalman_transition_matrices': None,  # çŠ¶æ€è½¬ç§»çŸ©é˜µ
            'kalman_observation_matrices': None,  # è§‚æµ‹çŸ©é˜µ
            'kalman_initial_state_mean': None,  # åˆå§‹çŠ¶æ€å‡å€¼
            'kalman_n_dim_state': 4,  # çŠ¶æ€ç»´åº¦
            'kalman_n_dim_obs': 1,  # è§‚æµ‹ç»´åº¦
            
            # ========== é£é™©ç®¡ç†å‚æ•° ==========
            'var_confidence_level': 0.95,  # VaRç½®ä¿¡æ°´å¹³
            'expected_shortfall_threshold': 0.05,  # æœŸæœ›æŸå¤±é˜ˆå€¼
            'maximum_drawdown_threshold': 0.15,  # æœ€å¤§å›æ’¤é˜ˆå€¼
            'volatility_threshold': 0.25,  # æ³¢åŠ¨ç‡é˜ˆå€¼
            'correlation_threshold': 0.7,  # ç›¸å…³æ€§é˜ˆå€¼
            
            # ========== åè½¬æ£€æµ‹å‚æ•° ==========
            'reversal_confidence_threshold': 0.75,  # åè½¬ç½®ä¿¡åº¦é˜ˆå€¼
            'reversal_magnitude_threshold': 0.1,  # åè½¬å¹…åº¦é˜ˆå€¼
            'reversal_timing_tolerance': 3,  # åè½¬æ—¶æœºå®¹å¿åº¦
            'pattern_matching_threshold': 0.8,  # æ¨¡å¼åŒ¹é…é˜ˆå€¼
            'signal_convergence_threshold': 0.7,  # ä¿¡å·æ”¶æ•›é˜ˆå€¼
            
            # ========== è‡ªé€‚åº”å­¦ä¹ å‚æ•° ==========
            'learning_rate': 0.01,  # å­¦ä¹ ç‡
            'adaptation_speed': 0.05,  # è‡ªé€‚åº”é€Ÿåº¦
            'memory_decay_factor': 0.95,  # è®°å¿†è¡°å‡å› å­
            'performance_window': 50,  # æ€§èƒ½è¯„ä¼°çª—å£
            'threshold_adjustment_sensitivity': 0.1,  # é˜ˆå€¼è°ƒæ•´æ•æ„Ÿåº¦
            'model_update_frequency': 10,  # æ¨¡å‹æ›´æ–°é¢‘ç‡
            
            # ========== è®¡ç®—æ€§èƒ½å‚æ•° ==========
            'parallel_processing': True,  # å¹¶è¡Œå¤„ç†
            'max_workers': mp.cpu_count() - 1,  # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
            'chunk_size': 100,  # æ•°æ®å—å¤§å°
            'memory_limit_gb': 8,  # å†…å­˜é™åˆ¶
            'cache_enabled': True,  # ç¼“å­˜å¯ç”¨
            'cache_size': 1000,  # ç¼“å­˜å¤§å°
            
            # ========== è°ƒè¯•å’Œç›‘æ§å‚æ•° ==========
            'debug_mode': False,  # è°ƒè¯•æ¨¡å¼
            'verbose_level': 1,  # è¯¦ç»†ç¨‹åº¦
            'log_predictions': True,  # è®°å½•é¢„æµ‹
            'save_intermediate_results': False,  # ä¿å­˜ä¸­é—´ç»“æœ
            'performance_monitoring': True,  # æ€§èƒ½ç›‘æ§
            'memory_monitoring': True,  # å†…å­˜ç›‘æ§
            
            # ========== é‡å­åŒ–æŒ‡æ ‡å‚æ•° ==========
            'quantum_coherence_threshold': 0.8,  # é‡å­ç›¸å¹²æ€§é˜ˆå€¼
            'entanglement_measure_type': 'von_neumann',  # çº ç¼ åº¦é‡ç±»å‹
            'quantum_state_dimensions': 4,  # é‡å­æ€ç»´åº¦
            'measurement_basis': 'computational',  # æµ‹é‡åŸº
            'decoherence_time': 100,  # å»ç›¸å¹²æ—¶é—´
            
            # ========== å¸‚åœºå¾®è§‚ç»“æ„å‚æ•° ==========
            'bid_ask_spread_threshold': 0.001,  # ä¹°å–ä»·å·®é˜ˆå€¼
            'market_depth_levels': 5,  # å¸‚åœºæ·±åº¦å±‚æ•°
            'order_flow_window': 20,  # è®¢å•æµçª—å£
            'trade_intensity_smoothing': 0.3,  # äº¤æ˜“å¼ºåº¦å¹³æ»‘
            'volatility_clustering_metric': 'garch',  # æ³¢åŠ¨æ€§èšé›†åº¦é‡
            
            # ========== æ¨¡å¼è¯†åˆ«å‚æ•° ==========
            'pattern_library_size': 50,  # æ¨¡å¼åº“å¤§å°
            'pattern_similarity_threshold': 0.85,  # æ¨¡å¼ç›¸ä¼¼æ€§é˜ˆå€¼
            'pattern_evolution_tracking': True,  # æ¨¡å¼æ¼”åŒ–è·Ÿè¸ª
            'geometric_pattern_tolerance': 0.05,  # å‡ ä½•æ¨¡å¼å®¹å¿åº¦
            'temporal_pattern_weight': 0.6,  # æ—¶é—´æ¨¡å¼æƒé‡
            
            # ========== ä¿¡å·èåˆå‚æ•° ==========
            'signal_fusion_method': 'weighted_average',  # ä¿¡å·èåˆæ–¹æ³•
            'signal_weights': {  # ä¿¡å·æƒé‡
                'technical': 0.25,
                'wavelet': 0.2,
                'fourier': 0.15,
                'nonlinear': 0.15,
                'ml': 0.15,
                'quantum': 0.1
            },
            'signal_conflict_resolution': 'majority_vote',  # ä¿¡å·å†²çªè§£å†³æ–¹æ³•
            'signal_quality_threshold': SignalQuality.MODERATE,  # ä¿¡å·è´¨é‡é˜ˆå€¼
        }
    

    def _set_random_seeds(self, seed: int):
        """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
        np.random.seed(seed)
        random.seed(seed)
        if TORCH_AVAILABLE:
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
                torch.cuda.manual_seed_all(seed)
        if TF_AVAILABLE:
            tf.random.set_seed(seed)
    
    def _initialize_ml_models(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹é›†åˆ"""
        models = {}
        
        # é›†æˆæ–¹æ³•
        models['random_forest'] = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        models['gradient_boosting'] = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )
        
        models['extra_trees'] = ExtraTreesRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # çº¿æ€§æ¨¡å‹
        models['ridge'] = Ridge(alpha=1.0, random_state=42)
        models['lasso'] = Lasso(alpha=1.0, random_state=42, max_iter=1000)
        models['elastic_net'] = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42, max_iter=1000)
        models['bayesian_ridge'] = BayesianRidge(alpha_1=1e-6, alpha_2=1e-6, lambda_1=1e-6, lambda_2=1e-6)
        models['huber'] = HuberRegressor(epsilon=1.35, max_iter=100, alpha=0.0001)
        models['theil_sen'] = TheilSenRegressor(random_state=42, max_iter=300)
        
        # æ”¯æŒå‘é‡æœº
        models['svr'] = SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=0.1)
        models['nu_svr'] = NuSVR(kernel='rbf', C=1.0, gamma='scale', nu=0.5)
        
        # è¿‘é‚»æ–¹æ³•
        models['knn'] = KNeighborsRegressor(n_neighbors=5, weights='distance', algorithm='auto')
        
        # é«˜æ–¯è¿‡ç¨‹
        models['gaussian_process'] = GaussianProcessRegressor(
            alpha=1e-10,
            normalize_y=True,
            random_state=42
        )
        
        # ç¥ç»ç½‘ç»œ
        models['mlp'] = MLPRegressor(
            hidden_layer_sizes=(100, 50),
            activation='relu',
            solver='adam',
            alpha=0.0001,
            batch_size='auto',
            learning_rate='constant',
            learning_rate_init=0.001,
            max_iter=200,
            random_state=42
        )
        
        return models
    
    def _initialize_feature_extractors(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–ç‰¹å¾æå–å™¨"""
        extractors = {}
        
        # æ•°æ®é¢„å¤„ç†
        extractors['standard_scaler'] = StandardScaler()
        extractors['robust_scaler'] = RobustScaler()
        extractors['minmax_scaler'] = MinMaxScaler()
        
        # é™ç»´
        extractors['pca'] = PCA(n_components=0.95, random_state=42)
        extractors['ica'] = FastICA(n_components=None, random_state=42, max_iter=200)
        extractors['nmf'] = NMF(n_components=10, random_state=42, max_iter=200)
        
        # èšç±»
        extractors['kmeans'] = KMeans(n_clusters=5, random_state=42, n_init=10)
        extractors['dbscan'] = DBSCAN(eps=0.5, min_samples=5)
        extractors['agglomerative'] = AgglomerativeClustering(n_clusters=5)
        
        # å¼‚å¸¸æ£€æµ‹
        extractors['isolation_forest'] = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        
        return extractors
    
    def _initialize_pytorch_models(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–PyTorchæ¨¡å‹"""
        if not TORCH_AVAILABLE:
            return {}
        
        models = {}
        
        # å®šä¹‰LSTMæ¨¡å‹
        class LSTMPredictor(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
                super(LSTMPredictor, self).__init__()
                self.hidden_size = hidden_size
                self.num_layers = num_layers
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                                  batch_first=True, dropout=dropout)
                self.fc = nn.Linear(hidden_size, output_size)
                self.dropout = nn.Dropout(dropout)
                
            def forward(self, x):
                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
                c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
                out, _ = self.lstm(x, (h0, c0))
                out = self.dropout(out[:, -1, :])
                out = self.fc(out)
                return out
        
        # å®šä¹‰GRUæ¨¡å‹
        class GRUPredictor(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
                super(GRUPredictor, self).__init__()
                self.hidden_size = hidden_size
                self.num_layers = num_layers
                self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                                batch_first=True, dropout=dropout)
                self.fc = nn.Linear(hidden_size, output_size)
                self.dropout = nn.Dropout(dropout)
                
            def forward(self, x):
                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)
                out, _ = self.gru(x, h0)
                out = self.dropout(out[:, -1, :])
                out = self.fc(out)
                return out
        
        # å®šä¹‰Transformeræ¨¡å‹
        class TransformerPredictor(nn.Module):
            def __init__(self, input_size, d_model, nhead, num_layers, output_size, dropout=0.1):
                super(TransformerPredictor, self).__init__()
                self.input_projection = nn.Linear(input_size, d_model)
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model, 
                    nhead=nhead, 
                    dropout=dropout,
                    batch_first=True
                )
                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
                self.fc = nn.Linear(d_model, output_size)
                self.dropout = nn.Dropout(dropout)
                
            def forward(self, x):
                x = self.input_projection(x)
                x = self.transformer(x)
                x = self.dropout(x.mean(dim=1))  # Global average pooling
                x = self.fc(x)
                return x
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        models['lstm'] = LSTMPredictor(
            input_size=20, 
            hidden_size=64, 
            num_layers=2, 
            output_size=1,
            dropout=0.2
        )
        
        models['gru'] = GRUPredictor(
            input_size=20, 
            hidden_size=64, 
            num_layers=2, 
            output_size=1,
            dropout=0.2
        )
        
        models['transformer'] = TransformerPredictor(
            input_size=20, 
            d_model=128, 
            nhead=8, 
            num_layers=3, 
            output_size=1,
            dropout=0.1
        )
        
        return models
    
    def _initialize_tensorflow_models(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–TensorFlowæ¨¡å‹"""
        if not TF_AVAILABLE:
            return {}
        
        models = {}
        
        # LSTMæ¨¡å‹
        lstm_model = Sequential([
            LSTM(64, return_sequences=True, input_shape=(30, 20)),
            Dropout(0.2),
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1, activation='linear')
        ])
        lstm_model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])
        models['lstm'] = lstm_model
        
        # GRUæ¨¡å‹
        gru_model = Sequential([
            GRU(64, return_sequences=True, input_shape=(30, 20)),
            Dropout(0.2),
            GRU(32, return_sequences=False),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1, activation='linear')
        ])
        gru_model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])
        models['gru'] = gru_model
        
        # CNN-LSTMæ¨¡å‹
        cnn_lstm_model = Sequential([
            Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(30, 20)),
            Conv1D(filters=32, kernel_size=3, activation='relu'),
            LSTM(50, return_sequences=False),
            Dropout(0.2),
            Dense(25, activation='relu'),
            Dense(1, activation='linear')
        ])
        cnn_lstm_model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])
        models['cnn_lstm'] = cnn_lstm_model
        
        return models
    
    def _create_kalman_filter(self) -> KalmanFilter:
        """åˆ›å»ºå¡å°”æ›¼æ»¤æ³¢å™¨"""
        transition_matrices = np.array([[1, 1, 0, 0],
                                      [0, 1, 1, 0],
                                      [0, 0, 1, 1],
                                      [0, 0, 0, 1]])
        
        observation_matrices = np.array([[1, 0, 0, 0]])
        
        initial_state_mean = np.array([0, 0, 0, 0])
        
        initial_state_covariance = np.eye(4) * 1000
        
        transition_covariance = np.eye(4) * 0.01
        
        observation_covariance = np.array([[1]])
        
        kf = KalmanFilter(
            transition_matrices=transition_matrices,
            observation_matrices=observation_matrices,
            initial_state_mean=initial_state_mean,
            initial_state_covariance=initial_state_covariance,
            transition_covariance=transition_covariance,
            observation_covariance=observation_covariance
        )
        
        return kf
    
    def _initialize_advanced_pattern_library(self) -> Dict[str, Callable]:
        """åˆå§‹åŒ–é«˜çº§æ¨¡å¼è¯†åˆ«åº“"""
        return {
            # ç»å…¸æŠ€æœ¯åˆ†ææ¨¡å¼
            'double_top': self._detect_double_top_advanced,
            'double_bottom': self._detect_double_bottom_advanced,
            'head_shoulders': self._detect_head_shoulders_advanced,
            'inverse_head_shoulders': self._detect_inverse_head_shoulders,
            'triple_top': self._detect_triple_top,
            'triple_bottom': self._detect_triple_bottom,
            'ascending_triangle': self._detect_ascending_triangle,
            'descending_triangle': self._detect_descending_triangle,
            'symmetric_triangle': self._detect_symmetric_triangle,
            'rising_wedge': self._detect_rising_wedge,
            'falling_wedge': self._detect_falling_wedge,
            'flag': self._detect_flag_advanced,
            'pennant': self._detect_pennant,
            'cup_handle': self._detect_cup_handle,
            'rounding_top': self._detect_rounding_top,
            'rounding_bottom': self._detect_rounding_bottom,
            
            # æ—¥æœ¬èœ¡çƒ›å›¾æ¨¡å¼
            'doji': self._detect_doji,
            'hammer': self._detect_hammer,
            'hanging_man': self._detect_hanging_man,
            'shooting_star': self._detect_shooting_star,
            'engulfing_bullish': self._detect_engulfing_bullish,
            'engulfing_bearish': self._detect_engulfing_bearish,
            'morning_star': self._detect_morning_star,
            'evening_star': self._detect_evening_star,
            'three_white_soldiers': self._detect_three_white_soldiers,
            'three_black_crows': self._detect_three_black_crows,
            
            # æ³¢æµªç†è®ºæ¨¡å¼
            'elliott_wave_1': self._detect_elliott_wave_1,
            'elliott_wave_2': self._detect_elliott_wave_2,
            'elliott_wave_3': self._detect_elliott_wave_3,
            'elliott_wave_4': self._detect_elliott_wave_4,
            'elliott_wave_5': self._detect_elliott_wave_5,
            'corrective_wave_a': self._detect_corrective_wave_a,
            'corrective_wave_b': self._detect_corrective_wave_b,
            'corrective_wave_c': self._detect_corrective_wave_c,
            
            # åˆ†å½¢å‡ ä½•æ¨¡å¼
            'fractal_support': self._detect_fractal_support,
            'fractal_resistance': self._detect_fractal_resistance,
            'chaos_theory_pattern': self._detect_chaos_pattern,
            'mandelbrot_pattern': self._detect_mandelbrot_pattern,
            
            # é‡å­åŒ–æ¨¡å¼
            'quantum_superposition': self._detect_quantum_superposition,
            'quantum_entanglement': self._detect_quantum_entanglement,
            'quantum_coherence': self._detect_quantum_coherence,
            
            # æœºå™¨å­¦ä¹ å‘ç°çš„æ¨¡å¼
            'ml_discovered_pattern_1': self._detect_ml_pattern_1,
            'ml_discovered_pattern_2': self._detect_ml_pattern_2,
            'ml_discovered_pattern_3': self._detect_ml_pattern_3,
            
            # æ—¶é—´åºåˆ—æ¨¡å¼
            'seasonal_pattern': self._detect_seasonal_pattern,
            'cyclical_pattern': self._detect_cyclical_pattern,
            'trend_break_pattern': self._detect_trend_break_pattern,
            'mean_reversion_pattern': self._detect_mean_reversion_pattern,
            
            # å¤æ‚ç³»ç»Ÿæ¨¡å¼
            'emergence_pattern': self._detect_emergence_pattern,
            'self_organization': self._detect_self_organization,
            'phase_transition': self._detect_phase_transition,
            'critical_point': self._detect_critical_point,
        }
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO if self.config['debug_mode'] else logging.WARNING,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('anti_trend_hunter.log')
            ]
        )
        self.logger = logging.getLogger('AntiTrendHunter')

    def predict(self, candidate_tails: Tuple[int], historical_data_hash: str, 
               prediction_horizon: PredictionHorizon = PredictionHorizon.SHORT) -> Dict[str, Any]:
        """
        ç§‘ç ”çº§ä¸»é¢„æµ‹æ–¹æ³• - å¤šç»´åº¦åˆ†æå€™é€‰å°¾æ•°çš„åè½¬æ½œåŠ›
        
        Args:
            candidate_tails: ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰åçš„å€™é€‰å°¾æ•°å…ƒç»„
            historical_data_hash: å†å²æ•°æ®çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç¼“å­˜ï¼‰
            prediction_horizon: é¢„æµ‹æ—¶é—´èŒƒå›´
            
        Returns:
            åŒ…å«è¯¦ç»†åˆ†æç»“æœçš„é¢„æµ‹å­—å…¸
        """
        start_time = time.time()
        
        # å°†å…ƒç»„è½¬æ¢å›åˆ—è¡¨ï¼ˆä¸ºäº†å…¼å®¹ï¼‰
        candidate_tails_list = list(candidate_tails)
        
        # ä»ç¼“å­˜æ¢å¤å†å²æ•°æ®
        historical_data = self._get_historical_data_from_hash(historical_data_hash)
        
        if not candidate_tails_list or len(historical_data) < 10:
            return self._create_failure_result('insufficient_data')
        
        self.logger.info(f"ğŸ¯ ç§‘ç ”çº§åè¶‹åŠ¿çŒæ‰‹å¼€å§‹æ·±åº¦åˆ†æ")
        self.logger.info(f"   å€™é€‰å°¾æ•°: {sorted(candidate_tails_list)}")
        self.logger.info(f"   é¢„æµ‹èŒƒå›´: {prediction_horizon.name}")
        self.logger.info(f"   æ•°æ®é•¿åº¦: {len(historical_data)}")
        
        try:
            # ========== é˜¶æ®µ1: æ•°æ®é¢„å¤„ç†ä¸è´¨é‡æ£€æŸ¥ ==========
            preprocessed_data = self._preprocess_data_advanced(historical_data)
            data_quality_score = self._assess_data_quality(preprocessed_data)
            
            if data_quality_score < 0.6:
                return self._create_failure_result('poor_data_quality', 
                                                 details={'quality_score': data_quality_score})
            
            # ========== é˜¶æ®µ2: å¤šç»´åº¦ç‰¹å¾æå– ==========
            feature_matrix = self._extract_comprehensive_features(
                candidate_tails_list, preprocessed_data, prediction_horizon
            )
            
            # ========== é˜¶æ®µ3: é«˜çº§æŠ€æœ¯åˆ†æ ==========
            technical_analysis_results = {}
            for tail in candidate_tails_list:
                technical_analysis_results[tail] = self._perform_advanced_technical_analysis(
                    tail, preprocessed_data, prediction_horizon
                )
            
            # ========== é˜¶æ®µ4: å°æ³¢åˆ†æ ==========
            wavelet_analysis_results = {}
            if self.config['debug_mode']:
                print("ğŸŒŠ æ‰§è¡Œå°æ³¢åˆ†æ...")
            
            for tail in candidate_tails_list:
                wavelet_analysis_results[tail] = self._perform_wavelet_analysis(
                    tail, preprocessed_data
                )
            
            # ========== é˜¶æ®µ5: å‚…é‡Œå¶é¢‘åŸŸåˆ†æ ==========
            fourier_analysis_results = {}
            if self.config['debug_mode']:
                print("ğŸ“¡ æ‰§è¡Œå‚…é‡Œå¶åˆ†æ...")
            
            for tail in candidate_tails_list:
                fourier_analysis_results[tail] = self._perform_fourier_analysis(
                    tail, preprocessed_data
                )
            
            # ========== é˜¶æ®µ6: éçº¿æ€§åŠ¨åŠ›å­¦åˆ†æ ==========
            nonlinear_analysis_results = {}
            if self.config['debug_mode']:
                print("ğŸ”¬ æ‰§è¡Œéçº¿æ€§åŠ¨åŠ›å­¦åˆ†æ...")
            
            for tail in candidate_tails_list:
                nonlinear_analysis_results[tail] = self._perform_nonlinear_dynamics_analysis(
                    tail, preprocessed_data
                )
            
            # ========== é˜¶æ®µ7: æœºå™¨å­¦ä¹ é›†æˆé¢„æµ‹ ==========
            ml_predictions = {}
            if self.config['debug_mode']:
                print("ğŸ¤– æ‰§è¡Œæœºå™¨å­¦ä¹ é¢„æµ‹...")
            
            for tail in candidate_tails_list:
                ml_predictions[tail] = self._perform_ml_ensemble_prediction(
                    tail, feature_matrix[tail], preprocessed_data
                )
            
            # ========== é˜¶æ®µ8: æ·±åº¦å­¦ä¹ é¢„æµ‹ ==========
            dl_predictions = {}
            if TORCH_AVAILABLE or TF_AVAILABLE:
                if self.config['debug_mode']:
                    print("ğŸ§  æ‰§è¡Œæ·±åº¦å­¦ä¹ é¢„æµ‹...")
                
                for tail in candidate_tails_list:
                    dl_predictions[tail] = self._perform_deep_learning_prediction(
                        tail, feature_matrix[tail], preprocessed_data
                    )
            
            # ========== é˜¶æ®µ9: é‡å­åŒ–åˆ†æ ==========
            quantum_analysis_results = {}
            if self.config['debug_mode']:
                print("âš¡ æ‰§è¡Œé‡å­åŒ–åˆ†æ...")
            
            for tail in candidate_tails_list:
                quantum_analysis_results[tail] = self._perform_quantum_analysis(
                    tail, preprocessed_data
                )
            
            # ========== é˜¶æ®µ10: å¸‚åœºå¾®è§‚ç»“æ„åˆ†æ ==========
            microstructure_analysis_results = {}
            if self.config['debug_mode']:
                print("ğŸ’¹ æ‰§è¡Œå¸‚åœºå¾®è§‚ç»“æ„åˆ†æ...")
            
            for tail in candidate_tails_list:
                microstructure_analysis_results[tail] = self._perform_microstructure_analysis(
                    tail, preprocessed_data
                )
            
            # ========== é˜¶æ®µ11: å¡å°”æ›¼æ»¤æ³¢çŠ¶æ€ä¼°è®¡ ==========
            kalman_predictions = {}
            if self.config['debug_mode']:
                print("ğŸ”„ æ‰§è¡Œå¡å°”æ›¼æ»¤æ³¢...")
            
            for tail in candidate_tails_list:
                kalman_predictions[tail] = self._perform_kalman_filtering(
                    tail, preprocessed_data
                )
            
            # ========== é˜¶æ®µ12: é«˜çº§æ¨¡å¼è¯†åˆ« ==========
            pattern_analysis_results = {}
            if self.config['debug_mode']:
                print("ğŸ” æ‰§è¡Œé«˜çº§æ¨¡å¼è¯†åˆ«...")
            
            for tail in candidate_tails_list:
                pattern_analysis_results[tail] = self._perform_advanced_pattern_recognition(
                    tail, preprocessed_data
                )
            
            # ========== é˜¶æ®µ13: å¤šä¿¡å·èåˆ ==========
            if self.config['debug_mode']:
                print("ğŸ”€ æ‰§è¡Œå¤šä¿¡å·èåˆ...")
            
            fusion_results = self._perform_signal_fusion(
                candidate_tails_list,
                {
                    'technical': technical_analysis_results,
                    'wavelet': wavelet_analysis_results,
                    'fourier': fourier_analysis_results,
                    'nonlinear': nonlinear_analysis_results,
                    'ml': ml_predictions,
                    'dl': dl_predictions,
                    'quantum': quantum_analysis_results,
                    'microstructure': microstructure_analysis_results,
                    'kalman': kalman_predictions,
                    'pattern': pattern_analysis_results
                }
            )
            
            # ========== é˜¶æ®µ14: é£é™©è¯„ä¼° ==========
            risk_assessments = {}
            for tail in candidate_tails_list:
                risk_assessments[tail] = self._perform_comprehensive_risk_assessment(
                    tail, fusion_results[tail], preprocessed_data
                )
            
            # ========== é˜¶æ®µ15: æœ€ä¼˜é€‰æ‹©ä¸æ’åº ==========
            final_scores = {}
            detailed_analysis = {}
            
            for tail in candidate_tails_list:
                # è®¡ç®—ç»¼åˆå¾—åˆ†
                final_score = self._calculate_comprehensive_score(
                    fusion_results[tail],
                    risk_assessments[tail],
                    prediction_horizon
                )
                
                final_scores[tail] = final_score
                
                # æ„å»ºè¯¦ç»†åˆ†æ
                detailed_analysis[tail] = {
                    'technical_score': technical_analysis_results[tail].get('composite_score', 0.0),
                    'wavelet_score': wavelet_analysis_results[tail].get('reversal_probability', 0.0),
                    'fourier_score': fourier_analysis_results[tail].get('frequency_reversal_score', 0.0),
                    'nonlinear_score': nonlinear_analysis_results[tail].get('chaos_reversal_score', 0.0),
                    'ml_score': ml_predictions[tail].get('ensemble_confidence', 0.0),
                    'dl_score': dl_predictions.get(tail, {}).get('prediction_confidence', 0.0),
                    'quantum_score': quantum_analysis_results[tail].get('quantum_reversal_probability', 0.0),
                    'microstructure_score': microstructure_analysis_results[tail].get('structure_score', 0.0),
                    'kalman_score': kalman_predictions[tail].get('state_confidence', 0.0),
                    'pattern_score': pattern_analysis_results[tail].get('pattern_strength', 0.0),
                    'fusion_score': fusion_results[tail].get('consensus_score', 0.0),
                    'risk_score': risk_assessments[tail].get('risk_level', 0.0),
                    'final_score': final_score,
                    'signal_quality': self._assess_signal_quality(fusion_results[tail]),
                    'prediction_horizon': prediction_horizon.name,
                    'analysis_timestamp': datetime.now().isoformat()
                }
                
                if self.config['debug_mode']:
                    print(f"   å°¾æ•°{tail}: ç»¼åˆå¾—åˆ†={final_score:.4f}")
            
            # ========== é˜¶æ®µ16: ç»“æœç”Ÿæˆä¸éªŒè¯ ==========
            if not final_scores:
                return self._create_failure_result('no_valid_predictions')
            
            # é€‰æ‹©æœ€ä½³å€™é€‰
            best_tail = max(final_scores.keys(), key=lambda t: final_scores[t])
            best_score = final_scores[best_tail]
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_advanced_confidence(
                best_score, 
                detailed_analysis[best_tail],
                data_quality_score
            )
            
            # ç”Ÿæˆè¯¦ç»†æ¨ç†
            reasoning = self._generate_comprehensive_reasoning(
                best_tail, 
                detailed_analysis[best_tail],
                fusion_results[best_tail]
            )
            
            # é¢„æµ‹æ—¶æœºå»ºè®®
            timing_analysis = self._analyze_optimal_timing(
                best_tail,
                detailed_analysis[best_tail],
                prediction_horizon
            )
            
            # ä¸ç¡®å®šæ€§é‡åŒ–
            uncertainty_analysis = self._quantify_prediction_uncertainty(
                best_tail,
                fusion_results[best_tail],
                risk_assessments[best_tail]
            )
            
            # ========== é˜¶æ®µ17: è‡ªé€‚åº”å­¦ä¹ æ›´æ–° ==========
            self._update_adaptive_parameters(best_tail, confidence, final_scores)
            
            # ========== æ€§èƒ½ç›‘æ§ ==========
            execution_time = time.time() - start_time
            self._update_performance_metrics(execution_time, confidence, len(candidate_tails_list))
            
            # ========== æ„å»ºæœ€ç»ˆç»“æœ ==========
            result = {
                'success': True,
                'recommended_tail': best_tail,
                'confidence': confidence,
                'final_score': best_score,
                'prediction_horizon': prediction_horizon.name,
                'reasoning': reasoning,
                'detailed_analysis': detailed_analysis,
                'all_scores': final_scores,
                'signal_fusion_results': fusion_results,
                'risk_assessments': risk_assessments,
                'timing_analysis': timing_analysis,
                'uncertainty_analysis': uncertainty_analysis,
                'data_quality_score': data_quality_score,
                'execution_time': execution_time,
                'feature_importance': self._get_feature_importance_summary(),
                'model_contributions': self._get_model_contribution_summary(),
                'market_regime': self._detect_current_market_regime(preprocessed_data),
                'volatility_forecast': self._forecast_volatility(preprocessed_data),
                'reversal_probability_distribution': self._calculate_reversal_probability_distribution(final_scores),
                'alternative_scenarios': self._generate_alternative_scenarios(final_scores, confidence),
                'backtesting_similarity': self._find_historical_similarities(best_tail, preprocessed_data),
                'early_warning_signals': self._detect_early_warning_signals(preprocessed_data),
                'regime_change_probability': self._calculate_regime_change_probability(preprocessed_data),
                'systemic_risk_indicators': self._calculate_systemic_risk_indicators(preprocessed_data),
                'complexity_measures': self._calculate_complexity_measures(preprocessed_data),
                'metadata': {
                    'algorithm_version': '2.0.0-research',
                    'analysis_depth': 'comprehensive',
                    'prediction_method': 'multi_dimensional_fusion',
                    'total_models_used': len(self.ml_models) + len(self.pytorch_models) + len(self.tensorflow_models),
                    'feature_count': sum(len(features) for features in feature_matrix.values()),
                    'pattern_matches': sum(len(patterns) for patterns in pattern_analysis_results.values()),
                    'computational_complexity': 'O(nÂ²log n)',
                    'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024
                }
            }
            
            # ç¼“å­˜ç»“æœ
            if self.config['cache_enabled']:
                self._cache_prediction_result(candidate_tails, historical_data_hash, result)
            
            self.logger.info(f"âœ… ç§‘ç ”çº§é¢„æµ‹å®Œæˆ: æ¨èå°¾æ•°={best_tail}, ç½®ä¿¡åº¦={confidence:.3f}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ é¢„æµ‹è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
            return self._create_failure_result('prediction_error', details={'error': str(e)})
    
    def _preprocess_data_advanced(self, historical_data: List[Dict]) -> np.ndarray:
        """é«˜çº§æ•°æ®é¢„å¤„ç†"""
        # è½¬æ¢ä¸ºæ•°å€¼çŸ©é˜µ
        data_matrix = np.zeros((len(historical_data), 10))
        for i, period in enumerate(historical_data):
            for tail in range(10):
                data_matrix[i, tail] = 1 if tail in period.get('tails', []) else 0
        
        # æ•°æ®å¹³æ»‘
        if len(data_matrix) > self.config['data_smoothing_window']:
            for tail in range(10):
                data_matrix[:, tail] = savgol_filter(
                    data_matrix[:, tail], 
                    self.config['data_smoothing_window'], 
                    3
                )
        
        # å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†
        for tail in range(10):
            column = data_matrix[:, tail]
            Q1 = np.percentile(column, 25)
            Q3 = np.percentile(column, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # ä½¿ç”¨ä¸­ä½æ•°æ›¿æ¢å¼‚å¸¸å€¼
            median = np.median(column)
            outliers = (column < lower_bound) | (column > upper_bound)
            data_matrix[outliers, tail] = median
        
        return data_matrix
    
    def _assess_data_quality(self, data_matrix: np.ndarray) -> float:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        quality_scores = []
        
        # å®Œæ•´æ€§æ£€æŸ¥
        completeness = 1.0 - np.sum(np.isnan(data_matrix)) / data_matrix.size
        quality_scores.append(completeness)
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        consistency = 1.0 - np.std(np.sum(data_matrix, axis=1)) / np.mean(np.sum(data_matrix, axis=1))
        quality_scores.append(min(1.0, consistency))
        
        # å˜å¼‚æ€§æ£€æŸ¥
        variance_scores = []
        for tail in range(10):
            variance = np.var(data_matrix[:, tail])
            variance_scores.append(min(1.0, variance * 10))  # å½’ä¸€åŒ–
        variability = np.mean(variance_scores)
        quality_scores.append(variability)
        
        # æ—¶é—´ä¸€è‡´æ€§
        temporal_consistency = 1.0
        if len(data_matrix) > 5:
            for tail in range(10):
                autocorr = np.corrcoef(data_matrix[:-1, tail], data_matrix[1:, tail])[0, 1]
                if not np.isnan(autocorr):
                    temporal_consistency *= (1.0 + autocorr) / 2
        quality_scores.append(temporal_consistency)
        
        return np.mean(quality_scores)
    
    def _extract_comprehensive_features(self, candidate_tails: List[int], 
                                      data_matrix: np.ndarray, 
                                      prediction_horizon: PredictionHorizon) -> Dict[int, np.ndarray]:
        """æå–ç»¼åˆç‰¹å¾çŸ©é˜µ"""
        feature_matrix = {}
        
        for tail in candidate_tails:
            features = []
            tail_data = data_matrix[:, tail]
            
            # ========== ç»Ÿè®¡ç‰¹å¾ ==========
            if len(tail_data) > 0:
                features.extend([
                    np.mean(tail_data),
                    np.std(tail_data),
                    np.var(tail_data),
                    np.median(tail_data),
                    np.percentile(tail_data, 25),
                    np.percentile(tail_data, 75),
                    stats.skew(tail_data),
                    stats.kurtosis(tail_data),
                    np.min(tail_data),
                    np.max(tail_data)
                ])
            else:
                features.extend([0.0] * 10)
            
            # ========== æ—¶é—´åºåˆ—ç‰¹å¾ ==========
            if len(tail_data) > 10:
                # è‡ªç›¸å…³
                autocorr_lags = [1, 3, 5, 7, 10]
                for lag in autocorr_lags:
                    if len(tail_data) > lag:
                        autocorr = np.corrcoef(tail_data[:-lag], tail_data[lag:])[0, 1]
                        features.append(autocorr if not np.isnan(autocorr) else 0.0)
                    else:
                        features.append(0.0)
                
                # åè‡ªç›¸å…³ï¼ˆç®€åŒ–ç‰ˆï¼‰
                for lag in autocorr_lags:
                    if len(tail_data) > lag * 2:
                        pacf_val = self._calculate_partial_autocorr(tail_data, lag)
                        features.append(pacf_val)
                    else:
                        features.append(0.0)
                
                # è¶‹åŠ¿ç‰¹å¾
                trend_slope = self._calculate_trend_slope(tail_data)
                features.append(trend_slope)
                
                # å­£èŠ‚æ€§æ£€æµ‹
                seasonality_strength = self._detect_seasonality(tail_data)
                features.append(seasonality_strength)
                
                # å¹³ç¨³æ€§æ£€æµ‹
                stationarity_score = self._test_stationarity(tail_data)
                features.append(stationarity_score)
            else:
                features.extend([0.0] * 13)
            
            # ========== é¢‘åŸŸç‰¹å¾ ==========
            if len(tail_data) > 8:
                fft_features = self._extract_fft_features(tail_data)
                features.extend(fft_features)
            else:
                features.extend([0.0] * 6)
            
            # ========== éçº¿æ€§ç‰¹å¾ ==========
            if len(tail_data) > 20:
                nonlinear_features = self._extract_nonlinear_features(tail_data)
                features.extend(nonlinear_features)
            else:
                features.extend([0.0] * 8)
            
            # ========== æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ ==========
            technical_features = self._extract_technical_features(tail_data)
            features.extend(technical_features)
            
            # ========== ç›¸å¯¹ç‰¹å¾ ==========
            relative_features = self._extract_relative_features(tail, data_matrix)
            features.extend(relative_features)
            
            # ========== æ—¶é—´çª—å£ç‰¹å¾ ==========
            window_features = self._extract_window_features(tail_data, prediction_horizon)
            features.extend(window_features)
            
            feature_matrix[tail] = np.array(features)
        
        return feature_matrix
    
    def _perform_advanced_technical_analysis(self, tail: int, data_matrix: np.ndarray, 
                                           prediction_horizon: PredictionHorizon) -> Dict[str, Any]:
        """æ‰§è¡Œé«˜çº§æŠ€æœ¯åˆ†æ"""
        tail_data = data_matrix[:, tail]
        analysis_result = {}
        
        # ========== RSIåˆ†æ ==========
        rsi_values = self._calculate_rsi_advanced(tail_data)
        analysis_result['rsi'] = {
            'current': rsi_values[-1] if len(rsi_values) > 0 else 50,
            'trend': self._calculate_rsi_trend(rsi_values),
            'divergence': self._detect_rsi_divergence(tail_data, rsi_values),
            'overbought_oversold': self._classify_rsi_level(rsi_values[-1] if len(rsi_values) > 0 else 50)
        }
        
        # ========== MACDåˆ†æ ==========
        macd_line, macd_signal, macd_histogram = self._calculate_macd_advanced(tail_data)
        analysis_result['macd'] = {
            'line': macd_line[-1] if len(macd_line) > 0 else 0,
            'signal': macd_signal[-1] if len(macd_signal) > 0 else 0,
            'histogram': macd_histogram[-1] if len(macd_histogram) > 0 else 0,
            'crossover': self._detect_macd_crossover(macd_line, macd_signal),
            'divergence': self._detect_macd_divergence(tail_data, macd_line)
        }
        
        # ========== å¸ƒæ—å¸¦åˆ†æ ==========
        bb_upper, bb_middle, bb_lower = self._calculate_bollinger_bands_advanced(tail_data)
        analysis_result['bollinger'] = {
            'position': self._calculate_bollinger_position(tail_data[-1] if len(tail_data) > 0 else 0, 
                                                         bb_upper[-1] if len(bb_upper) > 0 else 1,
                                                         bb_lower[-1] if len(bb_lower) > 0 else 0),
            'squeeze': self._detect_bollinger_squeeze(bb_upper, bb_lower),
            'breakout': self._detect_bollinger_breakout(tail_data, bb_upper, bb_lower)
        }
        
        # ========== éšæœºæŒ‡æ ‡åˆ†æ ==========
        stoch_k, stoch_d = self._calculate_stochastic_advanced(tail_data)
        analysis_result['stochastic'] = {
            'k': stoch_k[-1] if len(stoch_k) > 0 else 50,
            'd': stoch_d[-1] if len(stoch_d) > 0 else 50,
            'crossover': self._detect_stochastic_crossover(stoch_k, stoch_d),
            'divergence': self._detect_stochastic_divergence(tail_data, stoch_k)
        }
        
        # ========== ADXè¶‹åŠ¿å¼ºåº¦åˆ†æ ==========
        adx_values = self._calculate_adx_advanced(tail_data)
        analysis_result['adx'] = {
            'strength': adx_values[-1] if len(adx_values) > 0 else 25,
            'trend_classification': self._classify_trend_strength(adx_values[-1] if len(adx_values) > 0 else 25)
        }
        
        # ========== æˆäº¤é‡åˆ†æ ==========
        volume_analysis = self._analyze_volume_patterns(tail, data_matrix)
        analysis_result['volume'] = volume_analysis
        
        # ========== æ”¯æ’‘é˜»åŠ›åˆ†æ ==========
        support_resistance = self._identify_support_resistance_levels(tail_data)
        analysis_result['support_resistance'] = support_resistance
        
        # ========== ç»¼åˆè¯„åˆ† ==========
        analysis_result['composite_score'] = self._calculate_technical_composite_score(analysis_result)
        
        return analysis_result
    
    def _perform_wavelet_analysis(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œå°æ³¢åˆ†æ"""
        tail_data = data_matrix[:, tail]
        
        if len(tail_data) < 16:
            return {'error': 'insufficient_data', 'reversal_probability': 0.0}
        
        try:
            # å°æ³¢åˆ†è§£
            wavelet_type = self.config['wavelet_type']
            levels = min(self.config['decomposition_levels'], int(np.log2(len(tail_data))))
            
            coeffs = pywt.wavedec(tail_data, wavelet_type, level=levels)
            
            # åˆ†æå„å±‚ç»†èŠ‚ç³»æ•°
            detail_analysis = []
            for i, detail in enumerate(coeffs[1:], 1):
                detail_analysis.append({
                    'level': i,
                    'energy': np.sum(detail**2),
                    'variance': np.var(detail),
                    'max_coeff': np.max(np.abs(detail)),
                    'entropy': self._calculate_wavelet_entropy(detail)
                })
            
            # å¥‡å¼‚è°±åˆ†æ
            singularity_spectrum = self._calculate_singularity_spectrum(tail_data)
            
            # å°æ³¢ç›¸å…³æ€§åˆ†æ
            wavelet_correlation = self._analyze_wavelet_correlation(coeffs)
            
            # é‡æ„è¯¯å·®åˆ†æ
            reconstructed = pywt.waverec(coeffs, wavelet_type)
            reconstruction_error = np.mean((tail_data[:len(reconstructed)] - reconstructed)**2)
            
            # åè½¬æ¦‚ç‡è®¡ç®—
            reversal_probability = self._calculate_wavelet_reversal_probability(
                coeffs, detail_analysis, singularity_spectrum
            )
            
            return {
                'coefficients_summary': {
                    'approximation_energy': np.sum(coeffs[0]**2),
                    'total_detail_energy': sum(da['energy'] for da in detail_analysis),
                    'energy_distribution': [da['energy'] for da in detail_analysis]
                },
                'detail_analysis': detail_analysis,
                'singularity_spectrum': singularity_spectrum,
                'wavelet_correlation': wavelet_correlation,
                'reconstruction_error': reconstruction_error,
                'reversal_probability': reversal_probability,
                'dominant_scales': self._identify_dominant_scales(coeffs),
                'multiscale_entropy': self._calculate_multiscale_entropy(coeffs)
            }
            
        except Exception as e:
            self.logger.error(f"å°æ³¢åˆ†æé”™è¯¯: {str(e)}")
            return {'error': str(e), 'reversal_probability': 0.0}
    
    def _perform_fourier_analysis(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œå‚…é‡Œå¶é¢‘åŸŸåˆ†æ"""
        tail_data = data_matrix[:, tail]
        
        if len(tail_data) < 8:
            return {'error': 'insufficient_data', 'frequency_reversal_score': 0.0}
        
        try:
            # FFTå˜æ¢
            fft_values = np.fft.fft(tail_data)
            frequencies = np.fft.fftfreq(len(tail_data))
            
            # åŠŸç‡è°±å¯†åº¦
            power_spectrum = np.abs(fft_values)**2
            
            # ç›¸ä½è°±
            phase_spectrum = np.angle(fft_values)
            
            # ä¸»å¯¼é¢‘ç‡è¯†åˆ«
            dominant_frequencies = self._identify_dominant_frequencies(frequencies, power_spectrum)
            
            # é¢‘ç‡ç¨³å®šæ€§åˆ†æ
            frequency_stability = self._analyze_frequency_stability(tail_data)
            
            # è°æ³¢åˆ†æ
            harmonic_analysis = self._perform_harmonic_analysis(fft_values, frequencies)
            
            # é¢‘åŸŸç‰¹å¾æå–
            spectral_features = {
                'spectral_centroid': self._calculate_spectral_centroid(power_spectrum, frequencies),
                'spectral_rolloff': self._calculate_spectral_rolloff(power_spectrum, frequencies),
                'spectral_flux': self._calculate_spectral_flux(power_spectrum),
                'spectral_bandwidth': self._calculate_spectral_bandwidth(power_spectrum, frequencies)
            }
            
            # é¢‘åŸŸåè½¬è¯„åˆ†
            frequency_reversal_score = self._calculate_frequency_reversal_score(
                dominant_frequencies, frequency_stability, harmonic_analysis
            )
            
            return {
                'power_spectrum': power_spectrum.tolist(),
                'phase_spectrum': phase_spectrum.tolist(),
                'dominant_frequencies': dominant_frequencies,
                'frequency_stability': frequency_stability,
                'harmonic_analysis': harmonic_analysis,
                'spectral_features': spectral_features,
                'frequency_reversal_score': frequency_reversal_score,
                'nyquist_frequency': 0.5,
                'frequency_resolution': 1.0 / len(tail_data)
            }
            
        except Exception as e:
            self.logger.error(f"å‚…é‡Œå¶åˆ†æé”™è¯¯: {str(e)}")
            return {'error': str(e), 'frequency_reversal_score': 0.0}
    
    def _perform_nonlinear_dynamics_analysis(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œéçº¿æ€§åŠ¨åŠ›å­¦åˆ†æ"""
        tail_data = data_matrix[:, tail]
        
        if len(tail_data) < 30:
            return {'error': 'insufficient_data', 'chaos_reversal_score': 0.0}
        
        try:
            # æé›…æ™®è¯ºå¤«æŒ‡æ•°è®¡ç®—
            lyapunov_exponent = self._calculate_lyapunov_exponent(tail_data)
            
            # å…³è”ç»´æ•°è®¡ç®—
            correlation_dimension = self._calculate_correlation_dimension(tail_data)
            
            # èµ«æ–¯ç‰¹æŒ‡æ•°è®¡ç®—
            hurst_exponent = self._calculate_hurst_exponent(tail_data)
            
            # åˆ†å½¢ç»´æ•°è®¡ç®—
            fractal_dimension = self._calculate_fractal_dimension(tail_data)
            
            # ç†µå€¼è®¡ç®—
            entropy_measures = {
                'shannon_entropy': self._calculate_shannon_entropy(tail_data),
                'approximate_entropy': self._calculate_approximate_entropy(tail_data),
                'sample_entropy': self._calculate_sample_entropy(tail_data),
                'permutation_entropy': self._calculate_permutation_entropy(tail_data)
            }
            
            # é€’å½’å›¾åˆ†æ
            recurrence_analysis = self._perform_recurrence_analysis(tail_data)
            
            # ç›¸ç©ºé—´é‡æ„
            phase_space = self._reconstruct_phase_space(tail_data)
            
            # åºåŠ è±æˆªé¢åˆ†æ
            poincare_analysis = self._analyze_poincare_section(phase_space)
            
            # æ··æ²Œç‰¹å¾è¯†åˆ«
            chaos_indicators = self._identify_chaos_indicators(
                lyapunov_exponent, correlation_dimension, entropy_measures
            )
            
            # éçº¿æ€§åè½¬è¯„åˆ†
            chaos_reversal_score = self._calculate_chaos_reversal_score(
                lyapunov_exponent, hurst_exponent, entropy_measures, chaos_indicators
            )
            
            return {
                'lyapunov_exponent': lyapunov_exponent,
                'correlation_dimension': correlation_dimension,
                'hurst_exponent': hurst_exponent,
                'fractal_dimension': fractal_dimension,
                'entropy_measures': entropy_measures,
                'recurrence_analysis': recurrence_analysis,
                'phase_space_properties': {
                    'embedding_dimension': phase_space.shape[1],
                    'trajectory_length': phase_space.shape[0],
                    'attractor_dimension': self._estimate_attractor_dimension(phase_space)
                },
                'poincare_analysis': poincare_analysis,
                'chaos_indicators': chaos_indicators,
                'chaos_reversal_score': chaos_reversal_score,
                'predictability_horizon': self._estimate_predictability_horizon(lyapunov_exponent)
            }
            
        except Exception as e:
            self.logger.error(f"éçº¿æ€§åŠ¨åŠ›å­¦åˆ†æé”™è¯¯: {str(e)}")
            return {'error': str(e), 'chaos_reversal_score': 0.0}

    def _perform_ml_ensemble_prediction(self, tail: int, features: np.ndarray, 
                                       data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œæœºå™¨å­¦ä¹ é›†æˆé¢„æµ‹"""
        try:
            if len(features) == 0:
                return {'error': 'no_features', 'ensemble_confidence': 0.0}
            
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 10:
                return {'error': 'insufficient_data', 'ensemble_confidence': 0.0}
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X, y = self._prepare_ml_training_data(features, tail_data)
            
            if len(X) == 0:
                return {'error': 'no_training_data', 'ensemble_confidence': 0.0}
            
            # ç‰¹å¾é¢„å¤„ç†
            X_scaled = self.feature_extractors['standard_scaler'].fit_transform(X.reshape(-1, 1)).flatten()
            
            # é›†æˆé¢„æµ‹
            predictions = {}
            model_performances = {}
            
            for model_name, model in self.ml_models.items():
                try:
                    # äº¤å‰éªŒè¯è¯„ä¼°
                    cv_scores = self._cross_validate_model(model, X_scaled.reshape(-1, 1), y)
                    
                    # è®­ç»ƒæ¨¡å‹
                    model.fit(X_scaled.reshape(-1, 1), y)
                    
                    # é¢„æµ‹
                    prediction = model.predict(X_scaled[-1:].reshape(-1, 1))[0]
                    predictions[model_name] = prediction
                    
                    # è®°å½•æ€§èƒ½
                    model_performances[model_name] = {
                        'cv_mean': np.mean(cv_scores),
                        'cv_std': np.std(cv_scores),
                        'prediction': prediction
                    }
                    
                except Exception as e:
                    self.logger.warning(f"æ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {str(e)}")
                    continue
            
            if not predictions:
                return {'error': 'all_models_failed', 'ensemble_confidence': 0.0}
            
            # é›†æˆç­–ç•¥
            ensemble_prediction = self._ensemble_predictions(predictions, model_performances)
            
            # ç‰¹å¾é‡è¦æ€§åˆ†æ
            feature_importance = self._analyze_feature_importance(X_scaled.reshape(-1, 1), y)
            
            # é¢„æµ‹ç½®ä¿¡åº¦è®¡ç®—
            ensemble_confidence = self._calculate_ensemble_confidence(model_performances)
            
            # é¢„æµ‹ä¸ç¡®å®šæ€§é‡åŒ–
            prediction_uncertainty = self._quantify_prediction_uncertainty_ml(predictions)
            
            return {
                'ensemble_prediction': ensemble_prediction,
                'individual_predictions': predictions,
                'model_performances': model_performances,
                'feature_importance': feature_importance,
                'ensemble_confidence': ensemble_confidence,
                'prediction_uncertainty': prediction_uncertainty,
                'best_model': max(model_performances.keys(), 
                                key=lambda k: model_performances[k]['cv_mean']),
                'model_agreement': self._calculate_model_agreement(predictions),
                'prediction_stability': self._assess_prediction_stability(predictions)
            }
            
        except Exception as e:
            self.logger.error(f"æœºå™¨å­¦ä¹ é›†æˆé¢„æµ‹é”™è¯¯: {str(e)}")
            return {'error': str(e), 'ensemble_confidence': 0.0}
    
    def _perform_deep_learning_prediction(self, tail: int, features: np.ndarray, 
                                        data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦å­¦ä¹ é¢„æµ‹"""
        try:
            if not (TORCH_AVAILABLE or TF_AVAILABLE):
                return {'error': 'no_dl_framework', 'prediction_confidence': 0.0}
            
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 30:
                return {'error': 'insufficient_data', 'prediction_confidence': 0.0}
            
            # å‡†å¤‡åºåˆ—æ•°æ®
            sequence_data = self._prepare_sequence_data(tail_data, 
                                                      self.config['lstm_sequence_length'])
            
            predictions = {}
            
            # PyTorchæ¨¡å‹é¢„æµ‹
            if TORCH_AVAILABLE and self.pytorch_models:
                pytorch_predictions = self._pytorch_ensemble_predict(sequence_data, tail_data)
                predictions.update(pytorch_predictions)
            
            # TensorFlowæ¨¡å‹é¢„æµ‹
            if TF_AVAILABLE and self.tensorflow_models:
                tensorflow_predictions = self._tensorflow_ensemble_predict(sequence_data, tail_data)
                predictions.update(tensorflow_predictions)
            
            if not predictions:
                return {'error': 'no_predictions', 'prediction_confidence': 0.0}
            
            # æ·±åº¦å­¦ä¹ é›†æˆ
            dl_ensemble_prediction = np.mean(list(predictions.values()))
            
            # æ³¨æ„åŠ›æƒé‡åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            attention_analysis = self._analyze_attention_weights(sequence_data)
            
            # é¢„æµ‹ç½®ä¿¡åº¦
            prediction_confidence = self._calculate_dl_confidence(predictions)
            
            # æ¨¡å‹è§£é‡Šæ€§åˆ†æ
            interpretability_analysis = self._analyze_dl_interpretability(sequence_data, predictions)
            
            return {
                'ensemble_prediction': dl_ensemble_prediction,
                'individual_predictions': predictions,
                'attention_analysis': attention_analysis,
                'prediction_confidence': prediction_confidence,
                'interpretability_analysis': interpretability_analysis,
                'sequence_length': len(sequence_data),
                'model_complexity': self._assess_model_complexity(),
                'gradient_information': self._extract_gradient_information(sequence_data)
            }
            
        except Exception as e:
            self.logger.error(f"æ·±åº¦å­¦ä¹ é¢„æµ‹é”™è¯¯: {str(e)}")
            return {'error': str(e), 'prediction_confidence': 0.0}
    
    def _perform_quantum_analysis(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œé‡å­åŒ–åˆ†æ"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 4:
                return {'error': 'insufficient_data', 'quantum_reversal_probability': 0.0}
            
            # é‡å­æ€æ„å»º
            quantum_state = self._construct_quantum_state(tail_data)
            
            # é‡å­ç›¸å¹²æ€§æµ‹é‡
            coherence_measure = self._calculate_quantum_coherence(quantum_state)
            
            # çº ç¼ ç†µè®¡ç®—
            entanglement_entropy = self._calculate_entanglement_entropy(quantum_state)
            
            # é‡å­ä¿çœŸåº¦
            quantum_fidelity = self._calculate_quantum_fidelity(quantum_state)
            
            # é‡å­å åŠ æ€åˆ†æ
            superposition_analysis = self._analyze_quantum_superposition(quantum_state)
            
            # é‡å­æµ‹é‡æ¦‚ç‡
            measurement_probabilities = self._calculate_measurement_probabilities(quantum_state)
            
            # é‡å­å¹²æ¶‰æ•ˆåº”
            interference_effects = self._analyze_quantum_interference(tail_data)
            
            # é‡å­é€€ç›¸å¹²åˆ†æ
            decoherence_analysis = self._analyze_quantum_decoherence(quantum_state)
            
            # é‡å­çº é”™èƒ½åŠ›è¯„ä¼°
            error_correction_capacity = self._assess_quantum_error_correction(quantum_state)
            
            # é‡å­åè½¬æ¦‚ç‡
            quantum_reversal_probability = self._calculate_quantum_reversal_probability(
                coherence_measure, entanglement_entropy, superposition_analysis
            )
            
            return {
                'quantum_state_properties': {
                    'dimension': len(quantum_state),
                    'purity': self._calculate_quantum_purity(quantum_state),
                    'trace': np.trace(quantum_state) if hasattr(quantum_state, 'trace') else 1.0
                },
                'coherence_measure': coherence_measure,
                'entanglement_entropy': entanglement_entropy,
                'quantum_fidelity': quantum_fidelity,
                'superposition_analysis': superposition_analysis,
                'measurement_probabilities': measurement_probabilities,
                'interference_effects': interference_effects,
                'decoherence_analysis': decoherence_analysis,
                'error_correction_capacity': error_correction_capacity,
                'quantum_reversal_probability': quantum_reversal_probability,
                'quantum_advantage': self._assess_quantum_advantage(tail_data),
                'bell_state_similarity': self._calculate_bell_state_similarity(quantum_state)
            }
            
        except Exception as e:
            self.logger.error(f"é‡å­åˆ†æé”™è¯¯: {str(e)}")
            return {'error': str(e), 'quantum_reversal_probability': 0.0}
    
    def _perform_microstructure_analysis(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œå¸‚åœºå¾®è§‚ç»“æ„åˆ†æ"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 10:
                return {'error': 'insufficient_data', 'structure_score': 0.0}
            
            # ä¹°å–ä»·å·®æ¨¡æ‹Ÿ
            bid_ask_spread = self._simulate_bid_ask_spread(tail_data)
            
            # å¸‚åœºæ·±åº¦åˆ†æ
            market_depth = self._analyze_market_depth(tail_data, data_matrix)
            
            # è®¢å•æµä¸å¹³è¡¡
            order_flow_imbalance = self._calculate_order_flow_imbalance(tail_data)
            
            # äº¤æ˜“å¼ºåº¦åˆ†æ
            trade_intensity = self._analyze_trade_intensity(tail_data)
            
            # ä»·æ ¼å½±å“åˆ†æ
            price_impact = self._analyze_price_impact(tail_data)
            
            # æµåŠ¨æ€§åº¦é‡
            liquidity_measures = self._calculate_liquidity_measures(tail_data)
            
            # ä¿¡æ¯ä¸å¯¹ç§°æ£€æµ‹
            information_asymmetry = self._detect_information_asymmetry(tail_data, data_matrix)
            
            # å¸‚åœºæ“çºµæ£€æµ‹
            manipulation_indicators = self._detect_market_manipulation(tail_data)
            
            # æ³¢åŠ¨æ€§èšé›†åˆ†æ
            volatility_clustering = self._analyze_volatility_clustering(tail_data)
            
            # è·³è·ƒæ£€æµ‹
            jump_detection = self._detect_price_jumps(tail_data)
            
            # å¾®è§‚ç»“æ„è¯„åˆ†
            structure_score = self._calculate_microstructure_score(
                bid_ask_spread, market_depth, order_flow_imbalance, 
                trade_intensity, liquidity_measures
            )
            
            return {
                'bid_ask_spread': bid_ask_spread,
                'market_depth': market_depth,
                'order_flow_imbalance': order_flow_imbalance,
                'trade_intensity': trade_intensity,
                'price_impact': price_impact,
                'liquidity_measures': liquidity_measures,
                'information_asymmetry': information_asymmetry,
                'manipulation_indicators': manipulation_indicators,
                'volatility_clustering': volatility_clustering,
                'jump_detection': jump_detection,
                'structure_score': structure_score,
                'market_efficiency': self._assess_market_efficiency(tail_data),
                'transaction_cost_analysis': self._analyze_transaction_costs(tail_data)
            }
            
        except Exception as e:
            self.logger.error(f"å¾®è§‚ç»“æ„åˆ†æé”™è¯¯: {str(e)}")
            return {'error': str(e), 'structure_score': 0.0}
    
    def _perform_kalman_filtering(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œå¡å°”æ›¼æ»¤æ³¢çŠ¶æ€ä¼°è®¡"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return {'error': 'insufficient_data', 'state_confidence': 0.0}
            
            # è·å–è¯¥å°¾æ•°çš„å¡å°”æ›¼æ»¤æ³¢å™¨
            kf = self.kalman_filters[tail]
            
            # çŠ¶æ€ä¼°è®¡
            state_means, state_covariances = kf.em(tail_data.reshape(-1, 1))
            
            # é¢„æµ‹ä¸‹ä¸€æ­¥
            next_state_mean, next_state_covariance = kf.filter_update(
                state_means[-1], state_covariances[-1], tail_data[-1]
            )
            
            # å¹³æ»‘ä¼°è®¡
            smoothed_means, smoothed_covariances = kf.smooth()
            
            # çŠ¶æ€ç½®ä¿¡åº¦
            state_confidence = self._calculate_kalman_confidence(
                state_covariances, next_state_covariance
            )
            
            # æ–°æ¯åˆ†æ
            innovations = self._calculate_kalman_innovations(tail_data, state_means)
            
            # ä¼¼ç„¶è¯„ä¼°
            log_likelihood = self._calculate_kalman_likelihood(innovations)
            
            # æ¨¡å‹é€‚åº”æ€§è¯„ä¼°
            model_adaptability = self._assess_kalman_adaptability(
                state_covariances, innovations
            )
            
            return {
                'current_state': state_means[-1],
                'predicted_state': next_state_mean,
                'state_uncertainty': np.trace(next_state_covariance),
                'state_confidence': state_confidence,
                'innovations': {
                    'mean': np.mean(innovations),
                    'std': np.std(innovations),
                    'autocorrelation': self._calculate_innovation_autocorr(innovations)
                },
                'log_likelihood': log_likelihood,
                'model_adaptability': model_adaptability,
                'filter_gain': self._calculate_kalman_gain(kf),
                'prediction_interval': self._calculate_prediction_interval(
                    next_state_mean, next_state_covariance
                )
            }
            
        except Exception as e:
            self.logger.error(f"å¡å°”æ›¼æ»¤æ³¢é”™è¯¯: {str(e)}")
            return {'error': str(e), 'state_confidence': 0.0}
    
    def _perform_advanced_pattern_recognition(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œé«˜çº§æ¨¡å¼è¯†åˆ«"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return {'error': 'insufficient_data', 'pattern_strength': 0.0}
            
            detected_patterns = {}
            pattern_confidences = {}
            
            # éå†æ¨¡å¼åº“è¿›è¡Œæ£€æµ‹
            for pattern_name, pattern_detector in self.pattern_library.items():
                try:
                    detection_result = pattern_detector(tail, data_matrix)
                    if detection_result:
                        detected_patterns[pattern_name] = detection_result
                        # è®¡ç®—æ¨¡å¼ç½®ä¿¡åº¦
                        confidence = self._calculate_pattern_confidence(
                            pattern_name, detection_result, tail_data
                        )
                        pattern_confidences[pattern_name] = confidence
                except Exception as e:
                    self.logger.warning(f"æ¨¡å¼ {pattern_name} æ£€æµ‹å¤±è´¥: {str(e)}")
                    continue
            
            # æ¨¡å¼æ¼”åŒ–åˆ†æ
            pattern_evolution = self._analyze_pattern_evolution(tail, detected_patterns)
            
            # æ¨¡å¼å¼ºåº¦è®¡ç®—
            pattern_strength = np.mean(list(pattern_confidences.values())) if pattern_confidences else 0.0
            
            # æ¨¡å¼ç»„åˆåˆ†æ
            pattern_combinations = self._analyze_pattern_combinations(detected_patterns)
            
            # æ¨¡å¼ç¨³å®šæ€§è¯„ä¼°
            pattern_stability = self._assess_pattern_stability(detected_patterns, tail_data)
            
            # å‡ ä½•æ¨¡å¼åˆ†æ
            geometric_patterns = self._detect_geometric_patterns(tail_data)
            
            # æ—¶é—´æ¨¡å¼åˆ†æ
            temporal_patterns = self._detect_temporal_patterns(tail_data)
            
            # åˆ†å½¢æ¨¡å¼åˆ†æ
            fractal_patterns = self._detect_fractal_patterns(tail_data)
            
            return {
                'detected_patterns': detected_patterns,
                'pattern_confidences': pattern_confidences,
                'pattern_evolution': pattern_evolution,
                'pattern_strength': pattern_strength,
                'pattern_combinations': pattern_combinations,
                'pattern_stability': pattern_stability,
                'geometric_patterns': geometric_patterns,
                'temporal_patterns': temporal_patterns,
                'fractal_patterns': fractal_patterns,
                'pattern_count': len(detected_patterns),
                'dominant_pattern': max(pattern_confidences.keys(), 
                                      key=lambda k: pattern_confidences[k]) if pattern_confidences else None,
                'pattern_diversity': self._calculate_pattern_diversity(detected_patterns)
            }
            
        except Exception as e:
            self.logger.error(f"é«˜çº§æ¨¡å¼è¯†åˆ«é”™è¯¯: {str(e)}")
            return {'error': str(e), 'pattern_strength': 0.0}
        
    def _perform_signal_fusion(self, candidate_tails: List[int], 
                              analysis_results: Dict[str, Dict]) -> Dict[int, Dict]:
        """æ‰§è¡Œå¤šä¿¡å·èåˆ"""
        fusion_results = {}
        
        for tail in candidate_tails:
            # æå–å„åˆ†æå™¨çš„ä¿¡å·
            signals = {}
            weights = {}
            
            # æŠ€æœ¯åˆ†æä¿¡å·
            if 'technical' in analysis_results and tail in analysis_results['technical']:
                tech_result = analysis_results['technical'][tail]
                signals['technical'] = tech_result.get('composite_score', 0.0)
                weights['technical'] = self.signal_fusion['signal_weights']['technical']
            
            # å°æ³¢åˆ†æä¿¡å·
            if 'wavelet' in analysis_results and tail in analysis_results['wavelet']:
                wavelet_result = analysis_results['wavelet'][tail]
                signals['wavelet'] = wavelet_result.get('reversal_probability', 0.0)
                weights['wavelet'] = self.signal_fusion['signal_weights']['wavelet']
            
            # å‚…é‡Œå¶åˆ†æä¿¡å·
            if 'fourier' in analysis_results and tail in analysis_results['fourier']:
                fourier_result = analysis_results['fourier'][tail]
                signals['fourier'] = fourier_result.get('frequency_reversal_score', 0.0)
                weights['fourier'] = self.signal_fusion['signal_weights']['fourier']
            
            # éçº¿æ€§åŠ¨åŠ›å­¦ä¿¡å·
            if 'nonlinear' in analysis_results and tail in analysis_results['nonlinear']:
                nonlinear_result = analysis_results['nonlinear'][tail]
                signals['nonlinear'] = nonlinear_result.get('chaos_reversal_score', 0.0)
                weights['nonlinear'] = self.signal_fusion['signal_weights']['nonlinear']
            
            # æœºå™¨å­¦ä¹ ä¿¡å·
            if 'ml' in analysis_results and tail in analysis_results['ml']:
                ml_result = analysis_results['ml'][tail]
                signals['ml'] = ml_result.get('ensemble_confidence', 0.0)
                weights['ml'] = self.signal_fusion['signal_weights']['ml']
            
            # æ·±åº¦å­¦ä¹ ä¿¡å·
            if 'dl' in analysis_results and tail in analysis_results['dl']:
                dl_result = analysis_results['dl'][tail]
                signals['dl'] = dl_result.get('prediction_confidence', 0.0)
                weights['dl'] = self.signal_fusion['signal_weights'].get('dl', 0.1)
            
            # é‡å­åˆ†æä¿¡å·
            if 'quantum' in analysis_results and tail in analysis_results['quantum']:
                quantum_result = analysis_results['quantum'][tail]
                signals['quantum'] = quantum_result.get('quantum_reversal_probability', 0.0)
                weights['quantum'] = self.signal_fusion['signal_weights']['quantum']
            
            # æ‰§è¡Œä¿¡å·èåˆ
            fusion_result = self._fuse_signals(signals, weights)
            
            # ä¿¡å·è´¨é‡è¯„ä¼°
            signal_quality = self._assess_signal_quality_comprehensive(signals)
            
            # ä¿¡å·ä¸€è‡´æ€§æ£€æŸ¥
            signal_consistency = self._check_signal_consistency(signals)
            
            # ä¿¡å·å¯é æ€§è¯„ä¼°
            signal_reliability = self._assess_signal_reliability(signals, weights)
            
            # è‡ªé€‚åº”æƒé‡è°ƒæ•´
            adapted_weights = self._adapt_signal_weights(signals, weights, tail)
            
            # é‡æ–°èåˆï¼ˆä½¿ç”¨è‡ªé€‚åº”æƒé‡ï¼‰
            adapted_fusion_result = self._fuse_signals(signals, adapted_weights)
            
            fusion_results[tail] = {
                'original_signals': signals,
                'original_weights': weights,
                'adapted_weights': adapted_weights,
                'consensus_score': fusion_result['consensus_score'],
                'adapted_consensus_score': adapted_fusion_result['consensus_score'],
                'signal_quality': signal_quality,
                'signal_consistency': signal_consistency,
                'signal_reliability': signal_reliability,
                'signal_count': len(signals),
                'dominant_signal': max(signals.keys(), key=lambda k: signals[k]) if signals else None,
                'signal_variance': np.var(list(signals.values())) if signals else 0.0,
                'fusion_confidence': fusion_result['confidence'],
                'adaptation_improvement': adapted_fusion_result['consensus_score'] - fusion_result['consensus_score']
            }
        
        return fusion_results
    
    def _perform_comprehensive_risk_assessment(self, tail: int, fusion_result: Dict, 
                                             data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œç»¼åˆé£é™©è¯„ä¼°"""
        try:
            tail_data = data_matrix[:, tail]
            
            # VaRè®¡ç®—
            var_analysis = self._calculate_value_at_risk(tail_data)
            
            # æ¡ä»¶é£é™©ä»·å€¼
            cvar_analysis = self._calculate_conditional_var(tail_data)
            
            # æœ€å¤§å›æ’¤åˆ†æ
            drawdown_analysis = self._analyze_maximum_drawdown(tail_data)
            
            # æ³¢åŠ¨ç‡é£é™©
            volatility_risk = self._assess_volatility_risk(tail_data)
            
            # æµåŠ¨æ€§é£é™©
            liquidity_risk = self._assess_liquidity_risk(tail_data)
            
            # æ¨¡å‹é£é™©
            model_risk = self._assess_model_risk(fusion_result)
            
            # ç³»ç»Ÿæ€§é£é™©
            systemic_risk = self._assess_systemic_risk(tail, data_matrix)
            
            # æ“ä½œé£é™©
            operational_risk = self._assess_operational_risk(fusion_result)
            
            # ä¿¡ç”¨é£é™©ï¼ˆæ¨¡æ‹Ÿï¼‰
            credit_risk = self._assess_credit_risk_simulation(tail_data)
            
            # å¸‚åœºé£é™©
            market_risk = self._assess_market_risk(tail_data, data_matrix)
            
            # é£é™©é›†ä¸­åº¦
            risk_concentration = self._assess_risk_concentration(tail, data_matrix)
            
            # å‹åŠ›æµ‹è¯•
            stress_test_results = self._perform_stress_testing(tail_data)
            
            # æƒ…æ™¯åˆ†æ
            scenario_analysis = self._perform_scenario_analysis(tail_data)
            
            # é£é™©è°ƒæ•´æ”¶ç›Š
            risk_adjusted_returns = self._calculate_risk_adjusted_returns(tail_data)
            
            # ç»¼åˆé£é™©è¯„åˆ†
            overall_risk_score = self._calculate_overall_risk_score(
                var_analysis, volatility_risk, model_risk, systemic_risk
            )
            
            return {
                'value_at_risk': var_analysis,
                'conditional_var': cvar_analysis,
                'maximum_drawdown': drawdown_analysis,
                'volatility_risk': volatility_risk,
                'liquidity_risk': liquidity_risk,
                'model_risk': model_risk,
                'systemic_risk': systemic_risk,
                'operational_risk': operational_risk,
                'credit_risk': credit_risk,
                'market_risk': market_risk,
                'risk_concentration': risk_concentration,
                'stress_test_results': stress_test_results,
                'scenario_analysis': scenario_analysis,
                'risk_adjusted_returns': risk_adjusted_returns,
                'overall_risk_score': overall_risk_score,
                'risk_level': self._classify_risk_level(overall_risk_score),
                'risk_capacity': self._assess_risk_capacity(tail_data),
                'risk_tolerance_alignment': self._check_risk_tolerance_alignment(overall_risk_score)
            }
            
        except Exception as e:
            self.logger.error(f"é£é™©è¯„ä¼°é”™è¯¯: {str(e)}")
            return {'error': str(e), 'overall_risk_score': 1.0, 'risk_level': 'high'}
    
    def _calculate_comprehensive_score(self, fusion_result: Dict, risk_assessment: Dict, 
                                     prediction_horizon: PredictionHorizon) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        try:
            # åŸºç¡€èåˆå¾—åˆ†
            base_score = fusion_result.get('adapted_consensus_score', 0.0)
            
            # ä¿¡å·è´¨é‡è°ƒæ•´
            quality_multiplier = fusion_result.get('signal_quality', 0.5)
            
            # ä¸€è‡´æ€§è°ƒæ•´
            consistency_multiplier = fusion_result.get('signal_consistency', 0.5)
            
            # å¯é æ€§è°ƒæ•´
            reliability_multiplier = fusion_result.get('signal_reliability', 0.5)
            
            # é£é™©è°ƒæ•´
            risk_score = risk_assessment.get('overall_risk_score', 0.5)
            risk_multiplier = 1.0 - risk_score  # é£é™©è¶Šé«˜ï¼Œå¾—åˆ†è¶Šä½
            
            # æ—¶é—´èŒƒå›´è°ƒæ•´
            horizon_multiplier = self._get_horizon_multiplier(prediction_horizon)
            
            # è®¡ç®—è°ƒæ•´åå¾—åˆ†
            adjusted_score = (base_score * 
                            quality_multiplier * 
                            consistency_multiplier * 
                            reliability_multiplier * 
                            risk_multiplier * 
                            horizon_multiplier)
            
            # åº”ç”¨éçº¿æ€§å˜æ¢å¢å¼ºåŒºåˆ†åº¦
            if adjusted_score > 0.8:
                final_score = 0.8 + (adjusted_score - 0.8) * 2.0
            elif adjusted_score < 0.2:
                final_score = adjusted_score * 0.5
            else:
                final_score = adjusted_score
            
            return min(1.0, max(0.0, final_score))
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆå¾—åˆ†è®¡ç®—é”™è¯¯: {str(e)}")
            return 0.0
    
    def _calculate_advanced_confidence(self, score: float, analysis: Dict, 
                                     data_quality: float) -> float:
        """è®¡ç®—é«˜çº§ç½®ä¿¡åº¦"""
        try:
            confidence_factors = []
            
            # åŸºç¡€å¾—åˆ†è´¡çŒ®
            confidence_factors.append(score)
            
            # æ•°æ®è´¨é‡è´¡çŒ®
            confidence_factors.append(data_quality)
            
            # ä¿¡å·ä¸€è‡´æ€§è´¡çŒ®
            signal_consistency = analysis.get('signal_consistency', 0.5)
            confidence_factors.append(signal_consistency)
            
            # æ¨¡å‹ä¸€è‡´æ€§è´¡çŒ®
            if 'ml_score' in analysis and 'dl_score' in analysis:
                model_agreement = 1.0 - abs(analysis['ml_score'] - analysis['dl_score'])
                confidence_factors.append(model_agreement)
            
            # åˆ†ææ·±åº¦è´¡çŒ®
            analysis_depth = len([k for k, v in analysis.items() 
                                if k.endswith('_score') and v > 0])
            depth_factor = min(1.0, analysis_depth / 8.0)
            confidence_factors.append(depth_factor)
            
            # é£é™©è°ƒæ•´
            risk_factor = 1.0 - analysis.get('risk_score', 0.5)
            confidence_factors.append(risk_factor)
            
            # æ—¶é—´ä¸€è‡´æ€§
            if 'temporal_patterns' in analysis:
                temporal_strength = analysis['temporal_patterns'].get('strength', 0.5)
                confidence_factors.append(temporal_strength)
            
            # è®¡ç®—åŠ æƒå¹³å‡ç½®ä¿¡åº¦
            weights = [0.25, 0.15, 0.15, 0.15, 0.1, 0.1, 0.1]
            if len(confidence_factors) < len(weights):
                weights = weights[:len(confidence_factors)]
                weights = [w / sum(weights) for w in weights]
            
            confidence = sum(f * w for f, w in zip(confidence_factors, weights))
            
            # åº”ç”¨éçº¿æ€§è°ƒæ•´
            if confidence > 0.9:
                confidence = 0.9 + (confidence - 0.9) * 0.5
            elif confidence < 0.1:
                confidence = confidence * 2.0
            
            return min(1.0, max(0.0, confidence))
            
        except Exception as e:
            self.logger.error(f"ç½®ä¿¡åº¦è®¡ç®—é”™è¯¯: {str(e)}")
            return 0.5
    
    def _generate_comprehensive_reasoning(self, tail: int, analysis: Dict, 
                                        fusion_result: Dict) -> str:
        """ç”Ÿæˆç»¼åˆæ¨ç†è¯´æ˜"""
        try:
            reasons = []
            
            # æŠ€æœ¯åˆ†æç†ç”±
            if analysis.get('technical_score', 0) > 0.6:
                reasons.append(f"æŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºå¼ºçƒˆåè½¬ä¿¡å·(å¾—åˆ†:{analysis['technical_score']:.2f})")
            
            # å°æ³¢åˆ†æç†ç”±
            if analysis.get('wavelet_score', 0) > 0.6:
                reasons.append(f"å°æ³¢åˆ†ææ£€æµ‹åˆ°å¤šå°ºåº¦åè½¬æ¨¡å¼(æ¦‚ç‡:{analysis['wavelet_score']:.2f})")
            
            # å‚…é‡Œå¶åˆ†æç†ç”±
            if analysis.get('fourier_score', 0) > 0.6:
                reasons.append(f"é¢‘åŸŸåˆ†ææ­ç¤ºå‘¨æœŸæ€§åè½¬ç‰¹å¾(å¾—åˆ†:{analysis['fourier_score']:.2f})")
            
            # éçº¿æ€§åŠ¨åŠ›å­¦ç†ç”±
            if analysis.get('nonlinear_score', 0) > 0.6:
                reasons.append(f"æ··æ²ŒåŠ¨åŠ›å­¦æ˜¾ç¤ºç³»ç»Ÿæ¥è¿‘ä¸´ç•Œç‚¹(å¾—åˆ†:{analysis['nonlinear_score']:.2f})")
            
            # æœºå™¨å­¦ä¹ ç†ç”±
            if analysis.get('ml_score', 0) > 0.6:
                reasons.append(f"æœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆé¢„æµ‹é«˜ç½®ä¿¡åº¦åè½¬(ç½®ä¿¡åº¦:{analysis['ml_score']:.2f})")
            
            # æ·±åº¦å­¦ä¹ ç†ç”±
            if analysis.get('dl_score', 0) > 0.6:
                reasons.append(f"æ·±åº¦ç¥ç»ç½‘ç»œè¯†åˆ«å¤æ‚åè½¬æ¨¡å¼(ç½®ä¿¡åº¦:{analysis['dl_score']:.2f})")
            
            # é‡å­åˆ†æç†ç”±
            if analysis.get('quantum_score', 0) > 0.6:
                reasons.append(f"é‡å­æ€åˆ†ææ˜¾ç¤ºç›¸å¹²æ€§å´©å¡Œè¿¹è±¡(æ¦‚ç‡:{analysis['quantum_score']:.2f})")
            
            # æ¨¡å¼è¯†åˆ«ç†ç”±
            if analysis.get('pattern_score', 0) > 0.6:
                reasons.append(f"è¯†åˆ«å‡ºç»å…¸åè½¬å½¢æ€æ¨¡å¼(å¼ºåº¦:{analysis['pattern_score']:.2f})")
            
            # ä¿¡å·èåˆç†ç”±
            consensus_score = fusion_result.get('adapted_consensus_score', 0)
            if consensus_score > 0.7:
                reasons.append(f"å¤šç»´åº¦ä¿¡å·é«˜åº¦ä¸€è‡´æŒ‡å‘åè½¬(ä¸€è‡´æ€§:{consensus_score:.2f})")
            
            # é£é™©è°ƒæ•´ç†ç”±
            risk_score = analysis.get('risk_score', 0.5)
            if risk_score < 0.3:
                reasons.append(f"ç»¼åˆé£é™©è¯„ä¼°ä¸ºä½é£é™©æ°´å¹³(é£é™©åˆ†:{risk_score:.2f})")
            
            # å¸‚åœºçŠ¶æ€ç†ç”±
            if 'market_regime' in analysis:
                reasons.append(f"å½“å‰å¸‚åœºçŠ¶æ€: {analysis['market_regime']}")
            
            # æ„å»ºæœ€ç»ˆæ¨ç†
            if reasons:
                main_reason = "ï¼›".join(reasons[:3])  # å–å‰ä¸‰ä¸ªæœ€é‡è¦çš„ç†ç”±
                
                if len(reasons) > 3:
                    additional_count = len(reasons) - 3
                    main_reason += f"ï¼›å¦æœ‰{additional_count}ä¸ªæ”¯æŒä¿¡å·"
                
                return f"å°¾æ•°{tail}åè½¬æ¨èåŸºäºï¼š{main_reason}"
            else:
                return f"å°¾æ•°{tail}åŸºäºç»¼åˆæŠ€æœ¯åˆ†ææ˜¾ç¤ºåè½¬æœºä¼š"
                
        except Exception as e:
            self.logger.error(f"æ¨ç†ç”Ÿæˆé”™è¯¯: {str(e)}")
            return f"å°¾æ•°{tail}ç»¼åˆåˆ†ææ˜¾ç¤ºåè½¬æ½œåŠ›"
    
    def _analyze_optimal_timing(self, tail: int, analysis: Dict, 
                              prediction_horizon: PredictionHorizon) -> Dict[str, Any]:
        """åˆ†ææœ€ä¼˜æ—¶æœº"""
        try:
            timing_factors = []
            
            # æŠ€æœ¯æŒ‡æ ‡æ—¶æœº
            if analysis.get('technical_score', 0) > 0.7:
                timing_factors.append(('immediate', 0.8))
            elif analysis.get('technical_score', 0) > 0.5:
                timing_factors.append(('next_1_2_periods', 0.6))
            
            # æ¨¡å¼è¯†åˆ«æ—¶æœº
            if analysis.get('pattern_score', 0) > 0.8:
                timing_factors.append(('immediate', 0.9))
            
            # æœºå™¨å­¦ä¹ é¢„æµ‹æ—¶æœº
            if analysis.get('ml_score', 0) > 0.7:
                ml_timing = self._estimate_ml_timing(analysis)
                timing_factors.append((ml_timing, 0.7))
            
            # é‡å­åˆ†ææ—¶æœº
            if analysis.get('quantum_score', 0) > 0.6:
                timing_factors.append(('quantum_superposition_collapse', 0.8))
            
            # é£é™©è°ƒæ•´æ—¶æœº
            risk_score = analysis.get('risk_score', 0.5)
            if risk_score < 0.2:
                timing_factors.append(('low_risk_window', 0.9))
            elif risk_score > 0.8:
                timing_factors.append(('wait_for_risk_reduction', 0.3))
            
            # ç»¼åˆæ—¶æœºè¯„ä¼°
            if timing_factors:
                immediate_score = sum(score for timing, score in timing_factors 
                                    if timing == 'immediate')
                delayed_score = sum(score for timing, score in timing_factors 
                                  if 'next' in timing or 'wait' in timing)
                
                if immediate_score > delayed_score:
                    optimal_timing = 'immediate'
                    timing_confidence = immediate_score / len(timing_factors)
                else:
                    optimal_timing = 'next_1_3_periods'
                    timing_confidence = delayed_score / len(timing_factors)
            else:
                optimal_timing = 'monitor_closely'
                timing_confidence = 0.5
            
            return {
                'optimal_timing': optimal_timing,
                'timing_confidence': timing_confidence,
                'timing_factors': timing_factors,
                'prediction_horizon': prediction_horizon.name,
                'recommended_monitoring_frequency': self._recommend_monitoring_frequency(optimal_timing),
                'entry_signals': self._identify_entry_signals(analysis),
                'exit_signals': self._identify_exit_signals(analysis),
                'timing_risk': self._assess_timing_risk(optimal_timing, analysis)
            }
            
        except Exception as e:
            self.logger.error(f"æ—¶æœºåˆ†æé”™è¯¯: {str(e)}")
            return {
                'optimal_timing': 'monitor_closely',
                'timing_confidence': 0.3,
                'error': str(e)
            }
        
    def _quantify_prediction_uncertainty(self, tail: int, fusion_result: Dict, 
                                        risk_assessment: Dict) -> Dict[str, Any]:
        """é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§"""
        try:
            uncertainty_sources = {}
            
            # æ¨¡å‹ä¸ç¡®å®šæ€§
            model_uncertainty = self._quantify_model_uncertainty(fusion_result)
            uncertainty_sources['model'] = model_uncertainty
            
            # æ•°æ®ä¸ç¡®å®šæ€§
            data_uncertainty = self._quantify_data_uncertainty(fusion_result)
            uncertainty_sources['data'] = data_uncertainty
            
            # å‚æ•°ä¸ç¡®å®šæ€§
            parameter_uncertainty = self._quantify_parameter_uncertainty()
            uncertainty_sources['parameter'] = parameter_uncertainty
            
            # è®¤çŸ¥ä¸ç¡®å®šæ€§
            epistemic_uncertainty = self._quantify_epistemic_uncertainty(fusion_result)
            uncertainty_sources['epistemic'] = epistemic_uncertainty
            
            # éšæœºä¸ç¡®å®šæ€§
            aleatoric_uncertainty = self._quantify_aleatoric_uncertainty(risk_assessment)
            uncertainty_sources['aleatoric'] = aleatoric_uncertainty
            
            # è®¡ç®—æ€»ä¸ç¡®å®šæ€§
            total_uncertainty = self._calculate_total_uncertainty(uncertainty_sources)
            
            # ä¸ç¡®å®šæ€§ä¼ æ’­åˆ†æ
            uncertainty_propagation = self._analyze_uncertainty_propagation(uncertainty_sources)
            
            # æ•æ„Ÿæ€§åˆ†æ
            sensitivity_analysis = self._perform_sensitivity_analysis(fusion_result)
            
            # ç½®ä¿¡åŒºé—´è®¡ç®—
            confidence_intervals = self._calculate_confidence_intervals(
                fusion_result, total_uncertainty
            )
            
            return {
                'uncertainty_sources': uncertainty_sources,
                'total_uncertainty': total_uncertainty,
                'uncertainty_propagation': uncertainty_propagation,
                'sensitivity_analysis': sensitivity_analysis,
                'confidence_intervals': confidence_intervals,
                'uncertainty_ranking': self._rank_uncertainty_sources(uncertainty_sources),
                'uncertainty_reduction_suggestions': self._suggest_uncertainty_reduction(uncertainty_sources),
                'prediction_reliability': 1.0 - total_uncertainty
            }
            
        except Exception as e:
            self.logger.error(f"ä¸ç¡®å®šæ€§é‡åŒ–é”™è¯¯: {str(e)}")
            return {'error': str(e), 'total_uncertainty': 0.5}
    
    def _update_adaptive_parameters(self, tail: int, confidence: float, scores: Dict[int, float]):
        """æ›´æ–°è‡ªé€‚åº”å‚æ•°"""
        try:
            # æ›´æ–°é¢„æµ‹ç»Ÿè®¡
            self.total_predictions += 1
            
            # æ›´æ–°åŠ¨æ€é˜ˆå€¼
            if confidence > 0.8:
                # é«˜ç½®ä¿¡åº¦æ—¶ç•¥å¾®æé«˜é˜ˆå€¼
                for key in self.dynamic_thresholds.dynamic_thresholds:
                    current = self.dynamic_thresholds.dynamic_thresholds[key]
                    adaptive_rate = self.dynamic_thresholds.adaptive_rates[key]
                    self.dynamic_thresholds.dynamic_thresholds[key] = min(0.95, 
                        current + adaptive_rate * 0.1)
            elif confidence < 0.4:
                # ä½ç½®ä¿¡åº¦æ—¶é™ä½é˜ˆå€¼
                for key in self.dynamic_thresholds.dynamic_thresholds:
                    current = self.dynamic_thresholds.dynamic_thresholds[key]
                    adaptive_rate = self.dynamic_thresholds.adaptive_rates[key]
                    self.dynamic_thresholds.dynamic_thresholds[key] = max(0.3, 
                        current - adaptive_rate * 0.1)
            
            # æ›´æ–°ä¿¡å·æƒé‡
            self._update_signal_weights(tail, confidence, scores)
            
            # è®°å½•å†å²
            self.dynamic_thresholds.threshold_history[tail].append({
                'timestamp': datetime.now(),
                'confidence': confidence,
                'thresholds': self.dynamic_thresholds.dynamic_thresholds.copy()
            })
            
            # æ›´æ–°æ¨¡å‹æ€§èƒ½
            self._update_model_performance_tracking(tail, confidence)
            
        except Exception as e:
            self.logger.error(f"è‡ªé€‚åº”å‚æ•°æ›´æ–°é”™è¯¯: {str(e)}")
    
    def _update_performance_metrics(self, execution_time: float, confidence: float, 
                                  candidate_count: int):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        try:
            # è®°å½•æ‰§è¡Œæ—¶é—´
            self.performance_monitor['execution_times']['prediction'].append(execution_time)
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            memory_usage = psutil.Process().memory_info().rss / 1024 / 1024
            self.performance_monitor['memory_usage'].append(memory_usage)
            
            # è®°å½•ç½®ä¿¡åº¦å†å²
            self.model_confidence_history.append(confidence)
            
            # æ›´æ–°å¹³å‡æ€§èƒ½æŒ‡æ ‡
            if len(self.performance_monitor['execution_times']['prediction']) > 0:
                avg_execution_time = np.mean(self.performance_monitor['execution_times']['prediction'])
                self.logger.info(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_execution_time:.3f}ç§’")
            
            # å†…å­˜ç›‘æ§è­¦å‘Š
            if memory_usage > self.config['memory_limit_gb'] * 1024:
                self.logger.warning(f"å†…å­˜ä½¿ç”¨è¶…é™: {memory_usage:.1f}MB")
                self._trigger_memory_cleanup()
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½æŒ‡æ ‡æ›´æ–°é”™è¯¯: {str(e)}")
    
    # ========== è¾…åŠ©è®¡ç®—æ–¹æ³• ==========
    
    def _create_data_hash(self, historical_data: List[Dict]) -> str:
        """åˆ›å»ºå†å²æ•°æ®å“ˆå¸Œå€¼"""
        import hashlib
        data_str = str(historical_data)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _get_historical_data_from_hash(self, data_hash: str) -> List[Dict]:
        """ä»å“ˆå¸Œå€¼æ¢å¤å†å²æ•°æ®ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä»ç¼“å­˜ä¸­æ¢å¤æ•°æ®
        # è¿™é‡Œè¿”å›ç©ºåˆ—è¡¨ä½œä¸ºå ä½ç¬¦
        return []
    
    def _create_failure_result(self, reason: str, details: Dict = None) -> Dict[str, Any]:
        """åˆ›å»ºå¤±è´¥ç»“æœ"""
        return {
            'success': False,
            'recommended_tail': None,
            'confidence': 0.0,
            'reasoning': reason,
            'details': details or {},
            'timestamp': datetime.now().isoformat()
        }
    
    def _get_horizon_multiplier(self, horizon: PredictionHorizon) -> float:
        """è·å–æ—¶é—´èŒƒå›´ä¹˜æ•°"""
        multipliers = {
            PredictionHorizon.ULTRA_SHORT: 1.0,
            PredictionHorizon.SHORT: 0.95,
            PredictionHorizon.MEDIUM: 0.9,
            PredictionHorizon.LONG: 0.85,
            PredictionHorizon.ULTRA_LONG: 0.8
        }
        return multipliers.get(horizon, 0.9)
    
    def _calculate_partial_autocorr(self, data: np.ndarray, lag: int) -> float:
        """è®¡ç®—åè‡ªç›¸å…³ç³»æ•°"""
        try:
            if len(data) <= lag:
                return 0.0
            
            # ä½¿ç”¨Yule-Walkeræ–¹ç¨‹è®¡ç®—åè‡ªç›¸å…³
            autocorrs = [1.0]
            for k in range(1, lag + 1):
                if len(data) > k:
                    autocorr = np.corrcoef(data[:-k], data[k:])[0, 1]
                    autocorrs.append(autocorr if not np.isnan(autocorr) else 0.0)
                else:
                    autocorrs.append(0.0)
            
            # ç®€åŒ–çš„åè‡ªç›¸å…³è®¡ç®—
            if len(autocorrs) > lag:
                return autocorrs[lag]
            else:
                return 0.0
                
        except Exception:
            return 0.0
    
    def _calculate_trend_slope(self, data: np.ndarray) -> float:
        """è®¡ç®—è¶‹åŠ¿æ–œç‡"""
        try:
            if len(data) < 2:
                return 0.0
            
            x = np.arange(len(data))
            slope, _ = np.polyfit(x, data, 1)
            return slope
            
        except Exception:
            return 0.0
    
    def _detect_seasonality(self, data: np.ndarray) -> float:
        """æ£€æµ‹å­£èŠ‚æ€§å¼ºåº¦"""
        try:
            if len(data) < 8:
                return 0.0
            
            # ä½¿ç”¨FFTæ£€æµ‹å‘¨æœŸæ€§
            fft = np.fft.fft(data)
            freqs = np.fft.fftfreq(len(data))
            power = np.abs(fft)**2
            
            # æ’é™¤DCåˆ†é‡
            power[0] = 0
            
            # æ‰¾å‡ºä¸»å¯¼é¢‘ç‡
            max_power = np.max(power)
            total_power = np.sum(power)
            
            if total_power > 0:
                seasonality_strength = max_power / total_power
            else:
                seasonality_strength = 0.0
            
            return min(1.0, seasonality_strength * 4)
            
        except Exception:
            return 0.0
    
    def _test_stationarity(self, data: np.ndarray) -> float:
        """æµ‹è¯•å¹³ç¨³æ€§"""
        try:
            if len(data) < 10:
                return 0.5
            
            # ADFæµ‹è¯•
            try:
                adf_stat, adf_pvalue, _, _, _, _ = adfuller(data)
                adf_score = 1.0 - adf_pvalue  # på€¼è¶Šå°ï¼Œè¶Šå¹³ç¨³
            except:
                adf_score = 0.5
            
            # KPSSæµ‹è¯•
            try:
                kpss_stat, kpss_pvalue, _, _ = kpss(data)
                kpss_score = kpss_pvalue  # på€¼è¶Šå¤§ï¼Œè¶Šå¹³ç¨³
            except:
                kpss_score = 0.5
            
            # ç»„åˆè¯„åˆ†
            stationarity_score = (adf_score + kpss_score) / 2
            return min(1.0, max(0.0, stationarity_score))
            
        except Exception:
            return 0.5
    
    def _extract_fft_features(self, data: np.ndarray) -> List[float]:
        """æå–FFTç‰¹å¾"""
        try:
            fft = np.fft.fft(data)
            freqs = np.fft.fftfreq(len(data))
            power = np.abs(fft)**2
            
            # å»é™¤ç›´æµåˆ†é‡
            power[0] = 0
            
            features = []
            
            # ä¸»å¯¼é¢‘ç‡
            if len(power) > 1:
                dominant_freq_idx = np.argmax(power[1:]) + 1
                features.append(freqs[dominant_freq_idx])
                features.append(power[dominant_freq_idx])
            else:
                features.extend([0.0, 0.0])
            
            # åŠŸç‡è°±ç‰¹å¾
            if np.sum(power) > 0:
                features.append(np.sum(power))  # æ€»åŠŸç‡
                features.append(np.mean(power))  # å¹³å‡åŠŸç‡
                features.append(np.std(power))   # åŠŸç‡æ ‡å‡†å·®
                features.append(np.max(power))   # æœ€å¤§åŠŸç‡
            else:
                features.extend([0.0, 0.0, 0.0, 0.0])
            
            return features
            
        except Exception:
            return [0.0] * 6
    
    def _extract_nonlinear_features(self, data: np.ndarray) -> List[float]:
        """æå–éçº¿æ€§ç‰¹å¾"""
        try:
            features = []
            
            # æé›…æ™®è¯ºå¤«æŒ‡æ•°ï¼ˆç®€åŒ–ï¼‰
            lyapunov = self._calculate_lyapunov_exponent(data)
            features.append(lyapunov)
            
            # èµ«æ–¯ç‰¹æŒ‡æ•°
            hurst = self._calculate_hurst_exponent(data)
            features.append(hurst)
            
            # è¿‘ä¼¼ç†µ
            approx_entropy = self._calculate_approximate_entropy(data)
            features.append(approx_entropy)
            
            # æ ·æœ¬ç†µ
            sample_entropy = self._calculate_sample_entropy(data)
            features.append(sample_entropy)
            
            # åˆ†å½¢ç»´æ•°
            fractal_dim = self._calculate_fractal_dimension(data)
            features.append(fractal_dim)
            
            # å»è¶‹åŠ¿æ³¢åŠ¨åˆ†æ
            dfa_alpha = self._calculate_dfa_alpha(data)
            features.append(dfa_alpha)
            
            # é€’å½’é‡åŒ–åˆ†ææŒ‡æ ‡
            rr, det = self._calculate_rqa_measures(data)
            features.extend([rr, det])
            
            return features
            
        except Exception:
            return [0.0] * 8
    
    def _extract_technical_features(self, data: np.ndarray) -> List[float]:
        """æå–æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        try:
            features = []
            
            if len(data) < 5:
                return [0.0] * 10
            
            # RSI
            rsi = self._calculate_rsi_simple(data)
            features.append(rsi)
            
            # MACD
            if len(data) >= 26:
                macd, macd_signal, macd_hist = self._calculate_macd_simple(data)
                features.extend([macd, macd_signal, macd_hist])
            else:
                features.extend([0.0, 0.0, 0.0])
            
            # å¸ƒæ—å¸¦ä½ç½®
            if len(data) >= 20:
                bb_position = self._calculate_bollinger_position_simple(data)
                features.append(bb_position)
            else:
                features.append(0.5)
            
            # éšæœºæŒ‡æ ‡
            if len(data) >= 14:
                stoch_k = self._calculate_stochastic_simple(data)
                features.append(stoch_k)
            else:
                features.append(50.0)
            
            # Williams %R
            if len(data) >= 14:
                williams_r = self._calculate_williams_r_simple(data)
                features.append(williams_r)
            else:
                features.append(-50.0)
            
            # CCI
            if len(data) >= 20:
                cci = self._calculate_cci_simple(data)
                features.append(cci)
            else:
                features.append(0.0)
            
            # ROC (Rate of Change)
            if len(data) >= 10:
                roc = (data[-1] - data[-10]) / data[-10] if data[-10] != 0 else 0.0
                features.append(roc)
            else:
                features.append(0.0)
            
            # åŠ¨é‡æŒ‡æ ‡
            if len(data) >= 5:
                momentum = data[-1] - data[-5]
                features.append(momentum)
            else:
                features.append(0.0)
            
            return features
            
        except Exception:
            return [0.0] * 10
    
    def _extract_relative_features(self, tail: int, data_matrix: np.ndarray) -> List[float]:
        """æå–ç›¸å¯¹ç‰¹å¾"""
        try:
            features = []
            tail_data = data_matrix[:, tail]
            
            # ä¸å…¶ä»–å°¾æ•°çš„ç›¸å…³æ€§
            correlations = []
            for other_tail in range(10):
                if other_tail != tail:
                    other_data = data_matrix[:, other_tail]
                    if len(tail_data) > 1 and len(other_data) > 1:
                        corr = np.corrcoef(tail_data, other_data)[0, 1]
                        correlations.append(corr if not np.isnan(corr) else 0.0)
            
            if correlations:
                features.extend([
                    np.mean(correlations),
                    np.std(correlations),
                    np.max(correlations),
                    np.min(correlations)
                ])
            else:
                features.extend([0.0, 0.0, 0.0, 0.0])
            
            # ç›¸å¯¹å¼ºåº¦
            if len(tail_data) > 0:
                total_appearances = np.sum(data_matrix, axis=1)
                relative_strength = np.mean(tail_data) / np.mean(total_appearances) if np.mean(total_appearances) > 0 else 0.0
                features.append(relative_strength)
            else:
                features.append(0.0)
            
            # ç›¸å¯¹å˜å¼‚ç³»æ•°
            if len(tail_data) > 1:
                cv = np.std(tail_data) / np.mean(tail_data) if np.mean(tail_data) > 0 else 0.0
                features.append(cv)
            else:
                features.append(0.0)
            
            return features
            
        except Exception:
            return [0.0] * 6
    
    def _extract_window_features(self, data: np.ndarray, horizon: PredictionHorizon) -> List[float]:
        """æå–æ—¶é—´çª—å£ç‰¹å¾"""
        try:
            features = []
            window_size = horizon.value
            
            if len(data) < window_size:
                return [0.0] * 5
            
            # æœ€è¿‘çª—å£ç»Ÿè®¡
            recent_window = data[-window_size:]
            features.extend([
                np.mean(recent_window),
                np.std(recent_window),
                np.max(recent_window) - np.min(recent_window),  # èŒƒå›´
                np.sum(recent_window > np.mean(data)),  # é«˜äºæ•´ä½“å‡å€¼çš„æ•°é‡
                np.sum(np.diff(recent_window) > 0)      # ä¸Šå‡è¶‹åŠ¿æ•°é‡
            ])
            
            return features
            
        except Exception:
            return [0.0] * 5
        
# ========== é«˜çº§æŠ€æœ¯æŒ‡æ ‡å®ç° ==========
    
    def _calculate_rsi_advanced(self, data: np.ndarray, period: int = 14) -> np.ndarray:
        """è®¡ç®—é«˜çº§RSIæŒ‡æ ‡"""
        try:
            if len(data) < period + 1:
                return np.array([50.0])
            
            delta = np.diff(data)
            gain = np.where(delta > 0, delta, 0)
            loss = np.where(delta < 0, -delta, 0)
            
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            avg_gain = pd.Series(gain).ewm(span=period).mean().values
            avg_loss = pd.Series(loss).ewm(span=period).mean().values
            
            rs = avg_gain / (avg_loss + 1e-10)
            rsi = 100 - (100 / (1 + rs))
            
            return rsi
            
        except Exception:
            return np.array([50.0])
    
    def _calculate_macd_advanced(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """è®¡ç®—é«˜çº§MACDæŒ‡æ ‡"""
        try:
            if len(data) < 26:
                return np.array([0.0]), np.array([0.0]), np.array([0.0])
            
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            ema12 = pd.Series(data).ewm(span=12).mean().values
            ema26 = pd.Series(data).ewm(span=26).mean().values
            
            # MACDçº¿
            macd_line = ema12 - ema26
            
            # ä¿¡å·çº¿
            macd_signal = pd.Series(macd_line).ewm(span=9).mean().values
            
            # MACDæŸ±çŠ¶å›¾
            macd_histogram = macd_line - macd_signal
            
            return macd_line, macd_signal, macd_histogram
            
        except Exception:
            return np.array([0.0]), np.array([0.0]), np.array([0.0])
    
    def _calculate_bollinger_bands_advanced(self, data: np.ndarray, period: int = 20, 
                                          std_mult: float = 2.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """è®¡ç®—é«˜çº§å¸ƒæ—å¸¦"""
        try:
            if len(data) < period:
                mean_val = np.mean(data)
                std_val = np.std(data)
                return (np.array([mean_val + std_mult * std_val]), 
                       np.array([mean_val]), 
                       np.array([mean_val - std_mult * std_val]))
            
            # ç®€å•ç§»åŠ¨å¹³å‡
            sma = pd.Series(data).rolling(window=period).mean().values
            
            # æ ‡å‡†å·®
            std = pd.Series(data).rolling(window=period).std().values
            
            # å¸ƒæ—å¸¦
            upper_band = sma + (std_mult * std)
            lower_band = sma - (std_mult * std)
            
            return upper_band, sma, lower_band
            
        except Exception:
            mean_val = np.mean(data) if len(data) > 0 else 0.5
            return (np.array([mean_val + 0.1]), 
                   np.array([mean_val]), 
                   np.array([mean_val - 0.1]))
    
    def _calculate_stochastic_advanced(self, data: np.ndarray, period: int = 14) -> Tuple[np.ndarray, np.ndarray]:
        """è®¡ç®—é«˜çº§éšæœºæŒ‡æ ‡"""
        try:
            if len(data) < period:
                return np.array([50.0]), np.array([50.0])
            
            # è®¡ç®—%K
            lowest_low = pd.Series(data).rolling(window=period).min().values
            highest_high = pd.Series(data).rolling(window=period).max().values
            
            k_percent = 100 * ((data - lowest_low) / (highest_high - lowest_low + 1e-10))
            
            # è®¡ç®—%D (3æœŸç§»åŠ¨å¹³å‡)
            d_percent = pd.Series(k_percent).rolling(window=3).mean().values
            
            return k_percent, d_percent
            
        except Exception:
            return np.array([50.0]), np.array([50.0])
    
    def _calculate_adx_advanced(self, data: np.ndarray, period: int = 14) -> np.ndarray:
        """è®¡ç®—é«˜çº§ADXæŒ‡æ ‡"""
        try:
            if len(data) < period + 1:
                return np.array([25.0])
            
            # æ¨¡æ‹Ÿé«˜ä½ä»·ï¼ˆä½¿ç”¨æ•°æ®å˜åŒ–ï¼‰
            high = data + np.abs(np.random.normal(0, 0.01, len(data)))
            low = data - np.abs(np.random.normal(0, 0.01, len(data)))
            close = data
            
            # è®¡ç®—True Range
            tr1 = high[1:] - low[1:]
            tr2 = np.abs(high[1:] - close[:-1])
            tr3 = np.abs(low[1:] - close[:-1])
            tr = np.maximum(tr1, np.maximum(tr2, tr3))
            
            # è®¡ç®—æ–¹å‘æ€§ç§»åŠ¨
            dm_plus = np.where((high[1:] - high[:-1]) > (low[:-1] - low[1:]), 
                              np.maximum(high[1:] - high[:-1], 0), 0)
            dm_minus = np.where((low[:-1] - low[1:]) > (high[1:] - high[:-1]), 
                               np.maximum(low[:-1] - low[1:], 0), 0)
            
            # å¹³æ»‘å¤„ç†
            atr = pd.Series(tr).rolling(window=period).mean().values
            di_plus = 100 * pd.Series(dm_plus).rolling(window=period).mean().values / (atr + 1e-10)
            di_minus = 100 * pd.Series(dm_minus).rolling(window=period).mean().values / (atr + 1e-10)
            
            # è®¡ç®—ADX
            dx = 100 * np.abs(di_plus - di_minus) / (di_plus + di_minus + 1e-10)
            adx = pd.Series(dx).rolling(window=period).mean().values
            
            return adx
            
        except Exception:
            return np.array([25.0])
    
    # ========== å°æ³¢åˆ†æé«˜çº§å®ç° ==========
    
    def _calculate_wavelet_entropy(self, coeffs: np.ndarray) -> float:
        """è®¡ç®—å°æ³¢ç†µ"""
        try:
            if len(coeffs) == 0:
                return 0.0
            
            # å½’ä¸€åŒ–èƒ½é‡
            energy = coeffs**2
            total_energy = np.sum(energy)
            
            if total_energy == 0:
                return 0.0
            
            p = energy / total_energy
            p = p[p > 0]  # é¿å…log(0)
            
            entropy = -np.sum(p * np.log2(p))
            
            return entropy
            
        except Exception:
            return 0.0
    
    def _calculate_singularity_spectrum(self, data: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—å¥‡å¼‚è°±"""
        try:
            if len(data) < 16:
                return {'alpha_min': 0.0, 'alpha_max': 0.0, 'width': 0.0}
            
            # ä½¿ç”¨å°æ³¢å˜æ¢è¿›è¡Œå¤šåˆ†è¾¨ç‡åˆ†æ
            wavelet = 'db4'
            levels = min(4, int(np.log2(len(data))))
            
            coeffs = pywt.wavedec(data, wavelet, level=levels)
            
            # è®¡ç®—å±€éƒ¨å¥‡å¼‚æ€§æŒ‡æ•°
            alphas = []
            for level, detail in enumerate(coeffs[1:], 1):
                if len(detail) > 0:
                    # è®¡ç®—å±€éƒ¨æœ€å¤§å€¼
                    local_maxima = find_peaks(np.abs(detail))[0]
                    
                    for peak in local_maxima:
                        if peak < len(detail):
                            # è®¡ç®—å¥‡å¼‚æ€§æŒ‡æ•°
                            alpha = np.log2(np.abs(detail[peak]) + 1e-10) / level
                            alphas.append(alpha)
            
            if alphas:
                alpha_min = np.min(alphas)
                alpha_max = np.max(alphas)
                width = alpha_max - alpha_min
            else:
                alpha_min = alpha_max = width = 0.0
            
            return {
                'alpha_min': alpha_min,
                'alpha_max': alpha_max, 
                'width': width
            }
            
        except Exception:
            return {'alpha_min': 0.0, 'alpha_max': 0.0, 'width': 0.0}
    
    def _analyze_wavelet_correlation(self, coeffs: List[np.ndarray]) -> Dict[str, float]:
        """åˆ†æå°æ³¢ç›¸å…³æ€§"""
        try:
            if len(coeffs) < 2:
                return {'cross_correlation': 0.0, 'scale_correlation': 0.0}
            
            # è·¨å°ºåº¦ç›¸å…³æ€§
            correlations = []
            for i in range(len(coeffs) - 1):
                for j in range(i + 1, len(coeffs)):
                    if len(coeffs[i]) > 1 and len(coeffs[j]) > 1:
                        # é‡æ–°é‡‡æ ·åˆ°ç›¸åŒé•¿åº¦
                        min_len = min(len(coeffs[i]), len(coeffs[j]))
                        c1 = coeffs[i][:min_len]
                        c2 = coeffs[j][:min_len]
                        
                        corr = np.corrcoef(c1, c2)[0, 1]
                        if not np.isnan(corr):
                            correlations.append(abs(corr))
            
            cross_correlation = np.mean(correlations) if correlations else 0.0
            
            # å°ºåº¦é—´ç›¸å…³æ€§
            scale_correlations = []
            for i in range(1, len(coeffs)):
                if len(coeffs[i]) > 1:
                    autocorr = np.corrcoef(coeffs[i][:-1], coeffs[i][1:])[0, 1]
                    if not np.isnan(autocorr):
                        scale_correlations.append(abs(autocorr))
            
            scale_correlation = np.mean(scale_correlations) if scale_correlations else 0.0
            
            return {
                'cross_correlation': cross_correlation,
                'scale_correlation': scale_correlation
            }
            
        except Exception:
            return {'cross_correlation': 0.0, 'scale_correlation': 0.0}
    
    def _calculate_wavelet_reversal_probability(self, coeffs: List[np.ndarray], 
                                              detail_analysis: List[Dict],
                                              singularity_spectrum: Dict) -> float:
        """è®¡ç®—å°æ³¢åè½¬æ¦‚ç‡"""
        try:
            if not coeffs or not detail_analysis:
                return 0.0
            
            reversal_indicators = []
            
            # åŸºäºèƒ½é‡åˆ†å¸ƒ
            energies = [da['energy'] for da in detail_analysis]
            if energies:
                # é«˜é¢‘èƒ½é‡çªå¢è¡¨æ˜åè½¬
                high_freq_energy = sum(energies[:2]) if len(energies) >= 2 else 0
                total_energy = sum(energies)
                
                if total_energy > 0:
                    high_freq_ratio = high_freq_energy / total_energy
                    reversal_indicators.append(high_freq_ratio)
            
            # åŸºäºå¥‡å¼‚è°±å®½åº¦
            spectrum_width = singularity_spectrum.get('width', 0.0)
            if spectrum_width > 1.0:  # å®½è°±è¡¨æ˜å¤æ‚æ€§å¢åŠ 
                reversal_indicators.append(min(1.0, spectrum_width / 3.0))
            
            # åŸºäºæœ€å¤§ç³»æ•°
            max_coeffs = [da['max_coeff'] for da in detail_analysis]
            if max_coeffs:
                max_coeff_ratio = max(max_coeffs) / (np.mean(max_coeffs) + 1e-10)
                if max_coeff_ratio > 2.0:  # å¼‚å¸¸å¤§çš„ç³»æ•°
                    reversal_indicators.append(min(1.0, (max_coeff_ratio - 1) / 3.0))
            
            # åŸºäºç†µå€¼
            entropies = [da['entropy'] for da in detail_analysis]
            if entropies:
                avg_entropy = np.mean(entropies)
                if avg_entropy > 2.0:  # é«˜ç†µè¡¨æ˜æ··ä¹±åº¦å¢åŠ 
                    reversal_indicators.append(min(1.0, (avg_entropy - 1) / 3.0))
            
            return np.mean(reversal_indicators) if reversal_indicators else 0.0
            
        except Exception:
            return 0.0
    
    def _identify_dominant_scales(self, coeffs: List[np.ndarray]) -> List[int]:
        """è¯†åˆ«ä¸»å¯¼å°ºåº¦"""
        try:
            if not coeffs:
                return []
            
            # è®¡ç®—æ¯ä¸ªå°ºåº¦çš„èƒ½é‡
            energies = []
            for i, coeff in enumerate(coeffs[1:], 1):  # è·³è¿‡è¿‘ä¼¼ç³»æ•°
                energy = np.sum(coeff**2)
                energies.append((i, energy))
            
            # æŒ‰èƒ½é‡æ’åº
            energies.sort(key=lambda x: x[1], reverse=True)
            
            # è¿”å›å‰3ä¸ªä¸»å¯¼å°ºåº¦
            dominant_scales = [scale for scale, _ in energies[:3]]
            
            return dominant_scales
            
        except Exception:
            return []
    
    def _calculate_multiscale_entropy(self, coeffs: List[np.ndarray]) -> List[float]:
        """è®¡ç®—å¤šå°ºåº¦ç†µ"""
        try:
            entropies = []
            
            for coeff in coeffs:
                if len(coeff) > 0:
                    entropy = self._calculate_wavelet_entropy(coeff)
                    entropies.append(entropy)
                else:
                    entropies.append(0.0)
            
            return entropies
            
        except Exception:
            return [0.0]
    
    # ========== å‚…é‡Œå¶åˆ†æé«˜çº§å®ç° ==========
    
    def _identify_dominant_frequencies(self, frequencies: np.ndarray, 
                                     power_spectrum: np.ndarray) -> List[Tuple[float, float]]:
        """è¯†åˆ«ä¸»å¯¼é¢‘ç‡"""
        try:
            if len(power_spectrum) == 0:
                return []
            
            # æ‰¾å‡ºåŠŸç‡è°±çš„å³°å€¼
            peaks, _ = find_peaks(power_spectrum, height=np.max(power_spectrum) * 0.1)
            
            # è·å–å³°å€¼å¯¹åº”çš„é¢‘ç‡å’ŒåŠŸç‡
            dominant_freqs = []
            for peak in peaks:
                if peak < len(frequencies):
                    freq = frequencies[peak]
                    power = power_spectrum[peak]
                    dominant_freqs.append((freq, power))
            
            # æŒ‰åŠŸç‡æ’åº
            dominant_freqs.sort(key=lambda x: x[1], reverse=True)
            
            return dominant_freqs[:5]  # è¿”å›å‰5ä¸ªä¸»å¯¼é¢‘ç‡
            
        except Exception:
            return []
    
    def _analyze_frequency_stability(self, data: np.ndarray, window_size: int = 10) -> float:
        """åˆ†æé¢‘ç‡ç¨³å®šæ€§"""
        try:
            if len(data) < window_size * 2:
                return 0.5
            
            # æ»‘åŠ¨çª—å£åˆ†æ
            stability_measures = []
            
            for i in range(len(data) - window_size):
                window1 = data[i:i + window_size]
                window2 = data[i + 1:i + 1 + window_size]
                
                # è®¡ç®—æ¯ä¸ªçª—å£çš„ä¸»å¯¼é¢‘ç‡
                fft1 = np.fft.fft(window1)
                fft2 = np.fft.fft(window2)
                
                power1 = np.abs(fft1)**2
                power2 = np.abs(fft2)**2
                
                # æ‰¾å‡ºä¸»å¯¼é¢‘ç‡
                peak1 = np.argmax(power1[1:]) + 1
                peak2 = np.argmax(power2[1:]) + 1
                
                # è®¡ç®—é¢‘ç‡ç¨³å®šæ€§
                freq_diff = abs(peak1 - peak2) / max(peak1, peak2, 1)
                stability = 1.0 - freq_diff
                stability_measures.append(stability)
            
            return np.mean(stability_measures) if stability_measures else 0.5
            
        except Exception:
            return 0.5
    
    def _perform_harmonic_analysis(self, fft_values: np.ndarray, 
                                 frequencies: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œè°æ³¢åˆ†æ"""
        try:
            if len(fft_values) == 0:
                return {'harmonic_content': 0.0, 'fundamental_frequency': 0.0}
            
            power_spectrum = np.abs(fft_values)**2
            
            # æ‰¾å‡ºåŸºé¢‘
            fundamental_idx = np.argmax(power_spectrum[1:]) + 1
            fundamental_freq = frequencies[fundamental_idx] if fundamental_idx < len(frequencies) else 0.0
            
            # å¯»æ‰¾è°æ³¢
            harmonics = []
            for n in range(2, 6):  # 2-5æ¬¡è°æ³¢
                harmonic_freq = n * fundamental_freq
                # æ‰¾åˆ°æœ€æ¥è¿‘è°æ³¢é¢‘ç‡çš„ç´¢å¼•
                freq_diffs = np.abs(frequencies - harmonic_freq)
                harmonic_idx = np.argmin(freq_diffs)
                
                if freq_diffs[harmonic_idx] < 0.1:  # é¢‘ç‡è¯¯å·®å®¹å¿åº¦
                    harmonic_power = power_spectrum[harmonic_idx]
                    harmonics.append(harmonic_power)
            
            # è®¡ç®—è°æ³¢å«é‡
            fundamental_power = power_spectrum[fundamental_idx]
            total_harmonic_power = sum(harmonics)
            
            if fundamental_power > 0:
                harmonic_content = total_harmonic_power / fundamental_power
            else:
                harmonic_content = 0.0
            
            return {
                'harmonic_content': harmonic_content,
                'fundamental_frequency': fundamental_freq,
                'harmonic_count': len(harmonics),
                'total_harmonic_distortion': harmonic_content
            }
            
        except Exception:
            return {'harmonic_content': 0.0, 'fundamental_frequency': 0.0}
    
    def _calculate_spectral_centroid(self, power_spectrum: np.ndarray, 
                                   frequencies: np.ndarray) -> float:
        """è®¡ç®—é¢‘è°±é‡å¿ƒ"""
        try:
            if len(power_spectrum) == 0 or np.sum(power_spectrum) == 0:
                return 0.0
            
            centroid = np.sum(frequencies * power_spectrum) / np.sum(power_spectrum)
            return centroid
            
        except Exception:
            return 0.0
    
    def _calculate_spectral_rolloff(self, power_spectrum: np.ndarray, 
                                  frequencies: np.ndarray, threshold: float = 0.85) -> float:
        """è®¡ç®—é¢‘è°±æ»šé™ç‚¹"""
        try:
            if len(power_spectrum) == 0:
                return 0.0
            
            total_energy = np.sum(power_spectrum)
            cumulative_energy = np.cumsum(power_spectrum)
            
            rolloff_idx = np.where(cumulative_energy >= threshold * total_energy)[0]
            
            if len(rolloff_idx) > 0:
                return frequencies[rolloff_idx[0]]
            else:
                return frequencies[-1] if len(frequencies) > 0 else 0.0
                
        except Exception:
            return 0.0
    
    def _calculate_spectral_flux(self, power_spectrum: np.ndarray) -> float:
        """è®¡ç®—é¢‘è°±é€šé‡"""
        try:
            if len(power_spectrum) < 2:
                return 0.0
            
            # è®¡ç®—ç›¸é‚»å¸§ä¹‹é—´çš„åŠŸç‡è°±å·®å¼‚
            flux = np.sum(np.diff(power_spectrum)**2)
            return flux
            
        except Exception:
            return 0.0
    
    def _calculate_spectral_bandwidth(self, power_spectrum: np.ndarray, 
                                    frequencies: np.ndarray) -> float:
        """è®¡ç®—é¢‘è°±å¸¦å®½"""
        try:
            if len(power_spectrum) == 0:
                return 0.0
            
            centroid = self._calculate_spectral_centroid(power_spectrum, frequencies)
            
            if np.sum(power_spectrum) == 0:
                return 0.0
            
            bandwidth = np.sqrt(np.sum(((frequencies - centroid)**2) * power_spectrum) / 
                               np.sum(power_spectrum))
            
            return bandwidth
            
        except Exception:
            return 0.0
    
    def _calculate_frequency_reversal_score(self, dominant_frequencies: List[Tuple[float, float]], 
                                          frequency_stability: float, 
                                          harmonic_analysis: Dict) -> float:
        """è®¡ç®—é¢‘åŸŸåè½¬è¯„åˆ†"""
        try:
            score_components = []
            
            # ä¸»å¯¼é¢‘ç‡å˜åŒ–
            if dominant_frequencies:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜åŠŸç‡çš„é¢‘ç‡
                powers = [power for _, power in dominant_frequencies]
                if powers:
                    max_power = max(powers)
                    avg_power = np.mean(powers)
                    
                    if avg_power > 0:
                        power_ratio = max_power / avg_power
                        if power_ratio > 3.0:  # å¼‚å¸¸çªå‡ºçš„é¢‘ç‡
                            score_components.append(min(1.0, (power_ratio - 1) / 5.0))
            
            # é¢‘ç‡ä¸ç¨³å®šæ€§
            instability = 1.0 - frequency_stability
            if instability > 0.5:
                score_components.append(instability)
            
            # è°æ³¢å¤±çœŸ
            harmonic_content = harmonic_analysis.get('harmonic_content', 0.0)
            if harmonic_content > 0.3:
                score_components.append(min(1.0, harmonic_content))
            
            return np.mean(score_components) if score_components else 0.0
            
        except Exception:
            return 0.0
        
# ========== éçº¿æ€§åŠ¨åŠ›å­¦é«˜çº§å®ç° ==========
    
    def _calculate_lyapunov_exponent(self, data: np.ndarray, embedding_dim: int = 3, 
                                   time_delay: int = 1) -> float:
        """è®¡ç®—æé›…æ™®è¯ºå¤«æŒ‡æ•°"""
        try:
            if len(data) < 30:
                return 0.0
            
            # ç›¸ç©ºé—´é‡æ„
            embedded = self._embed_time_series(data, embedding_dim, time_delay)
            
            if len(embedded) < 10:
                return 0.0
            
            # å¯»æ‰¾æœ€è¿‘é‚»ç‚¹
            distances = []
            divergences = []
            
            for i in range(len(embedded) - 10):
                # è®¡ç®—åˆ°å…¶ä»–ç‚¹çš„è·ç¦»
                point = embedded[i]
                other_points = embedded[i+1:]
                
                dists = np.linalg.norm(other_points - point, axis=1)
                
                # æ‰¾æœ€è¿‘é‚»
                nearest_idx = np.argmin(dists)
                if nearest_idx < len(dists) - 5:
                    initial_distance = dists[nearest_idx]
                    
                    if initial_distance > 0:
                        # è·Ÿè¸ª5æ­¥åçš„è·ç¦»
                        future_distance = np.linalg.norm(
                            embedded[i + nearest_idx + 5] - embedded[i + 5]
                        )
                        
                        if future_distance > 0:
                            divergence = np.log(future_distance / initial_distance)
                            divergences.append(divergence)
            
            if divergences:
                lyapunov = np.mean(divergences) / 5  # é™¤ä»¥æ—¶é—´æ­¥é•¿
            else:
                lyapunov = 0.0
            
            return lyapunov
            
        except Exception:
            return 0.0
    
    def _calculate_correlation_dimension(self, data: np.ndarray, 
                                       embedding_dim: int = 5) -> float:
        """è®¡ç®—å…³è”ç»´æ•°"""
        try:
            if len(data) < 50:
                return 1.0
            
            # ç›¸ç©ºé—´é‡æ„
            embedded = self._embed_time_series(data, embedding_dim, 1)
            
            if len(embedded) < 10:
                return 1.0
            
            # è®¡ç®—å…³è”ç§¯åˆ†
            radii = np.logspace(-3, 0, 20)
            correlations = []
            
            for r in radii:
                count = 0
                total_pairs = 0
                
                for i in range(len(embedded)):
                    for j in range(i + 1, len(embedded)):
                        distance = np.linalg.norm(embedded[i] - embedded[j])
                        total_pairs += 1
                        
                        if distance < r:
                            count += 1
                
                if total_pairs > 0:
                    correlation = count / total_pairs
                    correlations.append(correlation)
                else:
                    correlations.append(0.0)
            
            # è®¡ç®—å…³è”ç»´æ•°ï¼ˆæ–œç‡ï¼‰
            log_radii = np.log(radii)
            log_correlations = np.log(np.array(correlations) + 1e-10)
            
            # çº¿æ€§æ‹Ÿåˆ
            valid_indices = ~np.isnan(log_correlations) & ~np.isinf(log_correlations)
            if np.sum(valid_indices) > 5:
                slope, _ = np.polyfit(log_radii[valid_indices], 
                                    log_correlations[valid_indices], 1)
                correlation_dimension = slope
            else:
                correlation_dimension = 1.0
            
            return max(0.0, min(10.0, correlation_dimension))
            
        except Exception:
            return 1.0
    
    def _calculate_hurst_exponent(self, data: np.ndarray) -> float:
        """è®¡ç®—èµ«æ–¯ç‰¹æŒ‡æ•°"""
        try:
            if len(data) < 10:
                return 0.5
            
            # R/Såˆ†æ
            N = len(data)
            mean_data = np.mean(data)
            
            # ç´¯ç§¯åå·®
            Y = np.cumsum(data - mean_data)
            
            # èŒƒå›´
            R = np.max(Y) - np.min(Y)
            
            # æ ‡å‡†å·®
            S = np.std(data)
            
            if S == 0:
                return 0.5
            
            # R/Sæ¯”ç‡
            rs_ratio = R / S
            
            if rs_ratio <= 0:
                return 0.5
            
            # èµ«æ–¯ç‰¹æŒ‡æ•°ä¼°è®¡
            hurst = np.log(rs_ratio) / np.log(N)
            
            # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            return max(0.0, min(1.0, hurst))
            
        except Exception:
            return 0.5
    
    def _calculate_fractal_dimension(self, data: np.ndarray) -> float:
        """è®¡ç®—åˆ†å½¢ç»´æ•°"""
        try:
            if len(data) < 4:
                return 1.0
            
            # ç›’è®¡æ•°æ³•
            # å°†æ•°æ®å½’ä¸€åŒ–åˆ°[0,1]
            normalized_data = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-10)
            
            # ä¸åŒç›’å­å¤§å°
            box_sizes = []
            box_counts = []
            
            for box_size in [1/4, 1/8, 1/16, 1/32, 1/64]:
                if box_size > 0:
                    # è®¡ç®—éœ€è¦çš„ç›’å­æ•°é‡
                    grid_size = int(1.0 / box_size)
                    boxes = set()
                    
                    for i, value in enumerate(normalized_data):
                        x_box = int(i * grid_size / len(normalized_data))
                        y_box = int(value * grid_size)
                        boxes.add((x_box, y_box))
                    
                    box_sizes.append(box_size)
                    box_counts.append(len(boxes))
            
            if len(box_sizes) > 2:
                # çº¿æ€§æ‹Ÿåˆè®¡ç®—ç»´æ•°
                log_sizes = np.log(box_sizes)
                log_counts = np.log(box_counts)
                
                slope, _ = np.polyfit(log_sizes, log_counts, 1)
                fractal_dimension = -slope
            else:
                fractal_dimension = 1.0
            
            return max(1.0, min(3.0, fractal_dimension))
            
        except Exception:
            return 1.0
    
    def _calculate_shannon_entropy(self, data: np.ndarray, bins: int = 50) -> float:
        """è®¡ç®—é¦™å†œç†µ"""
        try:
            if len(data) == 0:
                return 0.0
            
            # æ•°æ®åˆ†ç®±
            hist, _ = np.histogram(data, bins=bins, density=True)
            hist = hist + 1e-10  # é¿å…log(0)
            
            # å½’ä¸€åŒ–
            hist = hist / np.sum(hist)
            
            # è®¡ç®—ç†µ
            entropy = -np.sum(hist * np.log2(hist))
            
            return entropy
            
        except Exception:
            return 0.0
    
    def _calculate_approximate_entropy(self, data: np.ndarray, m: int = 2, 
                                     r: float = None) -> float:
        """è®¡ç®—è¿‘ä¼¼ç†µ"""
        try:
            if len(data) < 10:
                return 0.0
            
            N = len(data)
            if r is None:
                r = 0.2 * np.std(data)
            
            def _maxdist(xi, xj, m):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])
            
            def _phi(m):
                patterns = np.array([data[i:i + m] for i in range(N - m + 1)])
                C = np.zeros(N - m + 1)
                
                for i in range(N - m + 1):
                    template = patterns[i]
                    for j in range(N - m + 1):
                        if _maxdist(template, patterns[j], m) <= r:
                            C[i] += 1.0
                
                C = C / (N - m + 1.0)
                phi = np.mean(np.log(C + 1e-10))
                return phi
            
            approximate_entropy = _phi(m) - _phi(m + 1)
            return approximate_entropy
            
        except Exception:
            return 0.0
    
    def _calculate_sample_entropy(self, data: np.ndarray, m: int = 2, 
                                r: float = None) -> float:
        """è®¡ç®—æ ·æœ¬ç†µ"""
        try:
            if len(data) < 10:
                return 0.0
            
            N = len(data)
            if r is None:
                r = 0.2 * np.std(data)
            
            def _maxdist(xi, xj, m):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])
            
            patterns_m = np.array([data[i:i + m] for i in range(N - m + 1)])
            patterns_m1 = np.array([data[i:i + m + 1] for i in range(N - m)])
            
            A = 0
            B = 0
            
            for i in range(N - m + 1):
                for j in range(i + 1, N - m + 1):
                    if _maxdist(patterns_m[i], patterns_m[j], m) <= r:
                        B += 1
                        if j < N - m and _maxdist(patterns_m1[i], patterns_m1[j], m + 1) <= r:
                            A += 1
            
            if B == 0:
                return 0.0
            
            sample_entropy = -np.log(A / B)
            return sample_entropy
            
        except Exception:
            return 0.0
    
    def _calculate_permutation_entropy(self, data: np.ndarray, m: int = 3) -> float:
        """è®¡ç®—æ’åˆ—ç†µ"""
        try:
            if len(data) < m:
                return 0.0
            
            # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æ’åˆ—
            from itertools import permutations
            all_perms = list(permutations(range(m)))
            perm_counts = {perm: 0 for perm in all_perms}
            
            # è®¡ç®—æ¯ä¸ªæ¨¡å¼çš„å‡ºç°æ¬¡æ•°
            for i in range(len(data) - m + 1):
                segment = data[i:i + m]
                order = tuple(np.argsort(segment))
                
                if order in perm_counts:
                    perm_counts[order] += 1
            
            # è®¡ç®—æ¦‚ç‡
            total_patterns = len(data) - m + 1
            probabilities = [count / total_patterns for count in perm_counts.values()]
            
            # è®¡ç®—ç†µ
            entropy = 0.0
            for p in probabilities:
                if p > 0:
                    entropy -= p * np.log2(p)
            
            # å½’ä¸€åŒ–
            max_entropy = np.log2(len(all_perms))
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0
            
            return normalized_entropy
            
        except Exception:
            return 0.0
    
    def _perform_recurrence_analysis(self, data: np.ndarray) -> Dict[str, float]:
        """æ‰§è¡Œé€’å½’é‡åŒ–åˆ†æ"""
        try:
            if len(data) < 20:
                return {'recurrence_rate': 0.0, 'determinism': 0.0}
            
            # åµŒå…¥å‚æ•°
            embedding_dim = 3
            embedded = self._embed_time_series(data, embedding_dim, 1)
            
            if len(embedded) < 10:
                return {'recurrence_rate': 0.0, 'determinism': 0.0}
            
            # æ„å»ºé€’å½’çŸ©é˜µ
            N = len(embedded)
            threshold = 0.1 * np.std(data)
            recurrence_matrix = np.zeros((N, N))
            
            for i in range(N):
                for j in range(N):
                    distance = np.linalg.norm(embedded[i] - embedded[j])
                    if distance < threshold:
                        recurrence_matrix[i, j] = 1
            
            # è®¡ç®—é€’å½’ç‡
            recurrence_rate = np.sum(recurrence_matrix) / (N * N)
            
            # è®¡ç®—ç¡®å®šæ€§ï¼ˆå¯¹è§’çº¿ç»“æ„ï¼‰
            diagonal_lines = []
            for diag in range(1, N):
                line_length = 0
                for i in range(N - diag):
                    if recurrence_matrix[i, i + diag] == 1:
                        line_length += 1
                    else:
                        if line_length >= 2:
                            diagonal_lines.append(line_length)
                        line_length = 0
                
                if line_length >= 2:
                    diagonal_lines.append(line_length)
            
            if diagonal_lines:
                determinism = sum(diagonal_lines) / np.sum(recurrence_matrix)
            else:
                determinism = 0.0
            
            return {
                'recurrence_rate': recurrence_rate,
                'determinism': determinism,
                'average_diagonal_length': np.mean(diagonal_lines) if diagonal_lines else 0.0
            }
            
        except Exception:
            return {'recurrence_rate': 0.0, 'determinism': 0.0}
    
    def _reconstruct_phase_space(self, data: np.ndarray, embedding_dim: int = 3, 
                               time_delay: int = 1) -> np.ndarray:
        """é‡æ„ç›¸ç©ºé—´"""
        try:
            return self._embed_time_series(data, embedding_dim, time_delay)
        except Exception:
            return np.array([[0.0] * embedding_dim])
    
    def _embed_time_series(self, data: np.ndarray, embedding_dim: int, 
                          time_delay: int) -> np.ndarray:
        """æ—¶é—´åºåˆ—åµŒå…¥"""
        try:
            if len(data) < embedding_dim * time_delay:
                return np.array([])
            
            embedded_length = len(data) - (embedding_dim - 1) * time_delay
            embedded = np.zeros((embedded_length, embedding_dim))
            
            for i in range(embedded_length):
                for j in range(embedding_dim):
                    embedded[i, j] = data[i + j * time_delay]
            
            return embedded
            
        except Exception:
            return np.array([])
    
    def _analyze_poincare_section(self, phase_space: np.ndarray) -> Dict[str, float]:
        """åˆ†æåºåŠ è±æˆªé¢"""
        try:
            if len(phase_space) == 0 or phase_space.shape[1] < 2:
                return {'section_density': 0.0, 'return_map_correlation': 0.0}
            
            # ç®€åŒ–çš„åºåŠ è±æˆªé¢ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªåæ ‡çš„é›¶äº¤å‰ï¼‰
            x_coords = phase_space[:, 0]
            
            # æ‰¾é›¶äº¤å‰ç‚¹
            zero_crossings = []
            for i in range(len(x_coords) - 1):
                if x_coords[i] * x_coords[i + 1] < 0:
                    # çº¿æ€§æ’å€¼æ‰¾ç²¾ç¡®äº¤å‰ç‚¹
                    t = -x_coords[i] / (x_coords[i + 1] - x_coords[i])
                    if phase_space.shape[1] > 1:
                        y_cross = phase_space[i, 1] + t * (phase_space[i + 1, 1] - phase_space[i, 1])
                        zero_crossings.append(y_cross)
            
            if len(zero_crossings) < 2:
                return {'section_density': 0.0, 'return_map_correlation': 0.0}
            
            # è®¡ç®—æˆªé¢å¯†åº¦
            section_density = len(zero_crossings) / len(phase_space)
            
            # è¿”å›æ˜ å°„ç›¸å…³æ€§
            if len(zero_crossings) > 2:
                return_values = zero_crossings[1:]
                prev_values = zero_crossings[:-1]
                correlation = np.corrcoef(prev_values, return_values)[0, 1]
                if np.isnan(correlation):
                    correlation = 0.0
            else:
                correlation = 0.0
            
            return {
                'section_density': section_density,
                'return_map_correlation': correlation
            }
            
        except Exception:
            return {'section_density': 0.0, 'return_map_correlation': 0.0}
    
    def _identify_chaos_indicators(self, lyapunov: float, correlation_dim: float, 
                                 entropy_measures: Dict) -> Dict[str, bool]:
        """è¯†åˆ«æ··æ²Œç‰¹å¾æŒ‡æ ‡"""
        try:
            indicators = {}
            
            # æ­£æé›…æ™®è¯ºå¤«æŒ‡æ•°è¡¨æ˜æ··æ²Œ
            indicators['positive_lyapunov'] = lyapunov > 0.01
            
            # éæ•´æ•°å…³è”ç»´æ•°è¡¨æ˜åˆ†å½¢ç»“æ„
            indicators['fractal_dimension'] = abs(correlation_dim - round(correlation_dim)) > 0.1
            
            # é«˜ç†µè¡¨æ˜å¤æ‚æ€§
            shannon_entropy = entropy_measures.get('shannon_entropy', 0.0)
            indicators['high_entropy'] = shannon_entropy > 3.0
            
            # ä½è¿‘ä¼¼ç†µè¡¨æ˜è§„å¾‹æ€§ä¸§å¤±
            approx_entropy = entropy_measures.get('approximate_entropy', 0.0)
            indicators['low_approximate_entropy'] = approx_entropy < 0.5
            
            return indicators
            
        except Exception:
            return {'positive_lyapunov': False, 'fractal_dimension': False, 
                   'high_entropy': False, 'low_approximate_entropy': False}
    
    def _calculate_chaos_reversal_score(self, lyapunov: float, hurst: float, 
                                      entropy_measures: Dict, 
                                      chaos_indicators: Dict) -> float:
        """è®¡ç®—æ··æ²Œåè½¬è¯„åˆ†"""
        try:
            score_components = []
            
            # æé›…æ™®è¯ºå¤«æŒ‡æ•°è´¡çŒ®
            if lyapunov > 0.05:
                score_components.append(min(1.0, lyapunov * 10))
            
            # èµ«æ–¯ç‰¹æŒ‡æ•°åç¦»0.5è¡¨æ˜ééšæœºè¡Œä¸º
            hurst_deviation = abs(hurst - 0.5)
            if hurst_deviation > 0.2:
                score_components.append(hurst_deviation * 2)
            
            # ç†µå€¼è´¡çŒ®
            shannon_entropy = entropy_measures.get('shannon_entropy', 0.0)
            if shannon_entropy > 4.0:
                score_components.append(min(1.0, (shannon_entropy - 3) / 3))
            
            # æ··æ²ŒæŒ‡æ ‡æ•°é‡
            chaos_count = sum(chaos_indicators.values())
            if chaos_count >= 2:
                score_components.append(chaos_count / 4.0)
            
            return np.mean(score_components) if score_components else 0.0
            
        except Exception:
            return 0.0
    
    def _estimate_attractor_dimension(self, phase_space: np.ndarray) -> float:
        """ä¼°è®¡å¸å¼•å­ç»´æ•°"""
        try:
            if len(phase_space) == 0:
                return 0.0
            
            # ä½¿ç”¨å…³è”ç»´æ•°æ–¹æ³•
            return self._calculate_correlation_dimension(phase_space[:, 0])
            
        except Exception:
            return 0.0
    
    def _estimate_predictability_horizon(self, lyapunov: float) -> float:
        """ä¼°è®¡å¯é¢„æµ‹æ€§è§†ç•Œ"""
        try:
            if lyapunov <= 0:
                return float('inf')
            
            # å¯é¢„æµ‹æ€§è§†ç•Œ â‰ˆ 1/Î»
            horizon = 1.0 / lyapunov
            return min(100.0, horizon)  # é™åˆ¶æœ€å¤§å€¼
            
        except Exception:
            return 10.0
    
    # ========== é‡å­åˆ†æé«˜çº§å®ç° ==========
    
    def _construct_quantum_state(self, data: np.ndarray) -> np.ndarray:
        """æ„å»ºé‡å­æ€"""
        try:
            if len(data) == 0:
                return np.array([1.0, 0.0, 0.0, 0.0])
            
            # å½’ä¸€åŒ–æ•°æ®åˆ°[0,1]
            normalized_data = (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-10)
            
            # æ„å»º4ç»´é‡å­æ€ï¼ˆ2ä¸ªqubitç³»ç»Ÿï¼‰
            dim = 4
            
            # ä½¿ç”¨æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§æ„å»ºæ€
            mean_val = np.mean(normalized_data)
            std_val = np.std(normalized_data)
            skew_val = stats.skew(normalized_data) if len(normalized_data) > 2 else 0.0
            kurt_val = stats.kurtosis(normalized_data) if len(normalized_data) > 3 else 0.0
            
            # æ„å»ºå¤æ•°æŒ¯å¹…
            amplitudes = np.array([
                mean_val + 1j * std_val,
                (1 - mean_val) + 1j * abs(skew_val),
                std_val + 1j * abs(kurt_val),
                (1 - std_val) + 1j * (1 - abs(skew_val))
            ])
            
            # å½’ä¸€åŒ–
            norm = np.sqrt(np.sum(np.abs(amplitudes)**2))
            if norm > 0:
                quantum_state = amplitudes / norm
            else:
                quantum_state = np.array([1.0, 0.0, 0.0, 0.0])
            
            return quantum_state
            
        except Exception:
            return np.array([1.0, 0.0, 0.0, 0.0])
    
    def _calculate_quantum_coherence(self, quantum_state: np.ndarray) -> float:
        """è®¡ç®—é‡å­ç›¸å¹²æ€§"""
        try:
            if len(quantum_state) == 0:
                return 0.0
            
            # è®¡ç®—å¯†åº¦çŸ©é˜µ
            density_matrix = np.outer(quantum_state, np.conj(quantum_state))
            
            # ç›¸å¹²æ€§åº¦é‡ï¼šéå¯¹è§’å…ƒç´ çš„æ¨¡é•¿å’Œ
            coherence = 0.0
            n = density_matrix.shape[0]
            
            for i in range(n):
                for j in range(n):
                    if i != j:
                        coherence += abs(density_matrix[i, j])
            
            # å½’ä¸€åŒ–
            max_coherence = n * (n - 1)
            if max_coherence > 0:
                coherence = coherence / max_coherence
            
            return coherence
            
        except Exception:
            return 0.0
    
    def _calculate_entanglement_entropy(self, quantum_state: np.ndarray) -> float:
        """è®¡ç®—çº ç¼ ç†µ"""
        try:
            if len(quantum_state) != 4:  # 2-qubitç³»ç»Ÿ
                return 0.0
            
            # é‡å¡‘ä¸º2x2çŸ©é˜µè¡¨ç¤º2ä¸ªqubit
            state_matrix = quantum_state.reshape(2, 2)
            
            # è®¡ç®—çº¦åŒ–å¯†åº¦çŸ©é˜µï¼ˆç¬¬ä¸€ä¸ªqubitï¼‰
            reduced_dm = np.zeros((2, 2), dtype=complex)
            for i in range(2):
                for j in range(2):
                    reduced_dm[i, j] = np.sum(state_matrix[i, :] * np.conj(state_matrix[j, :]))
            
            # è®¡ç®—æœ¬å¾å€¼
            eigenvalues = np.linalg.eigvals(reduced_dm)
            eigenvalues = np.real(eigenvalues[eigenvalues > 1e-10])
            
            # è®¡ç®—å†¯è¯ºä¾æ›¼ç†µ
            if len(eigenvalues) > 0:
                entropy = -np.sum(eigenvalues * np.log2(eigenvalues + 1e-10))
            else:
                entropy = 0.0
            
            return entropy
            
        except Exception:
            return 0.0
    
    def _calculate_quantum_fidelity(self, quantum_state: np.ndarray) -> float:
        """è®¡ç®—é‡å­ä¿çœŸåº¦"""
        try:
            if len(quantum_state) == 0:
                return 0.0
            
            # ä¸æœ€å¤§çº ç¼ æ€çš„ä¿çœŸåº¦
            bell_state = np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)])
            
            if len(quantum_state) == len(bell_state):
                fidelity = abs(np.vdot(quantum_state, bell_state))**2
            else:
                fidelity = 0.0
            
            return fidelity
            
        except Exception:
            return 0.0
    
    def _analyze_quantum_superposition(self, quantum_state: np.ndarray) -> Dict[str, float]:
        """åˆ†æé‡å­å åŠ æ€"""
        try:
            if len(quantum_state) == 0:
                return {'superposition_degree': 0.0, 'phase_distribution': 0.0}
            
            # å åŠ åº¦ï¼šæŒ¯å¹…åˆ†å¸ƒçš„å‡åŒ€æ€§
            amplitudes = np.abs(quantum_state)
            if np.sum(amplitudes) > 0:
                normalized_amps = amplitudes / np.sum(amplitudes)
                superposition_degree = 1.0 - np.max(normalized_amps)
            else:
                superposition_degree = 0.0
            
            # ç›¸ä½åˆ†å¸ƒ
            phases = np.angle(quantum_state)
            phase_variance = np.var(phases)
            phase_distribution = min(1.0, phase_variance / (np.pi**2))
            
            return {
                'superposition_degree': superposition_degree,
                'phase_distribution': phase_distribution
            }
            
        except Exception:
            return {'superposition_degree': 0.0, 'phase_distribution': 0.0}
    
    def _calculate_measurement_probabilities(self, quantum_state: np.ndarray) -> np.ndarray:
        """è®¡ç®—æµ‹é‡æ¦‚ç‡"""
        try:
            if len(quantum_state) == 0:
                return np.array([])
            
            probabilities = np.abs(quantum_state)**2
            return probabilities
            
        except Exception:
            return np.array([])
    
    def _analyze_quantum_interference(self, data: np.ndarray) -> Dict[str, float]:
        """åˆ†æé‡å­å¹²æ¶‰æ•ˆåº”"""
        try:
            if len(data) < 4:
                return {'interference_pattern': 0.0, 'visibility': 0.0}
            
            # ä½¿ç”¨FFTæ£€æµ‹å¹²æ¶‰æ¨¡å¼
            fft_data = np.fft.fft(data)
            power_spectrum = np.abs(fft_data)**2
            
            # å¹²æ¶‰å›¾æ ·ï¼šå³°å€¼çš„è§„å¾‹æ€§
            peaks, _ = find_peaks(power_spectrum)
            
            if len(peaks) > 2:
                # è®¡ç®—å³°å€¼é—´è·çš„è§„å¾‹æ€§
                peak_intervals = np.diff(peaks)
                interval_variance = np.var(peak_intervals)
                max_interval = np.max(peak_intervals) if len(peak_intervals) > 0 else 1
                
                interference_pattern = 1.0 - (interval_variance / (max_interval + 1e-10))
                
                # å¯è§åº¦ï¼šæœ€å¤§æœ€å°å€¼çš„å¯¹æ¯”åº¦
                max_power = np.max(power_spectrum)
                min_power = np.min(power_spectrum)
                
                if max_power + min_power > 0:
                    visibility = (max_power - min_power) / (max_power + min_power)
                else:
                    visibility = 0.0
            else:
                interference_pattern = 0.0
                visibility = 0.0
            
            return {
                'interference_pattern': min(1.0, interference_pattern),
                'visibility': min(1.0, visibility)
            }
            
        except Exception:
            return {'interference_pattern': 0.0, 'visibility': 0.0}
    
    def _analyze_quantum_decoherence(self, quantum_state: np.ndarray) -> Dict[str, float]:
        """åˆ†æé‡å­é€€ç›¸å¹²"""
        try:
            if len(quantum_state) == 0:
                return {'decoherence_rate': 0.0, 'coherence_time': 0.0}
            
            # ç›¸å¹²æ€§æµ‹é‡
            coherence = self._calculate_quantum_coherence(quantum_state)
            
            # çº¯åº¦æµ‹é‡
            density_matrix = np.outer(quantum_state, np.conj(quantum_state))
            purity = np.real(np.trace(np.dot(density_matrix, density_matrix)))
            
            # é€€ç›¸å¹²ç‡ï¼ˆç®€åŒ–æ¨¡å‹ï¼‰
            decoherence_rate = 1.0 - purity
            
            # ç›¸å¹²æ—¶é—´ä¼°è®¡
            if decoherence_rate > 0:
                coherence_time = 1.0 / decoherence_rate
            else:
                coherence_time = float('inf')
            
            return {
                'decoherence_rate': decoherence_rate,
                'coherence_time': min(100.0, coherence_time),
                'purity': purity
            }
            
        except Exception:
            return {'decoherence_rate': 0.0, 'coherence_time': 0.0}
    
    def _assess_quantum_error_correction(self, quantum_state: np.ndarray) -> float:
        """è¯„ä¼°é‡å­çº é”™èƒ½åŠ›"""
        try:
            if len(quantum_state) == 0:
                return 0.0
            
            # åŸºäºçº ç¼ ç†µçš„çº é”™èƒ½åŠ›è¯„ä¼°
            entanglement_entropy = self._calculate_entanglement_entropy(quantum_state)
            
            # é«˜çº ç¼ ç†µè¡¨æ˜æ›´å¥½çš„çº é”™èƒ½åŠ›
            correction_capacity = entanglement_entropy / np.log2(len(quantum_state))
            
            return min(1.0, correction_capacity)
            
        except Exception:
            return 0.0
    
    def _calculate_quantum_reversal_probability(self, coherence: float, 
                                              entanglement_entropy: float,
                                              superposition_analysis: Dict) -> float:
        """è®¡ç®—é‡å­åè½¬æ¦‚ç‡"""
        try:
            probability_factors = []
            
            # ç›¸å¹²æ€§å´©å¡Œ
            if coherence < 0.3:
                probability_factors.append(1.0 - coherence)
            
            # çº ç¼ æ€å´©å¡Œ
            if entanglement_entropy < 0.5:
                probability_factors.append(1.0 - entanglement_entropy)
            
            # å åŠ æ€ä¸ç¨³å®š
            superposition_degree = superposition_analysis.get('superposition_degree', 0.0)
            if superposition_degree > 0.8:
                probability_factors.append(superposition_degree)
            
            # ç›¸ä½æ··ä¹±
            phase_distribution = superposition_analysis.get('phase_distribution', 0.0)
            if phase_distribution > 0.7:
                probability_factors.append(phase_distribution)
            
            return np.mean(probability_factors) if probability_factors else 0.0
            
        except Exception:
            return 0.0
    
    def _calculate_quantum_purity(self, quantum_state: np.ndarray) -> float:
        """è®¡ç®—é‡å­æ€çº¯åº¦"""
        try:
            if len(quantum_state) == 0:
                return 0.0
            
            density_matrix = np.outer(quantum_state, np.conj(quantum_state))
            purity = np.real(np.trace(np.dot(density_matrix, density_matrix)))
            
            return purity
            
        except Exception:
            return 0.0
    
    def _assess_quantum_advantage(self, data: np.ndarray) -> float:
        """è¯„ä¼°é‡å­ä¼˜åŠ¿"""
        try:
            if len(data) < 4:
                return 0.0
            
            # é‡å­ä¼˜åŠ¿åŸºäºéç»å…¸ç›¸å…³æ€§
            quantum_state = self._construct_quantum_state(data)
            
            # è®¡ç®—è´å°”ä¸ç­‰å¼è¿åç¨‹åº¦
            bell_violation = self._calculate_bell_inequality_violation(quantum_state)
            
            # é‡å­ä¼˜åŠ¿è¯„åˆ†
            advantage = min(1.0, bell_violation / 2.0)
            
            return advantage
            
        except Exception:
            return 0.0
    
    def _calculate_bell_state_similarity(self, quantum_state: np.ndarray) -> float:
        """è®¡ç®—ä¸è´å°”æ€çš„ç›¸ä¼¼æ€§"""
        try:
            if len(quantum_state) != 4:
                return 0.0
            
            # å››ä¸ªè´å°”æ€
            bell_states = [
                np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)]),
                np.array([1/np.sqrt(2), 0, 0, -1/np.sqrt(2)]),
                np.array([0, 1/np.sqrt(2), 1/np.sqrt(2), 0]),
                np.array([0, 1/np.sqrt(2), -1/np.sqrt(2), 0])
            ]
            
            # è®¡ç®—ä¸æ¯ä¸ªè´å°”æ€çš„ä¿çœŸåº¦
            fidelities = []
            for bell_state in bell_states:
                fidelity = abs(np.vdot(quantum_state, bell_state))**2
                fidelities.append(fidelity)
            
            # è¿”å›æœ€å¤§ç›¸ä¼¼æ€§
            return max(fidelities)
            
        except Exception:
            return 0.0
    
    def _calculate_bell_inequality_violation(self, quantum_state: np.ndarray) -> float:
        """è®¡ç®—è´å°”ä¸ç­‰å¼è¿åç¨‹åº¦"""
        try:
            if len(quantum_state) != 4:
                return 0.0
            
            # CHSHä¸ç­‰å¼çš„é‡å­è¿å
            # ç®€åŒ–è®¡ç®—
            entanglement_entropy = self._calculate_entanglement_entropy(quantum_state)
            
            # åŸºäºçº ç¼ ç†µä¼°è®¡è´å°”è¿å
            max_violation = 2 * np.sqrt(2) - 2  # é‡å­åŠ›å­¦æœ€å¤§è¿åå€¼å‡å»ç»å…¸ç•Œé™
            violation = entanglement_entropy * max_violation
            
            return violation
            
        except Exception:
            return 0.0
        
# ========== æœºå™¨å­¦ä¹ å…·ä½“å®ç° ==========
    
    def _prepare_ml_training_data(self, features: np.ndarray, target_data: np.ndarray, 
                                window_size: int = 5) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®"""
        try:
            if len(target_data) < window_size + 1:
                return np.array([]), np.array([])
            
            X = []
            y = []
            
            # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ›å»ºè®­ç»ƒæ ·æœ¬
            for i in range(len(target_data) - window_size):
                # ç‰¹å¾ï¼šçª—å£å†…çš„å†å²æ•°æ®
                window_features = target_data[i:i + window_size].tolist()
                
                # å¦‚æœæœ‰é¢å¤–ç‰¹å¾ï¼Œæ·»åŠ è¿›å»
                if len(features) > 0:
                    window_features.extend(features.tolist() if hasattr(features, 'tolist') else [features])
                
                X.append(window_features)
                # æ ‡ç­¾ï¼šä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„å€¼
                y.append(target_data[i + window_size])
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæ•°æ®å‡†å¤‡é”™è¯¯: {str(e)}")
            return np.array([]), np.array([])
    
    def _cross_validate_model(self, model, X: np.ndarray, y: np.ndarray, cv_folds: int = 5) -> np.ndarray:
        """äº¤å‰éªŒè¯æ¨¡å‹"""
        try:
            if len(X) == 0 or len(y) == 0:
                return np.array([0.0])
            
            from sklearn.model_selection import cross_val_score
            
            # ç¡®ä¿æ•°æ®ç»´åº¦æ­£ç¡®
            if X.ndim == 1:
                X = X.reshape(-1, 1)
            
            scores = cross_val_score(model, X, y, cv=min(cv_folds, len(X)), 
                                   scoring='neg_mean_squared_error')
            
            return -scores  # è½¬æ¢ä¸ºæ­£å€¼ï¼ˆMSEï¼‰
            
        except Exception as e:
            self.logger.error(f"äº¤å‰éªŒè¯é”™è¯¯: {str(e)}")
            return np.array([0.0])
    
    def _ensemble_predictions(self, predictions: Dict[str, float], 
                            performances: Dict[str, Dict]) -> float:
        """é›†æˆé¢„æµ‹ç»“æœ"""
        try:
            if not predictions:
                return 0.0
            
            # åŸºäºæ€§èƒ½çš„æƒé‡
            weights = {}
            total_weight = 0
            
            for model_name, pred in predictions.items():
                if model_name in performances:
                    # ä½¿ç”¨äº¤å‰éªŒè¯å¾—åˆ†ä½œä¸ºæƒé‡ï¼ˆåˆ†æ•°è¶Šé«˜æƒé‡è¶Šå¤§ï¼‰
                    cv_score = performances[model_name].get('cv_mean', 0.1)
                    weight = 1.0 / (cv_score + 1e-6)  # é”™è¯¯è¶Šå°æƒé‡è¶Šå¤§
                    weights[model_name] = weight
                    total_weight += weight
                else:
                    weights[model_name] = 1.0
                    total_weight += 1.0
            
            # åŠ æƒå¹³å‡
            weighted_sum = 0
            for model_name, pred in predictions.items():
                weight = weights[model_name] / total_weight
                weighted_sum += pred * weight
            
            return weighted_sum
            
        except Exception:
            return np.mean(list(predictions.values())) if predictions else 0.0
    
    def _analyze_feature_importance(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        try:
            if len(X) == 0 or len(y) == 0:
                return {}
            
            # ä½¿ç”¨éšæœºæ£®æ—åˆ†æç‰¹å¾é‡è¦æ€§
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            rf.fit(X, y)
            
            importance_dict = {}
            for i, importance in enumerate(rf.feature_importances_):
                importance_dict[f'feature_{i}'] = importance
            
            return importance_dict
            
        except Exception:
            return {}
    
    def _calculate_ensemble_confidence(self, performances: Dict[str, Dict]) -> float:
        """è®¡ç®—é›†æˆæ¨¡å‹ç½®ä¿¡åº¦"""
        try:
            if not performances:
                return 0.0
            
            confidence_factors = []
            
            # å¹³å‡äº¤å‰éªŒè¯å¾—åˆ†
            cv_scores = [perf.get('cv_mean', 0.0) for perf in performances.values()]
            if cv_scores:
                avg_cv_score = np.mean(cv_scores)
                confidence_factors.append(1.0 / (avg_cv_score + 1e-6))
            
            # æ¨¡å‹ä¸€è‡´æ€§
            predictions = [perf.get('prediction', 0.0) for perf in performances.values()]
            if len(predictions) > 1:
                prediction_std = np.std(predictions)
                consistency = 1.0 / (prediction_std + 1e-6)
                confidence_factors.append(min(1.0, consistency / 10))
            
            # æ¨¡å‹æ•°é‡
            model_count_factor = min(1.0, len(performances) / 10.0)
            confidence_factors.append(model_count_factor)
            
            return np.mean(confidence_factors) if confidence_factors else 0.0
            
        except Exception:
            return 0.0
    
    def _quantify_prediction_uncertainty_ml(self, predictions: Dict[str, float]) -> Dict[str, float]:
        """é‡åŒ–æœºå™¨å­¦ä¹ é¢„æµ‹ä¸ç¡®å®šæ€§"""
        try:
            if not predictions:
                return {'total_uncertainty': 1.0}
            
            pred_values = list(predictions.values())
            
            # é¢„æµ‹æ–¹å·®
            prediction_variance = np.var(pred_values)
            
            # é¢„æµ‹èŒƒå›´
            prediction_range = np.max(pred_values) - np.min(pred_values)
            
            # æ ‡å‡†åŒ–ä¸ç¡®å®šæ€§
            normalized_uncertainty = min(1.0, prediction_variance + prediction_range / 4.0)
            
            return {
                'prediction_variance': prediction_variance,
                'prediction_range': prediction_range,
                'total_uncertainty': normalized_uncertainty
            }
            
        except Exception:
            return {'total_uncertainty': 1.0}
    
    def _calculate_model_agreement(self, predictions: Dict[str, float]) -> float:
        """è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§"""
        try:
            if len(predictions) < 2:
                return 1.0
            
            pred_values = list(predictions.values())
            
            # è®¡ç®—æ‰€æœ‰æ¨¡å‹é¢„æµ‹çš„æ ‡å‡†å·®
            std_dev = np.std(pred_values)
            mean_pred = np.mean(pred_values)
            
            # ä¸€è‡´æ€§ = 1 - æ ‡å‡†å·®/å‡å€¼
            if mean_pred != 0:
                agreement = 1.0 - min(1.0, abs(std_dev / mean_pred))
            else:
                agreement = 1.0 - min(1.0, std_dev)
            
            return max(0.0, agreement)
            
        except Exception:
            return 0.0
    
    def _assess_prediction_stability(self, predictions: Dict[str, float]) -> float:
        """è¯„ä¼°é¢„æµ‹ç¨³å®šæ€§"""
        try:
            if not predictions:
                return 0.0
            
            pred_values = list(predictions.values())
            
            # ç¨³å®šæ€§åŸºäºé¢„æµ‹å€¼çš„ç¦»æ•£ç¨‹åº¦
            if len(pred_values) == 1:
                return 1.0
            
            # ä½¿ç”¨å˜å¼‚ç³»æ•°
            mean_val = np.mean(pred_values)
            std_val = np.std(pred_values)
            
            if mean_val != 0:
                cv = std_val / abs(mean_val)
                stability = 1.0 / (1.0 + cv)
            else:
                stability = 1.0 / (1.0 + std_val)
            
            return stability
            
        except Exception:
            return 0.0
    
    # ========== æ·±åº¦å­¦ä¹ å…·ä½“å®ç° ==========
    
    def _prepare_sequence_data(self, data: np.ndarray, sequence_length: int) -> np.ndarray:
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        try:
            if len(data) < sequence_length:
                return np.array([])
            
            sequences = []
            for i in range(len(data) - sequence_length + 1):
                seq = data[i:i + sequence_length]
                sequences.append(seq)
            
            return np.array(sequences)
            
        except Exception:
            return np.array([])
    
    def _pytorch_ensemble_predict(self, sequence_data: np.ndarray, 
                                 target_data: np.ndarray) -> Dict[str, float]:
        """PyTorché›†æˆé¢„æµ‹"""
        try:
            if not TORCH_AVAILABLE or len(sequence_data) == 0:
                return {}
            
            predictions = {}
            
            # å‡†å¤‡æ•°æ®
            if len(sequence_data.shape) == 1:
                sequence_data = sequence_data.reshape(1, -1)
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
            for model_name, model in self.pytorch_models.items():
                try:
                    model.eval()
                    
                    # æ•°æ®é¢„å¤„ç†
                    input_size = 20  # å‡è®¾è¾“å…¥ç‰¹å¾æ•°
                    if sequence_data.shape[-1] != input_size:
                        # è°ƒæ•´è¾“å…¥å¤§å°
                        if sequence_data.shape[-1] > input_size:
                            seq_input = sequence_data[:, :input_size]
                        else:
                            # å¡«å……åˆ°æ‰€éœ€å¤§å°
                            padding = np.zeros((sequence_data.shape[0], 
                                              input_size - sequence_data.shape[-1]))
                            seq_input = np.hstack([sequence_data, padding])
                    else:
                        seq_input = sequence_data
                    
                    # è½¬æ¢ä¸ºtensor
                    input_tensor = torch.FloatTensor(seq_input)
                    
                    # é¢„æµ‹
                    with torch.no_grad():
                        output = model(input_tensor)
                        prediction = output.item() if output.numel() == 1 else output.mean().item()
                    
                    predictions[f'pytorch_{model_name}'] = prediction
                    
                except Exception as e:
                    self.logger.warning(f"PyTorchæ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {str(e)}")
                    continue
            
            return predictions
            
        except Exception as e:
            self.logger.error(f"PyTorché›†æˆé¢„æµ‹é”™è¯¯: {str(e)}")
            return {}
    
    def _tensorflow_ensemble_predict(self, sequence_data: np.ndarray, 
                                   target_data: np.ndarray) -> Dict[str, float]:
        """TensorFlowé›†æˆé¢„æµ‹"""
        try:
            if not TF_AVAILABLE or len(sequence_data) == 0:
                return {}
            
            predictions = {}
            
            # å‡†å¤‡æ•°æ®
            if len(sequence_data.shape) == 1:
                sequence_data = sequence_data.reshape(1, 1, -1)
            elif len(sequence_data.shape) == 2:
                sequence_data = sequence_data.reshape(1, sequence_data.shape[0], sequence_data.shape[1])
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
            for model_name, model in self.tensorflow_models.items():
                try:
                    # è°ƒæ•´è¾“å…¥å½¢çŠ¶ä»¥åŒ¹é…æ¨¡å‹æœŸæœ›
                    expected_shape = model.input_shape
                    
                    if len(expected_shape) == 3:  # (batch, timesteps, features)
                        timesteps = expected_shape[1]
                        features = expected_shape[2]
                        
                        # è°ƒæ•´æ—¶é—´æ­¥é•¿
                        if sequence_data.shape[1] != timesteps:
                            if sequence_data.shape[1] > timesteps:
                                seq_input = sequence_data[:, :timesteps, :]
                            else:
                                # é‡å¤æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                                padding_steps = timesteps - sequence_data.shape[1]
                                last_step = sequence_data[:, -1:, :]
                                padding = np.repeat(last_step, padding_steps, axis=1)
                                seq_input = np.concatenate([sequence_data, padding], axis=1)
                        else:
                            seq_input = sequence_data
                        
                        # è°ƒæ•´ç‰¹å¾æ•°
                        if seq_input.shape[2] != features:
                            if seq_input.shape[2] > features:
                                seq_input = seq_input[:, :, :features]
                            else:
                                # å¡«å……ç‰¹å¾
                                padding_features = features - seq_input.shape[2]
                                padding = np.zeros((seq_input.shape[0], seq_input.shape[1], padding_features))
                                seq_input = np.concatenate([seq_input, padding], axis=2)
                    else:
                        seq_input = sequence_data
                    
                    # é¢„æµ‹
                    prediction = model.predict(seq_input, verbose=0)
                    pred_value = prediction.item() if prediction.size == 1 else np.mean(prediction)
                    
                    predictions[f'tensorflow_{model_name}'] = pred_value
                    
                except Exception as e:
                    self.logger.warning(f"TensorFlowæ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥: {str(e)}")
                    continue
            
            return predictions
            
        except Exception as e:
            self.logger.error(f"TensorFlowé›†æˆé¢„æµ‹é”™è¯¯: {str(e)}")
            return {}
    
    def _analyze_attention_weights(self, sequence_data: np.ndarray) -> Dict[str, Any]:
        """åˆ†ææ³¨æ„åŠ›æƒé‡"""
        try:
            if len(sequence_data) == 0:
                return {'attention_distribution': [], 'focus_indices': []}
            
            # ç®€åŒ–çš„æ³¨æ„åŠ›åˆ†æ
            # åŸºäºæ•°æ®å˜åŒ–ç‡è®¡ç®—æ³¨æ„åŠ›
            if len(sequence_data.shape) == 1:
                changes = np.abs(np.diff(sequence_data))
            else:
                changes = np.mean(np.abs(np.diff(sequence_data, axis=0)), axis=1)
            
            if len(changes) > 0:
                # å½’ä¸€åŒ–ä¸ºæ³¨æ„åŠ›æƒé‡
                attention_weights = changes / (np.sum(changes) + 1e-10)
                
                # æ‰¾å‡ºæ³¨æ„åŠ›é›†ä¸­çš„ä½ç½®
                focus_threshold = np.mean(attention_weights) + np.std(attention_weights)
                focus_indices = np.where(attention_weights > focus_threshold)[0].tolist()
            else:
                attention_weights = []
                focus_indices = []
            
            return {
                'attention_distribution': attention_weights.tolist() if len(attention_weights) > 0 else [],
                'focus_indices': focus_indices,
                'attention_entropy': self._calculate_attention_entropy(attention_weights),
                'focus_concentration': len(focus_indices) / max(1, len(attention_weights))
            }
            
        except Exception:
            return {'attention_distribution': [], 'focus_indices': []}
    
    def _calculate_attention_entropy(self, attention_weights: np.ndarray) -> float:
        """è®¡ç®—æ³¨æ„åŠ›ç†µ"""
        try:
            if len(attention_weights) == 0:
                return 0.0
            
            # ç¡®ä¿æƒé‡å’Œä¸º1
            weights = attention_weights / (np.sum(attention_weights) + 1e-10)
            weights = weights[weights > 0]  # ç§»é™¤é›¶æƒé‡
            
            if len(weights) == 0:
                return 0.0
            
            entropy = -np.sum(weights * np.log2(weights))
            
            return entropy
            
        except Exception:
            return 0.0
    
    def _calculate_dl_confidence(self, predictions: Dict[str, float]) -> float:
        """è®¡ç®—æ·±åº¦å­¦ä¹ ç½®ä¿¡åº¦"""
        try:
            if not predictions:
                return 0.0
            
            # åŸºäºé¢„æµ‹ä¸€è‡´æ€§çš„ç½®ä¿¡åº¦
            pred_values = list(predictions.values())
            
            if len(pred_values) == 1:
                return 0.7  # å•æ¨¡å‹ä¸­ç­‰ç½®ä¿¡åº¦
            
            # è®¡ç®—é¢„æµ‹çš„ä¸€è‡´æ€§
            mean_pred = np.mean(pred_values)
            std_pred = np.std(pred_values)
            
            # ä¸€è‡´æ€§è¶Šé«˜ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            if mean_pred != 0:
                consistency = 1.0 - min(1.0, std_pred / abs(mean_pred))
            else:
                consistency = 1.0 - min(1.0, std_pred)
            
            # è€ƒè™‘æ¨¡å‹æ•°é‡çš„å½±å“
            model_count_factor = min(1.0, len(predictions) / 5.0)
            
            confidence = consistency * 0.7 + model_count_factor * 0.3
            
            return max(0.0, min(1.0, confidence))
            
        except Exception:
            return 0.0
    
    def _analyze_dl_interpretability(self, sequence_data: np.ndarray, 
                                   predictions: Dict[str, float]) -> Dict[str, Any]:
        """åˆ†ææ·±åº¦å­¦ä¹ æ¨¡å‹å¯è§£é‡Šæ€§"""
        try:
            if len(sequence_data) == 0:
                return {'feature_attribution': {}, 'model_explanations': []}
            
            # ç®€åŒ–çš„ç‰¹å¾å½’å› åˆ†æ
            feature_attribution = {}
            
            # åŸºäºè¾“å…¥æ•°æ®çš„å˜åŒ–åˆ†æç‰¹å¾é‡è¦æ€§
            if len(sequence_data.shape) == 1:
                data_variance = np.var(sequence_data)
                for i, value in enumerate(sequence_data):
                    feature_attribution[f'timestep_{i}'] = abs(value) / (data_variance + 1e-10)
            else:
                for i in range(sequence_data.shape[-1]):
                    feature_variance = np.var(sequence_data[:, i])
                    feature_mean = np.mean(np.abs(sequence_data[:, i]))
                    feature_attribution[f'feature_{i}'] = feature_mean / (feature_variance + 1e-10)
            
            # æ¨¡å‹è§£é‡Š
            model_explanations = []
            for model_name, prediction in predictions.items():
                explanation = {
                    'model': model_name,
                    'prediction': prediction,
                    'confidence': abs(prediction),  # ç®€åŒ–çš„ç½®ä¿¡åº¦
                    'complexity': 'high' if 'lstm' in model_name or 'transformer' in model_name else 'medium'
                }
                model_explanations.append(explanation)
            
            return {
                'feature_attribution': feature_attribution,
                'model_explanations': model_explanations,
                'interpretability_score': self._calculate_interpretability_score(feature_attribution)
            }
            
        except Exception:
            return {'feature_attribution': {}, 'model_explanations': []}
    
    def _calculate_interpretability_score(self, feature_attribution: Dict[str, float]) -> float:
        """è®¡ç®—å¯è§£é‡Šæ€§è¯„åˆ†"""
        try:
            if not feature_attribution:
                return 0.0
            
            attributions = list(feature_attribution.values())
            
            # ç‰¹å¾å½’å› çš„é›†ä¸­åº¦
            max_attribution = max(attributions)
            mean_attribution = np.mean(attributions)
            
            if mean_attribution > 0:
                concentration = max_attribution / mean_attribution
                # é›†ä¸­åº¦è¶Šé«˜ï¼Œè¶Šå®¹æ˜“è§£é‡Š
                interpretability = min(1.0, concentration / 5.0)
            else:
                interpretability = 0.0
            
            return interpretability
            
        except Exception:
            return 0.0
    
    def _assess_model_complexity(self) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹å¤æ‚åº¦"""
        try:
            complexity_info = {
                'total_models': len(self.ml_models) + len(self.pytorch_models) + len(self.tensorflow_models),
                'ml_models': len(self.ml_models),
                'pytorch_models': len(self.pytorch_models),
                'tensorflow_models': len(self.tensorflow_models)
            }
            
            # è®¡ç®—å‚æ•°æ€»æ•°ï¼ˆç®€åŒ–ä¼°è®¡ï¼‰
            total_parameters = 0
            
            # PyTorchæ¨¡å‹å‚æ•°
            if TORCH_AVAILABLE:
                for model in self.pytorch_models.values():
                    try:
                        params = sum(p.numel() for p in model.parameters())
                        total_parameters += params
                    except:
                        total_parameters += 10000  # ä¼°è®¡å€¼
            
            # TensorFlowæ¨¡å‹å‚æ•°
            if TF_AVAILABLE:
                for model in self.tensorflow_models.values():
                    try:
                        params = model.count_params()
                        total_parameters += params
                    except:
                        total_parameters += 10000  # ä¼°è®¡å€¼
            
            complexity_info['total_parameters'] = total_parameters
            complexity_info['complexity_level'] = (
                'low' if total_parameters < 10000 else
                'medium' if total_parameters < 100000 else 'high'
            )
            
            return complexity_info
            
        except Exception:
            return {'total_models': 0, 'complexity_level': 'unknown'}
    
    def _extract_gradient_information(self, sequence_data: np.ndarray) -> Dict[str, Any]:
        """æå–æ¢¯åº¦ä¿¡æ¯"""
        try:
            if len(sequence_data) == 0:
                return {'gradient_norm': 0.0, 'gradient_direction': []}
            
            # è®¡ç®—æ•°å€¼æ¢¯åº¦
            if len(sequence_data.shape) == 1:
                gradients = np.gradient(sequence_data)
            else:
                gradients = np.gradient(sequence_data, axis=0)
            
            # æ¢¯åº¦èŒƒæ•°
            if gradients.ndim == 1:
                gradient_norm = np.linalg.norm(gradients)
                gradient_direction = gradients.tolist()
            else:
                gradient_norm = np.mean([np.linalg.norm(grad) for grad in gradients])
                gradient_direction = np.mean(gradients, axis=0).tolist()
            
            return {
                'gradient_norm': gradient_norm,
                'gradient_direction': gradient_direction,
                'gradient_stability': 1.0 / (np.std(gradients) + 1e-10) if gradients.size > 1 else 1.0
            }
            
        except Exception:
            return {'gradient_norm': 0.0, 'gradient_direction': []}
        

# ========== å¸‚åœºå¾®è§‚ç»“æ„åˆ†æå…·ä½“å®ç° ==========
    
    def _simulate_bid_ask_spread(self, data: np.ndarray) -> Dict[str, float]:
        """æ¨¡æ‹Ÿä¹°å–ä»·å·®"""
        try:
            if len(data) == 0:
                return {'spread': 0.0, 'relative_spread': 0.0}
            
            # åŸºäºæ•°æ®æ³¢åŠ¨æ€§æ¨¡æ‹Ÿä»·å·®
            volatility = np.std(data)
            mean_price = np.mean(data)
            
            # ä»·å·®é€šå¸¸ä¸æ³¢åŠ¨æ€§æˆæ­£æ¯”
            absolute_spread = volatility * 0.1  # ç®€åŒ–æ¨¡å‹
            relative_spread = absolute_spread / (mean_price + 1e-10)
            
            # ä»·å·®çš„æ—¶é—´åºåˆ—
            spread_series = []
            for i in range(len(data)):
                window_start = max(0, i - 5)
                window_data = data[window_start:i + 1]
                window_vol = np.std(window_data) if len(window_data) > 1 else volatility
                spread_series.append(window_vol * 0.1)
            
            return {
                'spread': absolute_spread,
                'relative_spread': relative_spread,
                'spread_volatility': np.std(spread_series) if len(spread_series) > 1 else 0.0,
                'spread_series': spread_series
            }
            
        except Exception:
            return {'spread': 0.0, 'relative_spread': 0.0}
    
    def _analyze_market_depth(self, tail_data: np.ndarray, 
                            all_data: np.ndarray) -> Dict[str, float]:
        """åˆ†æå¸‚åœºæ·±åº¦"""
        try:
            if len(tail_data) == 0:
                return {'depth_score': 0.0, 'liquidity_ratio': 0.0}
            
            # åŸºäºå°¾æ•°å‡ºç°é¢‘ç‡å’Œæ€»ä½“æ´»è·ƒåº¦åˆ†ææ·±åº¦
            tail_frequency = np.mean(tail_data)
            total_activity = np.mean(np.sum(all_data, axis=1)) if all_data.ndim > 1 else np.mean(all_data)
            
            # æ·±åº¦è¯„åˆ†ï¼šåŸºäºç›¸å¯¹æ´»è·ƒåº¦
            if total_activity > 0:
                relative_activity = tail_frequency / total_activity
                depth_score = min(1.0, relative_activity * 10)
            else:
                depth_score = 0.0
            
            # æµåŠ¨æ€§æ¯”ç‡ï¼šåŸºäºæ•°æ®å˜åŒ–çš„å¹³æ»‘ç¨‹åº¦
            if len(tail_data) > 1:
                changes = np.abs(np.diff(tail_data))
                avg_change = np.mean(changes)
                liquidity_ratio = 1.0 / (avg_change + 1e-10)
                liquidity_ratio = min(1.0, liquidity_ratio / 10)
            else:
                liquidity_ratio = 0.5
            
            return {
                'depth_score': depth_score,
                'liquidity_ratio': liquidity_ratio,
                'relative_activity': relative_activity if total_activity > 0 else 0.0,
                'activity_stability': 1.0 - np.std(tail_data) if len(tail_data) > 1 else 1.0
            }
            
        except Exception:
            return {'depth_score': 0.0, 'liquidity_ratio': 0.0}
    
    def _calculate_order_flow_imbalance(self, data: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—è®¢å•æµä¸å¹³è¡¡"""
        try:
            if len(data) < 2:
                return {'imbalance': 0.0, 'imbalance_trend': 0.0}
            
            # æ¨¡æ‹Ÿä¹°å–è®¢å•æµ
            changes = np.diff(data)
            
            # æ­£å˜åŒ–è§†ä¸ºä¹°å•ï¼Œè´Ÿå˜åŒ–è§†ä¸ºå–å•
            buy_flow = np.sum(changes[changes > 0])
            sell_flow = np.sum(np.abs(changes[changes < 0]))
            
            total_flow = buy_flow + sell_flow
            
            if total_flow > 0:
                imbalance = (buy_flow - sell_flow) / total_flow
            else:
                imbalance = 0.0
            
            # ä¸å¹³è¡¡è¶‹åŠ¿
            if len(changes) >= 5:
                recent_imbalance = self._calculate_recent_imbalance(changes[-5:])
                overall_imbalance = self._calculate_recent_imbalance(changes)
                imbalance_trend = recent_imbalance - overall_imbalance
            else:
                imbalance_trend = 0.0
            
            return {
                'imbalance': imbalance,
                'imbalance_trend': imbalance_trend,
                'buy_pressure': buy_flow / (total_flow + 1e-10),
                'sell_pressure': sell_flow / (total_flow + 1e-10)
            }
            
        except Exception:
            return {'imbalance': 0.0, 'imbalance_trend': 0.0}
    
    def _calculate_recent_imbalance(self, changes: np.ndarray) -> float:
        """è®¡ç®—æœ€è¿‘çš„ä¸å¹³è¡¡"""
        try:
            buy_flow = np.sum(changes[changes > 0])
            sell_flow = np.sum(np.abs(changes[changes < 0]))
            total_flow = buy_flow + sell_flow
            
            if total_flow > 0:
                return (buy_flow - sell_flow) / total_flow
            else:
                return 0.0
                
        except Exception:
            return 0.0
    
    def _analyze_trade_intensity(self, data: np.ndarray) -> Dict[str, float]:
        """åˆ†æäº¤æ˜“å¼ºåº¦"""
        try:
            if len(data) == 0:
                return {'intensity': 0.0, 'intensity_trend': 0.0}
            
            # äº¤æ˜“å¼ºåº¦åŸºäºæ•°æ®å˜åŒ–çš„é¢‘ç‡å’Œå¹…åº¦
            if len(data) > 1:
                changes = np.abs(np.diff(data))
                
                # å¼ºåº¦åº¦é‡
                intensity = np.mean(changes) + np.std(changes)
                
                # å¼ºåº¦è¶‹åŠ¿
                if len(changes) >= 10:
                    recent_intensity = np.mean(changes[-5:]) + np.std(changes[-5:])
                    historical_intensity = np.mean(changes[:-5]) + np.std(changes[:-5])
                    intensity_trend = recent_intensity - historical_intensity
                else:
                    intensity_trend = 0.0
                
                # å¼ºåº¦åˆ†å¸ƒåˆ†æ
                intensity_percentiles = np.percentile(changes, [25, 50, 75])
                
            else:
                intensity = np.abs(data[0])
                intensity_trend = 0.0
                intensity_percentiles = [intensity, intensity, intensity]
            
            return {
                'intensity': intensity,
                'intensity_trend': intensity_trend,
                'intensity_volatility': np.std(changes) if len(data) > 1 else 0.0,
                'intensity_percentiles': {
                    'q25': intensity_percentiles[0],
                    'q50': intensity_percentiles[1], 
                    'q75': intensity_percentiles[2]
                }
            }
            
        except Exception:
            return {'intensity': 0.0, 'intensity_trend': 0.0}
    
    def _analyze_price_impact(self, data: np.ndarray) -> Dict[str, float]:
        """åˆ†æä»·æ ¼å½±å“"""
        try:
            if len(data) < 3:
                return {'temporary_impact': 0.0, 'permanent_impact': 0.0}
            
            # ä»·æ ¼å½±å“åˆ†æ
            changes = np.diff(data)
            
            # ä¸´æ—¶å½±å“ï¼šçŸ­æœŸä»·æ ¼åå¼¹
            temporary_impacts = []
            for i in range(len(changes) - 1):
                if changes[i] != 0:
                    # æ£€æŸ¥ä¸‹ä¸€æœŸæ˜¯å¦æœ‰åå‘å˜åŒ–
                    if changes[i] * changes[i + 1] < 0:
                        impact = abs(changes[i + 1] / changes[i])
                        temporary_impacts.append(impact)
            
            temporary_impact = np.mean(temporary_impacts) if temporary_impacts else 0.0
            
            # æ°¸ä¹…å½±å“ï¼šè¶‹åŠ¿å»¶ç»­
            permanent_impacts = []
            for i in range(len(changes) - 2):
                if changes[i] != 0:
                    # æ£€æŸ¥åç»­å˜åŒ–æ˜¯å¦åŒå‘
                    same_direction = sum(1 for j in range(i + 1, min(i + 3, len(changes))) 
                                       if changes[i] * changes[j] > 0)
                    if same_direction > 0:
                        impact = same_direction / 2.0  # å½’ä¸€åŒ–
                        permanent_impacts.append(impact)
            
            permanent_impact = np.mean(permanent_impacts) if permanent_impacts else 0.0
            
            return {
                'temporary_impact': min(1.0, temporary_impact),
                'permanent_impact': min(1.0, permanent_impact),
                'impact_ratio': permanent_impact / (temporary_impact + 1e-10),
                'market_resilience': 1.0 - temporary_impact
            }
            
        except Exception:
            return {'temporary_impact': 0.0, 'permanent_impact': 0.0}
    
    def _calculate_liquidity_measures(self, data: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—æµåŠ¨æ€§åº¦é‡"""
        try:
            if len(data) == 0:
                return {'kyle_lambda': 0.0, 'amihud_ratio': 0.0}
            
            # Kyle's Lambda (ä»·æ ¼å½±å“åº¦é‡)
            if len(data) > 1:
                changes = np.diff(data)
                price_changes = np.abs(changes)
                volume_proxy = np.ones_like(price_changes)  # ç®€åŒ–ï¼šå‡è®¾å•ä½æˆäº¤é‡
                
                if np.sum(volume_proxy) > 0:
                    kyle_lambda = np.mean(price_changes) / np.mean(volume_proxy)
                else:
                    kyle_lambda = 0.0
            else:
                kyle_lambda = 0.0
            
            # AmihudéæµåŠ¨æ€§æ¯”ç‡
            if len(data) > 1:
                returns = np.diff(data) / (data[:-1] + 1e-10)
                abs_returns = np.abs(returns)
                dollar_volume = np.ones_like(abs_returns)  # ç®€åŒ–
                
                if np.sum(dollar_volume) > 0:
                    amihud_ratio = np.mean(abs_returns / (dollar_volume + 1e-10))
                else:
                    amihud_ratio = 0.0
            else:
                amihud_ratio = 0.0
            
            # å…¶ä»–æµåŠ¨æ€§æŒ‡æ ‡
            bid_ask_spread = np.std(data) * 0.1  # ç®€åŒ–çš„ä»·å·®ä¼°è®¡
            turnover_rate = 1.0  # ç®€åŒ–
            
            return {
                'kyle_lambda': kyle_lambda,
                'amihud_ratio': amihud_ratio,
                'bid_ask_spread': bid_ask_spread,
                'turnover_rate': turnover_rate,
                'liquidity_score': 1.0 / (kyle_lambda + amihud_ratio + 1e-10)
            }
            
        except Exception:
            return {'kyle_lambda': 0.0, 'amihud_ratio': 0.0}
    
    def _detect_information_asymmetry(self, tail_data: np.ndarray, 
                                    all_data: np.ndarray) -> Dict[str, float]:
        """æ£€æµ‹ä¿¡æ¯ä¸å¯¹ç§°"""
        try:
            if len(tail_data) == 0:
                return {'asymmetry_score': 0.0, 'informed_trading': 0.0}
            
            # åŸºäºäº¤æ˜“æ¨¡å¼æ£€æµ‹ä¿¡æ¯ä¸å¯¹ç§°
            # ä¿¡æ¯ä¸å¯¹ç§°é€šå¸¸è¡¨ç°ä¸ºééšæœºçš„äº¤æ˜“æ¨¡å¼
            
            # åºåˆ—ç›¸å…³æ€§æ£€æµ‹
            if len(tail_data) > 1:
                autocorr = np.corrcoef(tail_data[:-1], tail_data[1:])[0, 1]
                autocorr = autocorr if not np.isnan(autocorr) else 0.0
            else:
                autocorr = 0.0
            
            # ä¸å¸‚åœºæ•´ä½“çš„ç›¸å…³æ€§
            if all_data.ndim > 1:
                market_avg = np.mean(all_data, axis=1)
                if len(market_avg) == len(tail_data) and len(tail_data) > 1:
                    market_corr = np.corrcoef(tail_data, market_avg)[0, 1]
                    market_corr = market_corr if not np.isnan(market_corr) else 0.0
                else:
                    market_corr = 0.0
            else:
                market_corr = 0.0
            
            # ä¿¡æ¯ä¸å¯¹ç§°è¯„åˆ†
            asymmetry_score = abs(autocorr) + abs(market_corr - np.mean([autocorr, market_corr]))
            asymmetry_score = min(1.0, asymmetry_score)
            
            # çŸ¥æƒ…äº¤æ˜“æ¦‚ç‡ä¼°è®¡
            informed_trading = asymmetry_score * 0.5 + abs(autocorr) * 0.5
            
            return {
                'asymmetry_score': asymmetry_score,
                'informed_trading': informed_trading,
                'autocorrelation': autocorr,
                'market_correlation': market_corr
            }
            
        except Exception:
            return {'asymmetry_score': 0.0, 'informed_trading': 0.0}
    
    def _detect_market_manipulation(self, data: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹å¸‚åœºæ“çºµ"""
        try:
            if len(data) == 0:
                return {'manipulation_score': 0.0, 'anomaly_detected': False}
            
            manipulation_indicators = []
            
            # å¼‚å¸¸ä»·æ ¼æ¨¡å¼æ£€æµ‹
            if len(data) > 5:
                # æ£€æµ‹å¼‚å¸¸å°–å³°
                z_scores = np.abs(stats.zscore(data))
                outliers = np.sum(z_scores > 3)
                outlier_ratio = outliers / len(data)
                
                if outlier_ratio > 0.1:  # è¶…è¿‡10%çš„å¼‚å¸¸ç‚¹
                    manipulation_indicators.append(('price_spikes', outlier_ratio))
                
                # æ£€æµ‹äººä¸ºçš„å‘¨æœŸæ€§æ¨¡å¼
                if len(data) >= 8:
                    fft = np.fft.fft(data)
                    power = np.abs(fft)**2
                    dominant_freq_power = np.max(power[1:])  # æ’é™¤DCåˆ†é‡
                    total_power = np.sum(power[1:])
                    
                    if total_power > 0:
                        concentration = dominant_freq_power / total_power
                        if concentration > 0.5:  # è¿‡åº¦é›†ä¸­çš„é¢‘ç‡
                            manipulation_indicators.append(('artificial_pattern', concentration))
                
                # æ£€æµ‹ä»·æ ¼å›ºå®šæ¨¡å¼
                unique_values = len(np.unique(data))
                if unique_values < len(data) * 0.3:  # å€¼è¿‡äºé›†ä¸­
                    concentration_score = 1.0 - unique_values / len(data)
                    manipulation_indicators.append(('price_fixing', concentration_score))
            
            # ç»¼åˆæ“çºµè¯„åˆ†
            if manipulation_indicators:
                manipulation_score = np.mean([score for _, score in manipulation_indicators])
                anomaly_detected = manipulation_score > 0.3
            else:
                manipulation_score = 0.0
                anomaly_detected = False
            
            return {
                'manipulation_score': manipulation_score,
                'anomaly_detected': anomaly_detected,
                'indicators': manipulation_indicators,
                'risk_level': 'high' if manipulation_score > 0.7 else 
                            'medium' if manipulation_score > 0.3 else 'low'
            }
            
        except Exception:
            return {'manipulation_score': 0.0, 'anomaly_detected': False}
    
    def _analyze_volatility_clustering(self, data: np.ndarray) -> Dict[str, float]:
        """åˆ†ææ³¢åŠ¨æ€§èšé›†"""
        try:
            if len(data) < 10:
                return {'clustering_coefficient': 0.0, 'garch_effect': 0.0}
            
            # è®¡ç®—æ³¢åŠ¨æ€§ä»£ç†ï¼ˆç»å¯¹æ”¶ç›Šç‡ï¼‰
            returns = np.diff(data) / (data[:-1] + 1e-10)
            volatility_proxy = np.abs(returns)
            
            # æ³¢åŠ¨æ€§çš„è‡ªç›¸å…³
            if len(volatility_proxy) > 1:
                vol_autocorr = np.corrcoef(volatility_proxy[:-1], volatility_proxy[1:])[0, 1]
                vol_autocorr = vol_autocorr if not np.isnan(vol_autocorr) else 0.0
            else:
                vol_autocorr = 0.0
            
            # ARCHæ•ˆåº”æ£€æµ‹ï¼ˆç®€åŒ–ï¼‰
            if len(volatility_proxy) >= 5:
                # è®¡ç®—5æœŸæ»åçš„è‡ªç›¸å…³
                arch_correlations = []
                for lag in range(1, min(6, len(volatility_proxy))):
                    if len(volatility_proxy) > lag:
                        corr = np.corrcoef(volatility_proxy[:-lag], volatility_proxy[lag:])[0, 1]
                        if not np.isnan(corr):
                            arch_correlations.append(abs(corr))
                
                garch_effect = np.mean(arch_correlations) if arch_correlations else 0.0
            else:
                garch_effect = 0.0
            
            # èšé›†ç³»æ•°
            clustering_coefficient = (abs(vol_autocorr) + garch_effect) / 2
            
            return {
                'clustering_coefficient': clustering_coefficient,
                'garch_effect': garch_effect,
                'volatility_persistence': abs(vol_autocorr),
                'mean_volatility': np.mean(volatility_proxy)
            }
            
        except Exception:
            return {'clustering_coefficient': 0.0, 'garch_effect': 0.0}
    
    def _detect_price_jumps(self, data: np.ndarray, threshold_multiplier: float = 3.0) -> Dict[str, Any]:
        """æ£€æµ‹ä»·æ ¼è·³è·ƒ"""
        try:
            if len(data) < 2:
                return {'jump_detected': False, 'jump_count': 0}
            
            # è®¡ç®—æ”¶ç›Šç‡
            returns = np.diff(data) / (data[:-1] + 1e-10)
            
            # è·³è·ƒæ£€æµ‹é˜ˆå€¼
            return_std = np.std(returns)
            threshold = threshold_multiplier * return_std
            
            # è¯†åˆ«è·³è·ƒ
            jumps = np.abs(returns) > threshold
            jump_indices = np.where(jumps)[0]
            
            jump_magnitudes = returns[jumps]
            
            return {
                'jump_detected': len(jump_indices) > 0,
                'jump_count': len(jump_indices),
                'jump_indices': jump_indices.tolist(),
                'jump_magnitudes': jump_magnitudes.tolist(),
                'jump_frequency': len(jump_indices) / len(returns),
                'max_jump': np.max(np.abs(jump_magnitudes)) if len(jump_magnitudes) > 0 else 0.0
            }
            
        except Exception:
            return {'jump_detected': False, 'jump_count': 0}
    
    def _calculate_microstructure_score(self, bid_ask_spread: Dict, market_depth: Dict,
                                      order_flow: Dict, trade_intensity: Dict,
                                      liquidity: Dict) -> float:
        """è®¡ç®—å¾®è§‚ç»“æ„ç»¼åˆè¯„åˆ†"""
        try:
            score_components = []
            
            # ä»·å·®è¯„åˆ†ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            spread_score = 1.0 - min(1.0, bid_ask_spread.get('relative_spread', 0.0) * 10)
            score_components.append(spread_score * 0.2)
            
            # æ·±åº¦è¯„åˆ†
            depth_score = market_depth.get('depth_score', 0.0)
            score_components.append(depth_score * 0.25)
            
            # è®¢å•æµå¹³è¡¡è¯„åˆ†
            imbalance = abs(order_flow.get('imbalance', 0.0))
            balance_score = 1.0 - imbalance
            score_components.append(balance_score * 0.2)
            
            # äº¤æ˜“å¼ºåº¦è¯„åˆ†
            intensity_score = min(1.0, trade_intensity.get('intensity', 0.0))
            score_components.append(intensity_score * 0.15)
            
            # æµåŠ¨æ€§è¯„åˆ†
            liquidity_score = min(1.0, liquidity.get('liquidity_score', 0.0) / 10)
            score_components.append(liquidity_score * 0.2)
            
            return sum(score_components)
            
        except Exception:
            return 0.0
    
    def _assess_market_efficiency(self, data: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°å¸‚åœºæ•ˆç‡"""
        try:
            if len(data) < 10:
                return {'efficiency_score': 0.5, 'weak_form_efficiency': 0.5}
            
            # å¼±å¼æœ‰æ•ˆæ€§æ£€éªŒ
            returns = np.diff(data) / (data[:-1] + 1e-10)
            
            # åºåˆ—ç›¸å…³æ€§æ£€éªŒ
            if len(returns) > 1:
                autocorr = np.corrcoef(returns[:-1], returns[1:])[0, 1]
                autocorr = autocorr if not np.isnan(autocorr) else 0.0
            else:
                autocorr = 0.0
            
            # æ¸¸ç¨‹æ£€éªŒï¼ˆç®€åŒ–ï¼‰
            signs = np.sign(returns)
            runs = []
            current_run = 1
            
            for i in range(1, len(signs)):
                if signs[i] == signs[i-1]:
                    current_run += 1
                else:
                    runs.append(current_run)
                    current_run = 1
            runs.append(current_run)
            
            expected_runs = (len(returns) + 1) / 2
            actual_runs = len(runs)
            runs_ratio = actual_runs / expected_runs
            
            # æ•ˆç‡è¯„åˆ†
            weak_form_efficiency = 1.0 - abs(autocorr)
            runs_efficiency = 1.0 - abs(runs_ratio - 1.0)
            
            efficiency_score = (weak_form_efficiency + runs_efficiency) / 2
            
            return {
                'efficiency_score': efficiency_score,
                'weak_form_efficiency': weak_form_efficiency,
                'autocorrelation': autocorr,
                'runs_ratio': runs_ratio
            }
            
        except Exception:
            return {'efficiency_score': 0.5, 'weak_form_efficiency': 0.5}
    
    def _analyze_transaction_costs(self, data: np.ndarray) -> Dict[str, float]:
        """åˆ†æäº¤æ˜“æˆæœ¬"""
        try:
            if len(data) == 0:
                return {'total_cost': 0.0, 'cost_ratio': 0.0}
            
            # ä¼°è®¡å„ç§äº¤æ˜“æˆæœ¬
            volatility = np.std(data)
            
            # ä¹°å–ä»·å·®æˆæœ¬
            spread_cost = volatility * 0.05  # ç®€åŒ–æ¨¡å‹
            
            # å¸‚åœºå†²å‡»æˆæœ¬
            impact_cost = volatility * 0.03
            
            # æ—¶æœºæˆæœ¬
            timing_cost = volatility * 0.02
            
            # æ€»äº¤æ˜“æˆæœ¬
            total_cost = spread_cost + impact_cost + timing_cost
            
            # æˆæœ¬æ¯”ç‡
            mean_price = np.mean(data)
            cost_ratio = total_cost / (mean_price + 1e-10)
            
            return {
                'total_cost': total_cost,
                'cost_ratio': cost_ratio,
                'spread_cost': spread_cost,
                'impact_cost': impact_cost,
                'timing_cost': timing_cost,
                'cost_breakdown': {
                    'spread': spread_cost / (total_cost + 1e-10),
                    'impact': impact_cost / (total_cost + 1e-10),
                    'timing': timing_cost / (total_cost + 1e-10)
                }
            }
            
        except Exception:
            return {'total_cost': 0.0, 'cost_ratio': 0.0}
        
# ========== é£é™©ç®¡ç†å…·ä½“å®ç° ==========
    
    def _calculate_value_at_risk(self, data: np.ndarray, confidence_level: float = 0.95,
                               time_horizon: int = 1) -> Dict[str, float]:
        """è®¡ç®—é£é™©ä»·å€¼(VaR)"""
        try:
            if len(data) < 10:
                return {'var_normal': 0.0, 'var_historical': 0.0, 'var_monte_carlo': 0.0}
            
            # è®¡ç®—æ”¶ç›Šç‡
            returns = np.diff(data) / (data[:-1] + 1e-10)
            
            # 1. å‚æ•°æ³•VaRï¼ˆæ­£æ€åˆ†å¸ƒå‡è®¾ï¼‰
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            alpha = 1 - confidence_level
            
            # è°ƒæ•´æ—¶é—´èŒƒå›´
            scaled_mean = mean_return * time_horizon
            scaled_std = std_return * np.sqrt(time_horizon)
            
            # æ­£æ€åˆ†å¸ƒVaR
            z_score = stats.norm.ppf(alpha)
            var_normal = -(scaled_mean + z_score * scaled_std)
            
            # 2. å†å²æ¨¡æ‹Ÿæ³•VaR
            if len(returns) > 0:
                var_historical = -np.percentile(returns, alpha * 100)
            else:
                var_historical = 0.0
            
            # 3. è’™ç‰¹å¡æ´›æ¨¡æ‹ŸVaR
            np.random.seed(42)
            simulated_returns = np.random.normal(mean_return, std_return, 10000)
            var_monte_carlo = -np.percentile(simulated_returns, alpha * 100)
            
            # 4. ä¿®æ­£çš„Cornish-Fisher VaRï¼ˆè€ƒè™‘ååº¦å’Œå³°åº¦ï¼‰
            if len(returns) > 3:
                skewness = stats.skew(returns)
                kurt = stats.kurtosis(returns)
                
                # Cornish-Fisherè°ƒæ•´
                cf_adjustment = (z_score**2 - 1) * skewness / 6 + \
                               (z_score**3 - 3*z_score) * kurt / 24 - \
                               (2*z_score**3 - 5*z_score) * skewness**2 / 36
                
                adjusted_quantile = z_score + cf_adjustment
                var_cornish_fisher = -(scaled_mean + adjusted_quantile * scaled_std)
            else:
                var_cornish_fisher = var_normal
            
            return {
                'var_normal': var_normal,
                'var_historical': var_historical,
                'var_monte_carlo': var_monte_carlo,
                'var_cornish_fisher': var_cornish_fisher,
                'confidence_level': confidence_level,
                'time_horizon': time_horizon,
                'var_average': np.mean([var_normal, var_historical, var_monte_carlo])
            }
            
        except Exception as e:
            self.logger.error(f"VaRè®¡ç®—é”™è¯¯: {str(e)}")
            return {'var_normal': 0.0, 'var_historical': 0.0, 'var_monte_carlo': 0.0}
    
    def _calculate_conditional_var(self, data: np.ndarray, confidence_level: float = 0.95) -> Dict[str, float]:
        """è®¡ç®—æ¡ä»¶é£é™©ä»·å€¼(CVaR/Expected Shortfall)"""
        try:
            if len(data) < 10:
                return {'cvar': 0.0, 'expected_shortfall': 0.0}
            
            returns = np.diff(data) / (data[:-1] + 1e-10)
            alpha = 1 - confidence_level
            
            # 1. å†å²æ¨¡æ‹Ÿæ³•CVaR
            var_threshold = np.percentile(returns, alpha * 100)
            tail_returns = returns[returns <= var_threshold]
            
            if len(tail_returns) > 0:
                cvar_historical = -np.mean(tail_returns)
            else:
                cvar_historical = 0.0
            
            # 2. å‚æ•°æ³•CVaRï¼ˆæ­£æ€åˆ†å¸ƒï¼‰
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            
            z_alpha = stats.norm.ppf(alpha)
            phi_z = stats.norm.pdf(z_alpha)
            
            cvar_normal = -(mean_return - std_return * phi_z / alpha)
            
            # 3. è’™ç‰¹å¡æ´›CVaR
            np.random.seed(42)
            simulated_returns = np.random.normal(mean_return, std_return, 10000)
            mc_var_threshold = np.percentile(simulated_returns, alpha * 100)
            mc_tail_returns = simulated_returns[simulated_returns <= mc_var_threshold]
            
            if len(mc_tail_returns) > 0:
                cvar_monte_carlo = -np.mean(mc_tail_returns)
            else:
                cvar_monte_carlo = 0.0
            
            # å¹³å‡CVaR
            cvar_average = np.mean([cvar_historical, cvar_normal, cvar_monte_carlo])
            
            return {
                'cvar': cvar_average,
                'expected_shortfall': cvar_average,
                'cvar_historical': cvar_historical,
                'cvar_normal': cvar_normal,
                'cvar_monte_carlo': cvar_monte_carlo,
                'confidence_level': confidence_level,
                'tail_expectation': cvar_average
            }
            
        except Exception as e:
            self.logger.error(f"CVaRè®¡ç®—é”™è¯¯: {str(e)}")
            return {'cvar': 0.0, 'expected_shortfall': 0.0}
    
    def _analyze_maximum_drawdown(self, data: np.ndarray) -> Dict[str, float]:
        """åˆ†ææœ€å¤§å›æ’¤"""
        try:
            if len(data) < 2:
                return {'max_drawdown': 0.0, 'drawdown_duration': 0}
            
            # è®¡ç®—ç´¯ç§¯æ”¶ç›Š
            cumulative_returns = np.cumprod(1 + np.diff(data) / (data[:-1] + 1e-10))
            
            # è®¡ç®—æ»šåŠ¨æœ€é«˜ç‚¹
            running_max = np.maximum.accumulate(cumulative_returns)
            
            # è®¡ç®—å›æ’¤
            drawdowns = (cumulative_returns - running_max) / running_max
            
            # æœ€å¤§å›æ’¤
            max_drawdown = np.min(drawdowns)
            max_dd_index = np.argmin(drawdowns)
            
            # å›æ’¤æŒç»­æ—¶é—´
            if max_dd_index > 0:
                # æ‰¾åˆ°æœ€å¤§å›æ’¤å¼€å§‹çš„ç‚¹
                peak_index = np.argmax(running_max[:max_dd_index + 1])
                drawdown_duration = max_dd_index - peak_index
            else:
                drawdown_duration = 0
            
            # å½“å‰å›æ’¤
            current_drawdown = drawdowns[-1] if len(drawdowns) > 0 else 0.0
            
            # å›æ’¤ç»Ÿè®¡
            negative_drawdowns = drawdowns[drawdowns < 0]
            avg_drawdown = np.mean(negative_drawdowns) if len(negative_drawdowns) > 0 else 0.0
            
            # æ¢å¤æ—¶é—´ä¼°è®¡
            if current_drawdown < 0:
                recent_trend = np.mean(np.diff(data[-5:])) if len(data) >= 6 else 0
                if recent_trend > 0:
                    recovery_estimate = abs(current_drawdown) / (recent_trend + 1e-10)
                else:
                    recovery_estimate = float('inf')
            else:
                recovery_estimate = 0
            
            return {
                'max_drawdown': abs(max_drawdown),
                'drawdown_duration': drawdown_duration,
                'current_drawdown': abs(current_drawdown),
                'average_drawdown': abs(avg_drawdown),
                'recovery_estimate': min(100, recovery_estimate),
                'drawdown_frequency': len(negative_drawdowns) / len(drawdowns) if len(drawdowns) > 0 else 0,
                'max_drawdown_date': max_dd_index
            }
            
        except Exception as e:
            self.logger.error(f"æœ€å¤§å›æ’¤åˆ†æé”™è¯¯: {str(e)}")
            return {'max_drawdown': 0.0, 'drawdown_duration': 0}
    
    def _assess_volatility_risk(self, data: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°æ³¢åŠ¨ç‡é£é™©"""
        try:
            if len(data) < 2:
                return {'volatility': 0.0, 'volatility_risk_level': 'low'}
            
            returns = np.diff(data) / (data[:-1] + 1e-10)
            
            # å†å²æ³¢åŠ¨ç‡
            historical_vol = np.std(returns) * np.sqrt(252)  # å¹´åŒ–
            
            # å®ç°æ³¢åŠ¨ç‡ï¼ˆåŸºäºé«˜é¢‘æ”¶ç›Šç‡ï¼‰
            if len(returns) > 1:
                realized_vol = np.sqrt(np.sum(returns**2)) * np.sqrt(252)
            else:
                realized_vol = historical_vol
            
            # GARCHæ³¢åŠ¨ç‡é¢„æµ‹ï¼ˆç®€åŒ–ï¼‰
            garch_vol = self._estimate_garch_volatility(returns)
            
            # æ³¢åŠ¨ç‡èšé›†åº¦
            vol_clustering = self._analyze_volatility_clustering(data)
            clustering_coefficient = vol_clustering.get('clustering_coefficient', 0.0)
            
            # æ³¢åŠ¨ç‡é£é™©ç­‰çº§
            if historical_vol > 0.3:
                risk_level = 'high'
            elif historical_vol > 0.15:
                risk_level = 'medium'
            else:
                risk_level = 'low'
            
            # æ³¢åŠ¨ç‡é¢„æµ‹åŒºé—´
            vol_forecast_upper = garch_vol * 1.2
            vol_forecast_lower = garch_vol * 0.8
            
            return {
                'volatility': historical_vol,
                'realized_volatility': realized_vol,
                'garch_volatility': garch_vol,
                'volatility_clustering': clustering_coefficient,
                'volatility_risk_level': risk_level,
                'vol_forecast_range': {
                    'upper': vol_forecast_upper,
                    'lower': vol_forecast_lower
                },
                'volatility_of_volatility': np.std([historical_vol, realized_vol, garch_vol])
            }
            
        except Exception as e:
            self.logger.error(f"æ³¢åŠ¨ç‡é£é™©è¯„ä¼°é”™è¯¯: {str(e)}")
            return {'volatility': 0.0, 'volatility_risk_level': 'low'}
    
    def _estimate_garch_volatility(self, returns: np.ndarray, alpha: float = 0.1, 
                                 beta: float = 0.85) -> float:
        """ä¼°è®¡GARCHæ³¢åŠ¨ç‡"""
        try:
            if len(returns) < 5:
                return np.std(returns) if len(returns) > 1 else 0.0
            
            # ç®€åŒ–çš„GARCH(1,1)æ¨¡å‹
            omega = 0.01  # é•¿æœŸæ–¹å·®
            
            # åˆå§‹æ¡ä»¶æ³¢åŠ¨ç‡
            sigma_squared = np.var(returns)
            
            # é€’å½’è®¡ç®—GARCHæ³¢åŠ¨ç‡
            for i in range(len(returns)):
                sigma_squared = omega + alpha * returns[i]**2 + beta * sigma_squared
            
            return np.sqrt(sigma_squared * 252)  # å¹´åŒ–
            
        except Exception:
            return np.std(returns) * np.sqrt(252) if len(returns) > 1 else 0.0
    
    def _assess_liquidity_risk(self, data: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°æµåŠ¨æ€§é£é™©"""
        try:
            if len(data) == 0:
                return {'liquidity_risk': 0.0, 'liquidity_score': 1.0}
            
            # åŸºäºä»·æ ¼å˜åŒ–çš„æµåŠ¨æ€§è¯„ä¼°
            if len(data) > 1:
                changes = np.abs(np.diff(data))
                
                # æµåŠ¨æ€§æŒ‡æ ‡
                price_impact = np.mean(changes)  # ä»·æ ¼å½±å“
                volume_proxy = 1.0 / (np.std(changes) + 1e-10)  # æˆäº¤é‡ä»£ç†
                
                # ä¹°å–ä»·å·®ä¼°è®¡
                bid_ask_spread = np.std(data) * 0.1
                
                # å¸‚åœºæ·±åº¦ä¼°è®¡
                market_depth = volume_proxy
                
                # æµåŠ¨æ€§é£é™©è¯„åˆ†
                liquidity_risk = (price_impact + bid_ask_spread) / (market_depth + 1e-10)
                liquidity_score = 1.0 / (1.0 + liquidity_risk)
                
            else:
                liquidity_risk = 0.0
                liquidity_score = 1.0
            
            # æµåŠ¨æ€§é£é™©ç­‰çº§
            if liquidity_risk > 0.1:
                risk_level = 'high'
            elif liquidity_risk > 0.05:
                risk_level = 'medium'
            else:
                risk_level = 'low'
            
            return {
                'liquidity_risk': liquidity_risk,
                'liquidity_score': liquidity_score,
                'risk_level': risk_level,
                'price_impact_cost': price_impact if len(data) > 1 else 0.0,
                'market_depth_score': market_depth if len(data) > 1 else 1.0
            }
            
        except Exception:
            return {'liquidity_risk': 0.0, 'liquidity_score': 1.0}
    
    def _assess_model_risk(self, fusion_result: Dict) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹é£é™©"""
        try:
            model_risk_factors = []
            
            # æ¨¡å‹ä¸ç¡®å®šæ€§
            signal_variance = fusion_result.get('signal_variance', 0.0)
            model_risk_factors.append(signal_variance)
            
            # ä¿¡å·è´¨é‡
            signal_quality = fusion_result.get('signal_quality', 0.5)
            quality_risk = 1.0 - signal_quality
            model_risk_factors.append(quality_risk)
            
            # æ¨¡å‹ä¸€è‡´æ€§
            signal_consistency = fusion_result.get('signal_consistency', 0.5)
            consistency_risk = 1.0 - signal_consistency
            model_risk_factors.append(consistency_risk)
            
            # æ•°æ®å……åˆ†æ€§
            signal_count = fusion_result.get('signal_count', 0)
            data_sufficiency = min(1.0, signal_count / 5.0)
            data_risk = 1.0 - data_sufficiency
            model_risk_factors.append(data_risk)
            
            # æ¨¡å‹å¤æ‚åº¦é£é™©
            complexity_score = self._assess_model_complexity()
            complexity_risk = min(1.0, complexity_score.get('total_parameters', 0) / 1000000)
            model_risk_factors.append(complexity_risk)
            
            # ç»¼åˆæ¨¡å‹é£é™©
            overall_model_risk = np.mean(model_risk_factors)
            
            # é£é™©åˆ†è§£
            risk_breakdown = {
                'signal_variance_risk': signal_variance,
                'quality_risk': quality_risk,
                'consistency_risk': consistency_risk,
                'data_risk': data_risk,
                'complexity_risk': complexity_risk
            }
            
            return {
                'model_risk': overall_model_risk,
                'risk_breakdown': risk_breakdown,
                'risk_level': 'high' if overall_model_risk > 0.7 else 
                            'medium' if overall_model_risk > 0.4 else 'low',
                'model_confidence': 1.0 - overall_model_risk
            }
            
        except Exception:
            return {'model_risk': 0.5, 'risk_level': 'medium'}
    
    def _assess_systemic_risk(self, tail: int, data_matrix: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°ç³»ç»Ÿæ€§é£é™©"""
        try:
            if data_matrix.ndim < 2 or data_matrix.shape[1] < 2:
                return {'systemic_risk': 0.0, 'correlation_risk': 0.0}
            
            tail_data = data_matrix[:, tail]
            
            # ä¸å…¶ä»–å°¾æ•°çš„ç›¸å…³æ€§
            correlations = []
            for other_tail in range(data_matrix.shape[1]):
                if other_tail != tail:
                    other_data = data_matrix[:, other_tail]
                    if len(tail_data) > 1 and len(other_data) > 1:
                        corr = np.corrcoef(tail_data, other_data)[0, 1]
                        if not np.isnan(corr):
                            correlations.append(abs(corr))
            
            # ç³»ç»Ÿæ€§é£é™©æŒ‡æ ‡
            if correlations:
                avg_correlation = np.mean(correlations)
                max_correlation = np.max(correlations)
                correlation_risk = avg_correlation
            else:
                avg_correlation = 0.0
                max_correlation = 0.0
                correlation_risk = 0.0
            
            # å¸‚åœºé›†ä¸­åº¦é£é™©
            total_activity = np.sum(data_matrix, axis=1)
            tail_share = np.mean(tail_data) / (np.mean(total_activity) + 1e-10)
            concentration_risk = min(1.0, tail_share * 10)  # å½’ä¸€åŒ–
            
            # ç³»ç»Ÿæ€§å†²å‡»æ•æ„Ÿæ€§
            if len(tail_data) > 5:
                # æ£€æµ‹ä¸ç³»ç»Ÿæ€§å˜åŒ–çš„æ•æ„Ÿæ€§
                system_changes = np.diff(total_activity)
                tail_changes = np.diff(tail_data)
                
                if len(system_changes) > 0 and len(tail_changes) > 0:
                    sensitivity = np.corrcoef(system_changes, tail_changes)[0, 1]
                    sensitivity = sensitivity if not np.isnan(sensitivity) else 0.0
                    shock_sensitivity = abs(sensitivity)
                else:
                    shock_sensitivity = 0.0
            else:
                shock_sensitivity = 0.0
            
            # ç»¼åˆç³»ç»Ÿæ€§é£é™©
            systemic_risk = (correlation_risk * 0.4 + 
                           concentration_risk * 0.3 + 
                           shock_sensitivity * 0.3)
            
            return {
                'systemic_risk': systemic_risk,
                'correlation_risk': correlation_risk,
                'concentration_risk': concentration_risk,
                'shock_sensitivity': shock_sensitivity,
                'average_correlation': avg_correlation,
                'max_correlation': max_correlation,
                'market_share': tail_share
            }
            
        except Exception:
            return {'systemic_risk': 0.0, 'correlation_risk': 0.0}
    
    def _assess_operational_risk(self, fusion_result: Dict) -> Dict[str, float]:
        """è¯„ä¼°æ“ä½œé£é™©"""
        try:
            operational_risk_factors = []
            
            # æŠ€æœ¯æ•…éšœé£é™©
            model_count = fusion_result.get('signal_count', 0)
            if model_count < 3:
                tech_failure_risk = 0.3  # æ¨¡å‹è¿‡å°‘
            else:
                tech_failure_risk = 0.1
            operational_risk_factors.append(tech_failure_risk)
            
            # æ•°æ®è´¨é‡é£é™©
            signal_quality = fusion_result.get('signal_quality', 0.5)
            data_quality_risk = 1.0 - signal_quality
            operational_risk_factors.append(data_quality_risk * 0.5)
            
            # å¤„ç†å¤æ‚åº¦é£é™©
            complexity_score = self._assess_model_complexity()
            processing_risk = min(0.3, complexity_score.get('total_models', 0) / 100)
            operational_risk_factors.append(processing_risk)
            
            # äººä¸ºé”™è¯¯é£é™©ï¼ˆç®€åŒ–ï¼‰
            human_error_risk = 0.05  # åŸºç¡€äººä¸ºé”™è¯¯æ¦‚ç‡
            operational_risk_factors.append(human_error_risk)
            
            # ç³»ç»Ÿé›†æˆé£é™©
            integration_complexity = len(fusion_result.get('original_signals', {}))
            integration_risk = min(0.2, integration_complexity / 20)
            operational_risk_factors.append(integration_risk)
            
            # ç»¼åˆæ“ä½œé£é™©
            overall_operational_risk = sum(operational_risk_factors)
            
            return {
                'operational_risk': overall_operational_risk,
                'tech_failure_risk': tech_failure_risk,
                'data_quality_risk': data_quality_risk,
                'processing_risk': processing_risk,
                'human_error_risk': human_error_risk,
                'integration_risk': integration_risk,
                'risk_level': 'high' if overall_operational_risk > 0.3 else 
                            'medium' if overall_operational_risk > 0.15 else 'low'
            }
            
        except Exception:
            return {'operational_risk': 0.1, 'risk_level': 'low'}
    
    def _assess_credit_risk_simulation(self, data: np.ndarray) -> Dict[str, float]:
        """æ¨¡æ‹Ÿä¿¡ç”¨é£é™©è¯„ä¼°"""
        try:
            if len(data) == 0:
                return {'credit_risk': 0.0, 'default_probability': 0.0}
            
            # åŸºäºæ•°æ®ç¨³å®šæ€§æ¨¡æ‹Ÿä¿¡ç”¨è´¨é‡
            if len(data) > 1:
                volatility = np.std(data)
                trend = np.polyfit(range(len(data)), data, 1)[0]
                
                # è¿çº¦æ¦‚ç‡æ¨¡æ‹Ÿ
                # é«˜æ³¢åŠ¨æ€§å’Œè´Ÿè¶‹åŠ¿å¢åŠ è¿çº¦é£é™©
                volatility_factor = min(1.0, volatility * 10)
                trend_factor = max(0.0, -trend * 100) if trend < 0 else 0.0
                
                default_probability = min(1.0, (volatility_factor + trend_factor) / 2)
                
                # ä¿¡ç”¨è¯„çº§æ¨¡æ‹Ÿ
                if default_probability < 0.01:
                    credit_rating = 'AAA'
                    credit_risk = 0.01
                elif default_probability < 0.05:
                    credit_rating = 'AA'
                    credit_risk = 0.05
                elif default_probability < 0.1:
                    credit_rating = 'A'
                    credit_risk = 0.1
                elif default_probability < 0.2:
                    credit_rating = 'BBB'
                    credit_risk = 0.2
                else:
                    credit_rating = 'BB+'
                    credit_risk = 0.3
                    
            else:
                default_probability = 0.05  # é»˜è®¤å€¼
                credit_rating = 'A'
                credit_risk = 0.1
            
            return {
                'credit_risk': credit_risk,
                'default_probability': default_probability,
                'credit_rating': credit_rating,
                'volatility_component': volatility_factor if len(data) > 1 else 0.0,
                'trend_component': trend_factor if len(data) > 1 else 0.0
            }
            
        except Exception:
            return {'credit_risk': 0.1, 'default_probability': 0.05}
    
    def _assess_market_risk(self, tail_data: np.ndarray, all_data: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°å¸‚åœºé£é™©"""
        try:
            if len(tail_data) == 0:
                return {'market_risk': 0.0, 'beta': 1.0}
            
            # è®¡ç®—Betaç³»æ•°
            if all_data.ndim > 1:
                market_returns = np.diff(np.mean(all_data, axis=1))
                tail_returns = np.diff(tail_data)
                
                if len(market_returns) > 1 and len(tail_returns) > 1 and len(market_returns) == len(tail_returns):
                    market_var = np.var(market_returns)
                    if market_var > 0:
                        covariance = np.cov(tail_returns, market_returns)[0, 1]
                        beta = covariance / market_var
                    else:
                        beta = 1.0
                else:
                    beta = 1.0
            else:
                beta = 1.0
            
            # å¸‚åœºé£é™©è¯„ä¼°
            if len(tail_data) > 1:
                tail_volatility = np.std(np.diff(tail_data))
                
                # åŸºäºBetaå’Œæ³¢åŠ¨ç‡çš„å¸‚åœºé£é™©
                market_risk = abs(beta) * tail_volatility
                
                # ç³»ç»Ÿæ€§é£é™©vsç‰¹å¼‚æ€§é£é™©
                systematic_risk = abs(beta - 1.0)  # åç¦»å¸‚åœºçš„ç¨‹åº¦
                idiosyncratic_risk = tail_volatility * (1 - min(1.0, abs(beta)))
                
            else:
                market_risk = 0.0
                systematic_risk = 0.0
                idiosyncratic_risk = 0.0
            
            # å¸‚åœºé£é™©ç­‰çº§
            if market_risk > 0.15:
                risk_level = 'high'
            elif market_risk > 0.08:
                risk_level = 'medium'
            else:
                risk_level = 'low'
            
            return {
                'market_risk': market_risk,
                'beta': beta,
                'systematic_risk': systematic_risk,
                'idiosyncratic_risk': idiosyncratic_risk,
                'risk_level': risk_level,
                'market_correlation': min(1.0, abs(beta))
            }
            
        except Exception:
            return {'market_risk': 0.1, 'beta': 1.0}
    
    def _assess_risk_concentration(self, tail: int, data_matrix: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°é£é™©é›†ä¸­åº¦"""
        try:
            if data_matrix.ndim < 2:
                return {'concentration_risk': 0.0, 'diversification_ratio': 1.0}
            
            tail_data = data_matrix[:, tail]
            
            # åœ¨æ€»ä½“ä¸­çš„æ¯”é‡
            total_activity = np.sum(data_matrix, axis=1)
            tail_weight = np.mean(tail_data) / (np.mean(total_activity) + 1e-10)
            
            # é›†ä¸­åº¦é£é™©
            concentration_risk = min(1.0, tail_weight * 20)  # æ”¾å¤§é›†ä¸­åº¦æ•ˆåº”
            
            # å¤šæ ·åŒ–æ¯”ç‡
            # è®¡ç®—ä¸å…¶ä»–å°¾æ•°çš„ç›¸å…³æ€§
            correlations = []
            for other_tail in range(data_matrix.shape[1]):
                if other_tail != tail and len(data_matrix[:, other_tail]) > 1:
                    corr = np.corrcoef(tail_data, data_matrix[:, other_tail])[0, 1]
                    if not np.isnan(corr):
                        correlations.append(abs(corr))
            
            if correlations:
                avg_correlation = np.mean(correlations)
                diversification_ratio = 1.0 - avg_correlation
            else:
                diversification_ratio = 1.0
            
            # é£é™©åˆ†æ•£æ•ˆæœ
            if diversification_ratio > 0.8:
                diversification_level = 'excellent'
            elif diversification_ratio > 0.6:
                diversification_level = 'good'
            elif diversification_ratio > 0.4:
                diversification_level = 'moderate'
            else:
                diversification_level = 'poor'
            
            return {
                'concentration_risk': concentration_risk,
                'diversification_ratio': diversification_ratio,
                'portfolio_weight': tail_weight,
                'diversification_level': diversification_level,
                'correlation_with_others': avg_correlation if correlations else 0.0
            }
            
        except Exception:
            return {'concentration_risk': 0.0, 'diversification_ratio': 1.0}
    
    def _perform_stress_testing(self, data: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œå‹åŠ›æµ‹è¯•"""
        try:
            if len(data) < 2:
                return {'stress_scenarios': [], 'worst_case_loss': 0.0}
            
            base_returns = np.diff(data) / (data[:-1] + 1e-10)
            base_volatility = np.std(base_returns)
            base_mean = np.mean(base_returns)
            
            stress_scenarios = []
            
            # æƒ…æ™¯1ï¼šé«˜æ³¢åŠ¨ç‡æƒ…æ™¯
            high_vol_scenario = {
                'name': 'High Volatility Shock',
                'volatility_multiplier': 3.0,
                'expected_loss': abs(base_mean - 3 * base_volatility),
                'probability': 0.05,
                'description': 'æ³¢åŠ¨ç‡å¢åŠ 3å€çš„æç«¯æƒ…å†µ'
            }
            stress_scenarios.append(high_vol_scenario)
            
            # æƒ…æ™¯2ï¼šè¶‹åŠ¿åè½¬æƒ…æ™¯
            trend_reversal_scenario = {
                'name': 'Trend Reversal',
                'return_shift': -2 * abs(base_mean),
                'expected_loss': 2 * abs(base_mean),
                'probability': 0.1,
                'description': 'è¶‹åŠ¿å®Œå…¨åè½¬çš„æƒ…å†µ'
            }
            stress_scenarios.append(trend_reversal_scenario)
            
            # æƒ…æ™¯3ï¼šæµåŠ¨æ€§æ¯ç«­æƒ…æ™¯
            liquidity_crisis_scenario = {
                'name': 'Liquidity Crisis',
                'liquidity_impact': base_volatility * 2,
                'expected_loss': base_volatility * 2,
                'probability': 0.02,
                'description': 'å¸‚åœºæµåŠ¨æ€§æ¯ç«­æƒ…å†µ'
            }
            stress_scenarios.append(liquidity_crisis_scenario)
            
            # æƒ…æ™¯4ï¼šç³»ç»Ÿæ€§å†²å‡»
            systemic_shock_scenario = {
                'name': 'Systemic Shock',
                'correlation_increase': 0.8,
                'expected_loss': base_volatility * 4,
                'probability': 0.01,
                'description': 'ç³»ç»Ÿæ€§é£é™©å†²å‡»æƒ…å†µ'
            }
            stress_scenarios.append(systemic_shock_scenario)
            
            # è®¡ç®—æœ€åæƒ…å†µæŸå¤±
            worst_case_loss = max(scenario['expected_loss'] for scenario in stress_scenarios)
            
            # ç»¼åˆå‹åŠ›æµ‹è¯•å¾—åˆ†
            expected_stress_loss = sum(scenario['expected_loss'] * scenario['probability'] 
                                     for scenario in stress_scenarios)
            
            return {
                'stress_scenarios': stress_scenarios,
                'worst_case_loss': worst_case_loss,
                'expected_stress_loss': expected_stress_loss,
                'stress_test_passed': worst_case_loss < 0.5,  # é˜ˆå€¼
                'resilience_score': 1.0 - min(1.0, expected_stress_loss)
            }
            
        except Exception:
            return {'stress_scenarios': [], 'worst_case_loss': 0.0}
    
    def _perform_scenario_analysis(self, data: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œæƒ…æ™¯åˆ†æ"""
        try:
            if len(data) < 5:
                return {'scenarios': [], 'base_case_probability': 1.0}
            
            base_trend = np.polyfit(range(len(data)), data, 1)[0]
            base_volatility = np.std(data)
            
            scenarios = []
            
            # åŸºå‡†æƒ…æ™¯
            base_scenario = {
                'name': 'Base Case',
                'probability': 0.6,
                'expected_return': base_trend,
                'volatility': base_volatility,
                'description': 'å½“å‰è¶‹åŠ¿å»¶ç»­'
            }
            scenarios.append(base_scenario)
            
            # ä¹è§‚æƒ…æ™¯
            optimistic_scenario = {
                'name': 'Optimistic',
                'probability': 0.2,
                'expected_return': base_trend * 1.5,
                'volatility': base_volatility * 0.8,
                'description': 'ç§¯æå‘å±•è¶‹åŠ¿'
            }
            scenarios.append(optimistic_scenario)
            
            # æ‚²è§‚æƒ…æ™¯
            pessimistic_scenario = {
                'name': 'Pessimistic',
                'probability': 0.15,
                'expected_return': -abs(base_trend),
                'volatility': base_volatility * 1.5,
                'description': 'ä¸åˆ©å‘å±•è¶‹åŠ¿'
            }
            scenarios.append(pessimistic_scenario)
            
            # æç«¯æƒ…æ™¯
            extreme_scenario = {
                'name': 'Extreme',
                'probability': 0.05,
                'expected_return': -abs(base_trend) * 3,
                'volatility': base_volatility * 3,
                'description': 'æç«¯ä¸åˆ©æƒ…å†µ'
            }
            scenarios.append(extreme_scenario)
            
            # æœŸæœ›æ”¶ç›Šå’Œé£é™©
            expected_return = sum(s['probability'] * s['expected_return'] for s in scenarios)
            expected_volatility = sum(s['probability'] * s['volatility'] for s in scenarios)
            
            return {
                'scenarios': scenarios,
                'expected_return': expected_return,
                'expected_volatility': expected_volatility,
                'downside_probability': pessimistic_scenario['probability'] + extreme_scenario['probability'],
                'upside_probability': optimistic_scenario['probability'],
                'base_case_probability': base_scenario['probability']
            }
            
        except Exception:
            return {'scenarios': [], 'base_case_probability': 1.0}
    
    def _calculate_risk_adjusted_returns(self, data: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—é£é™©è°ƒæ•´æ”¶ç›Š"""
        try:
            if len(data) < 2:
                return {'sharpe_ratio': 0.0, 'sortino_ratio': 0.0}
            
            returns = np.diff(data) / (data[:-1] + 1e-10)
            
            # å‡è®¾æ— é£é™©åˆ©ç‡
            risk_free_rate = 0.02 / 252  # æ—¥åŒ–æ— é£é™©åˆ©ç‡
            
            excess_returns = returns - risk_free_rate
            mean_excess_return = np.mean(excess_returns)
            
            # å¤æ™®æ¯”ç‡
            if len(returns) > 1:
                return_volatility = np.std(returns)
                sharpe_ratio = mean_excess_return / (return_volatility + 1e-10)
            else:
                sharpe_ratio = 0.0
            
            # ç´¢æè¯ºæ¯”ç‡ï¼ˆåªè€ƒè™‘ä¸‹è¡Œé£é™©ï¼‰
            negative_returns = returns[returns < 0]
            if len(negative_returns) > 0:
                downside_deviation = np.std(negative_returns)
                sortino_ratio = mean_excess_return / (downside_deviation + 1e-10)
            else:
                sortino_ratio = float('inf') if mean_excess_return > 0 else 0.0
            
            # å¡å°”é©¬æ¯”ç‡
            max_dd_analysis = self._analyze_maximum_drawdown(data)
            max_drawdown = max_dd_analysis['max_drawdown']
            
            if max_drawdown > 0:
                calmar_ratio = np.mean(returns) * 252 / max_drawdown  # å¹´åŒ–
            else:
                calmar_ratio = float('inf') if np.mean(returns) > 0 else 0.0
            
            # ä¿¡æ¯æ¯”ç‡ï¼ˆç›¸å¯¹åŸºå‡†çš„è¶…é¢æ”¶ç›Šï¼‰
            benchmark_return = 0.0  # ç®€åŒ–ï¼šå‡è®¾åŸºå‡†ä¸º0
            tracking_error = np.std(returns - benchmark_return)
            information_ratio = (np.mean(returns) - benchmark_return) / (tracking_error + 1e-10)
            
            return {
                'sharpe_ratio': sharpe_ratio,
                'sortino_ratio': min(10.0, sortino_ratio),  # é™åˆ¶æå€¼
                'calmar_ratio': min(10.0, calmar_ratio),
                'information_ratio': information_ratio,
                'return_volatility': return_volatility if len(returns) > 1 else 0.0,
                'downside_deviation': downside_deviation if len(negative_returns) > 0 else 0.0
            }
            
        except Exception:
            return {'sharpe_ratio': 0.0, 'sortino_ratio': 0.0}
    
    def _calculate_overall_risk_score(self, var_analysis: Dict, volatility_risk: Dict,
                                    model_risk: Dict, systemic_risk: Dict) -> float:
        """è®¡ç®—æ€»ä½“é£é™©è¯„åˆ†"""
        try:
            risk_components = []
            
            # VaRé£é™©æƒé‡
            var_score = var_analysis.get('var_average', 0.0)
            normalized_var = min(1.0, abs(var_score) * 10)
            risk_components.append(normalized_var * 0.3)
            
            # æ³¢åŠ¨ç‡é£é™©æƒé‡
            vol_risk = volatility_risk.get('volatility', 0.0)
            normalized_vol = min(1.0, vol_risk)
            risk_components.append(normalized_vol * 0.25)
            
            # æ¨¡å‹é£é™©æƒé‡
            model_risk_score = model_risk.get('model_risk', 0.0)
            risk_components.append(model_risk_score * 0.2)
            
            # ç³»ç»Ÿæ€§é£é™©æƒé‡
            systemic_risk_score = systemic_risk.get('systemic_risk', 0.0)
            risk_components.append(systemic_risk_score * 0.25)
            
            overall_risk = sum(risk_components)
            
            return min(1.0, max(0.0, overall_risk))
            
        except Exception:
            return 0.5
    
    def _classify_risk_level(self, risk_score: float) -> str:
        """åˆ†ç±»é£é™©ç­‰çº§"""
        if risk_score > 0.7:
            return 'high'
        elif risk_score > 0.4:
            return 'medium'
        else:
            return 'low'
    
    def _assess_risk_capacity(self, data: np.ndarray) -> Dict[str, float]:
        """è¯„ä¼°é£é™©æ‰¿å—èƒ½åŠ›"""
        try:
            if len(data) == 0:
                return {'risk_capacity': 0.5, 'capacity_utilization': 0.5}
            
            # åŸºäºå†å²è¡¨ç°è¯„ä¼°é£é™©æ‰¿å—èƒ½åŠ›
            if len(data) > 1:
                returns = np.diff(data) / (data[:-1] + 1e-10)
                
                # å†å²æœ€å¤§æŸå¤±æ‰¿å—
                max_loss = abs(np.min(returns)) if len(returns) > 0 else 0.0
                
                # æ³¢åŠ¨ç‡è€å—æ€§
                volatility_tolerance = 1.0 / (np.std(returns) + 1e-10) if len(returns) > 1 else 1.0
                volatility_tolerance = min(1.0, volatility_tolerance / 10)
                
                # æŒç»­æ—¶é—´è€å—æ€§
                negative_periods = np.sum(returns < 0) / len(returns) if len(returns) > 0 else 0.0
                duration_tolerance = 1.0 - negative_periods
                
                # ç»¼åˆé£é™©æ‰¿å—èƒ½åŠ›
                risk_capacity = (volatility_tolerance * 0.4 + 
                               duration_tolerance * 0.3 + 
                               (1.0 - min(1.0, max_loss * 10)) * 0.3)
                
            else:
                risk_capacity = 0.5  # é»˜è®¤ä¸­ç­‰é£é™©æ‰¿å—èƒ½åŠ›
            
            # å½“å‰é£é™©åˆ©ç”¨ç‡
            current_risk = np.std(returns) if len(data) > 1 else 0.0
            capacity_utilization = min(1.0, current_risk / (risk_capacity + 1e-10))
            
            return {
                'risk_capacity': risk_capacity,
                'capacity_utilization': capacity_utilization,
                'available_capacity': max(0.0, risk_capacity - current_risk),
                'capacity_level': 'high' if risk_capacity > 0.7 else 
                                'medium' if risk_capacity > 0.4 else 'low'
            }
            
        except Exception:
            return {'risk_capacity': 0.5, 'capacity_utilization': 0.5}
    
    def _check_risk_tolerance_alignment(self, risk_score: float) -> Dict[str, Any]:
        """æ£€æŸ¥é£é™©å®¹å¿åº¦å¯¹é½"""
        try:
            target_risk_tolerance = self.config.get('risk_tolerance', 0.3)
            
            # é£é™©å¯¹é½åº¦
            risk_alignment = 1.0 - abs(risk_score - target_risk_tolerance)
            
            # å¯¹é½çŠ¶æ€
            if abs(risk_score - target_risk_tolerance) < 0.1:
                alignment_status = 'well_aligned'
            elif risk_score > target_risk_tolerance:
                alignment_status = 'risk_too_high'
            else:
                alignment_status = 'risk_too_low'
            
            # å»ºè®®è°ƒæ•´
            if risk_score > target_risk_tolerance + 0.1:
                recommendation = 'reduce_risk_exposure'
            elif risk_score < target_risk_tolerance - 0.1:
                recommendation = 'increase_risk_exposure'
            else:
                recommendation = 'maintain_current_level'
            
            return {
                'risk_alignment': risk_alignment,
                'alignment_status': alignment_status,
                'target_tolerance': target_risk_tolerance,
                'current_risk': risk_score,
                'risk_gap': risk_score - target_risk_tolerance,
                'recommendation': recommendation
            }
            
        except Exception:
            return {'risk_alignment': 0.5, 'alignment_status': 'unknown'}
        
# ========== ä¿¡å·èåˆå…·ä½“å®ç° ==========
    
    def _fuse_signals(self, signals: Dict[str, float], weights: Dict[str, float]) -> Dict[str, float]:
        """èåˆå¤šä¸ªä¿¡å·"""
        try:
            if not signals:
                return {'consensus_score': 0.0, 'confidence': 0.0}
            
            # å½’ä¸€åŒ–æƒé‡
            total_weight = sum(weights.get(key, 0.0) for key in signals.keys())
            if total_weight == 0:
                # ç­‰æƒé‡å¤„ç†
                normalized_weights = {key: 1.0/len(signals) for key in signals.keys()}
            else:
                normalized_weights = {key: weights.get(key, 0.0)/total_weight for key in signals.keys()}
            
            # åŠ æƒèåˆ
            consensus_score = sum(signals[key] * normalized_weights[key] for key in signals.keys())
            
            # è®¡ç®—èåˆç½®ä¿¡åº¦
            signal_values = list(signals.values())
            
            # ä¿¡å·ä¸€è‡´æ€§
            if len(signal_values) > 1:
                signal_std = np.std(signal_values)
                signal_mean = np.mean(signal_values)
                consistency = 1.0 - min(1.0, signal_std / (abs(signal_mean) + 1e-10))
            else:
                consistency = 1.0
            
            # ä¿¡å·å¼ºåº¦
            signal_strength = np.mean([abs(val) for val in signal_values])
            
            # ä¿¡å·æ•°é‡å› å­
            signal_count_factor = min(1.0, len(signals) / 5.0)
            
            # ç»¼åˆç½®ä¿¡åº¦
            confidence = (consistency * 0.4 + 
                         signal_strength * 0.35 + 
                         signal_count_factor * 0.25)
            
            return {
                'consensus_score': consensus_score,
                'confidence': confidence,
                'consistency': consistency,
                'signal_strength': signal_strength,
                'signal_count': len(signals)
            }
            
        except Exception as e:
            self.logger.error(f"ä¿¡å·èåˆé”™è¯¯: {str(e)}")
            return {'consensus_score': 0.0, 'confidence': 0.0}
    
    def _assess_signal_quality_comprehensive(self, signals: Dict[str, float]) -> float:
        """ç»¼åˆè¯„ä¼°ä¿¡å·è´¨é‡"""
        try:
            if not signals:
                return 0.0
            
            quality_factors = []
            
            # ä¿¡å·å¼ºåº¦è´¨é‡
            signal_values = list(signals.values())
            avg_strength = np.mean([abs(val) for val in signal_values])
            strength_quality = min(1.0, avg_strength * 2)
            quality_factors.append(strength_quality)
            
            # ä¿¡å·åˆ†å¸ƒè´¨é‡
            if len(signal_values) > 1:
                value_range = max(signal_values) - min(signal_values)
                distribution_quality = min(1.0, value_range)
                quality_factors.append(distribution_quality)
            
            # ä¿¡å·å®Œæ•´æ€§è´¨é‡
            expected_signals = ['technical', 'wavelet', 'fourier', 'nonlinear', 'ml', 'quantum']
            completeness = len([s for s in expected_signals if s in signals]) / len(expected_signals)
            quality_factors.append(completeness)
            
            # ä¿¡å·å¯é æ€§è´¨é‡
            reliable_signals = sum(1 for val in signal_values if abs(val) > 0.1)
            reliability = reliable_signals / len(signal_values) if signal_values else 0.0
            quality_factors.append(reliability)
            
            return np.mean(quality_factors)
            
        except Exception:
            return 0.0
    
    def _check_signal_consistency(self, signals: Dict[str, float]) -> float:
        """æ£€æŸ¥ä¿¡å·ä¸€è‡´æ€§"""
        try:
            if len(signals) < 2:
                return 1.0
            
            signal_values = list(signals.values())
            
            # æ–¹å‘ä¸€è‡´æ€§
            positive_signals = sum(1 for val in signal_values if val > 0.1)
            negative_signals = sum(1 for val in signal_values if val < -0.1)
            neutral_signals = len(signal_values) - positive_signals - negative_signals
            
            # ä¸»å¯¼æ–¹å‘
            if positive_signals > negative_signals:
                dominant_direction = positive_signals
                total_directional = positive_signals + negative_signals
            else:
                dominant_direction = negative_signals
                total_directional = positive_signals + negative_signals
            
            if total_directional > 0:
                direction_consistency = dominant_direction / total_directional
            else:
                direction_consistency = 0.0
            
            # å¹…åº¦ä¸€è‡´æ€§
            if len(signal_values) > 1:
                cv = np.std(signal_values) / (np.mean(np.abs(signal_values)) + 1e-10)
                magnitude_consistency = 1.0 / (1.0 + cv)
            else:
                magnitude_consistency = 1.0
            
            # ç»¼åˆä¸€è‡´æ€§
            overall_consistency = (direction_consistency * 0.6 + magnitude_consistency * 0.4)
            
            return overall_consistency
            
        except Exception:
            return 0.0
    
    def _assess_signal_reliability(self, signals: Dict[str, float], weights: Dict[str, float]) -> float:
        """è¯„ä¼°ä¿¡å·å¯é æ€§"""
        try:
            if not signals:
                return 0.0
            
            reliability_scores = []
            
            for signal_name, signal_value in signals.items():
                # åŸºäºå†å²æ€§èƒ½çš„å¯é æ€§
                historical_reliability = self._get_signal_historical_reliability(signal_name)
                
                # åŸºäºä¿¡å·å¼ºåº¦çš„å¯é æ€§
                strength_reliability = min(1.0, abs(signal_value) * 2)
                
                # åŸºäºæƒé‡çš„å¯é æ€§
                weight_reliability = weights.get(signal_name, 0.0)
                
                # ç»¼åˆå¯é æ€§
                signal_reliability = (historical_reliability * 0.4 + 
                                    strength_reliability * 0.3 + 
                                    weight_reliability * 0.3)
                
                reliability_scores.append(signal_reliability)
            
            return np.mean(reliability_scores) if reliability_scores else 0.0
            
        except Exception:
            return 0.0
    
    def _get_signal_historical_reliability(self, signal_name: str) -> float:
        """è·å–ä¿¡å·å†å²å¯é æ€§"""
        try:
            # ç®€åŒ–å®ç°ï¼šåŸºäºä¿¡å·ç±»å‹çš„é¢„è®¾å¯é æ€§
            reliability_map = {
                'technical': 0.7,
                'wavelet': 0.8,
                'fourier': 0.75,
                'nonlinear': 0.6,
                'ml': 0.85,
                'dl': 0.8,
                'quantum': 0.65,
                'microstructure': 0.7,
                'kalman': 0.75,
                'pattern': 0.8
            }
            
            return reliability_map.get(signal_name, 0.5)
            
        except Exception:
            return 0.5
    
    def _adapt_signal_weights(self, signals: Dict[str, float], 
                            weights: Dict[str, float], tail: int) -> Dict[str, float]:
        """è‡ªé€‚åº”è°ƒæ•´ä¿¡å·æƒé‡"""
        try:
            adapted_weights = weights.copy()
            
            # åŸºäºä¿¡å·å¼ºåº¦è°ƒæ•´
            signal_strengths = {name: abs(value) for name, value in signals.items()}
            max_strength = max(signal_strengths.values()) if signal_strengths else 1.0
            
            if max_strength > 0:
                for signal_name in signals.keys():
                    if signal_name in adapted_weights:
                        # å¼ºä¿¡å·å¢åŠ æƒé‡
                        strength_factor = signal_strengths[signal_name] / max_strength
                        adaptation = self.config['adaptation_factor'] * strength_factor
                        adapted_weights[signal_name] *= (1.0 + adaptation)
            
            # åŸºäºå†å²æ€§èƒ½è°ƒæ•´
            for signal_name in signals.keys():
                if signal_name in adapted_weights:
                    historical_performance = self._get_signal_performance(signal_name, tail)
                    performance_factor = historical_performance - 0.5  # åç¦»ä¸­æ€§çš„ç¨‹åº¦
                    adaptation = self.config['learning_rate'] * performance_factor
                    adapted_weights[signal_name] *= (1.0 + adaptation)
            
            # é‡æ–°å½’ä¸€åŒ–æƒé‡
            total_weight = sum(adapted_weights.values())
            if total_weight > 0:
                adapted_weights = {name: weight/total_weight 
                                 for name, weight in adapted_weights.items()}
            
            return adapted_weights
            
        except Exception:
            return weights
    
    def _get_signal_performance(self, signal_name: str, tail: int) -> float:
        """è·å–ä¿¡å·å†å²æ€§èƒ½"""
        try:
            # ä»æ€§èƒ½ç›‘æ§ä¸­è·å–å†å²æ•°æ®
            if tail in self.performance_monitor['signal_qualities']:
                signal_history = self.performance_monitor['signal_qualities'][tail]
                if signal_history:
                    return np.mean(signal_history)
            
            return 0.5  # é»˜è®¤ä¸­æ€§æ€§èƒ½
            
        except Exception:
            return 0.5
    
    # ========== é«˜çº§æ¨¡å¼è¯†åˆ«å…·ä½“å®ç° ==========
    
    def _detect_double_top_advanced(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é«˜çº§åŒé¡¶æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 20:
                return None
            
            # å¯»æ‰¾å³°å€¼
            peaks, properties = find_peaks(tail_data, 
                                         height=np.mean(tail_data) + 0.5*np.std(tail_data),
                                         distance=5)
            
            if len(peaks) < 2:
                return None
            
            # æ£€æŸ¥æœ€è¿‘çš„ä¸¤ä¸ªå³°å€¼
            recent_peaks = peaks[-2:]
            peak_heights = tail_data[recent_peaks]
            
            # åŒé¡¶æ¡ä»¶
            height_similarity = 1.0 - abs(peak_heights[0] - peak_heights[1]) / max(peak_heights)
            
            if height_similarity > 0.8:  # é«˜åº¦ç›¸ä¼¼
                # æ£€æŸ¥ä¸­é—´è°·å€¼
                valley_region = tail_data[recent_peaks[0]:recent_peaks[1]]
                if len(valley_region) > 0:
                    min_valley = np.min(valley_region)
                    valley_depth = min(peak_heights) - min_valley
                    
                    if valley_depth > 0.2 * np.std(tail_data):  # è¶³å¤Ÿæ·±çš„è°·å€¼
                        return {
                            'pattern_type': 'double_top',
                            'confidence': height_similarity,
                            'peak_positions': recent_peaks.tolist(),
                            'peak_heights': peak_heights.tolist(),
                            'valley_depth': valley_depth,
                            'reversal_target': min_valley
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_double_bottom_advanced(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é«˜çº§åŒåº•æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 20:
                return None
            
            # åè½¬æ•°æ®æ¥å¯»æ‰¾è°·å€¼ï¼ˆä½œä¸ºå³°å€¼ï¼‰
            inverted_data = -tail_data
            peaks, properties = find_peaks(inverted_data, 
                                         height=np.mean(inverted_data) + 0.5*np.std(inverted_data),
                                         distance=5)
            
            if len(peaks) < 2:
                return None
            
            # æ£€æŸ¥æœ€è¿‘çš„ä¸¤ä¸ªè°·å€¼
            recent_troughs = peaks[-2:]
            trough_depths = tail_data[recent_troughs]
            
            # åŒåº•æ¡ä»¶
            depth_similarity = 1.0 - abs(trough_depths[0] - trough_depths[1]) / (max(abs(trough_depths)) + 1e-10)
            
            if depth_similarity > 0.8:  # æ·±åº¦ç›¸ä¼¼
                # æ£€æŸ¥ä¸­é—´å³°å€¼
                peak_region = tail_data[recent_troughs[0]:recent_troughs[1]]
                if len(peak_region) > 0:
                    max_peak = np.max(peak_region)
                    peak_height = max_peak - max(trough_depths)
                    
                    if peak_height > 0.2 * np.std(tail_data):  # è¶³å¤Ÿé«˜çš„å³°å€¼
                        return {
                            'pattern_type': 'double_bottom',
                            'confidence': depth_similarity,
                            'trough_positions': recent_troughs.tolist(),
                            'trough_depths': trough_depths.tolist(),
                            'peak_height': peak_height,
                            'reversal_target': max_peak
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_head_shoulders_advanced(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é«˜çº§å¤´è‚©é¡¶æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 30:
                return None
            
            # å¯»æ‰¾ä¸‰ä¸ªä¸»è¦å³°å€¼
            peaks, properties = find_peaks(tail_data, 
                                         height=np.mean(tail_data) + 0.3*np.std(tail_data),
                                         distance=5)
            
            if len(peaks) < 3:
                return None
            
            # æ£€æŸ¥æœ€è¿‘çš„ä¸‰ä¸ªå³°å€¼
            recent_peaks = peaks[-3:]
            peak_heights = tail_data[recent_peaks]
            
            # å¤´è‚©é¡¶æ¡ä»¶ï¼šä¸­é—´å³°å€¼æœ€é«˜ï¼Œä¸¤ä¾§å³°å€¼ç›¸è¿‘
            if peak_heights[1] > peak_heights[0] and peak_heights[1] > peak_heights[2]:
                # æ£€æŸ¥è‚©éƒ¨é«˜åº¦ç›¸ä¼¼æ€§
                shoulder_similarity = 1.0 - abs(peak_heights[0] - peak_heights[2]) / max(peak_heights[0], peak_heights[2])
                
                if shoulder_similarity > 0.7:
                    # è®¡ç®—é¢ˆçº¿
                    left_valley_idx = recent_peaks[0] + np.argmin(tail_data[recent_peaks[0]:recent_peaks[1]])
                    right_valley_idx = recent_peaks[1] + np.argmin(tail_data[recent_peaks[1]:recent_peaks[2]])
                    
                    neckline_level = np.mean([tail_data[left_valley_idx], tail_data[right_valley_idx]])
                    
                    return {
                        'pattern_type': 'head_shoulders',
                        'confidence': shoulder_similarity,
                        'head_position': recent_peaks[1],
                        'head_height': peak_heights[1],
                        'shoulder_positions': [recent_peaks[0], recent_peaks[2]],
                        'shoulder_heights': [peak_heights[0], peak_heights[2]],
                        'neckline_level': neckline_level,
                        'reversal_target': neckline_level - (peak_heights[1] - neckline_level)
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_inverse_head_shoulders(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """åå‘å¤´è‚©åº•æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 30:
                return None
            
            # åè½¬æ•°æ®æ¥å¯»æ‰¾è°·å€¼
            inverted_data = -tail_data
            result = self._detect_head_shoulders_advanced(tail, np.column_stack([inverted_data]))
            
            if result:
                result['pattern_type'] = 'inverse_head_shoulders'
                # è°ƒæ•´åè½¬ç›®æ ‡
                if 'reversal_target' in result:
                    result['reversal_target'] = -result['reversal_target']
            
            return result
            
        except Exception:
            return None
    
    def _detect_triple_top(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸‰é‡é¡¶æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 30:
                return None
            
            peaks, _ = find_peaks(tail_data, 
                                height=np.mean(tail_data) + 0.5*np.std(tail_data),
                                distance=5)
            
            if len(peaks) < 3:
                return None
            
            recent_peaks = peaks[-3:]
            peak_heights = tail_data[recent_peaks]
            
            # ä¸‰é‡é¡¶æ¡ä»¶ï¼šä¸‰ä¸ªå³°å€¼é«˜åº¦ç›¸è¿‘
            max_height = np.max(peak_heights)
            min_height = np.min(peak_heights)
            
            height_consistency = 1.0 - (max_height - min_height) / (max_height + 1e-10)
            
            if height_consistency > 0.85:
                return {
                    'pattern_type': 'triple_top',
                    'confidence': height_consistency,
                    'peak_positions': recent_peaks.tolist(),
                    'peak_heights': peak_heights.tolist(),
                    'average_height': np.mean(peak_heights)
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_triple_bottom(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸‰é‡åº•æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            inverted_data = -tail_data
            result = self._detect_triple_top(tail, np.column_stack([inverted_data]))
            
            if result:
                result['pattern_type'] = 'triple_bottom'
            
            return result
            
        except Exception:
            return None
    
    def _detect_ascending_triangle(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸Šå‡ä¸‰è§’å½¢æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 20:
                return None
            
            # å¯»æ‰¾é«˜ç‚¹å’Œä½ç‚¹
            peaks, _ = find_peaks(tail_data, distance=3)
            troughs, _ = find_peaks(-tail_data, distance=3)
            
            if len(peaks) < 3 or len(troughs) < 2:
                return None
            
            # æ£€æŸ¥é«˜ç‚¹æ˜¯å¦æ°´å¹³ï¼ˆé˜»åŠ›çº¿ï¼‰
            recent_peaks = peaks[-3:]
            peak_heights = tail_data[recent_peaks]
            peak_trend = np.polyfit(recent_peaks, peak_heights, 1)[0]
            
            # æ£€æŸ¥ä½ç‚¹æ˜¯å¦ä¸Šå‡ï¼ˆæ”¯æ’‘çº¿ï¼‰
            recent_troughs = troughs[-2:]
            trough_depths = tail_data[recent_troughs]
            trough_trend = np.polyfit(recent_troughs, trough_depths, 1)[0]
            
            if abs(peak_trend) < 0.01 and trough_trend > 0.01:  # æ°´å¹³é˜»åŠ›ï¼Œä¸Šå‡æ”¯æ’‘
                convergence = (np.mean(peak_heights) - np.mean(trough_depths)) / len(tail_data)
                
                return {
                    'pattern_type': 'ascending_triangle',
                    'confidence': min(1.0, trough_trend * 100),
                    'resistance_level': np.mean(peak_heights),
                    'support_trend': trough_trend,
                    'convergence_rate': convergence
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_descending_triangle(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸‹é™ä¸‰è§’å½¢æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 20:
                return None
            
            peaks, _ = find_peaks(tail_data, distance=3)
            troughs, _ = find_peaks(-tail_data, distance=3)
            
            if len(peaks) < 2 or len(troughs) < 3:
                return None
            
            # æ£€æŸ¥ä½ç‚¹æ˜¯å¦æ°´å¹³ï¼ˆæ”¯æ’‘çº¿ï¼‰
            recent_troughs = troughs[-3:]
            trough_depths = tail_data[recent_troughs]
            trough_trend = np.polyfit(recent_troughs, trough_depths, 1)[0]
            
            # æ£€æŸ¥é«˜ç‚¹æ˜¯å¦ä¸‹é™ï¼ˆé˜»åŠ›çº¿ï¼‰
            recent_peaks = peaks[-2:]
            peak_heights = tail_data[recent_peaks]
            peak_trend = np.polyfit(recent_peaks, peak_heights, 1)[0]
            
            if abs(trough_trend) < 0.01 and peak_trend < -0.01:  # æ°´å¹³æ”¯æ’‘ï¼Œä¸‹é™é˜»åŠ›
                convergence = (np.mean(peak_heights) - np.mean(trough_depths)) / len(tail_data)
                
                return {
                    'pattern_type': 'descending_triangle',
                    'confidence': min(1.0, abs(peak_trend) * 100),
                    'support_level': np.mean(trough_depths),
                    'resistance_trend': peak_trend,
                    'convergence_rate': convergence
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_symmetric_triangle(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """å¯¹ç§°ä¸‰è§’å½¢æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 20:
                return None
            
            peaks, _ = find_peaks(tail_data, distance=3)
            troughs, _ = find_peaks(-tail_data, distance=3)
            
            if len(peaks) < 2 or len(troughs) < 2:
                return None
            
            # æ£€æŸ¥é«˜ç‚¹ä¸‹é™è¶‹åŠ¿
            recent_peaks = peaks[-2:]
            peak_heights = tail_data[recent_peaks]
            peak_trend = np.polyfit(recent_peaks, peak_heights, 1)[0]
            
            # æ£€æŸ¥ä½ç‚¹ä¸Šå‡è¶‹åŠ¿
            recent_troughs = troughs[-2:]
            trough_depths = tail_data[recent_troughs]
            trough_trend = np.polyfit(recent_troughs, trough_depths, 1)[0]
            
            if peak_trend < -0.01 and trough_trend > 0.01:  # æ”¶æ•›è¶‹åŠ¿
                convergence_rate = abs(peak_trend) + abs(trough_trend)
                symmetry = 1.0 - abs(abs(peak_trend) - abs(trough_trend)) / (abs(peak_trend) + abs(trough_trend))
                
                return {
                    'pattern_type': 'symmetric_triangle',
                    'confidence': symmetry,
                    'upper_trend': peak_trend,
                    'lower_trend': trough_trend,
                    'convergence_rate': convergence_rate,
                    'apex_estimate': len(tail_data) + (np.mean(peak_heights) - np.mean(trough_depths)) / convergence_rate
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_rising_wedge(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸Šå‡æ¥”å½¢æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 15:
                return None
            
            peaks, _ = find_peaks(tail_data, distance=3)
            troughs, _ = find_peaks(-tail_data, distance=3)
            
            if len(peaks) < 2 or len(troughs) < 2:
                return None
            
            # æ£€æŸ¥æ•´ä½“ä¸Šå‡è¶‹åŠ¿
            overall_trend = np.polyfit(range(len(tail_data)), tail_data, 1)[0]
            
            if overall_trend > 0:
                # æ£€æŸ¥é«˜ç‚¹å’Œä½ç‚¹éƒ½åœ¨ä¸Šå‡ï¼Œä½†é«˜ç‚¹ä¸Šå‡æ›´æ…¢
                peak_trend = np.polyfit(peaks[-2:], tail_data[peaks[-2:]], 1)[0]
                trough_trend = np.polyfit(troughs[-2:], tail_data[troughs[-2:]], 1)[0]
                
                if peak_trend > 0 and trough_trend > 0 and trough_trend > peak_trend:
                    convergence = trough_trend - peak_trend
                    
                    return {
                        'pattern_type': 'rising_wedge',
                        'confidence': min(1.0, convergence * 100),
                        'upper_trend': peak_trend,
                        'lower_trend': trough_trend,
                        'convergence_angle': convergence
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_falling_wedge(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸‹é™æ¥”å½¢æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 15:
                return None
            
            peaks, _ = find_peaks(tail_data, distance=3)
            troughs, _ = find_peaks(-tail_data, distance=3)
            
            if len(peaks) < 2 or len(troughs) < 2:
                return None
            
            # æ£€æŸ¥æ•´ä½“ä¸‹é™è¶‹åŠ¿
            overall_trend = np.polyfit(range(len(tail_data)), tail_data, 1)[0]
            
            if overall_trend < 0:
                # æ£€æŸ¥é«˜ç‚¹å’Œä½ç‚¹éƒ½åœ¨ä¸‹é™ï¼Œä½†ä½ç‚¹ä¸‹é™æ›´æ…¢
                peak_trend = np.polyfit(peaks[-2:], tail_data[peaks[-2:]], 1)[0]
                trough_trend = np.polyfit(troughs[-2:], tail_data[troughs[-2:]], 1)[0]
                
                if peak_trend < 0 and trough_trend < 0 and abs(peak_trend) > abs(trough_trend):
                    convergence = abs(peak_trend) - abs(trough_trend)
                    
                    return {
                        'pattern_type': 'falling_wedge',
                        'confidence': min(1.0, convergence * 100),
                        'upper_trend': peak_trend,
                        'lower_trend': trough_trend,
                        'convergence_angle': convergence
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_flag_advanced(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é«˜çº§æ——å½¢æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 15:
                return None
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼ºçƒˆçš„å…ˆå¯¼è¶‹åŠ¿ï¼ˆæ——æ†ï¼‰
            if len(tail_data) >= 10:
                flagpole_data = tail_data[-10:-5]  # æ——æ†éƒ¨åˆ†
                flag_data = tail_data[-5:]         # æ——å¸œéƒ¨åˆ†
                
                if len(flagpole_data) > 1 and len(flag_data) > 1:
                    flagpole_trend = np.polyfit(range(len(flagpole_data)), flagpole_data, 1)[0]
                    flag_trend = np.polyfit(range(len(flag_data)), flag_data, 1)[0]
                    flag_volatility = np.std(flag_data)
                    
                    # æ——å½¢æ¡ä»¶ï¼šå¼ºçƒˆæ——æ† + æ¨ªå‘æ•´ç†
                    if abs(flagpole_trend) > 0.05 and abs(flag_trend) < 0.02 and flag_volatility < 0.1:
                        flag_direction = 'bull_flag' if flagpole_trend > 0 else 'bear_flag'
                        
                        return {
                            'pattern_type': flag_direction,
                            'confidence': abs(flagpole_trend) * 10,
                            'flagpole_strength': abs(flagpole_trend),
                            'flag_consolidation': 1.0 - abs(flag_trend),
                            'breakout_target': tail_data[-1] + flagpole_trend * 5
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_pennant(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸‰è§’æ——æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 15:
                return None
            
            # ç±»ä¼¼æ——å½¢ï¼Œä½†æ•´ç†éƒ¨åˆ†æ˜¯æ”¶æ•›çš„ä¸‰è§’å½¢
            flagpole_data = tail_data[-10:-5]
            pennant_data = tail_data[-5:]
            
            if len(flagpole_data) > 1 and len(pennant_data) > 2:
                flagpole_trend = np.polyfit(range(len(flagpole_data)), flagpole_data, 1)[0]
                
                # æ£€æŸ¥ä¸‰è§’æ——çš„æ”¶æ•›æ€§
                pennant_range_start = max(pennant_data) - min(pennant_data)
                pennant_range_end = abs(pennant_data[-1] - pennant_data[-2])
                
                if pennant_range_start > 0:
                    convergence = 1.0 - pennant_range_end / pennant_range_start
                    
                    if abs(flagpole_trend) > 0.05 and convergence > 0.5:
                        pennant_type = 'bull_pennant' if flagpole_trend > 0 else 'bear_pennant'
                        
                        return {
                            'pattern_type': pennant_type,
                            'confidence': convergence,
                            'flagpole_strength': abs(flagpole_trend),
                            'convergence_rate': convergence,
                            'breakout_target': tail_data[-1] + flagpole_trend * 5
                        }
            
            return None
            
        except Exception:
            return None
        
# ========== æ›´å¤šé«˜çº§æ¨¡å¼è¯†åˆ« ==========
    
    def _detect_cup_handle(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ¯æŸ„å½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 30:
                return None
            
            # å¯»æ‰¾æ¯å­éƒ¨åˆ†ï¼ˆUå½¢åº•éƒ¨ï¼‰
            cup_data = tail_data[-25:-5]  # æ¯å­éƒ¨åˆ†
            handle_data = tail_data[-5:]   # æ‰‹æŸ„éƒ¨åˆ†
            
            if len(cup_data) < 15:
                return None
            
            # æ£€æŸ¥æ¯å­çš„Uå½¢ç‰¹å¾
            cup_start = cup_data[0]
            cup_end = cup_data[-1]
            cup_bottom = np.min(cup_data)
            cup_bottom_idx = np.argmin(cup_data)
            
            # Uå½¢æ¡ä»¶ï¼šä¸¤ç«¯é«˜åº¦ç›¸è¿‘ï¼Œä¸­é—´æœ‰æ˜æ˜¾ä½ç‚¹
            rim_similarity = 1.0 - abs(cup_start - cup_end) / (max(cup_start, cup_end) + 1e-10)
            depth = min(cup_start, cup_end) - cup_bottom
            
            if rim_similarity > 0.9 and depth > 0.1 * np.std(tail_data):
                # æ£€æŸ¥æ‰‹æŸ„çš„è½»å¾®ä¸‹å€¾
                if len(handle_data) > 2:
                    handle_trend = np.polyfit(range(len(handle_data)), handle_data, 1)[0]
                    handle_decline = handle_trend < 0 and abs(handle_trend) < 0.05
                    
                    if handle_decline:
                        return {
                            'pattern_type': 'cup_handle',
                            'confidence': rim_similarity,
                            'cup_depth': depth,
                            'rim_level': (cup_start + cup_end) / 2,
                            'handle_decline': abs(handle_trend),
                            'breakout_target': max(cup_start, cup_end) + depth
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_rounding_top(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """åœ†é¡¶å½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 20:
                return None
            
            # æ£€æŸ¥å€’Uå½¢ç‰¹å¾
            data_len = len(tail_data)
            if data_len < 10:
                return None
            
            # æ‹ŸåˆäºŒæ¬¡å‡½æ•°
            x = np.arange(data_len)
            coeffs = np.polyfit(x, tail_data, 2)
            
            # äºŒæ¬¡é¡¹ç³»æ•°ä¸ºè´Ÿè¡¨ç¤ºå€’Uå½¢
            if coeffs[0] < -0.001:  # è¶³å¤Ÿæ˜æ˜¾çš„æ›²ç‡
                # è®¡ç®—æ‹Ÿåˆä¼˜åº¦
                fitted_curve = np.polyval(coeffs, x)
                r_squared = 1 - np.sum((tail_data - fitted_curve)**2) / np.sum((tail_data - np.mean(tail_data))**2)
                
                if r_squared > 0.7:  # è‰¯å¥½æ‹Ÿåˆ
                    peak_x = -coeffs[1] / (2 * coeffs[0])
                    peak_y = np.polyval(coeffs, peak_x)
                    
                    return {
                        'pattern_type': 'rounding_top',
                        'confidence': r_squared,
                        'curvature': abs(coeffs[0]),
                        'peak_position': peak_x,
                        'peak_value': peak_y,
                        'symmetry': self._calculate_pattern_symmetry(tail_data, int(peak_x))
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_rounding_bottom(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """åœ†åº•å½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 20:
                return None
            
            # æ£€æŸ¥Uå½¢ç‰¹å¾
            data_len = len(tail_data)
            x = np.arange(data_len)
            coeffs = np.polyfit(x, tail_data, 2)
            
            # äºŒæ¬¡é¡¹ç³»æ•°ä¸ºæ­£è¡¨ç¤ºUå½¢
            if coeffs[0] > 0.001:  # è¶³å¤Ÿæ˜æ˜¾çš„æ›²ç‡
                fitted_curve = np.polyval(coeffs, x)
                r_squared = 1 - np.sum((tail_data - fitted_curve)**2) / np.sum((tail_data - np.mean(tail_data))**2)
                
                if r_squared > 0.7:  # è‰¯å¥½æ‹Ÿåˆ
                    trough_x = -coeffs[1] / (2 * coeffs[0])
                    trough_y = np.polyval(coeffs, trough_x)
                    
                    return {
                        'pattern_type': 'rounding_bottom',
                        'confidence': r_squared,
                        'curvature': coeffs[0],
                        'trough_position': trough_x,
                        'trough_value': trough_y,
                        'symmetry': self._calculate_pattern_symmetry(tail_data, int(trough_x))
                    }
            
            return None
            
        except Exception:
            return None
    
    def _calculate_pattern_symmetry(self, data: np.ndarray, center_idx: int) -> float:
        """è®¡ç®—æ¨¡å¼çš„å¯¹ç§°æ€§"""
        try:
            if center_idx <= 0 or center_idx >= len(data) - 1:
                return 0.0
            
            left_part = data[:center_idx]
            right_part = data[center_idx+1:]
            
            # å–è¾ƒçŸ­çš„ä¸€è¾¹è¿›è¡Œæ¯”è¾ƒ
            min_len = min(len(left_part), len(right_part))
            if min_len == 0:
                return 0.0
            
            left_compare = left_part[-min_len:]
            right_compare = right_part[:min_len]
            
            # è®¡ç®—å¯¹ç§°æ€§
            differences = np.abs(left_compare - right_compare[::-1])
            max_possible_diff = np.max(data) - np.min(data)
            
            if max_possible_diff > 0:
                symmetry = 1.0 - np.mean(differences) / max_possible_diff
            else:
                symmetry = 1.0
            
            return max(0.0, symmetry)
            
        except Exception:
            return 0.0
    
    # ========== æ—¥æœ¬èœ¡çƒ›å›¾æ¨¡å¼ ==========
    
    def _detect_doji(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """åå­—æ˜Ÿ(Doji)æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 3:
                return None
            
            # ç®€åŒ–ï¼šæ£€æŸ¥æœ€è¿‘å‡ æœŸçš„æ³¢åŠ¨æ€§
            recent_data = tail_data[-3:]
            current_value = recent_data[-1]
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å‡å€¼é™„è¿‘ï¼ˆåå­—æ˜Ÿç‰¹å¾ï¼‰
            mean_value = np.mean(recent_data)
            std_value = np.std(recent_data)
            
            if std_value > 0 and abs(current_value - mean_value) < 0.3 * std_value:
                # æ£€æŸ¥å‰æœŸæ˜¯å¦æœ‰æ˜æ˜¾è¶‹åŠ¿
                if len(tail_data) >= 5:
                    trend_data = tail_data[-5:-1]
                    trend = np.polyfit(range(len(trend_data)), trend_data, 1)[0]
                    
                    if abs(trend) > 0.02:  # æœ‰æ˜æ˜¾è¶‹åŠ¿
                        return {
                            'pattern_type': 'doji',
                            'confidence': 1.0 - abs(current_value - mean_value) / (std_value + 1e-10),
                            'trend_before': 'uptrend' if trend > 0 else 'downtrend',
                            'reversal_signal': True
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_hammer(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é”¤å¤´çº¿æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            current = tail_data[-1]
            previous = tail_data[-2]
            
            # é”¤å¤´ç‰¹å¾ï¼šä¸‹è·Œåçš„åè½¬ä¿¡å·
            if len(tail_data) >= 5:
                trend_data = tail_data[-5:-1]
                trend = np.polyfit(range(len(trend_data)), trend_data, 1)[0]
                
                # ä¸‹è·Œè¶‹åŠ¿ä¸­å‡ºç°åå¼¹
                if trend < -0.01 and current > previous:
                    reversal_strength = (current - previous) / (np.std(trend_data) + 1e-10)
                    
                    if reversal_strength > 1.0:  # å¼ºçƒˆåå¼¹
                        return {
                            'pattern_type': 'hammer',
                            'confidence': min(1.0, reversal_strength / 2),
                            'reversal_strength': reversal_strength,
                            'trend_before': 'downtrend'
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_hanging_man(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸ŠåŠçº¿æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            current = tail_data[-1]
            previous = tail_data[-2]
            
            # ä¸ŠåŠçº¿ç‰¹å¾ï¼šä¸Šæ¶¨åçš„åè½¬ä¿¡å·
            if len(tail_data) >= 5:
                trend_data = tail_data[-5:-1]
                trend = np.polyfit(range(len(trend_data)), trend_data, 1)[0]
                
                # ä¸Šæ¶¨è¶‹åŠ¿ä¸­å‡ºç°å›è½
                if trend > 0.01 and current < previous:
                    reversal_strength = (previous - current) / (np.std(trend_data) + 1e-10)
                    
                    if reversal_strength > 1.0:  # å¼ºçƒˆå›è½
                        return {
                            'pattern_type': 'hanging_man',
                            'confidence': min(1.0, reversal_strength / 2),
                            'reversal_strength': reversal_strength,
                            'trend_before': 'uptrend'
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_shooting_star(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æµæ˜Ÿçº¿æ£€æµ‹"""
        try:
            # ç®€åŒ–å®ç°ï¼šç±»ä¼¼ä¸ŠåŠçº¿ä½†åœ¨é«˜ä½
            result = self._detect_hanging_man(tail, data_matrix)
            if result:
                result['pattern_type'] = 'shooting_star'
            return result
            
        except Exception:
            return None
    
    def _detect_engulfing_bullish(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """çœ‹æ¶¨åæ²¡å½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 3:
                return None
            
            current = tail_data[-1]
            previous = tail_data[-2]
            before_previous = tail_data[-3]
            
            # çœ‹æ¶¨åæ²¡ï¼šå‰æœŸä¸‹è·Œï¼Œå½“æœŸå¼ºçƒˆä¸Šæ¶¨
            if before_previous > previous and current > before_previous:
                engulfing_strength = (current - previous) / (before_previous - previous + 1e-10)
                
                if engulfing_strength > 1.5:  # å®Œå…¨åæ²¡
                    return {
                        'pattern_type': 'engulfing_bullish',
                        'confidence': min(1.0, engulfing_strength / 3),
                        'engulfing_ratio': engulfing_strength
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_engulfing_bearish(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """çœ‹è·Œåæ²¡å½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 3:
                return None
            
            current = tail_data[-1]
            previous = tail_data[-2]
            before_previous = tail_data[-3]
            
            # çœ‹è·Œåæ²¡ï¼šå‰æœŸä¸Šæ¶¨ï¼Œå½“æœŸå¼ºçƒˆä¸‹è·Œ
            if before_previous < previous and current < before_previous:
                engulfing_strength = (previous - current) / (previous - before_previous + 1e-10)
                
                if engulfing_strength > 1.5:  # å®Œå…¨åæ²¡
                    return {
                        'pattern_type': 'engulfing_bearish',
                        'confidence': min(1.0, engulfing_strength / 3),
                        'engulfing_ratio': engulfing_strength
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_morning_star(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ™¨æ˜Ÿå½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 4:
                return None
            
            # ä¸‰æ ¹Kçº¿å½¢æ€ï¼šä¸‹è·Œ + ä½ä½æ•´ç† + ä¸Šæ¶¨
            third = tail_data[-1]   # å½“å‰
            second = tail_data[-2]  # ä¸­é—´
            first = tail_data[-3]   # ä¹‹å‰
            
            # æ™¨æ˜Ÿæ¡ä»¶
            if first > second and third > second and third > first:
                gap_down = (first - second) / (np.std(tail_data) + 1e-10)
                gap_up = (third - second) / (np.std(tail_data) + 1e-10)
                
                if gap_down > 0.5 and gap_up > 0.5:
                    return {
                        'pattern_type': 'morning_star',
                        'confidence': min(1.0, (gap_down + gap_up) / 4),
                        'star_position': second,
                        'reversal_strength': third - first
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_evening_star(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é»„æ˜æ˜Ÿå½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 4:
                return None
            
            # ä¸‰æ ¹Kçº¿å½¢æ€ï¼šä¸Šæ¶¨ + é«˜ä½æ•´ç† + ä¸‹è·Œ
            third = tail_data[-1]   # å½“å‰
            second = tail_data[-2]  # ä¸­é—´
            first = tail_data[-3]   # ä¹‹å‰
            
            # é»„æ˜æ˜Ÿæ¡ä»¶
            if first < second and third < second and third < first:
                gap_up = (second - first) / (np.std(tail_data) + 1e-10)
                gap_down = (second - third) / (np.std(tail_data) + 1e-10)
                
                if gap_up > 0.5 and gap_down > 0.5:
                    return {
                        'pattern_type': 'evening_star',
                        'confidence': min(1.0, (gap_up + gap_down) / 4),
                        'star_position': second,
                        'reversal_strength': first - third
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_three_white_soldiers(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸‰åªç™½é¸Ÿå½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 4:
                return None
            
            # è¿ç»­ä¸‰ä¸ªä¸Šæ¶¨
            recent_data = tail_data[-3:]
            
            if len(recent_data) == 3:
                increases = [recent_data[i+1] > recent_data[i] for i in range(2)]
                
                if all(increases):
                    # æ£€æŸ¥æ¶¨å¹…çš„ä¸€è‡´æ€§
                    gains = [recent_data[i+1] - recent_data[i] for i in range(2)]
                    gain_consistency = 1.0 - np.std(gains) / (np.mean(gains) + 1e-10)
                    
                    if gain_consistency > 0.7:
                        return {
                            'pattern_type': 'three_white_soldiers',
                            'confidence': gain_consistency,
                            'total_gain': recent_data[-1] - recent_data[0],
                            'average_gain': np.mean(gains)
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_three_black_crows(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸‰åªé»‘é¸¦å½¢æ€æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 4:
                return None
            
            # è¿ç»­ä¸‰ä¸ªä¸‹è·Œ
            recent_data = tail_data[-3:]
            
            if len(recent_data) == 3:
                decreases = [recent_data[i+1] < recent_data[i] for i in range(2)]
                
                if all(decreases):
                    # æ£€æŸ¥è·Œå¹…çš„ä¸€è‡´æ€§
                    losses = [recent_data[i] - recent_data[i+1] for i in range(2)]
                    loss_consistency = 1.0 - np.std(losses) / (np.mean(losses) + 1e-10)
                    
                    if loss_consistency > 0.7:
                        return {
                            'pattern_type': 'three_black_crows',
                            'confidence': loss_consistency,
                            'total_loss': recent_data[0] - recent_data[-1],
                            'average_loss': np.mean(losses)
                        }
            
            return None
            
        except Exception:
            return None
    
    # ========== æ³¢æµªç†è®ºæ¨¡å¼ï¼ˆç®€åŒ–å®ç°ï¼‰ ==========
    
    def _detect_elliott_wave_1(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è‰¾ç•¥ç‰¹æ³¢æµªç¬¬1æµªæ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 10:
                return None
            
            # ç¬¬1æµªç‰¹å¾ï¼šä»åº•éƒ¨å¼€å§‹çš„åˆå§‹ä¸Šæ¶¨
            recent_data = tail_data[-8:]
            
            # å¯»æ‰¾ä½ç‚¹å’Œé«˜ç‚¹
            min_idx = np.argmin(recent_data)
            max_idx = np.argmax(recent_data)
            
            # ç¬¬1æµªï¼šä½ç‚¹åœ¨å‰ï¼Œé«˜ç‚¹åœ¨å
            if min_idx < max_idx and max_idx - min_idx >= 3:
                wave_data = recent_data[min_idx:max_idx+1]
                wave_trend = np.polyfit(range(len(wave_data)), wave_data, 1)[0]
                
                if wave_trend > 0.02:  # æ˜æ˜¾ä¸Šå‡è¶‹åŠ¿
                    wave_strength = (recent_data[max_idx] - recent_data[min_idx]) / (np.std(recent_data) + 1e-10)
                    
                    return {
                        'pattern_type': 'elliott_wave_1',
                        'confidence': min(1.0, wave_strength / 3),
                        'wave_start': min_idx,
                        'wave_end': max_idx,
                        'wave_strength': wave_strength,
                        'impulse_direction': 'up'
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_elliott_wave_2(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è‰¾ç•¥ç‰¹æ³¢æµªç¬¬2æµªæ£€æµ‹"""
        try:
            # ç¬¬2æµªï¼šå¯¹ç¬¬1æµªçš„è°ƒæ•´ï¼Œé€šå¸¸å›è°ƒ50-78.6%
            wave1_result = self._detect_elliott_wave_1(tail, data_matrix)
            if not wave1_result:
                return None
            
            tail_data = data_matrix[:, tail]
            recent_data = tail_data[-8:]
            
            wave1_end = wave1_result['wave_end']
            if wave1_end < len(recent_data) - 2:
                # æ£€æŸ¥ç¬¬1æµªåçš„å›è°ƒ
                wave2_data = recent_data[wave1_end:]
                if len(wave2_data) >= 2:
                    wave2_trend = np.polyfit(range(len(wave2_data)), wave2_data, 1)[0]
                    
                    if wave2_trend < -0.01:  # æ˜æ˜¾å›è°ƒ
                        wave1_height = recent_data[wave1_result['wave_end']] - recent_data[wave1_result['wave_start']]
                        wave2_decline = recent_data[wave1_end] - recent_data[-1]
                        
                        retracement_ratio = wave2_decline / (wave1_height + 1e-10)
                        
                        if 0.3 <= retracement_ratio <= 0.8:  # å…¸å‹å›è°ƒå¹…åº¦
                            return {
                                'pattern_type': 'elliott_wave_2',
                                'confidence': 1.0 - abs(retracement_ratio - 0.618),
                                'retracement_ratio': retracement_ratio,
                                'corrective_type': 'abc_correction'
                            }
            
            return None
            
        except Exception:
            return None
    
    def _detect_elliott_wave_3(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è‰¾ç•¥ç‰¹æ³¢æµªç¬¬3æµªæ£€æµ‹"""
        try:
            # ç¬¬3æµªï¼šé€šå¸¸æ˜¯æœ€å¼ºçš„æ¨åŠ¨æµª
            wave2_result = self._detect_elliott_wave_2(tail, data_matrix)
            if not wave2_result:
                return None
            
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            # æ£€æŸ¥å½“å‰æ˜¯å¦æœ‰å¼ºçƒˆä¸Šæ¶¨
            recent_data = tail_data[-5:]
            current_trend = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
            
            if current_trend > 0.03:  # æ¯”ç¬¬1æµªæ›´å¼ºçš„ä¸Šæ¶¨
                trend_strength = abs(current_trend)
                
                return {
                    'pattern_type': 'elliott_wave_3',
                    'confidence': min(1.0, trend_strength * 20),
                    'trend_strength': trend_strength,
                    'expected_extension': True  # ç¬¬3æµªé€šå¸¸å»¶é•¿
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_elliott_wave_4(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è‰¾ç•¥ç‰¹æ³¢æµªç¬¬4æµªæ£€æµ‹"""
        try:
            # ç¬¬4æµªï¼šæ¨ªå‘è°ƒæ•´ï¼Œä¸ä¸ç¬¬1æµªé‡å 
            wave3_result = self._detect_elliott_wave_3(tail, data_matrix)
            if not wave3_result:
                return None
            
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            recent_data = tail_data[-5:]
            volatility = np.std(recent_data)
            trend = abs(np.polyfit(range(len(recent_data)), recent_data, 1)[0])
            
            # ç¬¬4æµªç‰¹å¾ï¼šä½æ³¢åŠ¨æ€§ï¼Œæ¨ªå‘æ•´ç†
            if volatility < 0.1 and trend < 0.01:
                return {
                    'pattern_type': 'elliott_wave_4',
                    'confidence': 1.0 - trend * 100,
                    'consolidation_type': 'sideways',
                    'volatility_compression': True
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_elliott_wave_5(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è‰¾ç•¥ç‰¹æ³¢æµªç¬¬5æµªæ£€æµ‹"""
        try:
            # ç¬¬5æµªï¼šæœ€åçš„æ¨åŠ¨æµªï¼Œå¯èƒ½å‡ºç°èƒŒç¦»
            wave4_result = self._detect_elliott_wave_4(tail, data_matrix)
            if not wave4_result:
                return None
            
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            recent_data = tail_data[-5:]
            current_trend = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
            
            if current_trend > 0.02:  # æœ€åçš„ä¸Šæ¶¨
                # æ£€æŸ¥æ˜¯å¦æœ‰èƒŒç¦»è¿¹è±¡
                momentum = recent_data[-1] - recent_data[-3]
                price_momentum = recent_data[-1] - recent_data[0]
                
                divergence = 1.0 - momentum / (price_momentum + 1e-10)
                
                return {
                    'pattern_type': 'elliott_wave_5',
                    'confidence': min(1.0, abs(current_trend) * 30),
                    'trend_strength': current_trend,
                    'divergence_signal': divergence > 0.3,
                    'completion_warning': True
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_corrective_wave_a(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è°ƒæ•´æµªAæ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 8:
                return None
            
            # Aæµªï¼šäº”æµªç»“æŸåçš„ç¬¬ä¸€ä¸ªè°ƒæ•´æµª
            recent_data = tail_data[-6:]
            trend = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
            
            if trend < -0.02:  # æ˜æ˜¾ä¸‹è·Œ
                return {
                    'pattern_type': 'corrective_wave_a',
                    'confidence': min(1.0, abs(trend) * 30),
                    'correction_strength': abs(trend),
                    'correction_type': 'impulse'
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_corrective_wave_b(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è°ƒæ•´æµªBæ£€æµ‹"""
        try:
            # Bæµªï¼šå¯¹Aæµªçš„åå¼¹
            wave_a_result = self._detect_corrective_wave_a(tail, data_matrix)
            if not wave_a_result:
                return None
            
            tail_data = data_matrix[:, tail]
            recent_data = tail_data[-4:]
            
            if len(recent_data) >= 3:
                trend = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
                
                if trend > 0.01:  # åå¼¹
                    return {
                        'pattern_type': 'corrective_wave_b',
                        'confidence': min(1.0, trend * 50),
                        'retracement_type': 'corrective_rally'
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_corrective_wave_c(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è°ƒæ•´æµªCæ£€æµ‹"""
        try:
            # Cæµªï¼šå®Œæˆè°ƒæ•´çš„æœ€åä¸€æµª
            wave_b_result = self._detect_corrective_wave_b(tail, data_matrix)
            if not wave_b_result:
                return None
            
            tail_data = data_matrix[:, tail]
            recent_data = tail_data[-4:]
            
            if len(recent_data) >= 3:
                trend = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
                
                if trend < -0.02:  # æœ€åä¸‹è·Œ
                    return {
                        'pattern_type': 'corrective_wave_c',
                        'confidence': min(1.0, abs(trend) * 30),
                        'completion_signal': True,
                        'new_cycle_preparation': True
                    }
            
            return None
            
        except Exception:
            return None
    
    # ========== å…¶ä»–é«˜çº§æ¨¡å¼ ==========
    
    def _detect_fractal_support(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """åˆ†å½¢æ”¯æ’‘æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            # åˆ†å½¢æ”¯æ’‘ï¼šä¸­å¿ƒä½ç‚¹è¢«ä¸¤ä¾§é«˜ç‚¹åŒ…å›´
            for i in range(2, len(tail_data) - 2):
                center = tail_data[i]
                left1, left2 = tail_data[i-1], tail_data[i-2]
                right1, right2 = tail_data[i+1], tail_data[i+2]
                
                if center < left1 and center < left2 and center < right1 and center < right2:
                    support_strength = min(left1, left2, right1, right2) - center
                    
                    return {
                        'pattern_type': 'fractal_support',
                        'confidence': min(1.0, support_strength * 10),
                        'support_level': center,
                        'support_index': i
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_fractal_resistance(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """åˆ†å½¢é˜»åŠ›æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            # åˆ†å½¢é˜»åŠ›ï¼šä¸­å¿ƒé«˜ç‚¹è¢«ä¸¤ä¾§ä½ç‚¹åŒ…å›´
            for i in range(2, len(tail_data) - 2):
                center = tail_data[i]
                left1, left2 = tail_data[i-1], tail_data[i-2]
                right1, right2 = tail_data[i+1], tail_data[i+2]
                
                if center > left1 and center > left2 and center > right1 and center > right2:
                    resistance_strength = center - max(left1, left2, right1, right2)
                    
                    return {
                        'pattern_type': 'fractal_resistance',
                        'confidence': min(1.0, resistance_strength * 10),
                        'resistance_level': center,
                        'resistance_index': i
                    }
            
            return None
            
        except Exception:
            return None
        
# ========== å‰©ä½™æ¨¡å¼è¯†åˆ«æ–¹æ³• ==========
    
    def _detect_chaos_pattern(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ··æ²Œç†è®ºæ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 10:
                return None
            
            # è®¡ç®—æé›…æ™®è¯ºå¤«æŒ‡æ•°
            lyapunov = self._calculate_lyapunov_exponent(tail_data)
            
            # è®¡ç®—åˆ†å½¢ç»´æ•°
            fractal_dim = self._calculate_fractal_dimension(tail_data)
            
            # æ··æ²Œç‰¹å¾
            if lyapunov > 0.01 and 1.5 < fractal_dim < 2.5:
                chaos_strength = lyapunov * fractal_dim
                
                return {
                    'pattern_type': 'chaos_pattern',
                    'confidence': min(1.0, chaos_strength),
                    'lyapunov_exponent': lyapunov,
                    'fractal_dimension': fractal_dim,
                    'predictability_horizon': 1.0 / lyapunov if lyapunov > 0 else float('inf')
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_mandelbrot_pattern(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ›¼å¾·å¸ƒç½—ç‰¹é›†åˆæ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 8:
                return None
            
            # ç®€åŒ–çš„åˆ†å½¢æ¨¡å¼æ£€æµ‹
            # æ£€æŸ¥è‡ªç›¸ä¼¼æ€§
            data_len = len(tail_data)
            half_len = data_len // 2
            
            if half_len > 2:
                first_half = tail_data[:half_len]
                second_half = tail_data[half_len:half_len*2]
                
                if len(first_half) == len(second_half):
                    # è®¡ç®—ç›¸ä¼¼æ€§
                    correlation = np.corrcoef(first_half, second_half)[0, 1]
                    if not np.isnan(correlation) and correlation > 0.7:
                        return {
                            'pattern_type': 'mandelbrot_pattern',
                            'confidence': correlation,
                            'self_similarity': correlation,
                            'fractal_nature': True
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_quantum_superposition(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é‡å­å åŠ æ€æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            quantum_state = self._construct_quantum_state(tail_data)
            
            # åˆ†æå åŠ æ€ç‰¹å¾
            superposition_analysis = self._analyze_quantum_superposition(quantum_state)
            
            if superposition_analysis['superposition_degree'] > 0.6:
                return {
                    'pattern_type': 'quantum_superposition',
                    'confidence': superposition_analysis['superposition_degree'],
                    'superposition_degree': superposition_analysis['superposition_degree'],
                    'phase_distribution': superposition_analysis['phase_distribution'],
                    'quantum_coherence': self._calculate_quantum_coherence(quantum_state)
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_quantum_entanglement(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é‡å­çº ç¼ æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            quantum_state = self._construct_quantum_state(tail_data)
            
            # è®¡ç®—çº ç¼ ç†µ
            entanglement_entropy = self._calculate_entanglement_entropy(quantum_state)
            
            if entanglement_entropy > 0.5:
                return {
                    'pattern_type': 'quantum_entanglement',
                    'confidence': entanglement_entropy,
                    'entanglement_entropy': entanglement_entropy,
                    'bell_state_similarity': self._calculate_bell_state_similarity(quantum_state)
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_quantum_coherence(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """é‡å­ç›¸å¹²æ€§æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            quantum_state = self._construct_quantum_state(tail_data)
            
            coherence = self._calculate_quantum_coherence(quantum_state)
            
            if coherence > 0.7:
                return {
                    'pattern_type': 'quantum_coherence',
                    'confidence': coherence,
                    'coherence_measure': coherence,
                    'decoherence_analysis': self._analyze_quantum_decoherence(quantum_state)
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_ml_pattern_1(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æœºå™¨å­¦ä¹ å‘ç°çš„æ¨¡å¼1"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 5:
                return None
            
            # ä½¿ç”¨èšç±»ç®—æ³•å‘ç°æ¨¡å¼
            if len(tail_data) >= 5:
                # ç®€åŒ–ï¼šåŸºäºæ•°æ®çš„ç»Ÿè®¡ç‰¹å¾
                features = [
                    np.mean(tail_data),
                    np.std(tail_data),
                    stats.skew(tail_data),
                    stats.kurtosis(tail_data)
                ]
                
                # æ¨¡å¼è¯†åˆ«ï¼ˆç®€åŒ–ï¼‰
                pattern_score = np.sum(np.abs(features)) / len(features)
                
                if pattern_score > 0.5:
                    return {
                        'pattern_type': 'ml_discovered_pattern_1',
                        'confidence': min(1.0, pattern_score),
                        'feature_vector': features,
                        'pattern_score': pattern_score
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_ml_pattern_2(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æœºå™¨å­¦ä¹ å‘ç°çš„æ¨¡å¼2"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 8:
                return None
            
            # åŸºäºè‡ªç¼–ç å™¨çš„å¼‚å¸¸æ£€æµ‹æ¨¡å¼
            # ç®€åŒ–ï¼šæ£€æµ‹æ•°æ®çš„é‡æ„è¯¯å·®
            if len(tail_data) >= 4:
                # ç®€å•çš„é‡æ„æµ‹è¯•
                mid_point = len(tail_data) // 2
                first_half = tail_data[:mid_point]
                second_half = tail_data[mid_point:]
                
                if len(first_half) == len(second_half):
                    reconstruction_error = np.mean((first_half - second_half)**2)
                    
                    if reconstruction_error > 0.1:
                        return {
                            'pattern_type': 'ml_discovered_pattern_2',
                            'confidence': min(1.0, reconstruction_error * 5),
                            'reconstruction_error': reconstruction_error,
                            'anomaly_detected': True
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_ml_pattern_3(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æœºå™¨å­¦ä¹ å‘ç°çš„æ¨¡å¼3"""
        try:
            tail_data = data_matrix[:, tail]
            
            # åŸºäºæ·±åº¦å­¦ä¹ çš„åºåˆ—æ¨¡å¼
            if len(tail_data) >= 6:
                # ç®€åŒ–ï¼šæ£€æµ‹åºåˆ—çš„å‘¨æœŸæ€§
                fft_result = np.fft.fft(tail_data)
                power_spectrum = np.abs(fft_result)**2
                
                # æ‰¾ä¸»å¯¼é¢‘ç‡
                dominant_freq_idx = np.argmax(power_spectrum[1:]) + 1
                dominant_power = power_spectrum[dominant_freq_idx]
                total_power = np.sum(power_spectrum[1:])
                
                if total_power > 0:
                    pattern_strength = dominant_power / total_power
                    
                    if pattern_strength > 0.3:
                        return {
                            'pattern_type': 'ml_discovered_pattern_3',
                            'confidence': pattern_strength,
                            'dominant_frequency': dominant_freq_idx,
                            'pattern_strength': pattern_strength,
                            'periodicity_detected': True
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_seasonal_pattern(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """å­£èŠ‚æ€§æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 12:
                return None
            
            # æ£€æµ‹å‘¨æœŸæ€§
            seasonality_strength = self._detect_seasonality(tail_data)
            
            if seasonality_strength > 0.5:
                # æ‰¾å‡ºå­£èŠ‚å‘¨æœŸ
                autocorr_values = []
                for lag in range(1, min(12, len(tail_data)//2)):
                    if len(tail_data) > lag:
                        autocorr = np.corrcoef(tail_data[:-lag], tail_data[lag:])[0, 1]
                        if not np.isnan(autocorr):
                            autocorr_values.append((lag, abs(autocorr)))
                
                if autocorr_values:
                    best_lag = max(autocorr_values, key=lambda x: x[1])
                    
                    return {
                        'pattern_type': 'seasonal_pattern',
                        'confidence': seasonality_strength,
                        'seasonality_strength': seasonality_strength,
                        'seasonal_period': best_lag[0],
                        'autocorrelation': best_lag[1]
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_cyclical_pattern(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """å‘¨æœŸæ€§æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 8:
                return None
            
            # ä½¿ç”¨FFTæ£€æµ‹å‘¨æœŸ
            fft_result = np.fft.fft(tail_data)
            frequencies = np.fft.fftfreq(len(tail_data))
            power_spectrum = np.abs(fft_result)**2
            
            # æ‰¾ä¸»å¯¼å‘¨æœŸ
            positive_freqs = frequencies[1:len(frequencies)//2]
            positive_power = power_spectrum[1:len(power_spectrum)//2]
            
            if len(positive_power) > 0:
                max_power_idx = np.argmax(positive_power)
                dominant_freq = positive_freqs[max_power_idx]
                
                if dominant_freq > 0:
                    cycle_length = 1.0 / dominant_freq
                    cycle_strength = positive_power[max_power_idx] / np.sum(positive_power)
                    
                    if cycle_strength > 0.2:
                        return {
                            'pattern_type': 'cyclical_pattern',
                            'confidence': cycle_strength,
                            'cycle_length': cycle_length,
                            'cycle_strength': cycle_strength,
                            'dominant_frequency': dominant_freq
                        }
            
            return None
            
        except Exception:
            return None
    
    def _detect_trend_break_pattern(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è¶‹åŠ¿çªç ´æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 10:
                return None
            
            # æ£€æµ‹è¶‹åŠ¿å˜åŒ–ç‚¹
            mid_point = len(tail_data) // 2
            early_trend = np.polyfit(range(mid_point), tail_data[:mid_point], 1)[0]
            late_trend = np.polyfit(range(mid_point), tail_data[mid_point:], 1)[0]
            
            # è¶‹åŠ¿çªç ´æ¡ä»¶
            trend_change = abs(late_trend - early_trend)
            
            if trend_change > 0.05:  # æ˜¾è‘—è¶‹åŠ¿å˜åŒ–
                break_strength = trend_change
                break_direction = 'upward' if late_trend > early_trend else 'downward'
                
                return {
                    'pattern_type': 'trend_break_pattern',
                    'confidence': min(1.0, break_strength * 10),
                    'break_strength': break_strength,
                    'break_direction': break_direction,
                    'early_trend': early_trend,
                    'late_trend': late_trend
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_mean_reversion_pattern(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """å‡å€¼å›å½’æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 8:
                return None
            
            # æ£€æµ‹åç¦»å‡å€¼çš„ç¨‹åº¦
            mean_value = np.mean(tail_data)
            current_value = tail_data[-1]
            std_value = np.std(tail_data)
            
            if std_value > 0:
                deviation = abs(current_value - mean_value) / std_value
                
                # æ£€æµ‹å›å½’è¶‹åŠ¿
                if len(tail_data) >= 5:
                    recent_data = tail_data[-5:]
                    # æ£€æŸ¥æ˜¯å¦åœ¨å‘å‡å€¼é è¿‘
                    distances_to_mean = [abs(val - mean_value) for val in recent_data]
                    
                    if len(distances_to_mean) > 1:
                        regression_trend = np.polyfit(range(len(distances_to_mean)), distances_to_mean, 1)[0]
                        
                        if deviation > 1.5 and regression_trend < 0:  # æ­£åœ¨å›å½’
                            return {
                                'pattern_type': 'mean_reversion_pattern',
                                'confidence': min(1.0, deviation / 3),
                                'deviation_level': deviation,
                                'regression_trend': abs(regression_trend),
                                'mean_level': mean_value,
                                'current_distance': abs(current_value - mean_value)
                            }
            
            return None
            
        except Exception:
            return None
    
    def _detect_emergence_pattern(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ¶Œç°æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 10:
                return None
            
            # æ£€æµ‹çªç„¶çš„è¡Œä¸ºå˜åŒ–ï¼ˆæ¶Œç°ç‰¹å¾ï¼‰
            recent_volatility = np.std(tail_data[-5:]) if len(tail_data) >= 5 else 0
            historical_volatility = np.std(tail_data[:-5]) if len(tail_data) > 5 else recent_volatility
            
            if historical_volatility > 0:
                volatility_change = recent_volatility / historical_volatility
                
                # æ¶Œç°æ¡ä»¶ï¼šçªç„¶çš„è¡Œä¸ºæ¨¡å¼æ”¹å˜
                if volatility_change > 2.0 or volatility_change < 0.5:
                    emergence_strength = abs(np.log(volatility_change))
                    
                    return {
                        'pattern_type': 'emergence_pattern',
                        'confidence': min(1.0, emergence_strength),
                        'volatility_change_ratio': volatility_change,
                        'emergence_type': 'complexity_increase' if volatility_change > 1 else 'order_emergence',
                        'emergence_strength': emergence_strength
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_self_organization(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """è‡ªç»„ç»‡æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 12:
                return None
            
            # æ£€æµ‹æ•°æ®çš„è‡ªç»„ç»‡ç‰¹å¾ï¼ˆç†µçš„å˜åŒ–ï¼‰
            early_data = tail_data[:len(tail_data)//2]
            late_data = tail_data[len(tail_data)//2:]
            
            early_entropy = self._calculate_shannon_entropy(early_data)
            late_entropy = self._calculate_shannon_entropy(late_data)
            
            entropy_change = early_entropy - late_entropy
            
            # è‡ªç»„ç»‡ï¼šç†µå‡å°‘ï¼ˆæ›´æœ‰åºï¼‰
            if entropy_change > 0.5:
                return {
                    'pattern_type': 'self_organization',
                    'confidence': min(1.0, entropy_change),
                    'entropy_reduction': entropy_change,
                    'organization_level': 1.0 - late_entropy / (early_entropy + 1e-10),
                    'early_entropy': early_entropy,
                    'late_entropy': late_entropy
                }
            
            return None
            
        except Exception:
            return None
    
    def _detect_phase_transition(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ç›¸å˜æ¨¡å¼æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 10:
                return None
            
            # æ£€æµ‹çªå˜ç‚¹ï¼ˆç›¸å˜ç‰¹å¾ï¼‰
            # ä½¿ç”¨æ»‘åŠ¨çª—å£æ£€æµ‹ç»Ÿè®¡ç‰¹æ€§çš„çªç„¶å˜åŒ–
            window_size = 4
            changes = []
            
            for i in range(window_size, len(tail_data) - window_size):
                before_window = tail_data[i-window_size:i]
                after_window = tail_data[i:i+window_size]
                
                before_mean = np.mean(before_window)
                after_mean = np.mean(after_window)
                before_std = np.std(before_window)
                after_std = np.std(after_window)
                
                mean_change = abs(after_mean - before_mean)
                std_change = abs(after_std - before_std)
                
                total_change = mean_change + std_change
                changes.append(total_change)
            
            if changes:
                max_change = max(changes)
                max_change_idx = changes.index(max_change)
                
                if max_change > 0.2:  # æ˜¾è‘—å˜åŒ–
                    return {
                        'pattern_type': 'phase_transition',
                        'confidence': min(1.0, max_change * 5),
                        'transition_point': max_change_idx + window_size,
                        'transition_magnitude': max_change,
                        'transition_type': 'critical_point'
                    }
            
            return None
            
        except Exception:
            return None
    
    def _detect_critical_point(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """ä¸´ç•Œç‚¹æ£€æµ‹"""
        try:
            tail_data = data_matrix[:, tail]
            if len(tail_data) < 8:
                return None
            
            # æ£€æµ‹ä¸´ç•Œç‚¹ç‰¹å¾ï¼šæ³¢åŠ¨æ€§æ€¥å‰§å¢åŠ 
            if len(tail_data) >= 6:
                recent_volatility = np.std(tail_data[-3:])
                previous_volatility = np.std(tail_data[-6:-3])
                
                if previous_volatility > 0:
                    volatility_ratio = recent_volatility / previous_volatility
                    
                    # ä¸´ç•Œç‚¹ï¼šæ³¢åŠ¨æ€§æ€¥å‰§å¢åŠ 
                    if volatility_ratio > 3.0:
                        criticality = np.log(volatility_ratio)
                        
                        return {
                            'pattern_type': 'critical_point',
                            'confidence': min(1.0, criticality / 3),
                            'volatility_explosion': volatility_ratio,
                            'criticality_level': criticality,
                            'system_instability': True
                        }
            
            return None
            
        except Exception:
            return None
    
    # ========== ç®€åŒ–æŠ€æœ¯æŒ‡æ ‡è®¡ç®—æ–¹æ³• ==========
    
    def _calculate_rsi_simple(self, data: np.ndarray, period: int = 14) -> float:
        """ç®€åŒ–RSIè®¡ç®—"""
        try:
            if len(data) < period + 1:
                return 50.0
            
            changes = np.diff(data)
            gains = np.where(changes > 0, changes, 0)
            losses = np.where(changes < 0, -changes, 0)
            
            avg_gain = np.mean(gains[-period:])
            avg_loss = np.mean(losses[-period:])
            
            if avg_loss == 0:
                return 100.0
            
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
            
            return rsi
            
        except Exception:
            return 50.0
    
    def _calculate_macd_simple(self, data: np.ndarray) -> Tuple[float, float, float]:
        """ç®€åŒ–MACDè®¡ç®—"""
        try:
            if len(data) < 26:
                return 0.0, 0.0, 0.0
            
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            ema12 = np.mean(data[-12:])  # ç®€åŒ–ä¸ºç®€å•å‡çº¿
            ema26 = np.mean(data[-26:])
            
            macd_line = ema12 - ema26
            
            # ä¿¡å·çº¿ï¼ˆç®€åŒ–ï¼‰
            if len(data) >= 35:
                signal_data = []
                for i in range(9):
                    if len(data) > 26 + i:
                        ema12_i = np.mean(data[-(12+i):len(data)-i])
                        ema26_i = np.mean(data[-(26+i):len(data)-i])
                        signal_data.append(ema12_i - ema26_i)
                
                macd_signal = np.mean(signal_data) if signal_data else macd_line
            else:
                macd_signal = macd_line
            
            macd_histogram = macd_line - macd_signal
            
            return macd_line, macd_signal, macd_histogram
            
        except Exception:
            return 0.0, 0.0, 0.0
    
    def _calculate_bollinger_position_simple(self, data: np.ndarray, period: int = 20) -> float:
        """ç®€åŒ–å¸ƒæ—å¸¦ä½ç½®è®¡ç®—"""
        try:
            if len(data) < period:
                return 0.5
            
            recent_data = data[-period:]
            mean_val = np.mean(recent_data)
            std_val = np.std(recent_data)
            
            upper_band = mean_val + 2 * std_val
            lower_band = mean_val - 2 * std_val
            current_price = data[-1]
            
            if upper_band != lower_band:
                position = (current_price - lower_band) / (upper_band - lower_band)
            else:
                position = 0.5
            
            return max(0.0, min(1.0, position))
            
        except Exception:
            return 0.5
    
    def _calculate_stochastic_simple(self, data: np.ndarray, period: int = 14) -> float:
        """ç®€åŒ–éšæœºæŒ‡æ ‡è®¡ç®—"""
        try:
            if len(data) < period:
                return 50.0
            
            recent_data = data[-period:]
            current_price = data[-1]
            
            highest_high = np.max(recent_data)
            lowest_low = np.min(recent_data)
            
            if highest_high != lowest_low:
                k_percent = 100 * (current_price - lowest_low) / (highest_high - lowest_low)
            else:
                k_percent = 50.0
            
            return k_percent
            
        except Exception:
            return 50.0
    
    def _calculate_williams_r_simple(self, data: np.ndarray, period: int = 14) -> float:
        """ç®€åŒ–Williams %Rè®¡ç®—"""
        try:
            if len(data) < period:
                return -50.0
            
            recent_data = data[-period:]
            current_price = data[-1]
            
            highest_high = np.max(recent_data)
            lowest_low = np.min(recent_data)
            
            if highest_high != lowest_low:
                williams_r = -100 * (highest_high - current_price) / (highest_high - lowest_low)
            else:
                williams_r = -50.0
            
            return williams_r
            
        except Exception:
            return -50.0
    
    def _calculate_cci_simple(self, data: np.ndarray, period: int = 20) -> float:
        """ç®€åŒ–CCIè®¡ç®—"""
        try:
            if len(data) < period:
                return 0.0
            
            recent_data = data[-period:]
            typical_price = data[-1]  # ç®€åŒ–ï¼šä½¿ç”¨å½“å‰ä»·æ ¼
            
            sma = np.mean(recent_data)
            mean_deviation = np.mean(np.abs(recent_data - sma))
            
            if mean_deviation != 0:
                cci = (typical_price - sma) / (0.015 * mean_deviation)
            else:
                cci = 0.0
            
            return cci
            
        except Exception:
            return 0.0
    
    def _calculate_dfa_alpha(self, data: np.ndarray) -> float:
        """å»è¶‹åŠ¿æ³¢åŠ¨åˆ†æ"""
        try:
            if len(data) < 10:
                return 0.5
            
            # ç®€åŒ–çš„DFAå®ç°
            N = len(data)
            y = np.cumsum(data - np.mean(data))
            
            # ä¸åŒçª—å£å¤§å°
            scales = np.logspace(0.5, np.log10(N//4), 10).astype(int)
            fluctuations = []
            
            for scale in scales:
                if scale >= 4:
                    segments = N // scale
                    if segments > 0:
                        F = 0
                        for v in range(segments):
                            start = v * scale
                            end = start + scale
                            segment = y[start:end]
                            
                            # çº¿æ€§å»è¶‹åŠ¿
                            x = np.arange(len(segment))
                            coeffs = np.polyfit(x, segment, 1)
                            trend = np.polyval(coeffs, x)
                            
                            F += np.mean((segment - trend)**2)
                        
                        F = np.sqrt(F / segments)
                        fluctuations.append(F)
                    else:
                        fluctuations.append(0)
                else:
                    fluctuations.append(0)
            
            # è®¡ç®—æ ‡åº¦æŒ‡æ•°
            valid_scales = []
            valid_fluctuations = []
            
            for i, f in enumerate(fluctuations):
                if f > 0:
                    valid_scales.append(scales[i])
                    valid_fluctuations.append(f)
            
            if len(valid_scales) > 3:
                log_scales = np.log10(valid_scales)
                log_fluctuations = np.log10(valid_fluctuations)
                
                alpha = np.polyfit(log_scales, log_fluctuations, 1)[0]
                return max(0.0, min(2.0, alpha))
            else:
                return 0.5
                
        except Exception:
            return 0.5
    
    def _calculate_rqa_measures(self, data: np.ndarray) -> Tuple[float, float]:
        """é€’å½’é‡åŒ–åˆ†ææŒ‡æ ‡"""
        try:
            if len(data) < 10:
                return 0.0, 0.0
            
            # ç®€åŒ–çš„RQAå®ç°
            threshold = 0.1 * np.std(data)
            N = len(data)
            
            # æ„å»ºé€’å½’çŸ©é˜µ
            recurrence_points = 0
            diagonal_points = 0
            
            for i in range(N):
                for j in range(N):
                    if abs(data[i] - data[j]) < threshold:
                        recurrence_points += 1
                        
                        # æ£€æŸ¥å¯¹è§’çº¿ç»“æ„
                        if abs(i - j) <= 2:
                            diagonal_points += 1
            
            # é€’å½’ç‡
            recurrence_rate = recurrence_points / (N * N)
            
            # ç¡®å®šæ€§
            determinism = diagonal_points / max(1, recurrence_points)
            
            return recurrence_rate, determinism
            
        except Exception:
            return 0.0, 0.0
    
# ========== æ›´å¤šè¾…åŠ©åˆ†ææ–¹æ³• ==========
    
    def _get_feature_importance_summary(self) -> Dict[str, float]:
        """è·å–ç‰¹å¾é‡è¦æ€§æ‘˜è¦"""
        try:
            importance_summary = {}
            
            # ä»å†å²è®°å½•ä¸­è·å–ç‰¹å¾é‡è¦æ€§
            for tail, importance_history in self.feature_importance_tracker.items():
                if importance_history:
                    avg_importance = {}
                    for importance_dict in importance_history:
                        for feature, value in importance_dict.items():
                            if feature not in avg_importance:
                                avg_importance[feature] = []
                            avg_importance[feature].append(value)
                    
                    # è®¡ç®—å¹³å‡é‡è¦æ€§
                    for feature, values in avg_importance.items():
                        avg_value = np.mean(values)
                        if feature not in importance_summary:
                            importance_summary[feature] = []
                        importance_summary[feature].append(avg_value)
            
            # å…¨å±€å¹³å‡
            global_importance = {}
            for feature, tail_values in importance_summary.items():
                global_importance[feature] = np.mean(tail_values)
            
            return global_importance
            
        except Exception:
            return {}
    
    def _get_model_contribution_summary(self) -> Dict[str, float]:
        """è·å–æ¨¡å‹è´¡çŒ®åº¦æ‘˜è¦"""
        try:
            contributions = {}
            
            # åŸºäºæ¨¡å‹ç±»å‹çš„é¢„è®¾æƒé‡
            model_weights = {
                'technical_analysis': 0.2,
                'wavelet_analysis': 0.15,
                'fourier_analysis': 0.12,
                'nonlinear_dynamics': 0.1,
                'machine_learning': 0.18,
                'deep_learning': 0.15,
                'quantum_analysis': 0.05,
                'pattern_recognition': 0.05
            }
            
            # æ ¹æ®å†å²æ€§èƒ½è°ƒæ•´æƒé‡
            for model_type, base_weight in model_weights.items():
                # ç®€åŒ–ï¼šä½¿ç”¨åŸºç¡€æƒé‡
                contributions[model_type] = base_weight
            
            return contributions
            
        except Exception:
            return {}
    
    def _detect_current_market_regime(self, data_matrix: np.ndarray) -> str:
        """æ£€æµ‹å½“å‰å¸‚åœºçŠ¶æ€"""
        try:
            if data_matrix.ndim < 2 or data_matrix.shape[0] < 10:
                return MarketRegime.SIDEWAYS.value
            
            # è®¡ç®—å¸‚åœºæ•´ä½“æ´»è·ƒåº¦
            total_activity = np.sum(data_matrix, axis=1)
            
            # è®¡ç®—è¶‹åŠ¿
            if len(total_activity) >= 10:
                recent_trend = np.polyfit(range(10), total_activity[-10:], 1)[0]
                overall_trend = np.polyfit(range(len(total_activity)), total_activity, 1)[0]
                
                # è®¡ç®—æ³¢åŠ¨ç‡
                recent_volatility = np.std(total_activity[-10:])
                historical_volatility = np.std(total_activity)
                
                # çŠ¶æ€åˆ¤æ–­
                if recent_volatility > historical_volatility * 1.5:
                    if abs(recent_trend) > 0.1:
                        return MarketRegime.CRISIS.value
                    else:
                        return MarketRegime.HIGH_VOLATILITY.value
                elif recent_volatility < historical_volatility * 0.5:
                    return MarketRegime.LOW_VOLATILITY.value
                elif recent_trend > 0.05:
                    return MarketRegime.BULL_MARKET.value
                elif recent_trend < -0.05:
                    return MarketRegime.BEAR_MARKET.value
                else:
                    return MarketRegime.SIDEWAYS.value
            
            return MarketRegime.SIDEWAYS.value
            
        except Exception:
            return MarketRegime.SIDEWAYS.value
    
    def _forecast_volatility(self, data_matrix: np.ndarray, horizon: int = 5) -> Dict[str, float]:
        """é¢„æµ‹æ³¢åŠ¨ç‡"""
        try:
            if data_matrix.ndim < 2 or data_matrix.shape[0] < 10:
                return {'forecast': 0.1, 'confidence': 0.0}
            
            # è®¡ç®—å†å²æ³¢åŠ¨ç‡
            total_activity = np.sum(data_matrix, axis=1)
            historical_volatility = np.std(total_activity)
            
            # ç®€å•çš„GARCHæ¨¡å‹é¢„æµ‹
            if len(total_activity) >= 5:
                recent_returns = np.diff(total_activity[-5:])
                recent_volatility = np.std(recent_returns) if len(recent_returns) > 1 else historical_volatility
                
                # æ³¢åŠ¨ç‡é¢„æµ‹ï¼ˆç®€åŒ–ï¼‰
                alpha = 0.1  # GARCHå‚æ•°
                beta = 0.85
                omega = 0.01
                
                # é€’æ¨é¢„æµ‹
                forecast_var = omega
                for i in range(horizon):
                    if len(recent_returns) > 0:
                        forecast_var = omega + alpha * recent_returns[-1]**2 + beta * forecast_var
                
                forecast_volatility = np.sqrt(forecast_var)
                
                # é¢„æµ‹ç½®ä¿¡åº¦
                data_length_factor = min(1.0, len(total_activity) / 50)
                trend_stability = 1.0 - abs(np.polyfit(range(len(total_activity)), total_activity, 1)[0])
                confidence = data_length_factor * trend_stability
                
            else:
                forecast_volatility = historical_volatility
                confidence = 0.3
            
            return {
                'forecast': forecast_volatility,
                'confidence': min(1.0, confidence),
                'horizon': horizon,
                'current_volatility': historical_volatility,
                'volatility_trend': 'increasing' if forecast_volatility > historical_volatility else 'decreasing'
            }
            
        except Exception:
            return {'forecast': 0.1, 'confidence': 0.0}
    
    def _calculate_reversal_probability_distribution(self, scores: Dict[int, float]) -> Dict[str, Any]:
        """è®¡ç®—åè½¬æ¦‚ç‡åˆ†å¸ƒ"""
        try:
            if not scores:
                return {'probabilities': {}, 'entropy': 0.0}
            
            # å°†å¾—åˆ†è½¬æ¢ä¸ºæ¦‚ç‡
            total_score = sum(scores.values())
            if total_score == 0:
                # å‡åŒ€åˆ†å¸ƒ
                prob = 1.0 / len(scores)
                probabilities = {tail: prob for tail in scores.keys()}
            else:
                probabilities = {tail: score/total_score for tail, score in scores.items()}
            
            # è®¡ç®—åˆ†å¸ƒç†µ
            prob_values = list(probabilities.values())
            prob_values = [p for p in prob_values if p > 0]
            
            if prob_values:
                entropy = -sum(p * np.log2(p) for p in prob_values)
                max_entropy = np.log2(len(scores))
                normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0
            else:
                entropy = 0.0
                normalized_entropy = 0.0
            
            # æ¦‚ç‡åˆ†æ
            max_prob_tail = max(probabilities.keys(), key=lambda k: probabilities[k])
            max_probability = probabilities[max_prob_tail]
            
            # é›†ä¸­åº¦åˆ†æ
            concentration = max_probability
            uncertainty = 1.0 - concentration
            
            return {
                'probabilities': probabilities,
                'entropy': entropy,
                'normalized_entropy': normalized_entropy,
                'max_probability_tail': max_prob_tail,
                'max_probability': max_probability,
                'concentration': concentration,
                'uncertainty': uncertainty,
                'distribution_type': self._classify_distribution_type(probabilities)
            }
            
        except Exception:
            return {'probabilities': {}, 'entropy': 0.0}
    
    def _classify_distribution_type(self, probabilities: Dict[int, float]) -> str:
        """åˆ†ç±»æ¦‚ç‡åˆ†å¸ƒç±»å‹"""
        try:
            if not probabilities:
                return 'unknown'
            
            prob_values = list(probabilities.values())
            max_prob = max(prob_values)
            
            if max_prob > 0.7:
                return 'concentrated'  # é«˜åº¦é›†ä¸­
            elif max_prob > 0.4:
                return 'moderate'      # ä¸­ç­‰é›†ä¸­
            else:
                return 'dispersed'     # åˆ†æ•£
                
        except Exception:
            return 'unknown'
    
    def _generate_alternative_scenarios(self, scores: Dict[int, float], 
                                      confidence: float) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¤‡é€‰æ–¹æ¡ˆ"""
        try:
            scenarios = []
            
            if not scores:
                return scenarios
            
            # ä¸»è¦æ–¹æ¡ˆ
            best_tail = max(scores.keys(), key=lambda k: scores[k])
            scenarios.append({
                'scenario': 'primary',
                'recommended_tail': best_tail,
                'confidence': confidence,
                'probability': confidence,
                'reasoning': 'ä¸»è¦æ¨èæ–¹æ¡ˆ'
            })
            
            # å¤‡é€‰æ–¹æ¡ˆ
            sorted_tails = sorted(scores.keys(), key=lambda k: scores[k], reverse=True)
            
            if len(sorted_tails) > 1:
                second_best = sorted_tails[1]
                second_confidence = scores[second_best] / scores[best_tail] * confidence
                
                scenarios.append({
                    'scenario': 'alternative',
                    'recommended_tail': second_best,
                    'confidence': second_confidence,
                    'probability': second_confidence * 0.7,
                    'reasoning': 'å¤‡é€‰æ¨èæ–¹æ¡ˆ'
                })
            
            # ä¿å®ˆæ–¹æ¡ˆï¼ˆå¦‚æœä¸»æ–¹æ¡ˆç½®ä¿¡åº¦ä¸é«˜ï¼‰
            if confidence < 0.6:
                # é€‰æ‹©é£é™©æœ€ä½çš„é€‰é¡¹
                conservative_tail = min(scores.keys(), key=lambda k: abs(scores[k] - 0.5))
                
                scenarios.append({
                    'scenario': 'conservative',
                    'recommended_tail': conservative_tail,
                    'confidence': 0.5,
                    'probability': 1.0 - confidence,
                    'reasoning': 'ä¿å®ˆæ–¹æ¡ˆ-ä½é£é™©é€‰æ‹©'
                })
            
            return scenarios
            
        except Exception:
            return []
    
    def _find_historical_similarities(self, tail: int, data_matrix: np.ndarray, 
                                    lookback: int = 100) -> Dict[str, Any]:
        """å¯»æ‰¾å†å²ç›¸ä¼¼æƒ…å†µ"""
        try:
            if data_matrix.ndim < 2 or data_matrix.shape[0] < 20:
                return {'similarities': [], 'average_outcome': 0.5}
            
            tail_data = data_matrix[:, tail]
            current_pattern = tail_data[-10:] if len(tail_data) >= 10 else tail_data
            
            similarities = []
            outcomes = []
            
            # åœ¨å†å²æ•°æ®ä¸­æœç´¢ç›¸ä¼¼æ¨¡å¼
            search_length = min(lookback, len(tail_data) - 15)
            
            for i in range(search_length):
                if i + len(current_pattern) + 5 < len(tail_data):
                    historical_pattern = tail_data[i:i + len(current_pattern)]
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦
                    if len(historical_pattern) == len(current_pattern):
                        correlation = np.corrcoef(current_pattern, historical_pattern)[0, 1]
                        
                        if not np.isnan(correlation) and correlation > 0.7:
                            # è·å–åç»­ç»“æœ
                            future_data = tail_data[i + len(current_pattern):i + len(current_pattern) + 5]
                            if len(future_data) > 0:
                                outcome = np.mean(future_data) - current_pattern[-1]
                                
                                similarities.append({
                                    'similarity': correlation,
                                    'historical_period': i,
                                    'outcome': outcome,
                                    'confidence': correlation
                                })
                                outcomes.append(outcome)
            
            # åˆ†æå†å²ç»“æœ
            if outcomes:
                average_outcome = np.mean(outcomes)
                outcome_std = np.std(outcomes)
                positive_outcomes = sum(1 for o in outcomes if o > 0)
                success_rate = positive_outcomes / len(outcomes)
            else:
                average_outcome = 0.0
                outcome_std = 0.0
                success_rate = 0.5
            
            return {
                'similarities': similarities[:5],  # è¿”å›å‰5ä¸ªæœ€ç›¸ä¼¼çš„
                'similarity_count': len(similarities),
                'average_outcome': average_outcome,
                'outcome_variance': outcome_std,
                'historical_success_rate': success_rate,
                'prediction_reliability': len(similarities) / max(1, search_length) * 100
            }
            
        except Exception:
            return {'similarities': [], 'average_outcome': 0.5}
    
    def _detect_early_warning_signals(self, data_matrix: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹æ—©æœŸè­¦å‘Šä¿¡å·"""
        try:
            if data_matrix.ndim < 2 or data_matrix.shape[0] < 15:
                return {'warning_signals': [], 'alert_level': 'low'}
            
            warning_signals = []
            alert_level = 'low'
            
            # ç³»ç»Ÿæ€§é£é™©ä¿¡å·
            total_activity = np.sum(data_matrix, axis=1)
            
            # 1. æ³¢åŠ¨æ€§æ€¥å‰§å¢åŠ 
            if len(total_activity) >= 10:
                recent_vol = np.std(total_activity[-5:])
                historical_vol = np.std(total_activity[-15:-5])
                
                if historical_vol > 0 and recent_vol / historical_vol > 2.0:
                    warning_signals.append({
                        'type': 'volatility_spike',
                        'severity': 'high',
                        'description': 'æ³¢åŠ¨æ€§æ€¥å‰§å¢åŠ ',
                        'ratio': recent_vol / historical_vol
                    })
                    alert_level = 'high'
            
            # 2. ç›¸å…³æ€§å¢åŠ ï¼ˆç³»ç»Ÿæ€§é£é™©ï¼‰
            if data_matrix.shape[1] > 2:
                correlations = []
                for i in range(data_matrix.shape[1]):
                    for j in range(i + 1, data_matrix.shape[1]):
                        if len(data_matrix) > 1:
                            corr = np.corrcoef(data_matrix[:, i], data_matrix[:, j])[0, 1]
                            if not np.isnan(corr):
                                correlations.append(abs(corr))
                
                if correlations:
                    avg_correlation = np.mean(correlations)
                    if avg_correlation > 0.8:
                        warning_signals.append({
                            'type': 'high_correlation',
                            'severity': 'medium',
                            'description': 'ç³»ç»Ÿç›¸å…³æ€§è¿‡é«˜',
                            'correlation': avg_correlation
                        })
                        if alert_level == 'low':
                            alert_level = 'medium'
            
            # 3. è¶‹åŠ¿ä¸€è‡´æ€§ä¸§å¤±
            trend_consistency = []
            for tail in range(data_matrix.shape[1]):
                tail_data = data_matrix[:, tail]
                if len(tail_data) >= 10:
                    recent_trend = np.polyfit(range(5), tail_data[-5:], 1)[0]
                    historical_trend = np.polyfit(range(10), tail_data[-15:-5], 1)[0]
                    
                    if abs(historical_trend) > 0.01:
                        consistency = 1.0 - abs(recent_trend - historical_trend) / abs(historical_trend)
                        trend_consistency.append(consistency)
            
            if trend_consistency:
                avg_consistency = np.mean(trend_consistency)
                if avg_consistency < 0.3:
                    warning_signals.append({
                        'type': 'trend_breakdown',
                        'severity': 'medium',
                        'description': 'è¶‹åŠ¿ä¸€è‡´æ€§ä¸§å¤±',
                        'consistency': avg_consistency
                    })
                    if alert_level == 'low':
                        alert_level = 'medium'
            
            # 4. æå€¼å‡ºç°é¢‘ç‡å¢åŠ 
            extreme_count = 0
            for tail in range(data_matrix.shape[1]):
                tail_data = data_matrix[:, tail]
                if len(tail_data) >= 10:
                    recent_data = tail_data[-5:]
                    historical_mean = np.mean(tail_data[-15:-5])
                    historical_std = np.std(tail_data[-15:-5])
                    
                    if historical_std > 0:
                        for value in recent_data:
                            if abs(value - historical_mean) > 2 * historical_std:
                                extreme_count += 1
            
            extreme_ratio = extreme_count / (data_matrix.shape[1] * 5)
            if extreme_ratio > 0.2:
                warning_signals.append({
                    'type': 'extreme_values',
                    'severity': 'high',
                    'description': 'æå€¼å‡ºç°é¢‘ç‡å¼‚å¸¸',
                    'extreme_ratio': extreme_ratio
                })
                alert_level = 'high'
            
            return {
                'warning_signals': warning_signals,
                'alert_level': alert_level,
                'signal_count': len(warning_signals),
                'system_stability': 1.0 - len(warning_signals) / 10.0
            }
            
        except Exception:
            return {'warning_signals': [], 'alert_level': 'low'}
    
    def _calculate_regime_change_probability(self, data_matrix: np.ndarray) -> float:
        """è®¡ç®—åˆ¶åº¦å˜è¿æ¦‚ç‡"""
        try:
            if data_matrix.ndim < 2 or data_matrix.shape[0] < 20:
                return 0.0
            
            change_indicators = []
            
            # 1. ç»Ÿè®¡ç‰¹æ€§å˜åŒ–
            total_activity = np.sum(data_matrix, axis=1)
            if len(total_activity) >= 20:
                early_period = total_activity[-20:-10]
                recent_period = total_activity[-10:]
                
                # å‡å€¼å˜åŒ–
                mean_change = abs(np.mean(recent_period) - np.mean(early_period))
                mean_change_ratio = mean_change / (np.std(early_period) + 1e-10)
                change_indicators.append(min(1.0, mean_change_ratio))
                
                # æ–¹å·®å˜åŒ–
                var_change = abs(np.var(recent_period) - np.var(early_period))
                var_change_ratio = var_change / (np.var(early_period) + 1e-10)
                change_indicators.append(min(1.0, var_change_ratio))
            
            # 2. ç›¸å…³ç»“æ„å˜åŒ–
            if data_matrix.shape[1] > 2:
                early_corr_matrix = np.corrcoef(data_matrix[-20:-10].T)
                recent_corr_matrix = np.corrcoef(data_matrix[-10:].T)
                
                # è®¡ç®—ç›¸å…³çŸ©é˜µçš„å·®å¼‚
                if not (np.isnan(early_corr_matrix).any() or np.isnan(recent_corr_matrix).any()):
                    corr_diff = np.mean(np.abs(recent_corr_matrix - early_corr_matrix))
                    change_indicators.append(min(1.0, corr_diff * 5))
            
            # 3. è¶‹åŠ¿æ–¹å‘å˜åŒ–
            trend_changes = 0
            for tail in range(data_matrix.shape[1]):
                tail_data = data_matrix[:, tail]
                if len(tail_data) >= 20:
                    early_trend = np.polyfit(range(10), tail_data[-20:-10], 1)[0]
                    recent_trend = np.polyfit(range(10), tail_data[-10:], 1)[0]
                    
                    if early_trend * recent_trend < 0:  # è¶‹åŠ¿åè½¬
                        trend_changes += 1
            
            trend_change_ratio = trend_changes / data_matrix.shape[1]
            change_indicators.append(trend_change_ratio)
            
            # ç»¼åˆæ¦‚ç‡
            if change_indicators:
                regime_change_prob = np.mean(change_indicators)
            else:
                regime_change_prob = 0.0
            
            return min(1.0, regime_change_prob)
            
        except Exception:
            return 0.0
    
    def _calculate_systemic_risk_indicators(self, data_matrix: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—ç³»ç»Ÿæ€§é£é™©æŒ‡æ ‡"""
        try:
            if data_matrix.ndim < 2:
                return {'overall_systemic_risk': 0.0}
            
            indicators = {}
            
            # 1. ç³»ç»Ÿæ€§ç›¸å…³æ€§
            if data_matrix.shape[1] > 1:
                correlations = []
                for i in range(data_matrix.shape[1]):
                    for j in range(i + 1, data_matrix.shape[1]):
                        if len(data_matrix) > 1:
                            corr = np.corrcoef(data_matrix[:, i], data_matrix[:, j])[0, 1]
                            if not np.isnan(corr):
                                correlations.append(abs(corr))
                
                if correlations:
                    indicators['average_correlation'] = np.mean(correlations)
                    indicators['max_correlation'] = np.max(correlations)
                else:
                    indicators['average_correlation'] = 0.0
                    indicators['max_correlation'] = 0.0
            
            # 2. é›†ä¸­åº¦é£é™©
            total_activity = np.sum(data_matrix, axis=1)
            if len(total_activity) > 0:
                tail_shares = []
                for tail in range(data_matrix.shape[1]):
                    tail_activity = np.sum(data_matrix[:, tail])
                    share = tail_activity / (np.sum(total_activity) + 1e-10)
                    tail_shares.append(share)
                
                # HHIæŒ‡æ•°
                hhi = sum(share**2 for share in tail_shares)
                indicators['concentration_index'] = hhi
            
            # 3. ä¼ æŸ“é£é™©
            contagion_risk = 0.0
            if data_matrix.shape[0] >= 5:
                # æ£€æµ‹çº§è”æ•ˆåº”
                for i in range(len(data_matrix) - 1):
                    current_volatility = np.std(data_matrix[i])
                    next_volatility = np.std(data_matrix[i + 1])
                    
                    if current_volatility > 0:
                        volatility_transmission = next_volatility / current_volatility
                        if volatility_transmission > 1.5:
                            contagion_risk += 0.1
                
                contagion_risk = min(1.0, contagion_risk)
            
            indicators['contagion_risk'] = contagion_risk
            
            # 4. ç³»ç»Ÿæ€§è„†å¼±æ€§
            vulnerabilities = []
            for tail in range(data_matrix.shape[1]):
                tail_data = data_matrix[:, tail]
                if len(tail_data) > 1:
                    # åŸºäºæ³¢åŠ¨æ€§çš„è„†å¼±æ€§
                    volatility = np.std(tail_data)
                    trend_stability = 1.0 - abs(np.polyfit(range(len(tail_data)), tail_data, 1)[0])
                    vulnerability = volatility * (1.0 - trend_stability)
                    vulnerabilities.append(vulnerability)
            
            if vulnerabilities:
                indicators['system_vulnerability'] = np.mean(vulnerabilities)
            else:
                indicators['system_vulnerability'] = 0.0
            
            # 5. ç»¼åˆç³»ç»Ÿæ€§é£é™©
            risk_components = [
                indicators.get('average_correlation', 0.0),
                indicators.get('concentration_index', 0.0),
                indicators.get('contagion_risk', 0.0),
                indicators.get('system_vulnerability', 0.0)
            ]
            
            indicators['overall_systemic_risk'] = np.mean(risk_components)
            
            return indicators
            
        except Exception:
            return {'overall_systemic_risk': 0.0}
    
    def _calculate_complexity_measures(self, data_matrix: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—å¤æ‚æ€§åº¦é‡"""
        try:
            if data_matrix.ndim < 2:
                return {'overall_complexity': 0.0}
            
            measures = {}
            
            # 1. ä¿¡æ¯ç†µ
            entropies = []
            for tail in range(data_matrix.shape[1]):
                tail_data = data_matrix[:, tail]
                if len(tail_data) > 0:
                    entropy = self._calculate_shannon_entropy(tail_data)
                    entropies.append(entropy)
            
            if entropies:
                measures['average_entropy'] = np.mean(entropies)
                measures['entropy_variance'] = np.var(entropies)
            else:
                measures['average_entropy'] = 0.0
                measures['entropy_variance'] = 0.0
            
            # 2. åˆ†å½¢ç»´æ•°
            fractal_dimensions = []
            for tail in range(min(5, data_matrix.shape[1])):  # é™åˆ¶è®¡ç®—é‡
                tail_data = data_matrix[:, tail]
                if len(tail_data) >= 10:
                    fractal_dim = self._calculate_fractal_dimension(tail_data)
                    fractal_dimensions.append(fractal_dim)
            
            if fractal_dimensions:
                measures['average_fractal_dimension'] = np.mean(fractal_dimensions)
            else:
                measures['average_fractal_dimension'] = 1.0
            
            # 3. ç½‘ç»œå¤æ‚æ€§
            if data_matrix.shape[1] > 2:
                # åŸºäºç›¸å…³ç½‘ç»œçš„å¤æ‚æ€§
                correlations = []
                for i in range(data_matrix.shape[1]):
                    for j in range(i + 1, data_matrix.shape[1]):
                        if len(data_matrix) > 1:
                            corr = np.corrcoef(data_matrix[:, i], data_matrix[:, j])[0, 1]
                            if not np.isnan(corr):
                                correlations.append(abs(corr))
                
                if correlations:
                    # ç½‘ç»œå¯†åº¦
                    network_density = np.mean(correlations)
                    # ç½‘ç»œå¼‚è´¨æ€§
                    network_heterogeneity = np.std(correlations)
                    
                    measures['network_density'] = network_density
                    measures['network_heterogeneity'] = network_heterogeneity
                    measures['network_complexity'] = network_density * network_heterogeneity
                else:
                    measures['network_complexity'] = 0.0
            
            # 4. æ—¶é—´å¤æ‚æ€§
            total_activity = np.sum(data_matrix, axis=1)
            if len(total_activity) > 10:
                # Lempel-Zivå¤æ‚æ€§ï¼ˆç®€åŒ–ï¼‰
                binary_sequence = [1 if x > np.median(total_activity) else 0 for x in total_activity]
                lz_complexity = self._calculate_lempel_ziv_complexity(binary_sequence)
                measures['temporal_complexity'] = lz_complexity
            else:
                measures['temporal_complexity'] = 0.0
            
            # 5. ç»¼åˆå¤æ‚æ€§
            complexity_components = [
                measures.get('average_entropy', 0.0) / 5.0,  # å½’ä¸€åŒ–
                (measures.get('average_fractal_dimension', 1.0) - 1.0) / 2.0,
                measures.get('network_complexity', 0.0),
                measures.get('temporal_complexity', 0.0)
            ]
            
            measures['overall_complexity'] = np.mean(complexity_components)
            
            return measures
            
        except Exception:
            return {'overall_complexity': 0.0}
    
    def _calculate_lempel_ziv_complexity(self, binary_sequence: List[int]) -> float:
        """è®¡ç®—Lempel-Zivå¤æ‚æ€§"""
        try:
            if len(binary_sequence) == 0:
                return 0.0
            
            # ç®€åŒ–çš„LZ77ç®—æ³•
            dictionary = []
            i = 0
            
            while i < len(binary_sequence):
                # å¯»æ‰¾æœ€é•¿åŒ¹é…
                max_match_len = 0
                match_pos = 0
                
                for j, pattern in enumerate(dictionary):
                    match_len = 0
                    for k in range(min(len(pattern), len(binary_sequence) - i)):
                        if pattern[k] == binary_sequence[i + k]:
                            match_len += 1
                        else:
                            break
                    
                    if match_len > max_match_len:
                        max_match_len = match_len
                        match_pos = j
                
                # æ·»åŠ æ–°æ¨¡å¼
                if max_match_len == 0:
                    new_pattern = [binary_sequence[i]]
                    i += 1
                else:
                    end_pos = min(i + max_match_len + 1, len(binary_sequence))
                    new_pattern = binary_sequence[i:end_pos]
                    i = end_pos
                
                dictionary.append(new_pattern)
            
            # å¤æ‚æ€§ = å­—å…¸å¤§å° / åºåˆ—é•¿åº¦
            complexity = len(dictionary) / len(binary_sequence) if len(binary_sequence) > 0 else 0.0
            
            return complexity
            
        except Exception:
            return 0.0
    
    # ========== ç³»ç»Ÿç®¡ç†æ–¹æ³• ==========
    
    def _update_signal_weights(self, tail: int, confidence: float, scores: Dict[int, float]):
        """æ›´æ–°ä¿¡å·æƒé‡"""
        try:
            # åŸºäºæ€§èƒ½æ›´æ–°æƒé‡
            weight_adjustment = self.config['learning_rate'] * (confidence - 0.5)
            
            # æ›´æ–°æƒé‡å†å²
            if tail not in self.signal_fusion['weight_history']:
                self.signal_fusion['weight_history'][tail] = deque(maxlen=100)
            
            current_weights = self.signal_fusion['signal_weights'].copy()
            
            # è°ƒæ•´æƒé‡
            for signal_type in current_weights:
                adjustment = weight_adjustment * np.random.normal(0, 0.1)  # æ·»åŠ å™ªå£°
                current_weights[signal_type] *= (1 + adjustment)
                current_weights[signal_type] = max(0.01, min(0.5, current_weights[signal_type]))
            
            # å½’ä¸€åŒ–æƒé‡
            total_weight = sum(current_weights.values())
            if total_weight > 0:
                for signal_type in current_weights:
                    current_weights[signal_type] /= total_weight
            
            # æ›´æ–°å…¨å±€æƒé‡
            self.signal_fusion['signal_weights'] = current_weights
            
            # è®°å½•æƒé‡å†å²
            self.signal_fusion['weight_history'][tail].append({
                'timestamp': datetime.now(),
                'weights': current_weights.copy(),
                'confidence': confidence
            })
            
        except Exception as e:
            self.logger.error(f"æƒé‡æ›´æ–°é”™è¯¯: {str(e)}")
    
    def _update_model_performance_tracking(self, tail: int, confidence: float):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½è·Ÿè¸ª"""
        try:
            # æ›´æ–°æ€§èƒ½å†å²
            if tail not in self.performance_monitor['model_performances']:
                self.performance_monitor['model_performances'][tail] = deque(maxlen=100)
            
            performance_record = {
                'timestamp': datetime.now(),
                'confidence': confidence,
                'prediction_quality': confidence  # ç®€åŒ–
            }
            
            self.performance_monitor['model_performances'][tail].append(performance_record)
            
            # æ›´æ–°å…¨å±€æ€§èƒ½æŒ‡æ ‡
            self.performance_monitor['prediction_accuracies'].append(confidence)
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½è·Ÿè¸ªæ›´æ–°é”™è¯¯: {str(e)}")
    
    def _trigger_memory_cleanup(self):
        """è§¦å‘å†…å­˜æ¸…ç†"""
        try:
            # æ¸…ç†ç¼“å­˜
            if hasattr(self, 'cache'):
                # ä¿ç•™æœ€è¿‘çš„ç¼“å­˜æ¡ç›®
                if len(self.cache) > self.config['cache_size']:
                    # ç§»é™¤æœ€æ—§çš„æ¡ç›®
                    while len(self.cache) > self.config['cache_size'] // 2:
                        self.cache.popitem(last=False)
            
            # æ¸…ç†å†å²æ•°æ®
            for tail in range(10):
                if len(self.trend_history[tail]) > self.config['history_window']:
                    # ä¿ç•™æœ€è¿‘çš„æ•°æ®
                    recent_data = list(self.trend_history[tail])[-self.config['history_window']//2:]
                    self.trend_history[tail].clear()
                    self.trend_history[tail].extend(recent_data)
            
            # åƒåœ¾å›æ”¶
            gc.collect()
            
            self.logger.info("å†…å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"å†…å­˜æ¸…ç†é”™è¯¯: {str(e)}")
    
    def _cache_prediction_result(self, candidate_tails: Tuple[int], data_hash: str, 
                               result: Dict[str, Any]):
        """ç¼“å­˜é¢„æµ‹ç»“æœ"""
        try:
            if not self.config['cache_enabled']:
                return
            
            cache_key = f"{candidate_tails}_{data_hash}"
            
            # ç®€åŒ–ç»“æœä»¥èŠ‚çœå†…å­˜
            cached_result = {
                'recommended_tail': result.get('recommended_tail'),
                'confidence': result.get('confidence'),
                'final_score': result.get('final_score'),
                'timestamp': datetime.now().isoformat()
            }
            
            self.cache[cache_key] = cached_result
            
            # ç¼“å­˜å¤§å°æ§åˆ¶
            if len(self.cache) > self.config['cache_size']:
                # ç§»é™¤æœ€æ—§çš„æ¡ç›®
                self.cache.popitem(last=False)
            
        except Exception as e:
            self.logger.error(f"ç¼“å­˜é”™è¯¯: {str(e)}")
    
    # ========== å­¦ä¹ å’Œé€‚åº”æ–¹æ³• ==========
    
    def learn_from_outcome(self, prediction: Dict, actual_tails: List[int]) -> Dict:
        """ä»ç»“æœä¸­å­¦ä¹ ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        try:
            if not prediction or 'recommended_tail' not in prediction:
                return {'learning_success': False, 'reason': 'invalid_prediction'}
            
            predicted_tail = prediction['recommended_tail']
            prediction_confidence = prediction.get('confidence', 0.0)
            was_correct = predicted_tail in actual_tails
            
            # æ›´æ–°åŸºç¡€ç»Ÿè®¡
            self.total_predictions += 1
            if was_correct:
                self.successful_reversals += 1
            else:
                self.false_signals += 1
            
            # è®¡ç®—å‡†ç¡®ç‡
            self.prediction_accuracy = self.successful_reversals / max(1, self.total_predictions)
            
            # å­¦ä¹ å¼ºåº¦åŸºäºé¢„æµ‹ç½®ä¿¡åº¦
            learning_intensity = prediction_confidence if was_correct else (1.0 - prediction_confidence)
            
            # æ›´æ–°æ¨¡å‹å‚æ•°
            self._adaptive_parameter_update(predicted_tail, was_correct, learning_intensity)
            
            # æ›´æ–°ä¿¡å·æƒé‡
            self._adaptive_signal_weight_update(prediction, was_correct, learning_intensity)
            
            # æ›´æ–°åŠ¨æ€é˜ˆå€¼
            self._adaptive_threshold_update(was_correct, prediction_confidence)
            
            # å­¦ä¹ ç»“æœè®°å½•
            learning_record = {
                'learning_success': True,
                'was_correct': was_correct,
                'prediction_confidence': prediction_confidence,
                'learning_intensity': learning_intensity,
                'current_accuracy': self.prediction_accuracy,
                'total_predictions': self.total_predictions,
                'successful_reversals': self.successful_reversals,
                'false_signals': self.false_signals,
                'adaptation_details': {
                    'parameter_updates': self._get_recent_parameter_changes(),
                    'weight_updates': self._get_recent_weight_changes(),
                    'threshold_updates': self._get_recent_threshold_changes()
                }
            }
            
            return learning_record
            
        except Exception as e:
            self.logger.error(f"å­¦ä¹ è¿‡ç¨‹é”™è¯¯: {str(e)}")
            return {'learning_success': False, 'reason': str(e)}
    
    def _adaptive_parameter_update(self, tail: int, was_correct: bool, intensity: float):
        """è‡ªé€‚åº”å‚æ•°æ›´æ–°"""
        try:
            adjustment_factor = intensity * self.config['learning_rate']
            
            if was_correct:
                # å¼ºåŒ–æˆåŠŸçš„å‚æ•°é…ç½®
                if self.config['reversal_threshold'] > 0.5:
                    self.config['reversal_threshold'] *= (1 - adjustment_factor * 0.1)
                
                self.config['momentum_sensitivity'] *= (1 + adjustment_factor * 0.1)
            else:
                # è°ƒæ•´å¤±è´¥çš„å‚æ•°é…ç½®
                self.config['reversal_threshold'] *= (1 + adjustment_factor * 0.1)
                self.config['momentum_sensitivity'] *= (1 - adjustment_factor * 0.1)
            
            # ç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…
            self.config['reversal_threshold'] = max(0.3, min(0.9, self.config['reversal_threshold']))
            self.config['momentum_sensitivity'] = max(0.5, min(3.0, self.config['momentum_sensitivity']))
            
        except Exception as e:
            self.logger.error(f"å‚æ•°æ›´æ–°é”™è¯¯: {str(e)}")
    
    def _adaptive_signal_weight_update(self, prediction: Dict, was_correct: bool, intensity: float):
        """è‡ªé€‚åº”ä¿¡å·æƒé‡æ›´æ–°"""
        try:
            detailed_analysis = prediction.get('detailed_analysis', {})
            
            if not detailed_analysis:
                return
            
            recommended_tail = prediction.get('recommended_tail')
            if recommended_tail not in detailed_analysis:
                return
            
            tail_analysis = detailed_analysis[recommended_tail]
            
            # æ ¹æ®å„ä¿¡å·çš„è´¡çŒ®è°ƒæ•´æƒé‡
            signals_to_adjust = ['technical', 'wavelet', 'fourier', 'nonlinear', 'ml', 'quantum']
            
            for signal_type in signals_to_adjust:
                signal_score_key = f'{signal_type}_score'
                if signal_score_key in tail_analysis:
                    signal_score = tail_analysis[signal_score_key]
                    
                    if signal_type in self.signal_fusion['signal_weights']:
                        current_weight = self.signal_fusion['signal_weights'][signal_type]
                        
                        if was_correct:
                            # æˆåŠŸæ—¶ï¼Œå¢å¼ºé«˜åˆ†ä¿¡å·çš„æƒé‡
                            if signal_score > 0.6:
                                adjustment = intensity * 0.05 * signal_score
                                new_weight = current_weight * (1 + adjustment)
                            else:
                                new_weight = current_weight
                        else:
                            # å¤±è´¥æ—¶ï¼Œé™ä½é«˜åˆ†ä¿¡å·çš„æƒé‡
                            if signal_score > 0.6:
                                adjustment = intensity * 0.05 * signal_score
                                new_weight = current_weight * (1 - adjustment)
                            else:
                                new_weight = current_weight
                        
                        self.signal_fusion['signal_weights'][signal_type] = max(0.01, min(0.5, new_weight))
            
            # é‡æ–°å½’ä¸€åŒ–æƒé‡
            total_weight = sum(self.signal_fusion['signal_weights'].values())
            if total_weight > 0:
                for signal_type in self.signal_fusion['signal_weights']:
                    self.signal_fusion['signal_weights'][signal_type] /= total_weight
            
        except Exception as e:
            self.logger.error(f"ä¿¡å·æƒé‡æ›´æ–°é”™è¯¯: {str(e)}")
    
    def _adaptive_threshold_update(self, was_correct: bool, confidence: float):
        """è‡ªé€‚åº”é˜ˆå€¼æ›´æ–°"""
        try:
            adaptation_rate = self.config['learning_rate'] * 0.5
            
            for threshold_name in self.dynamic_thresholds.dynamic_thresholds:
                current_threshold = self.dynamic_thresholds.dynamic_thresholds[threshold_name]
                
                if was_correct:
                    if confidence > 0.8:
                        # é«˜ç½®ä¿¡åº¦æˆåŠŸï¼Œå¯ä»¥ç•¥å¾®é™ä½é˜ˆå€¼
                        adjustment = -adaptation_rate * 0.1
                    else:
                        # ä½ç½®ä¿¡åº¦æˆåŠŸï¼Œä¿æŒå½“å‰é˜ˆå€¼
                        adjustment = 0.0
                else:
                    if confidence > 0.7:
                        # é«˜ç½®ä¿¡åº¦å¤±è´¥ï¼Œéœ€è¦æé«˜é˜ˆå€¼
                        adjustment = adaptation_rate * 0.2
                    else:
                        # ä½ç½®ä¿¡åº¦å¤±è´¥ï¼Œé€‚åº¦æé«˜é˜ˆå€¼
                        adjustment = adaptation_rate * 0.1
                
                new_threshold = current_threshold + adjustment
                self.dynamic_thresholds.dynamic_thresholds[threshold_name] = max(0.1, min(0.95, new_threshold))
            
        except Exception as e:
            self.logger.error(f"é˜ˆå€¼æ›´æ–°é”™è¯¯: {str(e)}")
    
    def _get_recent_parameter_changes(self) -> Dict[str, float]:
        """è·å–æœ€è¿‘çš„å‚æ•°å˜åŒ–"""
        return {
            'reversal_threshold': self.config['reversal_threshold'],
            'momentum_sensitivity': self.config['momentum_sensitivity']
        }
    
    def _get_recent_weight_changes(self) -> Dict[str, float]:
        """è·å–æœ€è¿‘çš„æƒé‡å˜åŒ–"""
        return self.signal_fusion['signal_weights'].copy()
    
    def _get_recent_threshold_changes(self) -> Dict[str, float]:
        """è·å–æœ€è¿‘çš„é˜ˆå€¼å˜åŒ–"""
        return self.dynamic_thresholds.dynamic_thresholds.copy()
    
    # ========== æŠ¥å‘Šå’ŒçŠ¶æ€æ–¹æ³• ==========
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            status = {
                'system_info': {
                    'version': '2.0.0-research',
                    'total_predictions': self.total_predictions,
                    'prediction_accuracy': self.prediction_accuracy,
                    'successful_reversals': self.successful_reversals,
                    'false_signals': self.false_signals
                },
                'performance_metrics': {
                    'average_confidence': np.mean(self.model_confidence_history) if self.model_confidence_history else 0.0,
                    'recent_accuracy': self._calculate_recent_accuracy(),
                    'prediction_trend': self._analyze_prediction_trend()
                },
                'model_status': {
                    'ml_models_count': len(self.ml_models),
                    'dl_models_available': TORCH_AVAILABLE or TF_AVAILABLE,
                    'technical_indicators_count': len(self.technical_indicators),
                    'pattern_library_size': len(self.pattern_library)
                },
                'system_health': {
                    'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                    'cache_hit_ratio': self._calculate_cache_hit_ratio(),
                    'processing_speed': self._estimate_processing_speed()
                },
                'configuration': {
                    'current_thresholds': self.dynamic_thresholds.dynamic_thresholds.copy(),
                    'signal_weights': self.signal_fusion['signal_weights'].copy(),
                    'key_parameters': {
                        'reversal_threshold': self.config['reversal_threshold'],
                        'momentum_sensitivity': self.config['momentum_sensitivity'],
                        'learning_rate': self.config['learning_rate']
                    }
                }
            }
            
            return status
            
        except Exception as e:
            self.logger.error(f"çŠ¶æ€è·å–é”™è¯¯: {str(e)}")
            return {'error': str(e)}
    
    def _calculate_recent_accuracy(self, window: int = 50) -> float:
        """è®¡ç®—æœ€è¿‘çš„å‡†ç¡®ç‡"""
        try:
            if len(self.performance_monitor['prediction_accuracies']) < window:
                return self.prediction_accuracy
            
            recent_predictions = list(self.performance_monitor['prediction_accuracies'])[-window:]
            return np.mean(recent_predictions) if recent_predictions else 0.0
            
        except Exception:
            return 0.0
    
    def _analyze_prediction_trend(self) -> str:
        """åˆ†æé¢„æµ‹è¶‹åŠ¿"""
        try:
            if len(self.performance_monitor['prediction_accuracies']) < 10:
                return 'insufficient_data'
            
            recent_data = list(self.performance_monitor['prediction_accuracies'])[-10:]
            trend = np.polyfit(range(len(recent_data)), recent_data, 1)[0]
            
            if trend > 0.01:
                return 'improving'
            elif trend < -0.01:
                return 'declining'
            else:
                return 'stable'
                
        except Exception:
            return 'unknown'
    
    def _calculate_cache_hit_ratio(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        try:
            if not hasattr(self, 'cache_hits') or not hasattr(self, 'cache_misses'):
                return 0.0
            
            total_requests = self.cache_hits + self.cache_misses
            if total_requests == 0:
                return 0.0
            
            return self.cache_hits / total_requests
            
        except Exception:
            return 0.0
    
    def _estimate_processing_speed(self) -> float:
        """ä¼°è®¡å¤„ç†é€Ÿåº¦"""
        try:
            if 'prediction' not in self.performance_monitor['execution_times']:
                return 0.0
            
            execution_times = list(self.performance_monitor['execution_times']['prediction'])
            if not execution_times:
                return 0.0
            
            avg_time = np.mean(execution_times)
            # è¿”å›æ¯ç§’é¢„æµ‹æ•°
            return 1.0 / (avg_time + 1e-10)
            
        except Exception:
            return 0.0
        
# ========== é—æ¼æ–¹æ³•çš„å®ç° ==========
    
    def _calculate_rsi_trend(self, rsi_values: np.ndarray) -> str:
        """è®¡ç®—RSIè¶‹åŠ¿"""
        try:
            if len(rsi_values) < 3:
                return 'insufficient_data'
            
            recent_trend = np.polyfit(range(len(rsi_values[-5:])), rsi_values[-5:], 1)[0]
            
            if recent_trend > 0.5:
                return 'strongly_rising'
            elif recent_trend > 0.1:
                return 'rising'
            elif recent_trend > -0.1:
                return 'sideways'
            elif recent_trend > -0.5:
                return 'falling'
            else:
                return 'strongly_falling'
                
        except Exception:
            return 'unknown'
    
    def _detect_rsi_divergence(self, price_data: np.ndarray, rsi_values: np.ndarray) -> bool:
        """æ£€æµ‹RSIèƒŒç¦»"""
        try:
            if len(price_data) < 5 or len(rsi_values) < 5:
                return False
            
            # ä»·æ ¼è¶‹åŠ¿
            price_trend = np.polyfit(range(len(price_data)), price_data, 1)[0]
            rsi_trend = np.polyfit(range(len(rsi_values)), rsi_values, 1)[0]
            
            # èƒŒç¦»ï¼šä»·æ ¼å’ŒRSIè¶‹åŠ¿ç›¸å
            return (price_trend > 0 and rsi_trend < 0) or (price_trend < 0 and rsi_trend > 0)
            
        except Exception:
            return False
    
    def _classify_rsi_level(self, rsi_value: float) -> str:
        """åˆ†ç±»RSIæ°´å¹³"""
        if rsi_value >= 70:
            return 'overbought'
        elif rsi_value >= 50:
            return 'bullish'
        elif rsi_value >= 30:
            return 'bearish'
        else:
            return 'oversold'
    
    def _detect_macd_crossover(self, macd_line: np.ndarray, macd_signal: np.ndarray) -> str:
        """æ£€æµ‹MACDäº¤å‰"""
        try:
            if len(macd_line) < 2 or len(macd_signal) < 2:
                return 'no_crossover'
            
            # æ£€æŸ¥æœ€è¿‘çš„äº¤å‰
            prev_diff = macd_line[-2] - macd_signal[-2]
            curr_diff = macd_line[-1] - macd_signal[-1]
            
            if prev_diff <= 0 and curr_diff > 0:
                return 'bullish_crossover'
            elif prev_diff >= 0 and curr_diff < 0:
                return 'bearish_crossover'
            else:
                return 'no_crossover'
                
        except Exception:
            return 'no_crossover'
    
    def _detect_macd_divergence(self, price_data: np.ndarray, macd_line: np.ndarray) -> bool:
        """æ£€æµ‹MACDèƒŒç¦»"""
        try:
            return self._detect_rsi_divergence(price_data, macd_line)
        except Exception:
            return False
    
    def _calculate_bollinger_position(self, price: float, upper: float, lower: float) -> float:
        """è®¡ç®—å¸ƒæ—å¸¦ä½ç½®"""
        try:
            if upper == lower:
                return 0.5
            
            position = (price - lower) / (upper - lower)
            return max(0.0, min(1.0, position))
            
        except Exception:
            return 0.5
    
    def _detect_bollinger_squeeze(self, upper_band: np.ndarray, lower_band: np.ndarray) -> bool:
        """æ£€æµ‹å¸ƒæ—å¸¦æ”¶ç¼©"""
        try:
            if len(upper_band) < 5 or len(lower_band) < 5:
                return False
            
            # è®¡ç®—å¸¦å®½
            recent_width = np.mean(upper_band[-3:] - lower_band[-3:])
            historical_width = np.mean(upper_band[-10:-3] - lower_band[-10:-3])
            
            return recent_width < historical_width * 0.7
            
        except Exception:
            return False
    
    def _detect_bollinger_breakout(self, price_data: np.ndarray, 
                                 upper_band: np.ndarray, lower_band: np.ndarray) -> str:
        """æ£€æµ‹å¸ƒæ—å¸¦çªç ´"""
        try:
            if len(price_data) == 0 or len(upper_band) == 0 or len(lower_band) == 0:
                return 'no_breakout'
            
            current_price = price_data[-1]
            current_upper = upper_band[-1]
            current_lower = lower_band[-1]
            
            if current_price > current_upper:
                return 'upper_breakout'
            elif current_price < current_lower:
                return 'lower_breakout'
            else:
                return 'no_breakout'
                
        except Exception:
            return 'no_breakout'
    
    def _detect_stochastic_crossover(self, stoch_k: np.ndarray, stoch_d: np.ndarray) -> str:
        """æ£€æµ‹éšæœºæŒ‡æ ‡äº¤å‰"""
        try:
            return self._detect_macd_crossover(stoch_k, stoch_d)
        except Exception:
            return 'no_crossover'
    
    def _detect_stochastic_divergence(self, price_data: np.ndarray, stoch_k: np.ndarray) -> bool:
        """æ£€æµ‹éšæœºæŒ‡æ ‡èƒŒç¦»"""
        try:
            return self._detect_rsi_divergence(price_data, stoch_k)
        except Exception:
            return False
    
    def _classify_trend_strength(self, adx_value: float) -> str:
        """åˆ†ç±»è¶‹åŠ¿å¼ºåº¦"""
        if adx_value >= 50:
            return 'very_strong'
        elif adx_value >= 25:
            return 'strong'
        elif adx_value >= 20:
            return 'moderate'
        else:
            return 'weak'
    
    def _analyze_volume_patterns(self, tail: int, data_matrix: np.ndarray) -> Dict[str, Any]:
        """åˆ†ææˆäº¤é‡æ¨¡å¼"""
        try:
            # ç®€åŒ–ï¼šç”¨æ€»æ´»è·ƒåº¦æ¨¡æ‹Ÿæˆäº¤é‡
            if data_matrix.ndim > 1:
                volume_proxy = np.sum(data_matrix, axis=1)
            else:
                volume_proxy = data_matrix
            
            if len(volume_proxy) < 5:
                return {'volume_trend': 'insufficient_data'}
            
            # æˆäº¤é‡è¶‹åŠ¿
            volume_trend = np.polyfit(range(len(volume_proxy)), volume_proxy, 1)[0]
            
            # æˆäº¤é‡æ³¢åŠ¨æ€§
            volume_volatility = np.std(volume_proxy)
            
            # æˆäº¤é‡å¼‚å¸¸
            recent_volume = np.mean(volume_proxy[-3:])
            avg_volume = np.mean(volume_proxy)
            volume_ratio = recent_volume / (avg_volume + 1e-10)
            
            return {
                'volume_trend': 'increasing' if volume_trend > 0 else 'decreasing',
                'volume_volatility': volume_volatility,
                'volume_ratio': volume_ratio,
                'volume_anomaly': volume_ratio > 1.5 or volume_ratio < 0.5
            }
            
        except Exception:
            return {'volume_trend': 'unknown'}
    
    def _identify_support_resistance_levels(self, data: np.ndarray) -> Dict[str, Any]:
        """è¯†åˆ«æ”¯æ’‘é˜»åŠ›æ°´å¹³"""
        try:
            if len(data) < 10:
                return {'support_levels': [], 'resistance_levels': []}
            
            # å¯»æ‰¾å±€éƒ¨æå€¼
            peaks, _ = find_peaks(data, distance=3)
            troughs, _ = find_peaks(-data, distance=3)
            
            # æ”¯æ’‘æ°´å¹³ï¼ˆä½ç‚¹ï¼‰
            support_levels = []
            if len(troughs) > 0:
                trough_values = data[troughs]
                # èšç±»ç›¸è¿‘çš„æ”¯æ’‘ä½
                unique_supports = []
                for val in trough_values:
                    if not any(abs(val - existing) < 0.1 * np.std(data) for existing in unique_supports):
                        unique_supports.append(val)
                support_levels = sorted(unique_supports)
            
            # é˜»åŠ›æ°´å¹³ï¼ˆé«˜ç‚¹ï¼‰
            resistance_levels = []
            if len(peaks) > 0:
                peak_values = data[peaks]
                # èšç±»ç›¸è¿‘çš„é˜»åŠ›ä½
                unique_resistances = []
                for val in peak_values:
                    if not any(abs(val - existing) < 0.1 * np.std(data) for existing in unique_resistances):
                        unique_resistances.append(val)
                resistance_levels = sorted(unique_resistances, reverse=True)
            
            return {
                'support_levels': support_levels[:3],  # å–å‰3ä¸ªæœ€å¼ºæ”¯æ’‘
                'resistance_levels': resistance_levels[:3],  # å–å‰3ä¸ªæœ€å¼ºé˜»åŠ›
                'current_position': self._classify_price_position(data[-1], support_levels, resistance_levels)
            }
            
        except Exception:
            return {'support_levels': [], 'resistance_levels': []}
    
    def _classify_price_position(self, current_price: float, 
                               support_levels: List[float], 
                               resistance_levels: List[float]) -> str:
        """åˆ†ç±»ä»·æ ¼ä½ç½®"""
        try:
            if not support_levels and not resistance_levels:
                return 'neutral'
            
            # æ‰¾æœ€è¿‘çš„æ”¯æ’‘å’Œé˜»åŠ›
            nearest_support = max(support_levels) if support_levels else 0
            nearest_resistance = min(resistance_levels) if resistance_levels else float('inf')
            
            if current_price <= nearest_support:
                return 'at_support'
            elif current_price >= nearest_resistance:
                return 'at_resistance'
            else:
                return 'between_levels'
                
        except Exception:
            return 'unknown'
    
    def _calculate_technical_composite_score(self, analysis: Dict[str, Any]) -> float:
        """è®¡ç®—æŠ€æœ¯åˆ†æç»¼åˆå¾—åˆ†"""
        try:
            scores = []
            
            # RSIå¾—åˆ†
            rsi_info = analysis.get('rsi', {})
            rsi_level = rsi_info.get('overbought_oversold', 'neutral')
            if rsi_level in ['overbought', 'oversold']:
                scores.append(0.8)
            else:
                scores.append(0.4)
            
            # MACDå¾—åˆ†
            macd_info = analysis.get('macd', {})
            macd_crossover = macd_info.get('crossover', 'no_crossover')
            if 'crossover' in macd_crossover and macd_crossover != 'no_crossover':
                scores.append(0.7)
            else:
                scores.append(0.3)
            
            # å¸ƒæ—å¸¦å¾—åˆ†
            bollinger_info = analysis.get('bollinger', {})
            bollinger_position = bollinger_info.get('position', 0.5)
            if bollinger_position > 0.8 or bollinger_position < 0.2:
                scores.append(0.8)
            else:
                scores.append(0.4)
            
            # æˆäº¤é‡å¾—åˆ†
            volume_info = analysis.get('volume', {})
            if volume_info.get('volume_anomaly', False):
                scores.append(0.6)
            else:
                scores.append(0.3)
            
            return np.mean(scores) if scores else 0.5
            
        except Exception:
            return 0.5
    
    def _estimate_ml_timing(self, analysis: Dict[str, Any]) -> str:
        """ä¼°è®¡æœºå™¨å­¦ä¹ é¢„æµ‹æ—¶æœº"""
        try:
            ml_score = analysis.get('ml_score', 0.0)
            
            if ml_score > 0.8:
                return 'immediate'
            elif ml_score > 0.6:
                return 'next_1_2_periods'
            elif ml_score > 0.4:
                return 'next_3_5_periods'
            else:
                return 'wait_for_better_signal'
                
        except Exception:
            return 'monitor_closely'
    
    def _recommend_monitoring_frequency(self, timing: str) -> str:
        """æ¨èç›‘æ§é¢‘ç‡"""
        frequency_map = {
            'immediate': 'real_time',
            'next_1_2_periods': 'every_period',
            'next_1_3_periods': 'every_period',
            'next_3_5_periods': 'every_2_periods',
            'wait_for_better_signal': 'every_5_periods',
            'monitor_closely': 'every_period'
        }
        
        return frequency_map.get(timing, 'every_period')
    
    def _identify_entry_signals(self, analysis: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«å…¥åœºä¿¡å·"""
        try:
            signals = []
            
            # æŠ€æœ¯ä¿¡å·
            if analysis.get('technical_score', 0) > 0.6:
                signals.append('technical_confirmation')
            
            # æ¨¡å¼ä¿¡å·
            if analysis.get('pattern_score', 0) > 0.7:
                signals.append('pattern_breakout')
            
            # æœºå™¨å­¦ä¹ ä¿¡å·
            if analysis.get('ml_score', 0) > 0.7:
                signals.append('ml_consensus')
            
            # é‡å­ä¿¡å·
            if analysis.get('quantum_score', 0) > 0.6:
                signals.append('quantum_coherence_break')
            
            return signals
            
        except Exception:
            return []
    
    def _identify_exit_signals(self, analysis: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«é€€å‡ºä¿¡å·"""
        try:
            signals = []
            
            # é£é™©ä¿¡å·
            if analysis.get('risk_score', 0) > 0.7:
                signals.append('high_risk_exit')
            
            # åå‘ä¿¡å·
            if analysis.get('reversal_confidence', 0) < 0.3:
                signals.append('reversal_failure')
            
            # æ—¶é—´è¡°å‡
            signals.append('time_decay_after_5_periods')
            
            return signals
            
        except Exception:
            return []
    
    def _assess_timing_risk(self, timing: str, analysis: Dict[str, Any]) -> str:
        """è¯„ä¼°æ—¶æœºé£é™©"""
        try:
            confidence = analysis.get('final_score', 0.5)
            
            if timing == 'immediate':
                if confidence > 0.8:
                    return 'low'
                elif confidence > 0.6:
                    return 'medium'
                else:
                    return 'high'
            elif 'wait' in timing:
                return 'low'
            else:
                return 'medium'
                
        except Exception:
            return 'medium'
    
    def _quantify_model_uncertainty(self, fusion_result: Dict) -> float:
        """é‡åŒ–æ¨¡å‹ä¸ç¡®å®šæ€§"""
        try:
            # åŸºäºä¿¡å·æ–¹å·®çš„æ¨¡å‹ä¸ç¡®å®šæ€§
            signal_variance = fusion_result.get('signal_variance', 0.0)
            
            # åŸºäºæ¨¡å‹ä¸€è‡´æ€§çš„ä¸ç¡®å®šæ€§
            consistency = fusion_result.get('signal_consistency', 1.0)
            consistency_uncertainty = 1.0 - consistency
            
            # ç»¼åˆä¸ç¡®å®šæ€§
            model_uncertainty = (signal_variance + consistency_uncertainty) / 2
            
            return min(1.0, model_uncertainty)
            
        except Exception:
            return 0.5
    
    def _quantify_data_uncertainty(self, fusion_result: Dict) -> float:
        """é‡åŒ–æ•°æ®ä¸ç¡®å®šæ€§"""
        try:
            # åŸºäºä¿¡å·æ•°é‡çš„æ•°æ®ä¸ç¡®å®šæ€§
            signal_count = fusion_result.get('signal_count', 0)
            if signal_count == 0:
                return 1.0
            
            # æœŸæœ›ä¿¡å·æ•°é‡
            expected_signals = 6  # technical, wavelet, fourier, nonlinear, ml, quantum
            
            data_sufficiency = signal_count / expected_signals
            data_uncertainty = 1.0 - min(1.0, data_sufficiency)
            
            return data_uncertainty
            
        except Exception:
            return 0.5
    
    def _quantify_parameter_uncertainty(self) -> float:
        """é‡åŒ–å‚æ•°ä¸ç¡®å®šæ€§"""
        try:
            # åŸºäºå‚æ•°å˜åŒ–å†å²çš„ä¸ç¡®å®šæ€§
            parameter_stability = 0.8  # ç®€åŒ–ï¼šå‡è®¾å‚æ•°ç›¸å¯¹ç¨³å®š
            
            return 1.0 - parameter_stability
            
        except Exception:
            return 0.2
    
    def _quantify_epistemic_uncertainty(self, fusion_result: Dict) -> float:
        """é‡åŒ–è®¤çŸ¥ä¸ç¡®å®šæ€§"""
        try:
            # åŸºäºæ¨¡å‹ç†è§£ç¨‹åº¦çš„ä¸ç¡®å®šæ€§
            signal_quality = fusion_result.get('signal_quality', 0.5)
            
            # è®¤çŸ¥ä¸ç¡®å®šæ€§ä¸ä¿¡å·è´¨é‡åç›¸å…³
            epistemic_uncertainty = 1.0 - signal_quality
            
            return epistemic_uncertainty
            
        except Exception:
            return 0.5
    
    def _quantify_aleatoric_uncertainty(self, risk_assessment: Dict) -> float:
        """é‡åŒ–éšæœºä¸ç¡®å®šæ€§"""
        try:
            # åŸºäºç³»ç»Ÿéšæœºæ€§çš„ä¸ç¡®å®šæ€§
            volatility_risk = risk_assessment.get('volatility_risk', {})
            volatility = volatility_risk.get('volatility', 0.1)
            
            # éšæœºä¸ç¡®å®šæ€§ä¸æ³¢åŠ¨ç‡ç›¸å…³
            aleatoric_uncertainty = min(1.0, volatility * 5)
            
            return aleatoric_uncertainty
            
        except Exception:
            return 0.3
    
    def _calculate_total_uncertainty(self, uncertainty_sources: Dict[str, float]) -> float:
        """è®¡ç®—æ€»ä¸ç¡®å®šæ€§"""
        try:
            # ä½¿ç”¨å¹³æ–¹å’Œå¼€æ–¹æ³•ç»„åˆä¸ç¡®å®šæ€§
            uncertainties = list(uncertainty_sources.values())
            total_uncertainty_squared = sum(u**2 for u in uncertainties)
            total_uncertainty = np.sqrt(total_uncertainty_squared / len(uncertainties))
            
            return min(1.0, total_uncertainty)
            
        except Exception:
            return 0.5
    
    def _analyze_uncertainty_propagation(self, uncertainty_sources: Dict[str, float]) -> Dict[str, Any]:
        """åˆ†æä¸ç¡®å®šæ€§ä¼ æ’­"""
        try:
            # ä¸ç¡®å®šæ€§ä¼ æ’­åˆ†æ
            propagation_analysis = {
                'dominant_source': max(uncertainty_sources.keys(), 
                                     key=lambda k: uncertainty_sources[k]),
                'uncertainty_distribution': uncertainty_sources,
                'propagation_factor': max(uncertainty_sources.values()) / 
                                    (np.mean(list(uncertainty_sources.values())) + 1e-10)
            }
            
            return propagation_analysis
            
        except Exception:
            return {'dominant_source': 'unknown'}
    
    def _perform_sensitivity_analysis(self, fusion_result: Dict) -> Dict[str, float]:
        """æ‰§è¡Œæ•æ„Ÿæ€§åˆ†æ"""
        try:
            sensitivity = {}
            
            # å¯¹å…³é”®å‚æ•°çš„æ•æ„Ÿæ€§
            base_score = fusion_result.get('consensus_score', 0.5)
            
            # æƒé‡æ•æ„Ÿæ€§ï¼ˆç®€åŒ–ï¼‰
            for signal_type in ['technical', 'wavelet', 'fourier', 'ml']:
                # æ¨¡æ‹Ÿæƒé‡å˜åŒ–çš„å½±å“
                sensitivity[f'{signal_type}_weight'] = abs(base_score - 0.5) * 0.1
            
            return sensitivity
            
        except Exception:
            return {}
    
    def _calculate_confidence_intervals(self, fusion_result: Dict, 
                                      uncertainty: float) -> Dict[str, Tuple[float, float]]:
        """è®¡ç®—ç½®ä¿¡åŒºé—´"""
        try:
            base_score = fusion_result.get('consensus_score', 0.5)
            
            # 95%ç½®ä¿¡åŒºé—´
            margin_of_error = 1.96 * uncertainty
            
            intervals = {
                '95%': (max(0.0, base_score - margin_of_error), 
                       min(1.0, base_score + margin_of_error)),
                '68%': (max(0.0, base_score - uncertainty), 
                       min(1.0, base_score + uncertainty))
            }
            
            return intervals
            
        except Exception:
            return {'95%': (0.0, 1.0), '68%': (0.25, 0.75)}
    
    def _rank_uncertainty_sources(self, uncertainty_sources: Dict[str, float]) -> List[Tuple[str, float]]:
        """æ’åºä¸ç¡®å®šæ€§æ¥æº"""
        try:
            return sorted(uncertainty_sources.items(), key=lambda x: x[1], reverse=True)
        except Exception:
            return []
    
    def _suggest_uncertainty_reduction(self, uncertainty_sources: Dict[str, float]) -> List[str]:
        """å»ºè®®ä¸ç¡®å®šæ€§å‡å°‘æ–¹æ³•"""
        try:
            suggestions = []
            
            for source, uncertainty in uncertainty_sources.items():
                if uncertainty > 0.5:
                    if source == 'model':
                        suggestions.append('å¢åŠ æ¨¡å‹æ•°é‡å’Œå¤šæ ·æ€§')
                    elif source == 'data':
                        suggestions.append('æ”¶é›†æ›´å¤šé«˜è´¨é‡æ•°æ®')
                    elif source == 'parameter':
                        suggestions.append('ä¼˜åŒ–å‚æ•°è®¾ç½®')
                    elif source == 'epistemic':
                        suggestions.append('æé«˜æ¨¡å‹ç†è®ºåŸºç¡€')
                    elif source == 'aleatoric':
                        suggestions.append('è€ƒè™‘ä½¿ç”¨æ¦‚ç‡æ¨¡å‹')
            
            return suggestions
            
        except Exception:
            return ['ä¼˜åŒ–æ•´ä½“ç³»ç»Ÿé…ç½®']
    
    # ========== å¼‚å¸¸å¤„ç†å’Œæ¢å¤æ–¹æ³• ==========
    
    def _handle_prediction_error(self, error: Exception) -> Dict[str, Any]:
        """å¤„ç†é¢„æµ‹é”™è¯¯"""
        try:
            error_type = type(error).__name__
            error_message = str(error)
            
            # è®°å½•é”™è¯¯
            self.logger.error(f"é¢„æµ‹é”™è¯¯ {error_type}: {error_message}")
            
            # å°è¯•æ¢å¤
            recovery_result = self._attempt_error_recovery(error_type)
            
            return {
                'success': False,
                'error_type': error_type,
                'error_message': error_message,
                'recovery_attempted': recovery_result['attempted'],
                'recovery_success': recovery_result['success'],
                'fallback_recommendation': self._get_fallback_recommendation()
            }
            
        except Exception as e:
            return {
                'success': False,
                'error_type': 'critical_error',
                'error_message': f'é”™è¯¯å¤„ç†å¤±è´¥: {str(e)}'
            }
    
    def _attempt_error_recovery(self, error_type: str) -> Dict[str, bool]:
        """å°è¯•é”™è¯¯æ¢å¤"""
        try:
            recovery_attempted = True
            recovery_success = False
            
            if error_type == 'MemoryError':
                # å†…å­˜é”™è¯¯æ¢å¤
                self._trigger_memory_cleanup()
                recovery_success = True
                
            elif error_type == 'ValueError':
                # æ•°å€¼é”™è¯¯æ¢å¤
                self._reset_problematic_parameters()
                recovery_success = True
                
            elif error_type == 'IndexError':
                # ç´¢å¼•é”™è¯¯æ¢å¤
                self._validate_data_structures()
                recovery_success = True
                
            else:
                recovery_attempted = False
            
            return {
                'attempted': recovery_attempted,
                'success': recovery_success
            }
            
        except Exception:
            return {'attempted': False, 'success': False}
    
    def _reset_problematic_parameters(self):
        """é‡ç½®é—®é¢˜å‚æ•°"""
        try:
            # é‡ç½®ä¸ºé»˜è®¤é…ç½®
            default_config = self._get_default_config()
            
            # é‡ç½®å…³é”®å‚æ•°
            critical_params = [
                'reversal_threshold', 'momentum_sensitivity', 
                'learning_rate', 'adaptation_factor'
            ]
            
            for param in critical_params:
                if param in default_config:
                    self.config[param] = default_config[param]
            
            self.logger.info("é—®é¢˜å‚æ•°å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
            
        except Exception as e:
            self.logger.error(f"å‚æ•°é‡ç½®å¤±è´¥: {str(e)}")
    
    def _validate_data_structures(self):
        """éªŒè¯æ•°æ®ç»“æ„"""
        try:
            # éªŒè¯å…³é”®æ•°æ®ç»“æ„
            structures_to_check = [
                'trend_history', 'technical_indicators', 
                'performance_monitor', 'signal_fusion'
            ]
            
            for structure_name in structures_to_check:
                if hasattr(self, structure_name):
                    structure = getattr(self, structure_name)
                    if not structure:
                        # é‡æ–°åˆå§‹åŒ–ç©ºç»“æ„
                        if structure_name == 'trend_history':
                            self.trend_history = defaultdict(lambda: deque(maxlen=self.config['history_window']))
                        # å…¶ä»–ç»“æ„çš„é‡æ–°åˆå§‹åŒ–...
            
            self.logger.info("æ•°æ®ç»“æ„éªŒè¯å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ•°æ®ç»“æ„éªŒè¯å¤±è´¥: {str(e)}")
    
    def _get_fallback_recommendation(self) -> Dict[str, Any]:
        """è·å–å¤‡ç”¨æ¨è"""
        try:
            # ç®€å•çš„å¤‡ç”¨ç­–ç•¥
            return {
                'recommended_tail': 5,  # ä¸­æ€§é€‰æ‹©
                'confidence': 0.3,
                'reasoning': 'ç³»ç»Ÿé”™è¯¯æ—¶çš„å¤‡ç”¨æ¨è',
                'recommendation_type': 'fallback',
                'risk_level': 'medium'
            }
            
        except Exception:
            return {
                'recommended_tail': None,
                'confidence': 0.0,
                'reasoning': 'æ— æ³•æä¾›æ¨è'
            }
    
    # ========== ç‰ˆæœ¬å…¼å®¹æ€§æ–¹æ³• ==========
    
    def predict_legacy(self, candidate_tails: List[int], historical_data: List[Dict]) -> Dict[str, Any]:
        """ä¼ ç»Ÿç‰ˆæœ¬å…¼å®¹çš„é¢„æµ‹æ–¹æ³•"""
        try:
            # è°ƒç”¨ç®€åŒ–é¢„æµ‹æ–¹æ³•
            return self.predict_simple(candidate_tails, historical_data)
            
        except Exception as e:
            return self._handle_prediction_error(e)
    
    def get_simple_status(self) -> Dict[str, Any]:
        """è·å–ç®€åŒ–çŠ¶æ€ä¿¡æ¯"""
        try:
            return {
                'total_predictions': self.total_predictions,
                'accuracy': self.prediction_accuracy,
                'system_health': 'normal' if self.prediction_accuracy > 0.5 else 'needs_attention'
            }
            
        except Exception:
            return {'status': 'error'}
    
    # ========== ç³»ç»Ÿå…³é—­å’Œæ¸…ç† ==========
    
    def shutdown(self):
        """ç³»ç»Ÿå…³é—­æ¸…ç†"""
        try:
            self.logger.info("å¼€å§‹ç³»ç»Ÿå…³é—­æ¸…ç†...")
            
            # å…³é—­çº¿ç¨‹æ± 
            if hasattr(self, 'thread_pool') and self.thread_pool:
                self.thread_pool.shutdown(wait=True)
                
            if hasattr(self, 'process_pool') and self.process_pool:
                self.process_pool.shutdown(wait=True)
            
            # æ¸…ç†ç¼“å­˜
            if hasattr(self, 'cache'):
                self.cache.clear()
            
            # æœ€ç»ˆå†…å­˜æ¸…ç†
            gc.collect()
            
            self.logger.info("ç³»ç»Ÿå…³é—­æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿå…³é—­æ¸…ç†é”™è¯¯: {str(e)}")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        try:
            self.shutdown()
        except:
            pass

# ========== æ¨¡å—çº§åˆ«çš„å·¥å…·å‡½æ•° ==========

def create_anti_trend_hunter(config: Dict[str, Any] = None) -> AntiTrendHunter:
    """åˆ›å»ºåè¶‹åŠ¿çŒæ‰‹å®ä¾‹çš„å·¥å‚å‡½æ•°"""
    try:
        return AntiTrendHunter(config)
    except Exception as e:
        print(f"âŒ åè¶‹åŠ¿çŒæ‰‹åˆ›å»ºå¤±è´¥: {str(e)}")
        return None

def validate_historical_data(historical_data: List[Dict]) -> bool:
    """éªŒè¯å†å²æ•°æ®æ ¼å¼"""
    try:
        if not isinstance(historical_data, list):
            return False
        
        if len(historical_data) == 0:
            return False
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        for period in historical_data:
            if not isinstance(period, dict):
                return False
            
            if 'tails' not in period:
                return False
            
            if not isinstance(period['tails'], list):
                return False
        
        return True
        
    except Exception:
        return False

def format_prediction_result(result: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–é¢„æµ‹ç»“æœä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    try:
        if not result.get('success', False):
            return f"âŒ é¢„æµ‹å¤±è´¥: {result.get('reasoning', 'æœªçŸ¥é”™è¯¯')}"
        
        tail = result.get('recommended_tail')
        confidence = result.get('confidence', 0.0)
        reasoning = result.get('reasoning', '')
        
        confidence_level = "é«˜" if confidence > 0.7 else "ä¸­" if confidence > 0.5 else "ä½"
        
        return f"""
ğŸ¯ åè¶‹åŠ¿é¢„æµ‹ç»“æœ
æ¨èå°¾æ•°: {tail}
ç½®ä¿¡åº¦: {confidence:.1%} ({confidence_level})
åˆ†æä¾æ®: {reasoning}
é¢„æµ‹ç±»å‹: {result.get('reversal_type', 'æŠ€æœ¯åè½¬')}
é£é™©è¯„ä¼°: {result.get('risk_assessment', {}).get('risk_level', 'ä¸­ç­‰')}
å»ºè®®æ—¶æœº: {result.get('timing_analysis', {}).get('optimal_timing', 'å¯†åˆ‡å…³æ³¨')}
"""
        
    except Exception:
        return "âŒ ç»“æœæ ¼å¼åŒ–å¤±è´¥"

# ========== æ¨¡å—åˆå§‹åŒ– ==========

if __name__ == "__main__":
    print("ğŸ§¬ ç§‘ç ”çº§åè¶‹åŠ¿çŒæ‰‹æ¨¡å—å·²åŠ è½½")
    print("   ç‰ˆæœ¬: 2.0.0-research")
    print("   åŠŸèƒ½: å¤šç»´åº¦åè¶‹åŠ¿åˆ†æä¸é¢„æµ‹")
    print("   ä½œè€…: AI Research Team")
    print("   ä½¿ç”¨: hunter = create_anti_trend_hunter()")
    
    # ç®€å•çš„ç³»ç»Ÿæµ‹è¯•
    try:
        test_hunter = create_anti_trend_hunter()
        if test_hunter:
            print("âœ… ç³»ç»Ÿåˆå§‹åŒ–æµ‹è¯•é€šè¿‡")
            test_hunter.shutdown()
        else:
            print("âŒ ç³»ç»Ÿåˆå§‹åŒ–æµ‹è¯•å¤±è´¥")
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿæµ‹è¯•é”™è¯¯: {str(e)}")
        
    def predict_simple(self, candidate_tails: List[int], historical_data: List[Dict]) -> Dict[str, Any]:
        """
        ç®€åŒ–ç‰ˆé¢„æµ‹æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        # åˆ›å»ºæ•°æ®å“ˆå¸Œ
        data_hash = self._create_data_hash(historical_data)
        
        # è°ƒç”¨å®Œæ•´ç‰ˆé¢„æµ‹æ–¹æ³•
        return self.predict(tuple(candidate_tails), data_hash, PredictionHorizon.SHORT)
    
    def _update_trend_states(self, historical_data: List[Dict]):
        """æ›´æ–°æ‰€æœ‰å°¾æ•°çš„è¶‹åŠ¿çŠ¶æ€"""
        for tail in range(10):
            # è®¡ç®—ä¸åŒæ—¶é—´çª—å£çš„å‡ºç°é¢‘ç‡
            frequencies = {}
            for window_name, window_size in self.analysis_windows.items():
                if len(historical_data) >= window_size:
                    count = sum(1 for i in range(window_size) 
                              if tail in historical_data[i].get('tails', []))
                    frequencies[window_name] = count / window_size
            
            # åˆ¤æ–­è¶‹åŠ¿çŠ¶æ€
            if frequencies:
                # è®¡ç®—è¶‹åŠ¿æ–œç‡
                if 'long' in frequencies and 'short' in frequencies:
                    trend_slope = frequencies['short'] - frequencies['long']
                    
                    if trend_slope > 0.3:
                        state = TrendState.STRONG_UPTREND
                    elif trend_slope > 0.15:
                        state = TrendState.MODERATE_UPTREND
                    elif trend_slope > 0.05:
                        state = TrendState.WEAK_UPTREND
                    elif trend_slope > -0.05:
                        state = TrendState.SIDEWAYS
                    elif trend_slope > -0.15:
                        state = TrendState.WEAK_DOWNTREND
                    elif trend_slope > -0.3:
                        state = TrendState.MODERATE_DOWNTREND
                    else:
                        state = TrendState.STRONG_DOWNTREND
                    
                    self.trend_states[tail] = {
                        'state': state,
                        'slope': trend_slope,
                        'frequencies': frequencies
                    }
    
    def _calculate_technical_indicators(self, historical_data: List[Dict]):
        """è®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡"""
        for tail in range(10):
            # RSIè®¡ç®—
            self._calculate_rsi(tail, historical_data)
            
            # MACDè®¡ç®—
            self._calculate_macd(tail, historical_data)
            
            # éšæœºæŒ‡æ ‡è®¡ç®—
            self._calculate_stochastic(tail, historical_data)
            
            # Williams %Rè®¡ç®—
            self._calculate_williams_r(tail, historical_data)
            
            # OBVè®¡ç®—
            self._calculate_obv(tail, historical_data)
            
            # CCIè®¡ç®—
            self._calculate_cci(tail, historical_data)
            
            # MFIè®¡ç®—
            self._calculate_mfi(tail, historical_data)
            
            # ADXè®¡ç®—
            self._calculate_adx(tail, historical_data)
    
    def _analyze_trend_strength(self, tail: int, historical_data: List[Dict]) -> Dict:
        """åˆ†æè¶‹åŠ¿å¼ºåº¦"""
        if tail not in self.trend_states:
            return {'state': 'unknown', 'strength': 0.0}
        
        trend_info = self.trend_states[tail]
        state = trend_info['state']
        
        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
        strength = abs(state.value) / 4.0  # å½’ä¸€åŒ–åˆ°0-1
        
        # è®¡ç®—è¶‹åŠ¿æŒç»­æ—¶é—´
        duration = self._calculate_trend_duration(tail, historical_data)
        
        # è®¡ç®—è¶‹åŠ¿ä¸€è‡´æ€§
        consistency = self._calculate_trend_consistency(tail, historical_data)
        
        # ç»¼åˆè¶‹åŠ¿å¼ºåº¦
        total_strength = (strength * 0.4 + 
                         min(1.0, duration / 10.0) * 0.3 + 
                         consistency * 0.3)
        
        return {
            'state': state.name,
            'strength': total_strength,
            'raw_strength': strength,
            'duration': duration,
            'consistency': consistency
        }
    
    def _analyze_momentum(self, tail: int, historical_data: List[Dict]) -> Dict:
        """åˆ†æåŠ¨é‡"""
        if len(historical_data) < self.config['momentum_period']:
            return {'momentum': 0.0, 'acceleration': 0.0}
        
        period = self.config['momentum_period']
        
        # è®¡ç®—åŠ¨é‡
        recent_count = sum(1 for i in range(period // 2) 
                         if tail in historical_data[i].get('tails', []))
        earlier_count = sum(1 for i in range(period // 2, period) 
                          if tail in historical_data[i].get('tails', []))
        
        if earlier_count > 0:
            momentum = (recent_count - earlier_count) / earlier_count
        else:
            momentum = recent_count / (period // 2)
        
        # è®¡ç®—åŠ é€Ÿåº¦
        if len(historical_data) >= period * 2:
            prev_momentum = self._calculate_previous_momentum(tail, historical_data, period)
            acceleration = momentum - prev_momentum
        else:
            acceleration = 0.0
        
        return {
            'momentum': momentum,
            'acceleration': acceleration,
            'is_decelerating': acceleration < -0.1,
            'is_accelerating': acceleration > 0.1
        }
    
    def _detect_trend_exhaustion(self, tail: int, historical_data: List[Dict]) -> Dict:
        """æ£€æµ‹è¶‹åŠ¿è€—å°½"""
        exhaustion_signals = []
        
        # 1. è¿ç»­å‡ºç°å¯¼è‡´çš„è€—å°½
        consecutive_count = 0
        for period in historical_data:
            if tail in period.get('tails', []):
                consecutive_count += 1
            else:
                break
        
        if consecutive_count >= 5:
            exhaustion_signals.append(('consecutive_exhaustion', min(1.0, consecutive_count / 8.0)))
        
        # 2. é¢‘ç‡æç«¯å¯¼è‡´çš„è€—å°½
        if len(historical_data) >= 10:
            recent_freq = sum(1 for i in range(10) 
                            if tail in historical_data[i].get('tails', [])) / 10.0
            if recent_freq > 0.7:
                exhaustion_signals.append(('frequency_exhaustion', recent_freq))
        
        # 3. RSIè¶…ä¹°è¶…å–
        if tail in self.technical_indicators['rsi'] and self.technical_indicators['rsi'][tail]:
            latest_rsi = self.technical_indicators['rsi'][tail][-1]
            if latest_rsi > self.config['rsi_overbought']:
                exhaustion_signals.append(('rsi_overbought', (latest_rsi - 50) / 50))
            elif latest_rsi < self.config['rsi_oversold']:
                exhaustion_signals.append(('rsi_oversold', (50 - latest_rsi) / 50))
        
        # 4. åŠ¨é‡è¡°å‡
        momentum_data = self._analyze_momentum(tail, historical_data)
        if momentum_data['is_decelerating']:
            exhaustion_signals.append(('momentum_decay', abs(momentum_data['acceleration'])))
        
        # ç»¼åˆè€—å°½ç¨‹åº¦
        if exhaustion_signals:
            exhaustion_level = np.mean([score for _, score in exhaustion_signals])
            exhaustion_types = [signal_type for signal_type, _ in exhaustion_signals]
        else:
            exhaustion_level = 0.0
            exhaustion_types = []
        
        return {
            'exhaustion': exhaustion_level,
            'signals': exhaustion_types,
            'is_exhausted': exhaustion_level > self.dynamic_thresholds['trend_exhaustion']
        }
    
    def _identify_reversal_signals(self, tail: int, historical_data: List[Dict]) -> Dict:
        """è¯†åˆ«åè½¬ä¿¡å·"""
        reversal_signals = []
        
        # 1. èƒŒç¦»ä¿¡å·
        divergence = self._check_divergence(tail, historical_data)
        if divergence['has_divergence']:
            reversal_signals.append(('divergence', divergence['strength']))
        
        # 2. æ”¯æ’‘é˜»åŠ›çªç ´
        sr_break = self._check_support_resistance_break(tail, historical_data)
        if sr_break['has_break']:
            reversal_signals.append(('sr_break', sr_break['strength']))
        
        # 3. å½¢æ€åè½¬ä¿¡å·
        pattern_reversal = self._check_pattern_reversal(tail, historical_data)
        if pattern_reversal['has_reversal']:
            reversal_signals.append(('pattern', pattern_reversal['confidence']))
        
        # 4. æˆäº¤é‡å¼‚å¸¸
        volume_anomaly = self._check_volume_anomaly(tail, historical_data)
        if volume_anomaly['has_anomaly']:
            reversal_signals.append(('volume', volume_anomaly['strength']))
        
        # 5. æå€¼åè½¬
        extreme_reversal = self._check_extreme_reversal(tail, historical_data)
        if extreme_reversal['is_extreme']:
            reversal_signals.append(('extreme', extreme_reversal['probability']))
        
        # ç»¼åˆåè½¬ç½®ä¿¡åº¦
        if reversal_signals:
            confidence = np.mean([score for _, score in reversal_signals])
            signal_types = [signal_type for signal_type, _ in reversal_signals]
        else:
            confidence = 0.0
            signal_types = []
        
        return {
            'confidence': confidence,
            'signals': signal_types,
            'signal_count': len(reversal_signals),
            'has_strong_signal': confidence > self.dynamic_thresholds['reversal_confidence']
        }
    
    def _analyze_breakout_potential(self, tail: int, historical_data: List[Dict]) -> Dict:
        """åˆ†æçªç ´æ½œåŠ›"""
        # è®¡ç®—è¿‘æœŸçš„éœ‡è¡åŒºé—´
        if len(historical_data) < 10:
            return {'potential': 0.0, 'direction': 'unknown'}
        
        # è®¡ç®—æœ€è¿‘10æœŸçš„å‡ºç°æƒ…å†µ
        appearances = []
        for i in range(10):
            appearances.append(1 if tail in historical_data[i].get('tails', []) else 0)
        
        # è®¡ç®—éœ‡è¡åŒºé—´
        high = max(appearances)
        low = min(appearances)
        current = appearances[0]
        
        # åˆ¤æ–­çªç ´æ–¹å‘å’Œæ½œåŠ›
        if current == high and high > np.mean(appearances):
            # å‘ä¸Šçªç ´
            potential = min(1.0, (current - np.mean(appearances)) / 0.5)
            direction = 'upward'
        elif current == low and low < np.mean(appearances):
            # å‘ä¸‹çªç ´
            potential = min(1.0, (np.mean(appearances) - current) / 0.5)
            direction = 'downward'
        else:
            potential = 0.0
            direction = 'sideways'
        
        # è®¡ç®—çªç ´å¼ºåº¦
        if len(historical_data) >= 20:
            long_term_mean = sum(1 for i in range(20) 
                               if tail in historical_data[i].get('tails', [])) / 20.0
            breakout_strength = abs(current - long_term_mean)
        else:
            breakout_strength = 0.0
        
        return {
            'potential': potential,
            'direction': direction,
            'strength': breakout_strength,
            'is_breakout': potential > 0.5
        }
    
    def _analyze_volume_divergence(self, tail: int, historical_data: List[Dict]) -> float:
        """åˆ†ææˆäº¤é‡èƒŒç¦»"""
        if len(historical_data) < 10:
            return 0.0
        
        # ç®€åŒ–çš„æˆäº¤é‡åˆ†æï¼ˆç”¨å‡ºç°çš„å°¾æ•°æ€»æ•°æ¨¡æ‹Ÿæˆäº¤é‡ï¼‰
        price_trend = []
        volume_trend = []
        
        for i in range(10):
            # "ä»·æ ¼"ç”¨å‡ºç°é¢‘ç‡è¡¨ç¤º
            price = 1 if tail in historical_data[i].get('tails', []) else 0
            price_trend.append(price)
            
            # "æˆäº¤é‡"ç”¨è¯¥æœŸæ€»å°¾æ•°è¡¨ç¤º
            volume = len(historical_data[i].get('tails', []))
            volume_trend.append(volume)
        
        # è®¡ç®—è¶‹åŠ¿ç›¸å…³æ€§
        if len(set(price_trend)) > 1 and len(set(volume_trend)) > 1:
            correlation = np.corrcoef(price_trend, volume_trend)[0, 1]
            
            # è´Ÿç›¸å…³è¡¨ç¤ºèƒŒç¦»
            if correlation < -0.3:
                divergence = abs(correlation)
            else:
                divergence = 0.0
        else:
            divergence = 0.0
        
        return divergence
    
    def _calculate_technical_score(self, tail: int) -> float:
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç»¼åˆå¾—åˆ†"""
        scores = []
        
        # RSIå¾—åˆ†
        if tail in self.technical_indicators['rsi'] and self.technical_indicators['rsi'][tail]:
            rsi = self.technical_indicators['rsi'][tail][-1]
            if rsi > self.config['rsi_overbought'] or rsi < self.config['rsi_oversold']:
                scores.append(0.8)
            else:
                scores.append(0.2)
        
        # MACDå¾—åˆ†
        if tail in self.technical_indicators['macd'] and self.technical_indicators['macd'][tail]:
            macd_signal = self.technical_indicators['macd'][tail][-1]
            if abs(macd_signal) > 0.5:
                scores.append(0.7)
            else:
                scores.append(0.3)
        
        # å…¶ä»–æŒ‡æ ‡å¾—åˆ†...
        
        return np.mean(scores) if scores else 0.5
    
    def _match_reversal_patterns(self, tail: int, historical_data: List[Dict]) -> List[Dict]:
        """åŒ¹é…åè½¬æ¨¡å¼"""
        matched_patterns = []
        
        for pattern_name, pattern_func in self.reversal_pattern_library.items():
            if pattern_func(tail, historical_data):
                matched_patterns.append({
                    'name': pattern_name,
                    'confidence': 0.7  # ç®€åŒ–å¤„ç†
                })
        
        return matched_patterns
    
    def _calculate_reversal_score(self, trend_analysis: Dict, momentum_analysis: Dict,
                                 exhaustion_analysis: Dict, reversal_signals: Dict,
                                 breakout_analysis: Dict, volume_divergence: float,
                                 technical_score: float, pattern_matches: List) -> float:
        """è®¡ç®—ç»¼åˆåè½¬å¾—åˆ†"""
        
        # åŸºç¡€åˆ†æ•°
        base_score = 0.0
        
        # 1. è¶‹åŠ¿å¼ºåº¦è´¡çŒ®ï¼ˆå¼ºè¶‹åŠ¿åè½¬æ›´æœ‰ä»·å€¼ï¼‰
        if trend_analysis['strength'] > 0.6:
            base_score += trend_analysis['strength'] * 0.2
        
        # 2. åŠ¨é‡è´¡çŒ®ï¼ˆè´ŸåŠ¨é‡æˆ–å‡é€Ÿå¢åŠ åè½¬å¯èƒ½ï¼‰
        if momentum_analysis['momentum'] < 0 or momentum_analysis['is_decelerating']:
            base_score += 0.15
        
        # 3. è€—å°½è´¡çŒ®
        base_score += exhaustion_analysis['exhaustion'] * 0.25
        
        # 4. åè½¬ä¿¡å·è´¡çŒ®
        base_score += reversal_signals['confidence'] * 0.2
        
        # 5. çªç ´æ½œåŠ›è´¡çŒ®
        if breakout_analysis['is_breakout']:
            base_score += breakout_analysis['potential'] * 0.1
        
        # 6. æˆäº¤é‡èƒŒç¦»è´¡çŒ®
        base_score += volume_divergence * 0.1
        
        # 7. æŠ€æœ¯æŒ‡æ ‡è´¡çŒ®
        base_score += technical_score * 0.1
        
        # 8. æ¨¡å¼åŒ¹é…è´¡çŒ®
        if pattern_matches:
            base_score += min(0.1, len(pattern_matches) * 0.03)
        
        # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
        final_score = min(1.0, max(0.0, base_score))
        
        # åº”ç”¨éçº¿æ€§å˜æ¢å¢å¼ºåŒºåˆ†åº¦
        if final_score > 0.7:
            final_score = 0.7 + (final_score - 0.7) * 1.5
        elif final_score < 0.3:
            final_score = final_score * 0.7
        
        return min(1.0, final_score)
    
    def _calculate_confidence(self, score: float, analysis: Dict) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # åŸºç¡€ç½®ä¿¡åº¦æ¥è‡ªå¾—åˆ†
        confidence_factors.append(score)
        
        # è¶‹åŠ¿å¼ºåº¦å½±å“ç½®ä¿¡åº¦
        if 'trend_strength' in analysis:
            confidence_factors.append(analysis['trend_strength'])
        
        # è€—å°½ç¨‹åº¦å½±å“ç½®ä¿¡åº¦
        if 'exhaustion_level' in analysis:
            confidence_factors.append(analysis['exhaustion_level'])
        
        # åè½¬ä¿¡å·å½±å“ç½®ä¿¡åº¦
        if 'reversal_confidence' in analysis:
            confidence_factors.append(analysis['reversal_confidence'])
        
        # æ¨¡å¼åŒ¹é…æ•°é‡å½±å“ç½®ä¿¡åº¦
        if 'pattern_matches' in analysis:
            pattern_confidence = min(1.0, analysis['pattern_matches'] / 3.0)
            confidence_factors.append(pattern_confidence)
        
        return np.mean(confidence_factors) if confidence_factors else score
    
    def _generate_reasoning(self, tail: int, analysis: Dict) -> str:
        """ç”Ÿæˆæ¨ç†è¯´æ˜"""
        reasons = []
        
        if analysis['trend_state'] in ['STRONG_UPTREND', 'MODERATE_UPTREND']:
            reasons.append(f"å°¾æ•°{tail}å¤„äº{analysis['trend_state']}ï¼Œå³å°†åè½¬")
        
        if analysis['exhaustion_level'] > 0.7:
            reasons.append(f"è¶‹åŠ¿è€—å°½ç¨‹åº¦è¾¾{analysis['exhaustion_level']:.0%}")
        
        if analysis['reversal_confidence'] > 0.6:
            reasons.append(f"åè½¬ä¿¡å·å¼ºåº¦{analysis['reversal_confidence']:.0%}")
        
        if analysis['volume_divergence'] > 0.3:
            reasons.append("å‡ºç°æ˜æ˜¾æˆäº¤é‡èƒŒç¦»")
        
        if analysis['pattern_matches'] > 0:
            reasons.append(f"åŒ¹é…{analysis['pattern_matches']}ä¸ªåè½¬æ¨¡å¼")
        
        return "ï¼›".join(reasons) if reasons else "ç»¼åˆæŠ€æœ¯åˆ†ææ˜¾ç¤ºåè½¬æœºä¼š"
    
    # === è¾…åŠ©æ–¹æ³•å®ç° ===
    
    def _initialize_reversal_patterns(self) -> Dict:
        """åˆå§‹åŒ–åè½¬æ¨¡å¼åº“"""
        return {
            'double_top': self._check_double_top,
            'double_bottom': self._check_double_bottom,
            'head_shoulders': self._check_head_shoulders,
            'wedge': self._check_wedge,
            'triangle': self._check_triangle,
            'flag': self._check_flag,
            'channel_break': self._check_channel_break
        }
    
    def _calculate_trend_duration(self, tail: int, historical_data: List[Dict]) -> int:
        """è®¡ç®—è¶‹åŠ¿æŒç»­æ—¶é—´"""
        if tail not in self.trend_states:
            return 0
        
        current_state = self.trend_states[tail]['state']
        duration = 0
        
        # å‘åæŸ¥æ‰¾ç›¸åŒè¶‹åŠ¿çŠ¶æ€çš„æŒç»­æ—¶é—´
        for i in range(len(historical_data)):
            # ç®€åŒ–ï¼šé€šè¿‡é¢‘ç‡å˜åŒ–åˆ¤æ–­è¶‹åŠ¿æ˜¯å¦æ”¹å˜
            period_has_tail = tail in historical_data[i].get('tails', [])
            
            if current_state.value > 0:  # ä¸Šå‡è¶‹åŠ¿
                if period_has_tail:
                    duration += 1
                else:
                    if duration >= self.config['min_trend_duration']:
                        break
            else:  # ä¸‹é™è¶‹åŠ¿
                if not period_has_tail:
                    duration += 1
                else:
                    if duration >= self.config['min_trend_duration']:
                        break
        
        return duration
    
    def _calculate_trend_consistency(self, tail: int, historical_data: List[Dict]) -> float:
        """è®¡ç®—è¶‹åŠ¿ä¸€è‡´æ€§"""
        if len(historical_data) < 5:
            return 0.0
        
        # è®¡ç®—æ–¹å‘ä¸€è‡´æ€§
        directions = []
        for i in range(len(historical_data) - 1):
            current = 1 if tail in historical_data[i].get('tails', []) else 0
            next_val = 1 if tail in historical_data[i + 1].get('tails', []) else 0
            
            if current > next_val:
                directions.append(1)  # ä¸Šå‡
            elif current < next_val:
                directions.append(-1)  # ä¸‹é™
            else:
                directions.append(0)  # æŒå¹³
        
        if not directions:
            return 0.0
        
        # è®¡ç®—ä¸€è‡´æ€§ï¼ˆæ–¹å‘ç›¸åŒçš„æ¯”ä¾‹ï¼‰
        most_common = max(set(directions), key=directions.count)
        consistency = directions.count(most_common) / len(directions)
        
        return consistency
    
    def _calculate_previous_momentum(self, tail: int, historical_data: List[Dict], period: int) -> float:
        """è®¡ç®—ä¹‹å‰çš„åŠ¨é‡ï¼ˆç”¨äºè®¡ç®—åŠ é€Ÿåº¦ï¼‰"""
        if len(historical_data) < period * 2:
            return 0.0
        
        # è®¡ç®—å‰ä¸€ä¸ªå‘¨æœŸçš„åŠ¨é‡
        recent_count = sum(1 for i in range(period, period + period // 2) 
                         if tail in historical_data[i].get('tails', []))
        earlier_count = sum(1 for i in range(period + period // 2, period * 2) 
                          if tail in historical_data[i].get('tails', []))
        
        if earlier_count > 0:
            momentum = (recent_count - earlier_count) / earlier_count
        else:
            momentum = recent_count / (period // 2)
        
        return momentum
    
    def _calculate_rsi(self, tail: int, historical_data: List[Dict], period: int = 14):
        """è®¡ç®—RSIæŒ‡æ ‡"""
        if len(historical_data) < period + 1:
            return
        
        gains = []
        losses = []
        
        for i in range(period):
            current = 1 if tail in historical_data[i].get('tails', []) else 0
            previous = 1 if tail in historical_data[i + 1].get('tails', []) else 0
            
            change = current - previous
            if change > 0:
                gains.append(change)
                losses.append(0)
            else:
                gains.append(0)
                losses.append(abs(change))
        
        avg_gain = np.mean(gains) if gains else 0
        avg_loss = np.mean(losses) if losses else 0
        
        if avg_loss == 0:
            rsi = 100
        else:
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))
        
        self.technical_indicators['rsi'][tail].append(rsi)
    
    def _calculate_macd(self, tail: int, historical_data: List[Dict]):
        """è®¡ç®—MACDæŒ‡æ ‡"""
        if len(historical_data) < 26:
            return
        
        # ç®€åŒ–çš„MACDè®¡ç®—
        fast_period = 12
        slow_period = 26
        
        fast_ma = sum(1 for i in range(fast_period) 
                     if tail in historical_data[i].get('tails', [])) / fast_period
        slow_ma = sum(1 for i in range(slow_period) 
                     if tail in historical_data[i].get('tails', [])) / slow_period
        
        macd = fast_ma - slow_ma
        self.technical_indicators['macd'][tail].append(macd)
    
    def _calculate_stochastic(self, tail: int, historical_data: List[Dict], period: int = 14):
        """è®¡ç®—éšæœºæŒ‡æ ‡"""
        if len(historical_data) < period:
            return
        
        # è·å–æœŸé—´å†…çš„æœ€é«˜å’Œæœ€ä½
        values = []
        for i in range(period):
            values.append(1 if tail in historical_data[i].get('tails', []) else 0)
        
        high = max(values)
        low = min(values)
        current = values[0]
        
        if high != low:
            k = ((current - low) / (high - low)) * 100
        else:
            k = 50
        
        self.technical_indicators['stochastic'][tail].append(k)
    
    def _calculate_williams_r(self, tail: int, historical_data: List[Dict], period: int = 14):
        """è®¡ç®—Williams %RæŒ‡æ ‡"""
        if len(historical_data) < period:
            return
        
        values = []
        for i in range(period):
            values.append(1 if tail in historical_data[i].get('tails', []) else 0)
        
        high = max(values)
        low = min(values)
        current = values[0]
        
        if high != low:
            williams_r = ((high - current) / (high - low)) * -100
        else:
            williams_r = -50
        
        self.technical_indicators['williams_r'][tail].append(williams_r)
    
    def _calculate_obv(self, tail: int, historical_data: List[Dict]):
        """è®¡ç®—OBVæŒ‡æ ‡"""
        if len(historical_data) < 2:
            return
        
        # ç®€åŒ–çš„OBVè®¡ç®—
        obv = 0
        for i in range(len(historical_data) - 1):
            current = 1 if tail in historical_data[i].get('tails', []) else 0
            previous = 1 if tail in historical_data[i + 1].get('tails', []) else 0
            volume = len(historical_data[i].get('tails', []))  # ç”¨å°¾æ•°æ€»æ•°ä½œä¸ºæˆäº¤é‡
            
            if current > previous:
                obv += volume
            elif current < previous:
                obv -= volume
        
        self.technical_indicators['obv'][tail].append(obv)
    
    def _calculate_cci(self, tail: int, historical_data: List[Dict], period: int = 20):
        """è®¡ç®—CCIæŒ‡æ ‡"""
        if len(historical_data) < period:
            return
        
        # ç®€åŒ–çš„CCIè®¡ç®—
        typical_prices = []
        for i in range(period):
            value = 1 if tail in historical_data[i].get('tails', []) else 0
            typical_prices.append(value)
        
        sma = np.mean(typical_prices)
        mean_deviation = np.mean([abs(tp - sma) for tp in typical_prices])
        
        if mean_deviation != 0:
            cci = (typical_prices[0] - sma) / (0.015 * mean_deviation)
        else:
            cci = 0
        
        self.technical_indicators['cci'][tail].append(cci)
    
    def _calculate_mfi(self, tail: int, historical_data: List[Dict], period: int = 14):
        """è®¡ç®—MFIæŒ‡æ ‡"""
        if len(historical_data) < period + 1:
            return
        
        positive_flow = 0
        negative_flow = 0
        
        for i in range(period):
            current = 1 if tail in historical_data[i].get('tails', []) else 0
            previous = 1 if tail in historical_data[i + 1].get('tails', []) else 0
            volume = len(historical_data[i].get('tails', []))
            
            money_flow = current * volume
            
            if current > previous:
                positive_flow += money_flow
            elif current < previous:
                negative_flow += money_flow
        
        if negative_flow == 0:
            mfi = 100
        else:
            money_ratio = positive_flow / negative_flow
            mfi = 100 - (100 / (1 + money_ratio))
        
        self.technical_indicators['mfi'][tail].append(mfi)
    
    def _calculate_adx(self, tail: int, historical_data: List[Dict], period: int = 14):
        """è®¡ç®—ADXæŒ‡æ ‡"""
        if len(historical_data) < period + 1:
            return
        
        # ç®€åŒ–çš„ADXè®¡ç®—
        dx_values = []
        
        for i in range(period):
            current = 1 if tail in historical_data[i].get('tails', []) else 0
            previous = 1 if tail in historical_data[i + 1].get('tails', []) else 0
            
            # è®¡ç®—æ–¹å‘æ€§æŒ‡æ ‡
            if current > previous:
                plus_dm = current - previous
                minus_dm = 0
            elif current < previous:
                plus_dm = 0
                minus_dm = previous - current
            else:
                plus_dm = 0
                minus_dm = 0
            
            # ç®€åŒ–çš„DXè®¡ç®—
            if plus_dm + minus_dm > 0:
                dx = abs(plus_dm - minus_dm) / (plus_dm + minus_dm) * 100
            else:
                dx = 0
            
            dx_values.append(dx)
        
        adx = np.mean(dx_values)
        self.technical_indicators['adx'][tail].append(adx)
    
    def _check_divergence(self, tail: int, historical_data: List[Dict]) -> Dict:
        """æ£€æŸ¥èƒŒç¦»ä¿¡å·"""
        if len(historical_data) < 10:
            return {'has_divergence': False, 'strength': 0.0}
        
        # ä»·æ ¼è¶‹åŠ¿ï¼ˆç”¨å‡ºç°é¢‘ç‡è¡¨ç¤ºï¼‰
        price_trend = []
        for i in range(10):
            price_trend.append(1 if tail in historical_data[i].get('tails', []) else 0)
        
        # åŠ¨é‡è¶‹åŠ¿ï¼ˆç”¨RSIè¡¨ç¤ºï¼‰
        if tail in self.technical_indicators['rsi'] and len(self.technical_indicators['rsi'][tail]) >= 10:
            momentum_trend = self.technical_indicators['rsi'][tail][-10:]
            
            # æ£€æŸ¥ä»·æ ¼å’ŒåŠ¨é‡çš„èƒŒç¦»
            price_direction = 1 if price_trend[0] > price_trend[-1] else -1
            momentum_direction = 1 if momentum_trend[0] > momentum_trend[-1] else -1
            
            if price_direction != momentum_direction:
                # å­˜åœ¨èƒŒç¦»
                strength = abs(price_trend[0] - price_trend[-1]) + abs(momentum_trend[0] - momentum_trend[-1]) / 100
                return {'has_divergence': True, 'strength': min(1.0, strength)}
        
        return {'has_divergence': False, 'strength': 0.0}
    
    def _check_support_resistance_break(self, tail: int, historical_data: List[Dict]) -> Dict:
        """æ£€æŸ¥æ”¯æ’‘é˜»åŠ›çªç ´"""
        if len(historical_data) < 20:
            return {'has_break': False, 'strength': 0.0}
        
        # è®¡ç®—æ”¯æ’‘å’Œé˜»åŠ›æ°´å¹³
        frequencies = []
        for i in range(20):
            count = sum(1 for j in range(max(0, i-5), min(i+5, len(historical_data)))
                      if tail in historical_data[j].get('tails', []))
            frequencies.append(count / min(10, len(historical_data) - max(0, i-5)))
        
        # æ‰¾å‡ºå…³é”®æ°´å¹³
        resistance = max(frequencies)
        support = min(frequencies)
        current = frequencies[0]
        
        # æ£€æŸ¥çªç ´
        if current > resistance * 0.95:
            # é˜»åŠ›çªç ´
            return {'has_break': True, 'strength': min(1.0, (current - resistance) / resistance)}
        elif current < support * 1.05:
            # æ”¯æ’‘çªç ´
            return {'has_break': True, 'strength': min(1.0, (support - current) / support)}
        
        return {'has_break': False, 'strength': 0.0}
    
    def _check_pattern_reversal(self, tail: int, historical_data: List[Dict]) -> Dict:
        """æ£€æŸ¥å½¢æ€åè½¬ä¿¡å·"""
        patterns_found = []
        
        # æ£€æŸ¥å„ç§åè½¬å½¢æ€
        if self._check_double_top(tail, historical_data):
            patterns_found.append('double_top')
        
        if self._check_double_bottom(tail, historical_data):
            patterns_found.append('double_bottom')
        
        if self._check_head_shoulders(tail, historical_data):
            patterns_found.append('head_shoulders')
        
        if patterns_found:
            return {
                'has_reversal': True,
                'confidence': min(1.0, len(patterns_found) * 0.3),
                'patterns': patterns_found
            }
        
        return {'has_reversal': False, 'confidence': 0.0}
    
    def _check_volume_anomaly(self, tail: int, historical_data: List[Dict]) -> Dict:
        """æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸"""
        if len(historical_data) < 10:
            return {'has_anomaly': False, 'strength': 0.0}
        
        # è®¡ç®—å¹³å‡æˆäº¤é‡ï¼ˆç”¨å°¾æ•°æ€»æ•°æ¨¡æ‹Ÿï¼‰
        volumes = [len(period.get('tails', [])) for period in historical_data[:10]]
        avg_volume = np.mean(volumes)
        current_volume = volumes[0]
        
        # æ£€æŸ¥å¼‚å¸¸
        if current_volume > avg_volume * 1.5:
            # æ”¾é‡
            return {'has_anomaly': True, 'strength': min(1.0, (current_volume - avg_volume) / avg_volume)}
        elif current_volume < avg_volume * 0.5:
            # ç¼©é‡
            return {'has_anomaly': True, 'strength': min(1.0, (avg_volume - current_volume) / avg_volume)}
        
        return {'has_anomaly': False, 'strength': 0.0}
    
    def _check_extreme_reversal(self, tail: int, historical_data: List[Dict]) -> Dict:
        """æ£€æŸ¥æå€¼åè½¬"""
        if len(historical_data) < 30:
            return {'is_extreme': False, 'probability': 0.0}
        
        # è®¡ç®—30æœŸå†…çš„é¢‘ç‡
        frequency = sum(1 for i in range(30) 
                      if tail in historical_data[i].get('tails', [])) / 30.0
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æå€¼
        if frequency > 0.8:
            # æåº¦è¶…ä¹°
            return {'is_extreme': True, 'probability': frequency}
        elif frequency < 0.2:
            # æåº¦è¶…å–
            return {'is_extreme': True, 'probability': 1 - frequency}
        
        return {'is_extreme': False, 'probability': 0.0}
    
    # === åè½¬å½¢æ€æ£€æµ‹æ–¹æ³• ===
    
    def _check_double_top(self, tail: int, historical_data: List[Dict]) -> bool:
        """æ£€æŸ¥åŒé¡¶å½¢æ€"""
        if len(historical_data) < 15:
            return False
        
        # ç®€åŒ–ï¼šæ£€æŸ¥ä¸¤ä¸ªç›¸ä¼¼çš„é«˜ç‚¹
        peaks = []
        for i in range(1, 14):
            prev = 1 if tail in historical_data[i-1].get('tails', []) else 0
            curr = 1 if tail in historical_data[i].get('tails', []) else 0
            next_val = 1 if tail in historical_data[i+1].get('tails', []) else 0
            
            if curr > prev and curr > next_val:
                peaks.append(i)
        
        return len(peaks) >= 2
    
    def _check_double_bottom(self, tail: int, historical_data: List[Dict]) -> bool:
        """æ£€æŸ¥åŒåº•å½¢æ€"""
        if len(historical_data) < 15:
            return False
        
        # ç®€åŒ–ï¼šæ£€æŸ¥ä¸¤ä¸ªç›¸ä¼¼çš„ä½ç‚¹
        troughs = []
        for i in range(1, 14):
            prev = 1 if tail in historical_data[i-1].get('tails', []) else 0
            curr = 1 if tail in historical_data[i].get('tails', []) else 0
            next_val = 1 if tail in historical_data[i+1].get('tails', []) else 0
            
            if curr < prev and curr < next_val:
                troughs.append(i)
        
        return len(troughs) >= 2
    
    def _check_head_shoulders(self, tail: int, historical_data: List[Dict]) -> bool:
        """æ£€æŸ¥å¤´è‚©å½¢æ€"""
        if len(historical_data) < 20:
            return False
        
        # ç®€åŒ–ï¼šæ£€æŸ¥ä¸‰ä¸ªå³°ï¼Œä¸­é—´æœ€é«˜
        peaks = []
        for i in range(1, 19):
            prev = 1 if tail in historical_data[i-1].get('tails', []) else 0
            curr = 1 if tail in historical_data[i].get('tails', []) else 0
            next_val = 1 if tail in historical_data[i+1].get('tails', []) else 0
            
            if curr > prev and curr > next_val:
                peaks.append((i, curr))
        
        if len(peaks) >= 3:
            # æ£€æŸ¥ä¸­é—´å³°æ˜¯å¦æœ€é«˜
            middle_idx = len(peaks) // 2
            if all(peaks[middle_idx][1] >= peak[1] for peak in peaks):
                return True
        
        return False
    
    def _check_wedge(self, tail: int, historical_data: List[Dict]) -> bool:
        """æ£€æŸ¥æ¥”å½¢å½¢æ€"""
        if len(historical_data) < 10:
            return False
        
        # ç®€åŒ–ï¼šæ£€æŸ¥æ”¶æ•›è¶‹åŠ¿
        high_points = []
        low_points = []
        
        for i in range(10):
            value = 1 if tail in historical_data[i].get('tails', []) else 0
            if i % 2 == 0:
                high_points.append(value)
            else:
                low_points.append(value)
        
        # æ£€æŸ¥æ˜¯å¦æ”¶æ•›
        if len(high_points) >= 2 and len(low_points) >= 2:
            high_trend = high_points[0] - high_points[-1]
            low_trend = low_points[-1] - low_points[0]
            
            if high_trend > 0 and low_trend > 0:
                return True
        
        return False
    
    def _check_triangle(self, tail: int, historical_data: List[Dict]) -> bool:
        """æ£€æŸ¥ä¸‰è§’å½¢å½¢æ€"""
        return self._check_wedge(tail, historical_data)  # ç®€åŒ–å¤„ç†
    
    def _check_flag(self, tail: int, historical_data: List[Dict]) -> bool:
        """æ£€æŸ¥æ——å½¢å½¢æ€"""
        if len(historical_data) < 8:
            return False
        
        # ç®€åŒ–ï¼šæ£€æŸ¥çŸ­æœŸç›˜æ•´
        values = []
        for i in range(8):
            values.append(1 if tail in historical_data[i].get('tails', []) else 0)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ç‹­çª„åŒºé—´éœ‡è¡
        if max(values) - min(values) <= 0.3:
            return True
        
        return False
    
    def _check_channel_break(self, tail: int, historical_data: List[Dict]) -> bool:
        """æ£€æŸ¥é€šé“çªç ´"""
        if len(historical_data) < 15:
            return False
        
        # è®¡ç®—é€šé“ä¸Šä¸‹è½¨
        upper_bound = []
        lower_bound = []
        
        for i in range(15):
            value = 1 if tail in historical_data[i].get('tails', []) else 0
            if value == 1:
                upper_bound.append(i)
            else:
                lower_bound.append(i)
        
        # æ£€æŸ¥æ˜¯å¦çªç ´é€šé“
        if upper_bound and lower_bound:
            current = 1 if tail in historical_data[0].get('tails', []) else 0
            avg_upper = np.mean([1 for _ in upper_bound])
            avg_lower = np.mean([0 for _ in lower_bound])
            
            if current > avg_upper * 1.1 or current < avg_lower * 0.9:
                return True
        
        return False
    
    def _classify_reversal_type(self, analysis: Dict) -> str:
        """åˆ†ç±»åè½¬ç±»å‹"""
        if analysis['exhaustion_level'] > 0.7:
            return 'exhaustion_reversal'
        elif analysis['momentum'] < -0.3:
            return 'momentum_reversal'
        elif analysis['pattern_matches'] > 2:
            return 'pattern_reversal'
        elif analysis['volume_divergence'] > 0.5:
            return 'volume_reversal'
        else:
            return 'technical_reversal'
    
    def _assess_risk(self, analysis: Dict) -> str:
        """è¯„ä¼°é£é™©æ°´å¹³"""
        risk_score = 0.0
        
        # è¶‹åŠ¿å¼ºåº¦é£é™©
        if analysis['trend_strength'] > 0.7:
            risk_score += 0.3  # å¼ºè¶‹åŠ¿åè½¬é£é™©è¾ƒé«˜
        
        # åè½¬ä¿¡å·ä¸è¶³é£é™©
        if analysis['reversal_confidence'] < 0.5:
            risk_score += 0.3
        
        # æŠ€æœ¯æŒ‡æ ‡å†²çªé£é™©
        if analysis['technical_score'] < 0.5:
            risk_score += 0.2
        
        # æ¨¡å¼ä¸è¶³é£é™©
        if analysis['pattern_matches'] == 0:
            risk_score += 0.2
        
        if risk_score > 0.7:
            return 'high'
        elif risk_score > 0.4:
            return 'medium'
        else:
            return 'low'
    
    def _suggest_timing(self, analysis: Dict) -> str:
        """å»ºè®®æ“ä½œæ—¶æœº"""
        if analysis['exhaustion_level'] > 0.8:
            return 'immediate'  # ç«‹å³åè½¬
        elif analysis['reversal_confidence'] > 0.7:
            return 'next_1_2_periods'  # 1-2æœŸå†…
        elif analysis['breakout_potential'] > 0.6:
            return 'wait_for_confirmation'  # ç­‰å¾…ç¡®è®¤
        else:
            return 'monitor_closely'  # å¯†åˆ‡å…³æ³¨
    
    def _update_learning_parameters(self, tail: int, confidence: float):
        """æ›´æ–°å­¦ä¹ å‚æ•°"""
        self.total_predictions += 1
        
        # æ›´æ–°åŠ¨æ€é˜ˆå€¼
        if confidence > 0.7:
            # é«˜ç½®ä¿¡åº¦æ—¶ï¼Œç•¥å¾®æé«˜é˜ˆå€¼
            for key in self.dynamic_thresholds:
                self.dynamic_thresholds[key] = min(0.9, 
                    self.dynamic_thresholds[key] * (1 + self.learning_rate * 0.1))
        elif confidence < 0.3:
            # ä½ç½®ä¿¡åº¦æ—¶ï¼Œé™ä½é˜ˆå€¼
            for key in self.dynamic_thresholds:
                self.dynamic_thresholds[key] = max(0.3, 
                    self.dynamic_thresholds[key] * (1 - self.learning_rate * 0.1))
    
    def learn_from_outcome(self, prediction: Dict, actual_tails: List[int]) -> Dict:
        """ä»ç»“æœä¸­å­¦ä¹ """
        if not prediction or 'recommended_tail' not in prediction:
            return {'learning_success': False}
        
        predicted_tail = prediction['recommended_tail']
        was_correct = predicted_tail in actual_tails
        
        # æ›´æ–°ç»Ÿè®¡
        if was_correct:
            self.successful_reversals += 1
        else:
            self.false_signals += 1
        
        # è®¡ç®—å‡†ç¡®ç‡
        if self.total_predictions > 0:
            self.prediction_accuracy = self.successful_reversals / self.total_predictions
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        if was_correct:
            # å¢å¼ºæˆåŠŸçš„å‚æ•°é…ç½®
            self.config['reversal_threshold'] *= 0.98  # ç•¥å¾®é™ä½é˜ˆå€¼
            self.config['momentum_sensitivity'] *= 1.02  # æé«˜æ•æ„Ÿåº¦
        else:
            # è°ƒæ•´å¤±è´¥çš„å‚æ•°é…ç½®
            self.config['reversal_threshold'] *= 1.02  # æé«˜é˜ˆå€¼
            self.config['momentum_sensitivity'] *= 0.98  # é™ä½æ•æ„Ÿåº¦
        
        # ç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…
        self.config['reversal_threshold'] = max(0.5, min(0.8, self.config['reversal_threshold']))
        self.config['momentum_sensitivity'] = max(1.0, min(2.0, self.config['momentum_sensitivity']))
        
        return {
            'learning_success': True,
            'was_correct': was_correct,
            'current_accuracy': self.prediction_accuracy,
            'total_predictions': self.total_predictions,
            'successful_reversals': self.successful_reversals,
            'false_signals': self.false_signals,
            'updated_thresholds': self.dynamic_thresholds.copy()
        }