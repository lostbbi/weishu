#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åæ“æ§é¢„æµ‹æ¨¡å‹é›† - ç§‘ç ”çº§å®Œæ•´å®ç°
ä¸“é—¨é’ˆå¯¹"æ€å¤šèµ”å°‘"ç­–ç•¥çš„äººä¸ºæ“æ§ç³»ç»Ÿ
"""

import numpy as np
import math
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass
from enum import Enum
import json
import statistics
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# æ•°æ®ç»“æ„å®šä¹‰
@dataclass
class ManipulationSignal:
    """æ“æ§ä¿¡å·æ•°æ®ç»“æ„"""
    timestamp: datetime
    signal_strength: float  # 0-1ï¼Œæ“æ§å¼ºåº¦
    signal_type: str       # 'kill_hot', 'protect_cold', 'random'
    confidence: float      # ç½®ä¿¡åº¦
    target_tails: List[int]  # è¢«æ“æ§çš„ç›®æ ‡å°¾æ•°
    evidence: Dict[str, Any]  # è¯æ®æ•°æ®

@dataclass
class BehaviorPattern:
    """åº„å®¶è¡Œä¸ºæ¨¡å¼æ•°æ®ç»“æ„"""
    pattern_id: str
    pattern_type: str      # 'weekly', 'monthly', 'seasonal', 'emergency'
    trigger_conditions: Dict[str, Any]
    typical_actions: List[str]
    success_rate: float
    last_seen: datetime
    frequency: int

class ManipulationIntensity(Enum):
    """æ“æ§å¼ºåº¦ç­‰çº§"""
    NATURAL = 0      # è‡ªç„¶éšæœº
    SUBTLE = 1       # å¾®å¦™æ“æ§
    MODERATE = 2     # ä¸­ç­‰æ“æ§
    STRONG = 3       # å¼ºçƒˆæ“æ§
    EXTREME = 4      # æç«¯æ“æ§

class BankerBehaviorAnalyzer:
    """
    åº„å®¶è¡Œä¸ºåˆ†æå™¨ - ç§‘ç ”çº§å®Œæ•´å®ç°
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¤šç»´åº¦æ“æ§æ£€æµ‹ç®—æ³•
    2. åº„å®¶è¡Œä¸ºæ¨¡å¼å­¦ä¹ 
    3. æ“æ§æ—¶æœºé¢„æµ‹
    4. æ“æ§å¼ºåº¦é‡åŒ–
    5. é•¿æœŸç­–ç•¥è¿½è¸ª
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–åº„å®¶è¡Œä¸ºåˆ†æå™¨"""
        self.config = config or self._get_default_config()
        
        # æ ¸å¿ƒæ•°æ®å­˜å‚¨
        self.historical_signals = deque(maxlen=self.config['max_signal_history'])
        self.behavior_patterns = {}
        self.manipulation_timeline = deque(maxlen=self.config['timeline_window'])
        self.tail_manipulation_matrix = np.zeros((10, 10))  # 10x10å°¾æ•°æ“æ§å…³è”çŸ©é˜µ
        
        # ç»Ÿè®¡åˆ†æç»„ä»¶
        self.statistical_analyzer = StatisticalManipulationAnalyzer()
        self.pattern_matcher = BehaviorPatternMatcher()
        self.intensity_calculator = ManipulationIntensityCalculator()
        
        # å­¦ä¹ çŠ¶æ€
        self.total_periods_analyzed = 0
        self.confirmed_manipulations = 0
        self.prediction_accuracy = 0.0
        self.model_confidence = 0.5
        
        # å¤šæ—¶é—´å°ºåº¦åˆ†æçª—å£
        self.analysis_windows = {
            'immediate': deque(maxlen=5),      # æœ€è¿‘5æœŸ
            'short_term': deque(maxlen=20),    # çŸ­æœŸ20æœŸ
            'medium_term': deque(maxlen=50),   # ä¸­æœŸ50æœŸ
            'long_term': deque(maxlen=200),    # é•¿æœŸ200æœŸ
        }
        
        # åº„å®¶å¿ƒç†æ¨¡å‹
        self.banker_psychology = BankerPsychologyModel()
        
        print(f"ğŸ¯ åº„å®¶è¡Œä¸ºåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - é…ç½®å‚æ•°: {len(self.config)}é¡¹")
        print(f"   - åˆ†æç»´åº¦: {len(self.analysis_windows)}ä¸ªæ—¶é—´çª—å£")
        print(f"   - æ£€æµ‹ç®—æ³•: {self.config['detection_algorithms']}ç§")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'max_signal_history': 1000,
            'timeline_window': 500,
            'detection_algorithms': 8,
            'manipulation_threshold': 0.65,
            'pattern_similarity_threshold': 0.75,
            'statistical_significance_level': 0.05,
            'adaptive_learning_rate': 0.1,
            'memory_decay_factor': 0.95,
            'outlier_detection_sensitivity': 2.5,
            'behavioral_consistency_weight': 0.3,
            'temporal_correlation_weight': 0.25,
            'frequency_deviation_weight': 0.2,
            'psychological_factor_weight': 0.25,
        }
    
    def analyze_period(self, period_data: Dict[str, Any], 
                    historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå•æœŸæ•°æ®ï¼Œæ£€æµ‹æ“æ§ä¿¡å·"""
        self.total_periods_analyzed += 1
        analysis_start_time = datetime.now()
        
        try:
            # æ›´æ–°æ‰€æœ‰æ—¶é—´çª—å£æ•°æ®
            for window in self.analysis_windows.values():
                window.append(period_data)
            
            # å¤šç»´åº¦æ“æ§æ£€æµ‹
            print("ğŸ” å¼€å§‹å¤šç»´åº¦æ“æ§æ£€æµ‹...")
            detection_results = self._multi_dimensional_manipulation_detection(
                period_data, historical_context
            )
            print("âœ… å¤šç»´åº¦æ“æ§æ£€æµ‹å®Œæˆ")
            
            # ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
            print("ğŸ“Š å¼€å§‹ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹...")
            try:
                statistical_anomalies = self.statistical_analyzer.detect_anomalies(
                    period_data, list(self.analysis_windows['medium_term'])
                )
                print("âœ… ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å®Œæˆ")
            except Exception as e:
                print(f"âŒ ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
                statistical_anomalies = {'anomaly_score': 0.3, 'anomaly_details': {}}
            
            # è¡Œä¸ºæ¨¡å¼åŒ¹é…
            print("ğŸ” å¼€å§‹è¡Œä¸ºæ¨¡å¼åŒ¹é…...")
            try:
                pattern_matches = self.pattern_matcher.find_matching_patterns(
                    period_data, self.behavior_patterns, historical_context
                )
                print("âœ… è¡Œä¸ºæ¨¡å¼åŒ¹é…å®Œæˆ")
            except Exception as e:
                print(f"âŒ è¡Œä¸ºæ¨¡å¼åŒ¹é…å¤±è´¥: {e}")
                pattern_matches = {'matched_patterns': [], 'similarity_scores': []}
            
            # æ“æ§å¼ºåº¦è®¡ç®—
            print("âš¡ å¼€å§‹æ“æ§å¼ºåº¦è®¡ç®—...")
            try:
                manipulation_intensity = self.intensity_calculator.calculate_intensity(
                    detection_results, statistical_anomalies, pattern_matches
                )
                print("âœ… æ“æ§å¼ºåº¦è®¡ç®—å®Œæˆ")
            except Exception as e:
                print(f"âŒ æ“æ§å¼ºåº¦è®¡ç®—å¤±è´¥: {e}")
                manipulation_intensity = ManipulationIntensity.NATURAL
            
            # åº„å®¶å¿ƒç†çŠ¶æ€åˆ†æ
            print("ğŸ§  å¼€å§‹åº„å®¶å¿ƒç†çŠ¶æ€åˆ†æ...")
            try:
                psychological_state = self.banker_psychology.analyze_state(
                    period_data, historical_context, manipulation_intensity
                )
                print("âœ… åº„å®¶å¿ƒç†çŠ¶æ€åˆ†æå®Œæˆ")
            except Exception as e:
                print(f"âŒ åº„å®¶å¿ƒç†çŠ¶æ€åˆ†æå¤±è´¥: {e}")
                psychological_state = {
                    'stress_level': 0.5,
                    'aggressiveness': 0.5, 
                    'risk_tolerance': 0.5,
                    'strategic_phase': 'observation'
                }
            
            # ç»¼åˆåˆ†æç»“æœ
            analysis_result = self._synthesize_analysis_results(
                period_data, detection_results, statistical_anomalies,
                pattern_matches, manipulation_intensity, psychological_state
            )
            
            # æ›´æ–°å­¦ä¹ æ¨¡å‹
            self._update_learning_models(analysis_result)
            
            # è®°å½•åˆ°æ“æ§æ—¶é—´çº¿
            if analysis_result['manipulation_probability'] > self.config['manipulation_threshold']:
                manipulation_signal = ManipulationSignal(
                    timestamp=period_data.get('timestamp', datetime.now()),
                    signal_strength=analysis_result['manipulation_probability'],
                    signal_type=analysis_result['manipulation_type'],
                    confidence=analysis_result['confidence'],
                    target_tails=analysis_result['target_tails'],
                    evidence=analysis_result['evidence']
                )
                self.manipulation_timeline.append(manipulation_signal)
                self.confirmed_manipulations += 1
            
            analysis_duration = (datetime.now() - analysis_start_time).total_seconds()
            analysis_result['analysis_duration'] = analysis_duration
            
            return analysis_result
        except Exception as e:
            print(f"âŒ åæ“æ§åˆ†ææ€»ä½“å¤±è´¥: {e}")
            return self._get_default_analysis_result()
    def _multi_dimensional_manipulation_detection(self, period_data: Dict[str, Any], 
                                                 historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¤šç»´åº¦æ“æ§æ£€æµ‹ç®—æ³•"""
        
        detection_results = {
            'frequency_deviation': self._detect_frequency_deviation(period_data, historical_context),
            'pattern_disruption': self._detect_pattern_disruption(period_data, historical_context),
            'temporal_clustering': self._detect_temporal_clustering(period_data, historical_context),
            'anti_probability': self._detect_anti_probability_events(period_data, historical_context),
            'psychological_traps': self._detect_psychological_traps(period_data, historical_context),
            'sequence_anomalies': self._detect_sequence_anomalies(period_data, historical_context),
            'correlation_breaks': self._detect_correlation_breaks(period_data, historical_context),
            'entropy_analysis': self._analyze_entropy_deviation(period_data, historical_context),
        }
        
        # è®¡ç®—ç»¼åˆæ£€æµ‹åˆ†æ•°
        detection_scores = [result.get('score', 0.0) for result in detection_results.values()]
        detection_results['combined_score'] = np.mean(detection_scores)
        detection_results['max_score'] = np.max(detection_scores)
        detection_results['detection_consensus'] = len([s for s in detection_scores if s > 0.6]) / len(detection_scores)
        
        return detection_results
    
    def _detect_frequency_deviation(self, period_data: Dict[str, Any], 
                                   historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é¢‘ç‡åå·®æ£€æµ‹ - æ£€æµ‹å°¾æ•°å‡ºç°é¢‘ç‡çš„å¼‚å¸¸åå·®"""
        
        if len(historical_context) < 20:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        # è®¡ç®—æ¯ä¸ªå°¾æ•°çš„å†å²æœŸæœ›é¢‘ç‡
        expected_frequencies = {}
        actual_recent_frequencies = {}
        
        # ä½¿ç”¨å¤šä¸ªæ—¶é—´çª—å£åˆ†æ
        windows = [10, 20, 30, 50]
        deviation_scores = []
        
        for window_size in windows:
            if len(historical_context) >= window_size:
                recent_context = historical_context[-window_size:]
                
                # è®¡ç®—æ¯ä¸ªå°¾æ•°åœ¨æ­¤çª—å£å†…çš„é¢‘ç‡
                tail_counts = defaultdict(int)
                for period in recent_context:
                    for tail in period.get('tails', []):
                        tail_counts[tail] += 1
                
                # è®¡ç®—æœŸæœ›vså®é™…åå·®
                window_deviations = []
                for tail in range(10):
                    expected_freq = tail_counts[tail] / window_size if window_size > 0 else 0
                    is_current = 1 if tail in current_tails else 0
                    
                    # ä½¿ç”¨å¡æ–¹æ£€éªŒçš„æ€æƒ³è®¡ç®—åå·®
                    if expected_freq > 0:
                        deviation = abs(is_current - expected_freq) / math.sqrt(expected_freq)
                        window_deviations.append(deviation)
                    else:
                        window_deviations.append(abs(is_current))
                
                if window_deviations:
                    avg_deviation = np.mean(window_deviations)
                    deviation_scores.append(avg_deviation)
        
        if not deviation_scores:
            return {'score': 0.0, 'details': 'calculation_failed'}
        
        # ç»¼åˆå¤šçª—å£åå·®åˆ†æ•°
        final_deviation = np.mean(deviation_scores)
        
        # è½¬æ¢ä¸º0-1åˆ†æ•°ï¼ˆä½¿ç”¨sigmoidå‡½æ•°ï¼‰
        manipulation_score = 1 / (1 + math.exp(-3 * (final_deviation - 1)))
        
        return {
            'score': float(manipulation_score),
            'deviation_value': float(final_deviation),
            'window_scores': [float(s) for s in deviation_scores],
            'details': 'frequency_deviation_analysis',
            'evidence': {
                'current_tails': list(current_tails),
                'deviation_windows': windows[:len(deviation_scores)],
                'max_deviation_window': windows[np.argmax(deviation_scores)] if deviation_scores else None
            }
        }
    
    def _detect_pattern_disruption(self, period_data: Dict[str, Any], 
                                  historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¨¡å¼ä¸­æ–­æ£€æµ‹ - æ£€æµ‹æ­£å¸¸æ¨¡å¼çš„çªç„¶ä¸­æ–­"""
        
        if len(historical_context) < 15:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        # åˆ†ææœ€è¿‘15æœŸçš„æ¨¡å¼
        recent_periods = historical_context[-15:]
        
        # æ£€æµ‹è¿ç»­æ¨¡å¼
        continuous_patterns = self._identify_continuous_patterns(recent_periods)
        
        # æ£€æµ‹å‘¨æœŸæ¨¡å¼
        cyclic_patterns = self._identify_cyclic_patterns(recent_periods)
        
        # æ£€æµ‹è¶‹åŠ¿æ¨¡å¼
        trend_patterns = self._identify_trend_patterns(recent_periods)
        
        # è¯„ä¼°å½“å‰æœŸæ˜¯å¦æ‰“ç ´äº†è¿™äº›æ¨¡å¼
        disruption_scores = []
        
        # è¿ç»­æ¨¡å¼ä¸­æ–­è¯„åˆ†
        for pattern in continuous_patterns:
            if self._is_pattern_disrupted(pattern, current_tails):
                disruption_strength = pattern.get('strength', 0.5)
                disruption_scores.append(disruption_strength * 0.8)
        
        # å‘¨æœŸæ¨¡å¼ä¸­æ–­è¯„åˆ†
        for pattern in cyclic_patterns:
            if self._is_cyclic_pattern_disrupted(pattern, current_tails, len(historical_context)):
                disruption_strength = pattern.get('strength', 0.5)
                disruption_scores.append(disruption_strength * 0.9)
        
        # è¶‹åŠ¿æ¨¡å¼ä¸­æ–­è¯„åˆ†
        for pattern in trend_patterns:
            if self._is_trend_pattern_disrupted(pattern, current_tails):
                disruption_strength = pattern.get('strength', 0.5)
                disruption_scores.append(disruption_strength * 0.7)
        
        # è®¡ç®—ç»¼åˆä¸­æ–­åˆ†æ•°
        if disruption_scores:
            disruption_score = min(1.0, np.mean(disruption_scores))
        else:
            disruption_score = 0.0
        
        return {
            'score': float(disruption_score),
            'continuous_patterns_disrupted': len([p for p in continuous_patterns 
                                                if self._is_pattern_disrupted(p, current_tails)]),
            'cyclic_patterns_disrupted': len([p for p in cyclic_patterns 
                                            if self._is_cyclic_pattern_disrupted(p, current_tails, len(historical_context))]),
            'trend_patterns_disrupted': len([p for p in trend_patterns 
                                           if self._is_trend_pattern_disrupted(p, current_tails)]),
            'details': 'pattern_disruption_analysis',
            'evidence': {
                'identified_patterns': len(continuous_patterns) + len(cyclic_patterns) + len(trend_patterns),
                'disruption_scores': [float(s) for s in disruption_scores]
            }
        }
    
    def _detect_temporal_clustering(self, period_data: Dict[str, Any], 
                                   historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ—¶é—´èšé›†æ£€æµ‹ - æ£€æµ‹å°¾æ•°åœ¨æ—¶é—´ä¸Šçš„å¼‚å¸¸èšé›†"""
        
        if len(historical_context) < 30:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        # åˆ†ææ¯ä¸ªå°¾æ•°çš„æ—¶é—´åˆ†å¸ƒ
        tail_temporal_patterns = {}
        
        for tail in range(10):
            # æ‰¾åˆ°è¯¥å°¾æ•°å‡ºç°çš„æ‰€æœ‰æ—¶é—´ä½ç½®
            positions = []
            for i, period in enumerate(historical_context):
                if tail in period.get('tails', []):
                    positions.append(i)
            
            if len(positions) >= 3:  # è‡³å°‘éœ€è¦3æ¬¡å‡ºç°æ‰èƒ½åˆ†æ
                # è®¡ç®—é—´éš”åˆ†å¸ƒ
                intervals = [positions[i+1] - positions[i] for i in range(len(positions)-1)]
                
                if intervals:
                    # ä½¿ç”¨å˜å¼‚ç³»æ•°æ£€æµ‹èšé›†ç¨‹åº¦
                    mean_interval = np.mean(intervals)
                    std_interval = np.std(intervals)
                    
                    if mean_interval > 0:
                        coefficient_of_variation = std_interval / mean_interval
                        
                        # è®¡ç®—æœ€è¿‘èšé›†åº¦ï¼ˆé‡ç‚¹å…³æ³¨æœ€è¿‘çš„å‡ºç°ï¼‰
                        recent_positions = [p for p in positions if p >= len(historical_context) - 10]
                        recent_clustering = 0.0
                        
                        if len(recent_positions) >= 2:
                            recent_intervals = [recent_positions[i+1] - recent_positions[i] for i in range(len(recent_positions)-1)]
                            if recent_intervals:
                                recent_mean = np.mean(recent_intervals)
                                # å¦‚æœæœ€è¿‘é—´éš”æ˜æ˜¾å°äºæ•´ä½“å¹³å‡é—´éš”ï¼Œè¯´æ˜å­˜åœ¨èšé›†
                                if mean_interval > 0:
                                    recent_clustering = max(0, (mean_interval - recent_mean) / mean_interval)
                        
                        tail_temporal_patterns[tail] = {
                            'coefficient_of_variation': coefficient_of_variation,
                            'recent_clustering': recent_clustering,
                            'total_appearances': len(positions),
                            'mean_interval': mean_interval,
                            'recent_appearances': len(recent_positions)
                        }
        
        # è¯„ä¼°å½“å‰æœŸçš„æ—¶é—´èšé›†å¼‚å¸¸ç¨‹åº¦
        clustering_scores = []
        
        for tail in current_tails:
            if tail in tail_temporal_patterns:
                pattern = tail_temporal_patterns[tail]
                
                # å¦‚æœè¯¥å°¾æ•°æœ€è¿‘é¢‘ç¹å‡ºç°ï¼ˆå¯èƒ½çš„èšé›†æ“æ§ï¼‰
                recent_clustering_score = pattern['recent_clustering']
                
                # å¦‚æœè¯¥å°¾æ•°æ‰“ç ´äº†æ­£å¸¸çš„æ—¶é—´åˆ†å¸ƒæ¨¡å¼
                cv_anomaly_score = 0.0
                if pattern['coefficient_of_variation'] < 0.5:  # é—´éš”è¿‡äºè§„å¾‹ï¼Œå¯èƒ½è¢«æ“æ§
                    cv_anomaly_score = 0.8
                elif pattern['coefficient_of_variation'] > 2.0:  # é—´éš”è¿‡äºä¸è§„å¾‹ï¼Œä¹Ÿå¯èƒ½è¢«æ“æ§
                    cv_anomaly_score = 0.6
                
                # ç»¼åˆèšé›†åˆ†æ•°
                tail_clustering_score = (recent_clustering_score * 0.7 + cv_anomaly_score * 0.3)
                clustering_scores.append(tail_clustering_score)
        
        # è®¡ç®—æ•´ä½“èšé›†å¼‚å¸¸åˆ†æ•°
        if clustering_scores:
            overall_clustering_score = np.mean(clustering_scores)
        else:
            overall_clustering_score = 0.0
        
        return {
            'score': float(min(1.0, overall_clustering_score)),
            'analyzed_tails': len(tail_temporal_patterns),
            'current_anomalous_tails': len([t for t in current_tails if t in tail_temporal_patterns]),
            'details': 'temporal_clustering_analysis',
            'evidence': {
                'tail_patterns': {str(k): v for k, v in tail_temporal_patterns.items()},
                'clustering_scores': [float(s) for s in clustering_scores]
            }
        }
    
    def _detect_anti_probability_events(self, period_data: Dict[str, Any], 
                                       historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åæ¦‚ç‡äº‹ä»¶æ£€æµ‹ - æ£€æµ‹è¿åæ­£å¸¸æ¦‚ç‡åˆ†å¸ƒçš„äº‹ä»¶"""
        
        if len(historical_context) < 25:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        # åŸºäºå¤šé¡¹å¼åˆ†å¸ƒè®¡ç®—æ¯ä¸ªå°¾æ•°çš„æœŸæœ›æ¦‚ç‡
        total_periods = len(historical_context)
        tail_probabilities = {}
        tail_actual_counts = defaultdict(int)
        
        # è®¡ç®—å†å²æ¦‚ç‡åˆ†å¸ƒ
        for period in historical_context:
            for tail in period.get('tails', []):
                tail_actual_counts[tail] += 1
        
        for tail in range(10):
            tail_probabilities[tail] = tail_actual_counts[tail] / total_periods if total_periods > 0 else 0.1
        
        # ä½¿ç”¨äºŒé¡¹æ£€éªŒçš„æ€æƒ³æ£€æµ‹åæ¦‚ç‡äº‹ä»¶
        anti_probability_scores = []
        
        for tail in range(10):
            is_present = tail in current_tails
            expected_prob = tail_probabilities[tail]
            
            # è®¡ç®—è¯¥å°¾æ•°å‡ºç°/ä¸å‡ºç°çš„æ¦‚ç‡å¼‚å¸¸ç¨‹åº¦
            if is_present:
                # å°¾æ•°å‡ºç°äº†ï¼Œä½†å†å²æ¦‚ç‡å¾ˆä½
                if expected_prob < 0.2:  # ä½æ¦‚ç‡å°¾æ•°å´å‡ºç°äº†
                    anomaly_score = (0.2 - expected_prob) / 0.2
                    anti_probability_scores.append(anomaly_score * 0.8)
            else:
                # å°¾æ•°æ²¡å‡ºç°ï¼Œä½†å†å²æ¦‚ç‡å¾ˆé«˜
                if expected_prob > 0.7:  # é«˜æ¦‚ç‡å°¾æ•°å´æ²¡å‡ºç°
                    anomaly_score = (expected_prob - 0.7) / 0.3
                    anti_probability_scores.append(anomaly_score * 0.9)
        
        # æ£€æµ‹ç»„åˆæ¦‚ç‡å¼‚å¸¸
        current_combination_probability = 1.0
        for tail in range(10):
            if tail in current_tails:
                current_combination_probability *= tail_probabilities[tail]
            else:
                current_combination_probability *= (1 - tail_probabilities[tail])
        
        # ä½¿ç”¨å¯¹æ•°æ¦‚ç‡é¿å…æ•°å€¼ä¸‹æº¢
        log_prob = math.log(max(current_combination_probability, 1e-10))
        
        # è®¡ç®—æœŸæœ›çš„å¯¹æ•°æ¦‚ç‡èŒƒå›´
        expected_log_prob_range = self._calculate_expected_log_prob_range(tail_probabilities)
        
        # å¦‚æœå½“å‰ç»„åˆçš„æ¦‚ç‡è¿‡ä½ï¼Œå¯èƒ½æ˜¯åæ¦‚ç‡æ“æ§
        combination_anomaly_score = 0.0
        if log_prob < expected_log_prob_range['lower_bound']:
            combination_anomaly_score = min(1.0, (expected_log_prob_range['lower_bound'] - log_prob) / 5.0)
        
        # ç»¼åˆåæ¦‚ç‡åˆ†æ•°
        if anti_probability_scores:
            individual_anomaly_score = np.mean(anti_probability_scores)
        else:
            individual_anomaly_score = 0.0
        
        overall_anti_prob_score = (individual_anomaly_score * 0.6 + combination_anomaly_score * 0.4)
        
        return {
            'score': float(min(1.0, overall_anti_prob_score)),
            'individual_anomalies': len(anti_probability_scores),
            'combination_log_probability': float(log_prob),
            'expected_log_prob_range': expected_log_prob_range,
            'details': 'anti_probability_analysis',
            'evidence': {
                'tail_probabilities': {str(k): float(v) for k, v in tail_probabilities.items()},
                'anomaly_scores': [float(s) for s in anti_probability_scores],
                'combination_anomaly_score': float(combination_anomaly_score)
            }
        }
    
    def _detect_psychological_traps(self, period_data: Dict[str, Any], 
                                   historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¿ƒç†é™·é˜±æ£€æµ‹ - æ£€æµ‹è®¾è®¡æ¥è¯¯å¯¼ç©å®¶çš„æ¨¡å¼"""
        
        if len(historical_context) < 10:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        psychological_trap_scores = []
        
        # 1. çƒ­é—¨é™·é˜±æ£€æµ‹
        hot_trap_score = self._detect_hot_number_trap(current_tails, historical_context)
        psychological_trap_scores.append(hot_trap_score)
        
        # 2. å†·é—¨å¤å‡ºé™·é˜±æ£€æµ‹
        cold_comeback_score = self._detect_cold_comeback_trap(current_tails, historical_context)
        psychological_trap_scores.append(cold_comeback_score)
        
        # 3. è¿ç»­æ€§æ–­è£‚é™·é˜±æ£€æµ‹
        continuity_break_score = self._detect_continuity_break_trap(current_tails, historical_context)
        psychological_trap_scores.append(continuity_break_score)
        
        # 4. å¯¹ç§°æ€§é™·é˜±æ£€æµ‹
        symmetry_trap_score = self._detect_symmetry_trap(current_tails, historical_context)
        psychological_trap_scores.append(symmetry_trap_score)
        
        # 5. æ•°å­—å¿ƒç†å­¦é™·é˜±æ£€æµ‹
        number_psychology_score = self._detect_number_psychology_trap(current_tails, historical_context)
        psychological_trap_scores.append(number_psychology_score)
        
        # ç»¼åˆå¿ƒç†é™·é˜±åˆ†æ•°
        overall_trap_score = np.mean(psychological_trap_scores)
        
        return {
            'score': float(overall_trap_score),
            'hot_trap_score': float(hot_trap_score),
            'cold_comeback_score': float(cold_comeback_score),
            'continuity_break_score': float(continuity_break_score),
            'symmetry_trap_score': float(symmetry_trap_score),
            'number_psychology_score': float(number_psychology_score),
            'details': 'psychological_trap_analysis',
            'evidence': {
                'trap_types_detected': len([s for s in psychological_trap_scores if s > 0.5]),
                'strongest_trap_type': ['hot', 'cold_comeback', 'continuity_break', 'symmetry', 'number_psychology'][
                    np.argmax(psychological_trap_scores)
                ]
            }
        }
    
    def predict_next_manipulation(self, current_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é¢„æµ‹ä¸‹æœŸæ“æ§è¡Œä¸º"""
        
        if not current_context or len(current_context) < 10:
            return {
                'manipulation_probability': 0.5,
                'predicted_type': 'unknown',
                'target_tails': [],
                'confidence': 0.0,
                'reasoning': 'insufficient_data'
            }
        
        # åˆ†æå½“å‰åº„å®¶å¿ƒç†çŠ¶æ€
        current_state = self.banker_psychology.analyze_state(
            current_context[-1], current_context, ManipulationIntensity.MODERATE
        )
        
        # åŸºäºå†å²æ“æ§æ¨¡å¼é¢„æµ‹
        pattern_prediction = self._predict_based_on_patterns(current_context)
        
        # åŸºäºç»Ÿè®¡æ¨¡å‹é¢„æµ‹
        statistical_prediction = self._predict_based_on_statistics(current_context)
        
        # åŸºäºå¿ƒç†æ¨¡å‹é¢„æµ‹
        psychological_prediction = self._predict_based_on_psychology(current_context, current_state)
        
        # èåˆå¤šç§é¢„æµ‹æ–¹æ³•
        final_prediction = self._fuse_predictions([
            pattern_prediction,
            statistical_prediction,
            psychological_prediction
        ])
        
        return final_prediction
    
    def get_anti_manipulation_recommendations(self, current_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è·å–åæ“æ§æŠ•æ³¨å»ºè®®"""
        
        # é¢„æµ‹ä¸‹æœŸæ“æ§è¡Œä¸º
        manipulation_prediction = self.predict_next_manipulation(current_context)
        
        # åŸºäºé¢„æµ‹ç”Ÿæˆåæ“æ§ç­–ç•¥
        recommendations = self._generate_anti_manipulation_strategy(
            manipulation_prediction, current_context
        )
        
        return {
            'manipulation_prediction': manipulation_prediction,
            'recommended_tails': recommendations['recommended_tails'],
            'avoid_tails': recommendations['avoid_tails'],
            'confidence': recommendations['confidence'],
            'strategy_type': recommendations['strategy_type'],
            'reasoning': recommendations['reasoning'],
            'risk_assessment': recommendations['risk_assessment']
        }
    
    # ========== è¾…åŠ©æ–¹æ³•å®ç° ==========
    
    def _identify_continuous_patterns(self, recent_periods: List[Dict]) -> List[Dict]:
        """è¯†åˆ«è¿ç»­æ¨¡å¼"""
        patterns = []
        
        # æ£€æµ‹è¿ç»­å‡ºç°çš„å°¾æ•°
        for tail in range(10):
            consecutive_count = 0
            max_consecutive = 0
            
            for period in reversed(recent_periods):  # ä»æœ€æ–°å¾€å‰çœ‹
                if tail in period.get('tails', []):
                    consecutive_count += 1
                    max_consecutive = max(max_consecutive, consecutive_count)
                else:
                    if consecutive_count >= 2:  # è‡³å°‘è¿ç»­2æœŸæ‰ç®—æ¨¡å¼
                        patterns.append({
                            'type': 'continuous',
                            'tail': tail,
                            'length': consecutive_count,
                            'strength': min(1.0, consecutive_count / 5.0)
                        })
                    consecutive_count = 0
            
            # æ£€æŸ¥æœ€åçš„è¿ç»­æ¨¡å¼
            if consecutive_count >= 2:
                patterns.append({
                    'type': 'continuous',
                    'tail': tail,
                    'length': consecutive_count,
                    'strength': min(1.0, consecutive_count / 5.0)
                })
        
        return patterns
    
    def _identify_cyclic_patterns(self, recent_periods: List[Dict]) -> List[Dict]:
        """è¯†åˆ«å‘¨æœŸæ¨¡å¼"""
        patterns = []
        
        # æ£€æµ‹ç®€å•çš„å‘¨æœŸæ¨¡å¼ï¼ˆæ¯NæœŸå‡ºç°ä¸€æ¬¡ï¼‰
        for tail in range(10):
            tail_positions = []
            for i, period in enumerate(recent_periods):
                if tail in period.get('tails', []):
                    tail_positions.append(i)
            
            if len(tail_positions) >= 3:
                # è®¡ç®—é—´éš”
                intervals = [tail_positions[i+1] - tail_positions[i] for i in range(len(tail_positions)-1)]
                
                # æ£€æµ‹æ˜¯å¦æœ‰è§„å¾‹çš„é—´éš”
                if intervals:
                    most_common_interval = max(set(intervals), key=intervals.count)
                    interval_consistency = intervals.count(most_common_interval) / len(intervals)
                    
                    if interval_consistency >= 0.6:  # 60%çš„é—´éš”ä¸€è‡´
                        patterns.append({
                            'type': 'cyclic',
                            'tail': tail,
                            'interval': most_common_interval,
                            'consistency': interval_consistency,
                            'strength': interval_consistency
                        })
        
        return patterns
    
    def _identify_trend_patterns(self, recent_periods: List[Dict]) -> List[Dict]:
        """è¯†åˆ«è¶‹åŠ¿æ¨¡å¼"""
        patterns = []
        
        # æ£€æµ‹é¢‘ç‡è¶‹åŠ¿ï¼ˆé€’å¢æˆ–é€’å‡ï¼‰
        window_sizes = [5, 8, 10]
        
        for window_size in window_sizes:
            if len(recent_periods) >= window_size * 2:
                for tail in range(10):
                    # è®¡ç®—å‰åŠæ®µå’ŒååŠæ®µçš„é¢‘ç‡
                    first_half = recent_periods[:window_size]
                    second_half = recent_periods[window_size:window_size*2]
                    
                    first_freq = sum(1 for p in first_half if tail in p.get('tails', [])) / window_size
                    second_freq = sum(1 for p in second_half if tail in p.get('tails', [])) / window_size
                    
                    # æ£€æµ‹è¶‹åŠ¿å¼ºåº¦
                    trend_strength = abs(second_freq - first_freq)
                    if trend_strength >= 0.3:  # é¢‘ç‡å˜åŒ–è¶…è¿‡30%
                        trend_direction = 'increasing' if second_freq > first_freq else 'decreasing'
                        
                        patterns.append({
                            'type': 'trend',
                            'tail': tail,
                            'direction': trend_direction,
                            'strength': trend_strength,
                            'window_size': window_size
                        })
        
        return patterns
    
    def _is_pattern_disrupted(self, pattern: Dict, current_tails: Set[int]) -> bool:
        """æ£€æŸ¥è¿ç»­æ¨¡å¼æ˜¯å¦è¢«ä¸­æ–­"""
        if pattern['type'] == 'continuous':
            tail = pattern['tail']
            # å¦‚æœæ˜¯è¿ç»­æ¨¡å¼ï¼Œå½“å‰æœŸåº”è¯¥åŒ…å«è¯¥å°¾æ•°ï¼Œå¦‚æœä¸åŒ…å«åˆ™è¢«ä¸­æ–­
            return tail not in current_tails
        return False
    
    def _is_cyclic_pattern_disrupted(self, pattern: Dict, current_tails: Set[int], period_index: int) -> bool:
        """æ£€æŸ¥å‘¨æœŸæ¨¡å¼æ˜¯å¦è¢«ä¸­æ–­"""
        if pattern['type'] == 'cyclic':
            tail = pattern['tail']
            interval = pattern['interval']
            # ç®€åŒ–æ£€æŸ¥ï¼šå¦‚æœæŒ‰å‘¨æœŸåº”è¯¥å‡ºç°ä½†æ²¡å‡ºç°ï¼Œæˆ–ä¸åº”è¯¥å‡ºç°ä½†å‡ºç°äº†
            expected_appearance = (period_index % interval) == 0
            actual_appearance = tail in current_tails
            return expected_appearance != actual_appearance
        return False
    
    def _is_trend_pattern_disrupted(self, pattern: Dict, current_tails: Set[int]) -> bool:
        """æ£€æŸ¥è¶‹åŠ¿æ¨¡å¼æ˜¯å¦è¢«ä¸­æ–­"""
        if pattern['type'] == 'trend':
            tail = pattern['tail']
            direction = pattern['direction']
            # ç®€åŒ–æ£€æŸ¥ï¼šå¦‚æœæ˜¯é€’å¢è¶‹åŠ¿ä½†å°¾æ•°æ²¡å‡ºç°ï¼Œæˆ–é€’å‡è¶‹åŠ¿ä½†å°¾æ•°å‡ºç°äº†
            if direction == 'increasing':
                return tail not in current_tails
            else:  # decreasing
                return tail in current_tails
        return False
    
    def _calculate_expected_log_prob_range(self, tail_probabilities: Dict) -> Dict:
        """è®¡ç®—æœŸæœ›å¯¹æ•°æ¦‚ç‡èŒƒå›´"""
        # ä½¿ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•ä¼°è®¡æ­£å¸¸èŒƒå›´
        simulated_log_probs = []
        
        for _ in range(1000):  # æ¨¡æ‹Ÿ1000æ¬¡
            simulated_tails = set()
            for tail in range(10):
                if np.random.random() < tail_probabilities[tail]:
                    simulated_tails.add(tail)
            
            # è®¡ç®—è¿™æ¬¡æ¨¡æ‹Ÿçš„å¯¹æ•°æ¦‚ç‡
            log_prob = 0.0
            for tail in range(10):
                if tail in simulated_tails:
                    log_prob += math.log(max(tail_probabilities[tail], 1e-10))
                else:
                    log_prob += math.log(max(1 - tail_probabilities[tail], 1e-10))
            
            simulated_log_probs.append(log_prob)
        
        # è®¡ç®—5%å’Œ95%åˆ†ä½æ•°ä½œä¸ºæ­£å¸¸èŒƒå›´
        lower_bound = np.percentile(simulated_log_probs, 5)
        upper_bound = np.percentile(simulated_log_probs, 95)
        
        return {
            'lower_bound': float(lower_bound),
            'upper_bound': float(upper_bound),
            'mean': float(np.mean(simulated_log_probs)),
            'std': float(np.std(simulated_log_probs))
        }
    
    def _detect_hot_number_trap(self, current_tails: Set[int], historical_context: List[Dict]) -> float:
        """
        ç§‘ç ”çº§çƒ­é—¨æ•°å­—é™·é˜±æ£€æµ‹ç®—æ³•
        åŸºäºå¤šæ—¶é—´å°ºåº¦é¢‘ç‡åˆ†æã€ç»Ÿè®¡åå·®æ£€æµ‹å’Œæ“æ§æ—¶æœºè¯†åˆ«
        """
        if len(historical_context) < 15:
            return 0.0
        
        # === å¤šæ—¶é—´å°ºåº¦é¢‘ç‡åˆ†æ ===
        time_windows = [5, 10, 15, 20, 30]
        frequency_profiles = {}
        
        for window_size in time_windows:
            if len(historical_context) >= window_size:
                window_data = historical_context[-window_size:]
                tail_frequencies = defaultdict(int)
                
                for period in window_data:
                    for tail in period.get('tails', []):
                        tail_frequencies[tail] += 1
                
                # è®¡ç®—ç›¸å¯¹é¢‘ç‡å’Œç»Ÿè®¡æŒ‡æ ‡
                frequency_profile = {}
                for tail in range(10):
                    freq = tail_frequencies[tail]
                    relative_freq = freq / window_size
                    frequency_profile[tail] = {
                        'absolute_freq': freq,
                        'relative_freq': relative_freq,
                        'z_score': (relative_freq - 0.5) / math.sqrt(0.5 * 0.5 / window_size) if window_size > 0 else 0
                    }
                
                frequency_profiles[window_size] = frequency_profile
        
        # === çƒ­é—¨å°¾æ•°è¯†åˆ«ä¸åˆ†å±‚ ===
        hot_number_tiers = {
            'extremely_hot': set(),  # Zåˆ†æ•° > 2.0
            'very_hot': set(),       # Zåˆ†æ•° > 1.5
            'moderately_hot': set(), # Zåˆ†æ•° > 1.0
            'trending_hot': set()    # é¢‘ç‡è¶‹åŠ¿é€’å¢
        }
        
        # åŸºäºZåˆ†æ•°åˆ†å±‚
        for window_size, profile in frequency_profiles.items():
            for tail, stats in profile.items():
                z_score = stats['z_score']
                if z_score > 2.0:
                    hot_number_tiers['extremely_hot'].add(tail)
                elif z_score > 1.5:
                    hot_number_tiers['very_hot'].add(tail)
                elif z_score > 1.0:
                    hot_number_tiers['moderately_hot'].add(tail)
        
        # === é¢‘ç‡è¶‹åŠ¿åˆ†æ ===
        if len(time_windows) >= 3:
            for tail in range(10):
                trend_scores = []
                for i in range(len(time_windows) - 1):
                    current_freq = frequency_profiles[time_windows[i]][tail]['relative_freq']
                    next_freq = frequency_profiles[time_windows[i+1]][tail]['relative_freq']
                    trend_scores.append(next_freq - current_freq)
                
                # å¦‚æœé¢‘ç‡æŒç»­é€’å¢ï¼Œæ ‡è®°ä¸ºè¶‹åŠ¿çƒ­é—¨
                if len(trend_scores) >= 2 and all(score > 0.05 for score in trend_scores):
                    hot_number_tiers['trending_hot'].add(tail)
        
        # === æ“æ§æ—¶æœºåˆ†æ ===
        manipulation_timing_score = 0.0
        current_period_analysis = {
            'hot_concentration': 0.0,
            'timing_suspicion': 0.0,
            'frequency_anomaly': 0.0
        }
        
        # åˆ†æå½“å‰æœŸçƒ­é—¨æ•°å­—çš„é›†ä¸­åº¦
        current_hot_count_by_tier = {}
        for tier_name, hot_set in hot_number_tiers.items():
            current_hot_count_by_tier[tier_name] = len(current_tails.intersection(hot_set))
        
        # è®¡ç®—çƒ­é—¨é›†ä¸­åº¦å¼‚å¸¸åˆ†æ•°
        total_hot_in_current = sum(current_hot_count_by_tier.values())
        if len(current_tails) > 0:
            hot_concentration_ratio = total_hot_in_current / len(current_tails)
            
            # ä½¿ç”¨è´å¶æ–¯æ–¹æ³•è®¡ç®—å¼‚å¸¸æ¦‚ç‡
            prior_hot_prob = 0.3  # å…ˆéªŒçƒ­é—¨æ¦‚ç‡
            observed_hot_ratio = hot_concentration_ratio
            
            # è´å¶æ–¯æ›´æ–°
            if observed_hot_ratio > 0.6:  # 60%ä»¥ä¸Šéƒ½æ˜¯çƒ­é—¨æ•°å­—
                manipulation_timing_score += 0.4
                current_period_analysis['hot_concentration'] = observed_hot_ratio
        
        # === é¢‘ç‡åˆ†å¸ƒå¼‚å¸¸æ£€æµ‹ ===
        # ä½¿ç”¨å¡æ–¹æ£€éªŒæ£€æµ‹é¢‘ç‡åˆ†å¸ƒå¼‚å¸¸
        expected_freq = len(historical_context[-10:]) / 10.0 if len(historical_context) >= 10 else 1.0
        chi_square_stats = []
        
        for tail in range(10):
            if 10 in frequency_profiles:
                observed_freq = frequency_profiles[10][tail]['absolute_freq']
                chi_square_component = ((observed_freq - expected_freq) ** 2) / expected_freq if expected_freq > 0 else 0
                chi_square_stats.append(chi_square_component)
        
        chi_square_value = sum(chi_square_stats)
        chi_square_critical = 16.919  # 9è‡ªç”±åº¦ï¼ŒÎ±=0.05
        
        if chi_square_value > chi_square_critical:
            manipulation_timing_score += 0.3
            current_period_analysis['frequency_anomaly'] = chi_square_value / chi_square_critical
        
        # === å¿ƒç†å­¦é™·é˜±æ£€æµ‹ ===
        psychological_trap_indicators = 0.0
        
        # æ£€æµ‹"è¿½çƒ­"é™·é˜±æ¨¡å¼
        if current_hot_count_by_tier['extremely_hot'] >= 2:
            psychological_trap_indicators += 0.25  # æçƒ­æ•°å­—åŒæ—¶å‡ºç°
        
        if current_hot_count_by_tier['very_hot'] >= 3:
            psychological_trap_indicators += 0.20  # å¾ˆçƒ­æ•°å­—å¤§é‡å‡ºç°
        
        # æ£€æµ‹"çƒ­é—¨å»¶ç»­"å‡è±¡
        consecutive_hot_periods = 0
        for i in range(min(5, len(historical_context))):
            period = historical_context[-(i+1)]
            period_tails = set(period.get('tails', []))
            hot_in_period = len(period_tails.intersection(hot_number_tiers['very_hot'].union(hot_number_tiers['extremely_hot'])))
            if hot_in_period >= 2:
                consecutive_hot_periods += 1
            else:
                break
        
        if consecutive_hot_periods >= 3:
            psychological_trap_indicators += 0.3  # è¿ç»­çƒ­é—¨å¯èƒ½æ˜¯é™·é˜±è®¾ç½®
        
        # === æ“æ§å¼ºåº¦é‡åŒ– ===
        # å¤šå› å­ç»¼åˆè¯„åˆ†æ¨¡å‹
        manipulation_factors = {
            'timing_factor': manipulation_timing_score * 0.35,
            'psychological_factor': psychological_trap_indicators * 0.25,
            'frequency_anomaly_factor': current_period_analysis['frequency_anomaly'] * 0.20,
            'concentration_factor': current_period_analysis['hot_concentration'] * 0.20
        }
        
        total_manipulation_score = sum(manipulation_factors.values())
        
        # åº”ç”¨éçº¿æ€§å˜æ¢å¢å¼ºæ£€æµ‹æ•æ„Ÿæ€§
        enhanced_score = 1 - math.exp(-2.5 * total_manipulation_score)
        final_score = min(0.95, max(0.05, enhanced_score))
        
        # === è¯¦ç»†è¯æ®è®°å½• ===
        evidence_package = {
            'hot_number_tiers': {k: list(v) for k, v in hot_number_tiers.items()},
            'frequency_profiles': frequency_profiles,
            'current_analysis': current_period_analysis,
            'manipulation_factors': manipulation_factors,
            'chi_square_test': {
                'value': chi_square_value,
                'critical': chi_square_critical,
                'significant': chi_square_value > chi_square_critical
            },
            'consecutive_hot_periods': consecutive_hot_periods,
            'total_hot_in_current': total_hot_in_current
        }
        
        return final_score

    def _detect_cold_comeback_trap(self, current_tails: Set[int], historical_context: List[Dict]) -> float:
        """
        ç§‘ç ”çº§å†·é—¨å¤å‡ºé™·é˜±æ£€æµ‹ç®—æ³•
        åŸºäºå†·é—¨å‘¨æœŸåˆ†æã€å¤å‡ºæ—¶æœºé¢„æµ‹å’Œæ“æ§åŠ¨æœºè¯†åˆ«
        """
        if len(historical_context) < 20:
            return 0.0
        
        # === å†·é—¨å°¾æ•°åŠ¨æ€è¯†åˆ«ç³»ç»Ÿ ===
        absence_analysis = {}
        
        for tail in range(10):
            # è®¡ç®—å„ç§ç¼ºå¸­æŒ‡æ ‡
            last_appearance_index = -1
            absence_streaks = []
            current_absence_length = 0
            total_appearances = 0
            
            for i, period in enumerate(reversed(historical_context)):
                if tail in period.get('tails', []):
                    if last_appearance_index == -1:
                        last_appearance_index = i
                    if current_absence_length > 0:
                        absence_streaks.append(current_absence_length)
                        current_absence_length = 0
                    total_appearances += 1
                else:
                    current_absence_length += 1
            
            # å¦‚æœå½“å‰è¿˜åœ¨ç¼ºå¸­ä¸­
            if current_absence_length > 0:
                absence_streaks.append(current_absence_length)
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_absence_length = np.mean(absence_streaks) if absence_streaks else 0
            absence_variance = np.var(absence_streaks) if len(absence_streaks) > 1 else 0
            appearance_frequency = total_appearances / len(historical_context)
            
            # å†·é—¨ç¨‹åº¦é‡åŒ–
            coldness_metrics = {
                'current_absence_length': current_absence_length,
                'last_appearance_index': last_appearance_index,
                'avg_absence_length': avg_absence_length,
                'absence_variance': absence_variance,
                'appearance_frequency': appearance_frequency,
                'absence_streaks_count': len(absence_streaks)
            }
            
            # ç»¼åˆå†·é—¨æŒ‡æ•°è®¡ç®—
            if appearance_frequency > 0:
                expected_absence = 1 / appearance_frequency - 1
                absence_anomaly = (current_absence_length - expected_absence) / (expected_absence + 1) if expected_absence > 0 else 0
                
                coldness_index = (
                    (current_absence_length / 20.0) * 0.4 +  # å½“å‰ç¼ºå¸­é•¿åº¦æƒé‡
                    (absence_anomaly if absence_anomaly > 0 else 0) * 0.3 +  # å¼‚å¸¸ç¼ºå¸­æƒé‡
                    (1 - appearance_frequency) * 0.3  # æ•´ä½“ä½é¢‘æƒé‡
                )
            else:
                coldness_index = 1.0
            
            coldness_metrics['coldness_index'] = min(1.0, max(0.0, coldness_index))
            absence_analysis[tail] = coldness_metrics
        
        # === å†·é—¨å°¾æ•°åˆ†å±‚åˆ†ç±» ===
        cold_tiers = {
            'extremely_cold': [],  # å†·é—¨æŒ‡æ•° > 0.8
            'very_cold': [],       # å†·é—¨æŒ‡æ•° > 0.6
            'moderately_cold': [], # å†·é—¨æŒ‡æ•° > 0.4
            'trending_cold': []    # ç¼ºå¸­è¶‹åŠ¿é€’å¢
        }
        
        for tail, metrics in absence_analysis.items():
            coldness = metrics['coldness_index']
            if coldness > 0.8:
                cold_tiers['extremely_cold'].append(tail)
            elif coldness > 0.6:
                cold_tiers['very_cold'].append(tail)
            elif coldness > 0.4:
                cold_tiers['moderately_cold'].append(tail)
        
        # === å¤å‡ºæ—¶æœºæ“æ§æ£€æµ‹ ===
        comeback_manipulation_score = 0.0
        
        # æ£€æµ‹å½“å‰æœŸçš„å†·é—¨å¤å‡ºæƒ…å†µ
        current_cold_comebacks = {
            'extremely_cold_comebacks': len(current_tails.intersection(set(cold_tiers['extremely_cold']))),
            'very_cold_comebacks': len(current_tails.intersection(set(cold_tiers['very_cold']))),
            'moderately_cold_comebacks': len(current_tails.intersection(set(cold_tiers['moderately_cold'])))
        }
        
        # === å¤å‡ºæ—¶æœºå¼‚å¸¸æ€§åˆ†æ ===
        # ä½¿ç”¨æ³Šæ¾åˆ†å¸ƒæ¨¡å‹åˆ†æå¤å‡ºæ—¶æœºçš„å¼‚å¸¸æ€§
        total_cold_comebacks = sum(current_cold_comebacks.values())
        
        # åˆå§‹åŒ–å˜é‡ï¼Œé¿å…ä½œç”¨åŸŸé”™è¯¯
        expected_comeback_rate = 0.1  # åŸºç¡€å¤å‡ºæ¦‚ç‡10%
        total_cold_numbers = sum(len(tier) for tier in cold_tiers.values())
        
        if total_cold_comebacks > 0:
            # è®¡ç®—æœŸæœ›å¤å‡ºæ¦‚ç‡
            if total_cold_numbers > 0:
                observed_comeback_rate = total_cold_comebacks / total_cold_numbers
                
                # ä½¿ç”¨è´å¶æ–¯å¼‚å¸¸æ£€æµ‹
                if observed_comeback_rate > expected_comeback_rate * 3:  # å¤å‡ºç‡å¼‚å¸¸é«˜
                    comeback_manipulation_score += 0.4
        
        # === å¤å‡ºæ¨¡å¼æ“æ§æ£€æµ‹ ===
        pattern_manipulation_indicators = 0.0
        
        # æ£€æµ‹"è¡¥ç¼º"é™·é˜±æ¨¡å¼
        if current_cold_comebacks['extremely_cold_comebacks'] >= 2:
            pattern_manipulation_indicators += 0.3  # å¤šä¸ªæå†·æ•°å­—åŒæ—¶å¤å‡º
        
        # æ£€æµ‹"è½®æ¢"æ“æ§æ¨¡å¼
        recent_comebacks = []
        for i in range(min(5, len(historical_context))):
            period = historical_context[-(i+1)]
            period_tails = set(period.get('tails', []))
            
            # ç»Ÿè®¡è¯¥æœŸçš„å†·é—¨å¤å‡º
            period_cold_comebacks = 0
            for tail in period_tails:
                if tail in absence_analysis and absence_analysis[tail]['coldness_index'] > 0.5:
                    period_cold_comebacks += 1
            
            recent_comebacks.append(period_cold_comebacks)
        
        # æ£€æµ‹æ˜¯å¦å­˜åœ¨å‘¨æœŸæ€§çš„å†·é—¨å¤å‡ºæ¨¡å¼
        if len(recent_comebacks) >= 3:
            comeback_variance = np.var(recent_comebacks)
            if comeback_variance < 0.5 and np.mean(recent_comebacks) > 1.5:  # è§„å¾‹æ€§å†·é—¨å¤å‡º
                pattern_manipulation_indicators += 0.25
        
        # === å¿ƒç†å­¦æ“æ§ç»´åº¦åˆ†æ ===
        psychological_manipulation = 0.0
        
        # "è¡¥å¿å¿ƒç†"åˆ©ç”¨æ£€æµ‹
        for tail in current_tails:
            if tail in absence_analysis:
                tail_metrics = absence_analysis[tail]
                if tail_metrics['current_absence_length'] > 15:  # é•¿æœŸç¼ºå¸­åçªç„¶å‡ºç°
                    psychological_manipulation += 0.15
        
        # "æœŸæœ›å®ç°"é™·é˜±æ£€æµ‹
        high_expectation_tails = [tail for tail, metrics in absence_analysis.items() 
                                if metrics['coldness_index'] > 0.7]
        expectation_fulfillment = len(current_tails.intersection(set(high_expectation_tails)))
        
        if expectation_fulfillment >= 2:
            psychological_manipulation += 0.2  # åŒæ—¶æ»¡è¶³å¤šä¸ªé«˜æœŸæœ›
        
        # === ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ ===
        # ä½¿ç”¨è¶…å‡ ä½•åˆ†å¸ƒæ£€éªŒå¤å‡ºçš„ç»Ÿè®¡æ˜¾è‘—æ€§
        population_size = 10  # æ€»å°¾æ•°æ•°é‡
        success_states = len([tail for tail, metrics in absence_analysis.items() 
                            if metrics['coldness_index'] > 0.5])  # å†·é—¨å°¾æ•°æ•°é‡
        sample_size = len(current_tails)  # å½“å‰æœŸå°¾æ•°æ•°é‡
        observed_successes = total_cold_comebacks  # è§‚å¯Ÿåˆ°çš„å†·é—¨å¤å‡ºæ•°é‡
        
        if success_states > 0 and sample_size > 0:
            # è®¡ç®—è¶…å‡ ä½•æ¦‚ç‡
            expected_successes = (success_states * sample_size) / population_size
            if observed_successes > expected_successes * 2:  # è§‚å¯Ÿå€¼æ˜¾è‘—é«˜äºæœŸæœ›
                comeback_manipulation_score += 0.25
        
        # === æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ ===
        # åˆ†æå¤å‡ºæ—¶é—´çš„è‡ªç›¸å…³æ€§
        comeback_timeline = []
        for i, period in enumerate(historical_context):
            period_tails = set(period.get('tails', []))
            comeback_count = 0
            for tail in period_tails:
                if tail in absence_analysis and absence_analysis[tail]['coldness_index'] > 0.6:
                    comeback_count += 1
            comeback_timeline.append(comeback_count)
        
        if len(comeback_timeline) >= 10:
            # è®¡ç®—è‡ªç›¸å…³ç³»æ•°
            autocorr = np.corrcoef(comeback_timeline[:-1], comeback_timeline[1:])[0, 1]
            if not np.isnan(autocorr) and abs(autocorr) > 0.3:  # å¼ºè‡ªç›¸å…³æ€§
                comeback_manipulation_score += 0.2
        
        # === ç»¼åˆè¯„åˆ†ä¸éçº¿æ€§å˜æ¢ ===
        manipulation_components = {
            'comeback_timing': comeback_manipulation_score * 0.35,
            'pattern_indicators': pattern_manipulation_indicators * 0.25,
            'psychological_factors': psychological_manipulation * 0.25,
            'statistical_anomaly': 0.15 if total_cold_comebacks > expected_comeback_rate * total_cold_numbers * 2 else 0.0
        }
        
        total_score = sum(manipulation_components.values())
        
        # åº”ç”¨Så‹å˜æ¢å‡½æ•°
        enhanced_score = total_score / (1 + math.exp(-5 * (total_score - 0.4)))
        final_score = min(0.95, max(0.05, enhanced_score))
        
        return final_score

    def _detect_continuity_break_trap(self, current_tails: Set[int], historical_context: List[Dict]) -> float:
        """
        ç§‘ç ”çº§è¿ç»­æ€§æ–­è£‚é™·é˜±æ£€æµ‹ç®—æ³•
        åŸºäºé©¬å°”å¯å¤«é“¾åˆ†æã€è¿ç»­æ¨¡å¼è¯†åˆ«å’Œæ–­è£‚æ—¶æœºæ£€æµ‹
        """
        if len(historical_context) < 10:
            return 0.0
        
        # === å¤šé˜¶é©¬å°”å¯å¤«é“¾è¿ç»­æ€§åˆ†æ ===
        continuity_patterns = {
            'first_order': {},   # ä¸€é˜¶ï¼šåŸºäºå‰ä¸€æœŸ
            'second_order': {},  # äºŒé˜¶ï¼šåŸºäºå‰ä¸¤æœŸ
            'third_order': {}    # ä¸‰é˜¶ï¼šåŸºäºå‰ä¸‰æœŸ
        }
        
        # æ„å»ºçŠ¶æ€è½¬ç§»çŸ©é˜µ
        for order in [1, 2, 3]:
            transitions = defaultdict(lambda: defaultdict(int))
            
            for i in range(order, len(historical_context)):
                # æ„å»ºå‰åºçŠ¶æ€
                prev_states = []
                for j in range(order):
                    prev_states.append(tuple(sorted(historical_context[i-j-1].get('tails', []))))
                
                prev_state = tuple(prev_states)
                current_state = tuple(sorted(historical_context[i].get('tails', [])))
                
                transitions[prev_state][current_state] += 1
            
            continuity_patterns[f'{["first", "second", "third"][order-1]}_order'] = dict(transitions)
        
        # === è¿ç»­æ¨¡å¼å¼ºåº¦é‡åŒ– ===
        pattern_strengths = {}
        
        for tail in range(10):
            tail_continuity_analysis = {
                'consecutive_appearances': [],
                'consecutive_absences': [],
                'alternating_patterns': [],
                'transition_probabilities': {}
            }
            
            # åˆ†æè¿ç»­å‡ºç°å’Œç¼ºå¸­æ¨¡å¼
            current_streak = 0
            streak_type = None  # 'appear' or 'absent'
            
            for period in reversed(historical_context):
                is_present = tail in period.get('tails', [])
                
                if streak_type is None:
                    streak_type = 'appear' if is_present else 'absent'
                    current_streak = 1
                elif (streak_type == 'appear' and is_present) or (streak_type == 'absent' and not is_present):
                    current_streak += 1
                else:
                    # æ¨¡å¼æ”¹å˜
                    if streak_type == 'appear':
                        tail_continuity_analysis['consecutive_appearances'].append(current_streak)
                    else:
                        tail_continuity_analysis['consecutive_absences'].append(current_streak)
                    
                    streak_type = 'appear' if is_present else 'absent'
                    current_streak = 1
            
            # æ·»åŠ æœ€åçš„è¿ç»­æ¨¡å¼
            if streak_type == 'appear':
                tail_continuity_analysis['consecutive_appearances'].append(current_streak)
            else:
                tail_continuity_analysis['consecutive_absences'].append(current_streak)
            
            # è®¡ç®—è¿ç»­æ€§å¼ºåº¦æŒ‡æ ‡
            if tail_continuity_analysis['consecutive_appearances']:
                avg_appear_streak = np.mean(tail_continuity_analysis['consecutive_appearances'])
                max_appear_streak = max(tail_continuity_analysis['consecutive_appearances'])
            else:
                avg_appear_streak = 0
                max_appear_streak = 0
            
            if tail_continuity_analysis['consecutive_absences']:
                avg_absent_streak = np.mean(tail_continuity_analysis['consecutive_absences'])
                max_absent_streak = max(tail_continuity_analysis['consecutive_absences'])
            else:
                avg_absent_streak = 0
                max_absent_streak = 0
            
            # è¿ç»­æ€§å¼ºåº¦ç»¼åˆæŒ‡æ•°
            continuity_strength = (
                (max_appear_streak / 10.0) * 0.3 +
                (avg_appear_streak / 5.0) * 0.25 +
                (max_absent_streak / 15.0) * 0.25 +
                (avg_absent_streak / 7.0) * 0.2
            )
            
            pattern_strengths[tail] = {
                'continuity_strength': min(1.0, continuity_strength),
                'avg_appear_streak': avg_appear_streak,
                'max_appear_streak': max_appear_streak,
                'avg_absent_streak': avg_absent_streak,
                'max_absent_streak': max_absent_streak,
                'pattern_consistency': len(tail_continuity_analysis['consecutive_appearances']) + len(tail_continuity_analysis['consecutive_absences'])
            }
        
        # === æ–­è£‚ç‚¹æ£€æµ‹ä¸åˆ†æ ===
        break_detection_score = 0.0
        current_breaks = []
        
        for tail in range(10):
            tail_present = tail in current_tails
            pattern_info = pattern_strengths[tail]
            
            # æ£€æµ‹è¿ç»­å‡ºç°çš„æ–­è£‚
            if pattern_info['continuity_strength'] > 0.6:  # å¼ºè¿ç»­æ€§æ¨¡å¼
                recent_appearance_pattern = []
                for i in range(min(5, len(historical_context))):
                    period = historical_context[-(i+1)]
                    recent_appearance_pattern.append(tail in period.get('tails', []))
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è¿ç»­å‡ºç°åçš„çªç„¶ä¸­æ–­
                if len(recent_appearance_pattern) >= 3:
                    recent_consecutive = 0
                    for appeared in reversed(recent_appearance_pattern):
                        if appeared:
                            recent_consecutive += 1
                        else:
                            break
                    
                    # å¦‚æœè¿ç»­å‡ºç°2æ¬¡ä»¥ä¸Šåçªç„¶ä¸­æ–­
                    if recent_consecutive >= 2 and not tail_present:
                        break_intensity = min(1.0, recent_consecutive / 5.0)
                        current_breaks.append({
                            'tail': tail,
                            'type': 'appearance_break',
                            'intensity': break_intensity,
                            'consecutive_count': recent_consecutive
                        })
                        break_detection_score += break_intensity * 0.2
            
            # æ£€æµ‹è¿ç»­ç¼ºå¸­çš„æ–­è£‚ï¼ˆå†·é—¨çªç„¶å‡ºç°ï¼‰
            if pattern_info['avg_absent_streak'] > 3:  # å¹³å‡ç¼ºå¸­è¾ƒé•¿
                recent_absence_count = 0
                for i in range(min(int(pattern_info['avg_absent_streak']) + 2, len(historical_context))):
                    period = historical_context[-(i+1)]
                    if tail not in period.get('tails', []):
                        recent_absence_count += 1
                    else:
                        break
                
                # å¦‚æœé•¿æœŸç¼ºå¸­åçªç„¶å‡ºç°
                if recent_absence_count >= pattern_info['avg_absent_streak'] * 0.8 and tail_present:
                    break_intensity = min(1.0, recent_absence_count / 10.0)
                    current_breaks.append({
                        'tail': tail,
                        'type': 'absence_break',
                        'intensity': break_intensity,
                        'absence_count': recent_absence_count
                    })
                    break_detection_score += break_intensity * 0.15
        
        # === ç³»ç»Ÿæ€§æ–­è£‚æ£€æµ‹ ===
        # æ£€æµ‹å¤šä¸ªå°¾æ•°åŒæ—¶å‘ç”Ÿè¿ç»­æ€§æ–­è£‚ï¼ˆå¯èƒ½çš„ç³»ç»Ÿæ€§æ“æ§ï¼‰
        if len(current_breaks) >= 2:
            system_break_multiplier = min(2.0, 1 + len(current_breaks) * 0.2)
            break_detection_score *= system_break_multiplier
        
        # === æ–­è£‚æ—¶æœºå¼‚å¸¸æ€§åˆ†æ ===
        timing_anomaly_score = 0.0
        
        # åˆ†ææ–­è£‚å‘ç”Ÿçš„æ—¶æœºæ¨¡å¼
        if current_breaks:
            # æ£€æµ‹æ˜¯å¦åœ¨ç‰¹å®šå‘¨æœŸå‘ç”Ÿæ–­è£‚
            period_position = len(historical_context) % 7  # å‘¨æœŸæ€§åˆ†æ
            
            # ç»Ÿè®¡å†å²ä¸Šè¯¥ä½ç½®çš„æ–­è£‚é¢‘ç‡
            historical_breaks_at_position = 0
            for check_period in range(period_position, len(historical_context), 7):
                if check_period < len(historical_context) - 1:
                    # æ£€æŸ¥è¯¥æœŸæ˜¯å¦å‘ç”Ÿäº†ç±»ä¼¼çš„æ–­è£‚
                    period_data = historical_context[check_period]
                    prev_period_data = historical_context[check_period - 1] if check_period > 0 else None
                    
                    if prev_period_data:
                        for tail in range(10):
                            tail_in_current = tail in period_data.get('tails', [])
                            tail_in_prev = tail in prev_period_data.get('tails', [])
                            
                            if tail_in_prev and not tail_in_current:  # æ–­è£‚å‘ç”Ÿ
                                historical_breaks_at_position += 1
                                break
            
            # å¦‚æœå½“å‰ä½ç½®çš„æ–­è£‚é¢‘ç‡å¼‚å¸¸é«˜
            expected_break_frequency = len(historical_context) / 7.0 * 0.3  # æœŸæœ›æ–­è£‚é¢‘ç‡
            if historical_breaks_at_position > expected_break_frequency * 1.5:
                timing_anomaly_score += 0.3
        
        # === é©¬å°”å¯å¤«é“¾è½¬ç§»å¼‚å¸¸æ£€æµ‹ ===
        markov_anomaly_score = 0.0
        
        # åŸºäºå†å²è½¬ç§»æ¦‚ç‡è®¡ç®—å½“å‰çŠ¶æ€çš„å¼‚å¸¸ç¨‹åº¦
        if len(historical_context) >= 2:
            prev_state = tuple(sorted(historical_context[-1].get('tails', [])))
            current_state = tuple(sorted(current_tails))
            
            # æŸ¥æ‰¾å†å²è½¬ç§»æ¨¡å¼
            transitions_from_prev = continuity_patterns['first_order'].get(prev_state, {})
            if transitions_from_prev:
                total_transitions = sum(transitions_from_prev.values())
                observed_transition_count = transitions_from_prev.get(current_state, 0)
                transition_probability = observed_transition_count / total_transitions
                
                # å¦‚æœè½¬ç§»æ¦‚ç‡å¼‚å¸¸ä½ï¼ˆå¯èƒ½çš„äººä¸ºæ–­è£‚ï¼‰
                if transition_probability < 0.1 and observed_transition_count == 0:
                    markov_anomaly_score += 0.25
        
        # === å¿ƒç†å­¦æ–­è£‚é™·é˜±æ£€æµ‹ ===
        psychological_break_score = 0.0
        
        # æ£€æµ‹"æœŸæœ›è½ç©º"é™·é˜±
        for break_info in current_breaks:
            if break_info['type'] == 'appearance_break':
                # è¿ç»­å‡ºç°åçªç„¶ä¸­æ–­ï¼Œåˆ©ç”¨ç©å®¶çš„å»¶ç»­æœŸæœ›
                psychological_break_score += 0.2
            elif break_info['type'] == 'absence_break':
                # é•¿æœŸç¼ºå¸­åçªç„¶å‡ºç°ï¼Œåˆ©ç”¨ç©å®¶çš„è¡¥å¿å¿ƒç†
                psychological_break_score += 0.15
        
        # æ£€æµ‹"å‡æ¨¡å¼"å»ºç«‹åçš„æ–­è£‚
        established_patterns = [tail for tail, info in pattern_strengths.items() 
                              if info['continuity_strength'] > 0.7]
        broken_established_patterns = [tail for tail in established_patterns 
                                     if any(b['tail'] == tail for b in current_breaks)]
        
        if len(broken_established_patterns) >= 2:
            psychological_break_score += 0.25  # å¤šä¸ªå¼ºæ¨¡å¼åŒæ—¶æ–­è£‚
        
        # === ç»¼åˆè¯„åˆ†æ¨¡å‹ ===
        manipulation_factors = {
            'break_detection': break_detection_score * 0.35,
            'timing_anomaly': timing_anomaly_score * 0.25,
            'markov_anomaly': markov_anomaly_score * 0.20,
            'psychological_manipulation': psychological_break_score * 0.20
        }
        
        total_score = sum(manipulation_factors.values())
        
        # åº”ç”¨éçº¿æ€§å¢å¼ºå‡½æ•°
        if total_score > 0.6:
            enhanced_score = total_score + (total_score - 0.6) * 0.5
        else:
            enhanced_score = total_score * 0.9
        
        final_score = min(0.95, max(0.05, enhanced_score))
        
        return final_score

    def _detect_symmetry_trap(self, current_tails: Set[int], historical_context: List[Dict]) -> float:
        """
        ç§‘ç ”çº§å¯¹ç§°æ€§é™·é˜±æ£€æµ‹ç®—æ³•
        åŸºäºå¤šç»´å¯¹ç§°æ€§åˆ†æã€ç¾¤è®ºåº”ç”¨å’Œå¯¹ç§°æ€§ç ´ç¼ºæ£€æµ‹
        """
        if len(historical_context) < 8:
            return 0.0
        
        # === å¤šç»´å¯¹ç§°æ€§å®šä¹‰ç³»ç»Ÿ ===
        symmetry_types = {
            'numerical_symmetry': {
                'mirror_pairs': [(0, 9), (1, 8), (2, 7), (3, 6), (4, 5)],
                'center_point': None
            },
            'visual_symmetry': {
                'rotational_180': [(6, 9), (0, 0), (1, 1), (8, 8)],  # è§†è§‰ä¸Š180åº¦æ—‹è½¬å¯¹ç§°
                'vertical_mirror': [(0, 0), (1, 1), (2, 5), (3, 3), (6, 9), (8, 8)]  # å‚ç›´é•œåƒå¯¹ç§°
            },
            'arithmetic_symmetry': {
                'sum_complement': [(i, 9-i) for i in range(5)],  # æ•°å­—å’Œä¸º9çš„å¯¹ç§°
                'parity_symmetry': {'even': [0, 2, 4, 6, 8], 'odd': [1, 3, 5, 7, 9]}
            },
            'positional_symmetry': {
                'keyboard_layout': {
                    'top_row': [1, 2, 3],
                    'middle_row': [4, 5, 6], 
                    'bottom_row': [7, 8, 9],
                    'special': [0]
                }
            }
        }
        
        # === å†å²å¯¹ç§°æ€§æ¨¡å¼åˆ†æ ===
        historical_symmetry_scores = []
        symmetry_pattern_tracker = defaultdict(list)
        
        for period in historical_context:
            period_tails = set(period.get('tails', []))
            period_symmetry_analysis = {}
            
            # 1. æ•°å€¼é•œåƒå¯¹ç§°åˆ†æ
            mirror_symmetry_score = 0.0
            mirror_pairs_found = []
            
            for pair in symmetry_types['numerical_symmetry']['mirror_pairs']:
                if pair[0] in period_tails and pair[1] in period_tails:
                    mirror_pairs_found.append(pair)
                    mirror_symmetry_score += 0.2
            
            period_symmetry_analysis['mirror_symmetry'] = {
                'score': min(1.0, mirror_symmetry_score),
                'pairs_found': mirror_pairs_found
            }
            
            # 2. å¥‡å¶å¯¹ç§°åˆ†æ
            even_count = len(period_tails.intersection(set(symmetry_types['arithmetic_symmetry']['parity_symmetry']['even'])))
            odd_count = len(period_tails.intersection(set(symmetry_types['arithmetic_symmetry']['parity_symmetry']['odd'])))
            
            parity_balance = abs(even_count - odd_count) / max(len(period_tails), 1)
            parity_symmetry_score = 1.0 - parity_balance  # è¶Šå¹³è¡¡åˆ†æ•°è¶Šé«˜
            
            period_symmetry_analysis['parity_symmetry'] = {
                'score': parity_symmetry_score,
                'even_count': even_count,
                'odd_count': odd_count,
                'balance_ratio': parity_balance
            }
            
            # 3. ä½ç½®å¯¹ç§°åˆ†æï¼ˆåŸºäºæ•°å­—åœ¨é”®ç›˜ä¸Šçš„ä½ç½®ï¼‰
            position_symmetry_score = 0.0
            keyboard_layout = symmetry_types['positional_symmetry']['keyboard_layout']
            
            row_distributions = {}
            for row_name, digits in keyboard_layout.items():
                row_count = len(period_tails.intersection(set(digits)))
                row_distributions[row_name] = row_count
            
            # æ£€æµ‹è¡Œé—´å¯¹ç§°æ€§
            top_bottom_symmetry = abs(row_distributions.get('top_row', 0) - row_distributions.get('bottom_row', 0))
            if top_bottom_symmetry == 0 and row_distributions.get('top_row', 0) > 0:
                position_symmetry_score += 0.4
            
            # æ£€æµ‹ä¸­å¿ƒå¯¹ç§°æ€§
            middle_dominance = row_distributions.get('middle_row', 0) / max(len(period_tails), 1)
            if middle_dominance > 0.5:
                position_symmetry_score += 0.3
            
            period_symmetry_analysis['position_symmetry'] = {
                'score': min(1.0, position_symmetry_score),
                'row_distributions': row_distributions
            }
            
            # 4. æ•°å­¦ç¾¤å¯¹ç§°åˆ†æ
            group_symmetry_score = 0.0
            
            # å¾ªç¯ç¾¤å¯¹ç§°ï¼ˆæ¨¡10ï¼‰
            tail_sum_mod5 = sum(period_tails) % 5
            if tail_sum_mod5 == 0:  # å’Œä¸º5çš„å€æ•°
                group_symmetry_score += 0.25
            
            # ç½®æ¢ç¾¤å¯¹ç§°
            sorted_tails = sorted(list(period_tails))
            if len(sorted_tails) >= 3:
                # æ£€æµ‹ç­‰å·®æ•°åˆ—
                differences = [sorted_tails[i+1] - sorted_tails[i] for i in range(len(sorted_tails)-1)]
                if len(set(differences)) <= 2:  # å·®å€¼ç§ç±»å°‘ï¼Œå¯èƒ½æœ‰å¯¹ç§°æ€§
                    group_symmetry_score += 0.2
            
            period_symmetry_analysis['group_symmetry'] = {
                'score': min(1.0, group_symmetry_score),
                'sum_mod5': tail_sum_mod5,
                'differences': differences if len(sorted_tails) >= 3 else []
            }
            
            # ç»¼åˆå¯¹ç§°æ€§åˆ†æ•°
            total_period_symmetry = (
                period_symmetry_analysis['mirror_symmetry']['score'] * 0.3 +
                period_symmetry_analysis['parity_symmetry']['score'] * 0.25 +
                period_symmetry_analysis['position_symmetry']['score'] * 0.25 +
                period_symmetry_analysis['group_symmetry']['score'] * 0.2
            )
            
            historical_symmetry_scores.append(total_period_symmetry)
            symmetry_pattern_tracker['mirror_pairs'].append(len(mirror_pairs_found))
            symmetry_pattern_tracker['parity_balance'].append(parity_symmetry_score)
        
        # === å½“å‰æœŸå¯¹ç§°æ€§åˆ†æ ===
        current_period_analysis = {}
        
        # å¯¹å½“å‰æœŸè¿›è¡Œç›¸åŒçš„å¯¹ç§°æ€§åˆ†æ
        current_tails_list = list(current_tails)
        
        # 1. å½“å‰æœŸé•œåƒå¯¹ç§°
        current_mirror_pairs = []
        current_mirror_score = 0.0
        
        for pair in symmetry_types['numerical_symmetry']['mirror_pairs']:
            if pair[0] in current_tails and pair[1] in current_tails:
                current_mirror_pairs.append(pair)
                current_mirror_score += 0.2
        
        # 2. å½“å‰æœŸå¥‡å¶å¯¹ç§°
        current_even_count = len(current_tails.intersection(set(symmetry_types['arithmetic_symmetry']['parity_symmetry']['even'])))
        current_odd_count = len(current_tails.intersection(set(symmetry_types['arithmetic_symmetry']['parity_symmetry']['odd'])))
        
        current_parity_balance = abs(current_even_count - current_odd_count) / max(len(current_tails), 1)
        current_parity_score = 1.0 - current_parity_balance
        
        # 3. å½“å‰æœŸä½ç½®å¯¹ç§°
        current_position_score = 0.0
        current_row_distributions = {}
        
        for row_name, digits in keyboard_layout.items():
            row_count = len(current_tails.intersection(set(digits)))
            current_row_distributions[row_name] = row_count
        
        current_top_bottom_symmetry = abs(current_row_distributions.get('top_row', 0) - current_row_distributions.get('bottom_row', 0))
        if current_top_bottom_symmetry == 0 and current_row_distributions.get('top_row', 0) > 0:
            current_position_score += 0.4
        
        current_middle_dominance = current_row_distributions.get('middle_row', 0) / max(len(current_tails), 1)
        if current_middle_dominance > 0.5:
            current_position_score += 0.3
        
        # 4. å½“å‰æœŸç¾¤å¯¹ç§°
        current_group_score = 0.0
        current_tail_sum_mod5 = sum(current_tails) % 5
        if current_tail_sum_mod5 == 0:
            current_group_score += 0.25
        
        current_sorted_tails = sorted(current_tails_list)
        if len(current_sorted_tails) >= 3:
            current_differences = [current_sorted_tails[i+1] - current_sorted_tails[i] for i in range(len(current_sorted_tails)-1)]
            if len(set(current_differences)) <= 2:
                current_group_score += 0.2
        
        # å½“å‰æœŸç»¼åˆå¯¹ç§°æ€§åˆ†æ•°
        current_total_symmetry = (
            min(1.0, current_mirror_score) * 0.3 +
            current_parity_score * 0.25 +
            min(1.0, current_position_score) * 0.25 +
            min(1.0, current_group_score) * 0.2
        )
        
        # === å¯¹ç§°æ€§å¼‚å¸¸æ£€æµ‹ ===
        symmetry_anomaly_score = 0.0
        
        # 1. è¿‡åº¦å¯¹ç§°æ£€æµ‹
        historical_avg_symmetry = np.mean(historical_symmetry_scores) if historical_symmetry_scores else 0.3
        historical_std_symmetry = np.std(historical_symmetry_scores) if len(historical_symmetry_scores) > 1 else 0.1
        
        # Z-scoreå¼‚å¸¸æ£€æµ‹
        if historical_std_symmetry > 0:
            symmetry_z_score = (current_total_symmetry - historical_avg_symmetry) / historical_std_symmetry
            if symmetry_z_score > 2.0:  # å¼‚å¸¸é«˜å¯¹ç§°æ€§
                symmetry_anomaly_score += 0.4
        
        # 2. ç‰¹å®šå¯¹ç§°ç±»å‹çš„å¼‚å¸¸
        if len(current_mirror_pairs) >= 3:  # 3å¯¹ä»¥ä¸Šé•œåƒå¯¹ç§°
            symmetry_anomaly_score += 0.3
        
        if current_parity_score > 0.9:  # æé«˜å¥‡å¶å¹³è¡¡
            symmetry_anomaly_score += 0.25
        
        # 3. åå¯¹ç§°é™·é˜±æ£€æµ‹ï¼ˆæ•…æ„æ‰“ç ´å¯¹ç§°ï¼‰
        anti_symmetry_score = 0.0
        
        # æ£€æµ‹æ˜¯å¦æ•…æ„é¿å…å¯¹ç§°
        potential_symmetry_break = 0
        for pair in symmetry_types['numerical_symmetry']['mirror_pairs']:
            if (pair[0] in current_tails) != (pair[1] in current_tails):  # åªæœ‰ä¸€ä¸ªåœ¨ï¼Œç ´åå¯¹ç§°
                potential_symmetry_break += 1
        
        if potential_symmetry_break >= 3:  # å¤šä¸ªå¯¹ç§°è¢«æ•…æ„ç ´å
            anti_symmetry_score += 0.3
        
        # === å¯¹ç§°æ€§å¿ƒç†é™·é˜±æ£€æµ‹ ===
        psychological_symmetry_score = 0.0
        
        # æ£€æµ‹"ç¾å­¦å¸å¼•"é™·é˜±
        if current_total_symmetry > 0.8:
            psychological_symmetry_score += 0.25  # è¿‡äºç¾è§‚çš„å¯¹ç§°ç»„åˆ
        
        # æ£€æµ‹"æ¨¡å¼æœŸå¾…"é™·é˜±
        recent_symmetry_trend = historical_symmetry_scores[-3:] if len(historical_symmetry_scores) >= 3 else []
        if recent_symmetry_trend and all(score > 0.6 for score in recent_symmetry_trend):
            if current_total_symmetry > 0.7:
                psychological_symmetry_score += 0.2  # å»¶ç»­é«˜å¯¹ç§°æ€§è¶‹åŠ¿
        
        # æ£€æµ‹"è¡¥å¿å¯¹ç§°"é™·é˜±
        if len(historical_symmetry_scores) >= 5:
            recent_avg_symmetry = np.mean(historical_symmetry_scores[-5:])
            if recent_avg_symmetry < 0.3 and current_total_symmetry > 0.7:
                psychological_symmetry_score += 0.25  # ä½å¯¹ç§°åçš„è¡¥å¿æ€§é«˜å¯¹ç§°
        
        # === ç»¼åˆè¯„åˆ†ä¸é£é™©è¯„ä¼° ===
        manipulation_components = {
            'anomaly_detection': symmetry_anomaly_score * 0.35,
            'anti_symmetry_manipulation': anti_symmetry_score * 0.25,
            'psychological_exploitation': psychological_symmetry_score * 0.25,
            'absolute_symmetry_level': (current_total_symmetry - 0.5) * 0.15 if current_total_symmetry > 0.5 else 0.0
        }
        
        total_manipulation_score = sum(manipulation_components.values())
        
        # åº”ç”¨å¯¹ç§°æ€§ç‰¹æœ‰çš„è¯„åˆ†å‡½æ•°
        if total_manipulation_score > 0.4:
            # é«˜åˆ†åŒºé—´ä½¿ç”¨æŒ‡æ•°å¢å¼º
            enhanced_score = 0.4 + (total_manipulation_score - 0.4) * 1.5
        else:
            # ä½åˆ†åŒºé—´ä½¿ç”¨çº¿æ€§æ˜ å°„
            enhanced_score = total_manipulation_score
        
        final_score = min(0.95, max(0.05, enhanced_score))
        
        return final_score

    def _detect_number_psychology_trap(self, current_tails: Set[int], historical_context: List[Dict]) -> float:
        """
        ç§‘ç ”çº§æ•°å­—å¿ƒç†å­¦é™·é˜±æ£€æµ‹ç®—æ³•
        åŸºäºè®¤çŸ¥å¿ƒç†å­¦ã€è¡Œä¸ºç»æµå­¦å’Œæ–‡åŒ–æ•°å­—è±¡å¾å­¦çš„ç»¼åˆåˆ†æ
        """
        if len(historical_context) < 12:
            return 0.0
        
        # === è®¤çŸ¥å¿ƒç†å­¦æ•°å­—åå¥½æ¨¡å‹ ===
        cognitive_preferences = {
            'anchoring_bias': {
                'small_numbers': [0, 1, 2, 3],      # é”šå®šæ•ˆåº”ï¼šå€¾å‘äºé€‰æ‹©è¾ƒå°æ•°å­—
                'round_numbers': [0, 5],            # æ•´æ•°åå¥½
                'middle_range': [4, 5, 6],          # ä¸­åº¸åå¥½
                'lucky_numbers': [6, 8, 9],         # æ–‡åŒ–å¹¸è¿æ•°å­—
                'unlucky_numbers': [4, 7]           # æ–‡åŒ–å¿Œè®³æ•°å­—
            },
            'pattern_recognition': {
                'sequences': [
                    [1, 2, 3], [2, 3, 4], [3, 4, 5],  # è¿ç»­åºåˆ—
                    [1, 3, 5], [2, 4, 6], [0, 2, 4],  # ç­‰å·®åºåˆ—
                    [1, 4, 7], [2, 5, 8], [3, 6, 9]   # æ¨¡è¿ç®—åºåˆ—
                ],
                'symmetrical': [
                    [1, 9], [2, 8], [3, 7], [4, 6],   # é•œåƒå¯¹ç§°
                    [0, 5], [1, 5, 9], [2, 5, 8]      # ä¸­å¿ƒå¯¹ç§°
                ]
            },
            'availability_heuristic': {
                'memorable_dates': [1, 2, 9],        # å®¹æ˜“è®°ä½çš„æ—¥æœŸæ•°å­—
                'significant_numbers': [0, 1, 5, 8], # ç¤¾ä¼šæ„ä¹‰æ•°å­—
                'geometric_appeal': [0, 6, 8, 9]     # è§†è§‰å¸å¼•åŠ›æ•°å­—
            }
        }
        
        # === è¡Œä¸ºç»æµå­¦åå·®æ£€æµ‹ ===
        behavioral_biases = {
            'loss_aversion': 0.0,      # æŸå¤±åŒæ¶
            'confirmation_bias': 0.0,   # ç¡®è®¤åè¯¯
            'gambler_fallacy': 0.0,     # èµŒå¾’è°¬è¯¯
            'hot_hand_fallacy': 0.0,    # çƒ­æ‰‹é”™è§‰
            'representativeness': 0.0   # ä»£è¡¨æ€§å¯å‘å¼
        }
        
        # === å†å²æ•°å­—å¿ƒç†æ¨¡å¼åˆ†æ ===
        historical_psychology_profiles = []
        
        for period in historical_context:
            period_tails = set(period.get('tails', []))
            period_psychology = {
                'cognitive_score': 0.0,
                'bias_indicators': {},
                'cultural_influence': 0.0
            }
            
            # 1. è®¤çŸ¥åå¥½åˆ†æ
            cognitive_score = 0.0
            
            # é”šå®šæ•ˆåº”æ£€æµ‹
            small_numbers_count = len(period_tails.intersection(set(cognitive_preferences['anchoring_bias']['small_numbers'])))
            if small_numbers_count > len(period_tails) * 0.6:  # 60%ä»¥ä¸Šæ˜¯å°æ•°å­—
                cognitive_score += 0.2
            
            # æ•´æ•°åå¥½æ£€æµ‹
            round_numbers_count = len(period_tails.intersection(set(cognitive_preferences['anchoring_bias']['round_numbers'])))
            if round_numbers_count > 0:
                cognitive_score += round_numbers_count * 0.15
            
            # æ¨¡å¼è¯†åˆ«å€¾å‘æ£€æµ‹
            pattern_matches = 0
            for sequence in cognitive_preferences['pattern_recognition']['sequences']:
                if set(sequence).issubset(period_tails):
                    pattern_matches += 1
                    cognitive_score += 0.1
            
            period_psychology['cognitive_score'] = min(1.0, cognitive_score)
            period_psychology['pattern_matches'] = pattern_matches
            
            # 2. æ–‡åŒ–å½±å“åˆ†æ
            cultural_score = 0.0
            
            lucky_count = len(period_tails.intersection(set(cognitive_preferences['anchoring_bias']['lucky_numbers'])))
            unlucky_count = len(period_tails.intersection(set(cognitive_preferences['anchoring_bias']['unlucky_numbers'])))
            
            # å¹¸è¿æ•°å­—è¿‡å¤šå¯èƒ½æ˜¯å¿ƒç†æ“æ§
            if lucky_count > len(period_tails) * 0.5:
                cultural_score += 0.3
            
            # å¿Œè®³æ•°å­—å¼‚å¸¸å‡ºç°ä¹Ÿå¯èƒ½æ˜¯åå‘å¿ƒç†æ“æ§
            if unlucky_count > len(period_tails) * 0.4:
                cultural_score += 0.25
            
            period_psychology['cultural_influence'] = min(1.0, cultural_score)
            
            historical_psychology_profiles.append(period_psychology)
        
        # === å½“å‰æœŸå¿ƒç†å­¦åˆ†æ ===
        current_psychology_analysis = {
            'cognitive_manipulation': 0.0,
            'bias_exploitation': {},
            'cultural_manipulation': 0.0,
            'psychological_complexity': 0.0
        }
        
        # 1. è®¤çŸ¥æ“æ§æ£€æµ‹
        current_cognitive_score = 0.0
        
        # æ£€æµ‹é”šå®šåå·®åˆ©ç”¨
        current_small_count = len(current_tails.intersection(set(cognitive_preferences['anchoring_bias']['small_numbers'])))
        if current_small_count > len(current_tails) * 0.7:  # å¼‚å¸¸é«˜çš„å°æ•°å­—æ¯”ä¾‹
            current_cognitive_score += 0.3
            current_psychology_analysis['bias_exploitation']['anchoring'] = current_small_count / len(current_tails)
        
        # æ£€æµ‹æ¨¡å¼è¯†åˆ«æ“æ§
        current_pattern_matches = 0
        complex_patterns_found = []
        
        for sequence in cognitive_preferences['pattern_recognition']['sequences']:
            if set(sequence).issubset(current_tails):
                current_pattern_matches += 1
                complex_patterns_found.append(sequence)
                current_cognitive_score += 0.15
        
        # å¤šé‡æ¨¡å¼ç»„åˆï¼ˆé«˜çº§å¿ƒç†æ“æ§ï¼‰
        if current_pattern_matches >= 2:
            current_cognitive_score += 0.2
            current_psychology_analysis['bias_exploitation']['pattern_overload'] = current_pattern_matches
        
        current_psychology_analysis['cognitive_manipulation'] = min(1.0, current_cognitive_score)
        
        # 2. æ–‡åŒ–å¿ƒç†æ“æ§æ£€æµ‹
        current_cultural_score = 0.0
        
        # å¹¸è¿æ•°å­—èšé›†æ£€æµ‹
        current_lucky_count = len(current_tails.intersection(set(cognitive_preferences['anchoring_bias']['lucky_numbers'])))
        lucky_concentration = current_lucky_count / len(current_tails) if current_tails else 0
        
        if lucky_concentration > 0.6:  # 60%ä»¥ä¸Šæ˜¯å¹¸è¿æ•°å­—
            current_cultural_score += 0.35
            current_psychology_analysis['bias_exploitation']['lucky_number_trap'] = lucky_concentration
        
        # è§†è§‰å¸å¼•åŠ›æ“æ§æ£€æµ‹
        geometric_numbers = len(current_tails.intersection(set(cognitive_preferences['availability_heuristic']['geometric_appeal'])))
        if geometric_numbers > len(current_tails) * 0.5:
            current_cultural_score += 0.25
            current_psychology_analysis['bias_exploitation']['visual_appeal'] = geometric_numbers / len(current_tails)
        
        current_psychology_analysis['cultural_manipulation'] = min(1.0, current_cultural_score)
        
        # === è¡Œä¸ºç»æµå­¦åå·®åˆ©ç”¨æ£€æµ‹ ===
        bias_exploitation_score = 0.0
        
        # 1. ä»£è¡¨æ€§å¯å‘å¼åå·®æ£€æµ‹
        current_tails_list = sorted(list(current_tails))
        if len(current_tails_list) >= 4:
            # æ£€æµ‹æ˜¯å¦è¿‡äº"éšæœº"ï¼ˆåå‘æ“æ§ï¼‰
            spacing_variance = np.var([current_tails_list[i+1] - current_tails_list[i] for i in range(len(current_tails_list)-1)])
            if spacing_variance > 8:  # é—´è·å˜å¼‚æ€§å¾ˆå¤§ï¼Œçœ‹èµ·æ¥"å¾ˆéšæœº"
                bias_exploitation_score += 0.2
                behavioral_biases['representativeness'] = spacing_variance / 12.0
        
        # 2. ç¡®è®¤åè¯¯åˆ©ç”¨æ£€æµ‹
        # åˆ†ææœ€è¿‘å‡ æœŸçš„è¶‹åŠ¿ï¼Œæ£€æµ‹æ˜¯å¦æ•…æ„å»¶ç»­æˆ–æ‰“ç ´è¶‹åŠ¿
        if len(historical_psychology_profiles) >= 3:
            recent_cultural_scores = [profile['cultural_influence'] for profile in historical_psychology_profiles[-3:]]
            recent_trend = np.mean(recent_cultural_scores)
            
            current_cultural_normalized = current_psychology_analysis['cultural_manipulation']
            
            # å¦‚æœå»¶ç»­äº†é«˜æ–‡åŒ–å½±å“è¶‹åŠ¿
            if recent_trend > 0.6 and current_cultural_normalized > 0.7:
                bias_exploitation_score += 0.25
                behavioral_biases['confirmation_bias'] = abs(current_cultural_normalized - recent_trend)
        
        # 3. èµŒå¾’è°¬è¯¯åˆ©ç”¨æ£€æµ‹
        # æ£€æµ‹æ˜¯å¦åœ¨é•¿æœŸæ¨¡å¼åæ•…æ„åè½¬
        if len(historical_context) >= 8:
            long_term_pattern_consistency = []
            for period in historical_context[-8:]:
                period_tails = set(period.get('tails', []))
                small_ratio = len(period_tails.intersection(set(cognitive_preferences['anchoring_bias']['small_numbers']))) / len(period_tails)
                long_term_pattern_consistency.append(small_ratio)
            
            pattern_stability = 1.0 - np.std(long_term_pattern_consistency)
            current_small_ratio = len(current_tails.intersection(set(cognitive_preferences['anchoring_bias']['small_numbers']))) / len(current_tails)
            
            # å¦‚æœé•¿æœŸç¨³å®šåçªç„¶åè½¬
            if pattern_stability > 0.7:
                historical_avg = np.mean(long_term_pattern_consistency)
                if abs(current_small_ratio - historical_avg) > 0.4:
                    bias_exploitation_score += 0.3
                    behavioral_biases['gambler_fallacy'] = abs(current_small_ratio - historical_avg)
        
        # === é«˜çº§å¿ƒç†æ“æ§æŠ€æœ¯æ£€æµ‹ ===
        advanced_manipulation_score = 0.0
        
        # 1. å¤šå±‚æ¬¡å¿ƒç†æ“æ§æ£€æµ‹
        psychological_layers = 0
        
        if current_psychology_analysis['cognitive_manipulation'] > 0.6:
            psychological_layers += 1
        if current_psychology_analysis['cultural_manipulation'] > 0.6:
            psychological_layers += 1
        if bias_exploitation_score > 0.4:
            psychological_layers += 1
        
        if psychological_layers >= 2:  # å¤šé‡å¿ƒç†æ“æ§åŒæ—¶è¿›è¡Œ
            advanced_manipulation_score += 0.4
        
        # 2. åå¿ƒç†å­¦æ“æ§æ£€æµ‹ï¼ˆæ•…æ„è¿åå¿ƒç†æœŸå¾…ï¼‰
        anti_psychology_indicators = 0
        
        # æ£€æµ‹æ˜¯å¦æ•…æ„é¿å…å¸¸è§å¿ƒç†åå¥½
        common_preferences = set([0, 1, 5, 6, 8, 9])  # å¸¸è§åå¥½æ•°å­—
        preference_avoidance = len(common_preferences - current_tails) / len(common_preferences)
        
        if preference_avoidance > 0.7:  # 70%çš„å¸¸è§åå¥½è¢«é¿å…
            anti_psychology_indicators += 1
            advanced_manipulation_score += 0.25
        
        # 3. å¿ƒç†å¤æ‚åº¦è¯„ä¼°
        complexity_factors = [
            len(complex_patterns_found),  # å¤æ‚æ¨¡å¼æ•°é‡
            psychological_layers,         # å¿ƒç†å±‚æ¬¡æ•°é‡
            len([bias for bias, value in behavioral_biases.items() if value > 0.3])  # æ˜¾è‘—åå·®æ•°é‡
        ]
        
        psychological_complexity = min(1.0, sum(complexity_factors) / 6.0)
        current_psychology_analysis['psychological_complexity'] = psychological_complexity
        
        if psychological_complexity > 0.7:
            advanced_manipulation_score += 0.2
        
        # === ç»¼åˆå¿ƒç†æ“æ§è¯„åˆ† ===
        total_psychology_manipulation = (
            current_psychology_analysis['cognitive_manipulation'] * 0.3 +
            current_psychology_analysis['cultural_manipulation'] * 0.25 +
            bias_exploitation_score * 0.25 +
            advanced_manipulation_score * 0.20
        )
        
        # åº”ç”¨å¿ƒç†å­¦ç‰¹æœ‰çš„éçº¿æ€§å˜æ¢
        # å¿ƒç†æ“æ§å¾€å¾€å…·æœ‰é˜ˆå€¼æ•ˆåº”
        if total_psychology_manipulation > 0.5:
            # è¶…è¿‡é˜ˆå€¼åå¿«é€Ÿä¸Šå‡
            enhanced_score = 0.5 + (total_psychology_manipulation - 0.5) * 2.0
        else:
            # é˜ˆå€¼ä»¥ä¸‹ç¼“æ…¢ä¸Šå‡
            enhanced_score = total_psychology_manipulation * 0.8
        
        final_score = min(0.95, max(0.05, enhanced_score))
        
        return final_score

    def _detect_sequence_anomalies(self, period_data: Dict[str, Any], historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç§‘ç ”çº§åºåˆ—å¼‚å¸¸æ£€æµ‹ç®—æ³•
        åŸºäºä¿¡æ¯è®ºã€åºåˆ—åˆ†æå’ŒåŠ¨æ€ç³»ç»Ÿç†è®ºçš„å¼‚å¸¸æ£€æµ‹
        """
        if len(historical_context) < 15:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        # === åºåˆ—ç†µåˆ†æ ===
        # æ„å»ºå°¾æ•°åºåˆ—
        tail_sequence = []
        for period in historical_context:
            period_tails = sorted(period.get('tails', []))
            tail_sequence.extend(period_tails)
        
        # è®¡ç®—åºåˆ—ç†µ
        sequence_entropy = 0.0
        if len(tail_sequence) > 0:
            tail_counts = defaultdict(int)
            for tail in tail_sequence:
                tail_counts[tail] += 1
            
            total_count = len(tail_sequence)
            for count in tail_counts.values():
                if count > 0:
                    probability = count / total_count
                    sequence_entropy -= probability * math.log2(probability)
        
        # ç†è®ºæœ€å¤§ç†µï¼ˆå®Œå…¨éšæœºï¼‰
        max_possible_entropy = math.log2(10)  # 10ä¸ªå°¾æ•°çš„æœ€å¤§ç†µ
        entropy_ratio = sequence_entropy / max_possible_entropy if max_possible_entropy > 0 else 0
        
        # === é©¬å°”å¯å¤«é“¾åºåˆ—åˆ†æ ===
        markov_anomaly_score = 0.0
        
        # æ„å»ºè½¬ç§»æ¦‚ç‡çŸ©é˜µ
        transition_matrix = defaultdict(lambda: defaultdict(int))
        for i in range(len(historical_context) - 1):
            current_period_tails = set(historical_context[i].get('tails', []))
            next_period_tails = set(historical_context[i + 1].get('tails', []))
            
            # è®°å½•å°¾æ•°çš„å‡ºç°/æ¶ˆå¤±è½¬ç§»
            for tail in range(10):
                current_state = tail in current_period_tails
                next_state = tail in next_period_tails
                transition_matrix[current_state][next_state] += 1
        
        # è®¡ç®—è½¬ç§»å¼‚å¸¸åº¦
        expected_transitions = len(historical_context) - 1
        for current_state in [True, False]:
            for next_state in [True, False]:
                observed = transition_matrix[current_state][next_state]
                expected = expected_transitions * 0.25  # ç†è®ºæœŸæœ›
                if expected > 0:
                    chi_square_component = ((observed - expected) ** 2) / expected
                    markov_anomaly_score += chi_square_component
        
        markov_anomaly_score = min(1.0, markov_anomaly_score / 20.0)  # å½’ä¸€åŒ–
        
        # === é•¿ç¨‹ç›¸å…³æ€§æ£€æµ‹ ===
        long_range_correlation = 0.0
        
        # åˆ†æå°¾æ•°å‡ºç°çš„é•¿ç¨‹ä¾èµ–æ€§
        for tail in range(10):
            appearance_sequence = []
            for period in historical_context:
                appearance_sequence.append(1 if tail in period.get('tails', []) else 0)
            
            if len(appearance_sequence) >= 10:
                # è®¡ç®—è‡ªç›¸å…³å‡½æ•°
                autocorrelations = []
                for lag in range(1, min(8, len(appearance_sequence) // 2)):
                    if len(appearance_sequence) > lag:
                        correlation = np.corrcoef(
                            appearance_sequence[:-lag], 
                            appearance_sequence[lag:]
                        )[0, 1]
                        if not np.isnan(correlation):
                            autocorrelations.append(abs(correlation))
                
                if autocorrelations:
                    max_correlation = max(autocorrelations)
                    if max_correlation > 0.4:  # å¼ºé•¿ç¨‹ç›¸å…³æ€§
                        long_range_correlation += max_correlation * 0.2
        
        long_range_correlation = min(1.0, long_range_correlation)
        
        # === å½“å‰æœŸåºåˆ—ä½ç½®å¼‚å¸¸æ£€æµ‹ ===
        positional_anomaly_score = 0.0
        
        # åˆ†æå½“å‰æœŸåœ¨æ•´ä¸ªåºåˆ—ä¸­çš„ä½ç½®å¼‚å¸¸æ€§
        total_periods = len(historical_context)
        current_position = total_periods  # å½“å‰æœŸçš„ä½ç½®
        
        # å‘¨æœŸæ€§åˆ†æ
        for cycle_length in [7, 10, 14, 21]:  # ä¸åŒå‘¨æœŸé•¿åº¦
            if total_periods >= cycle_length * 2:
                cycle_position = current_position % cycle_length
                
                # ç»Ÿè®¡è¯¥å‘¨æœŸä½ç½®çš„å†å²æ¨¡å¼
                historical_patterns_at_position = []
                for check_pos in range(cycle_position, total_periods, cycle_length):
                    if check_pos < len(historical_context):
                        period_tails = set(historical_context[check_pos].get('tails', []))
                        historical_patterns_at_position.append(period_tails)
                
                if len(historical_patterns_at_position) >= 3:
                    # è®¡ç®—å½“å‰æœŸä¸å†å²åŒä½ç½®æœŸçš„ç›¸ä¼¼åº¦
                    similarity_scores = []
                    for hist_pattern in historical_patterns_at_position:
                        similarity = len(current_tails.intersection(hist_pattern)) / len(current_tails.union(hist_pattern))
                        similarity_scores.append(similarity)
                    
                    avg_similarity = np.mean(similarity_scores)
                    if avg_similarity < 0.2:  # ä¸å†å²åŒä½ç½®æ¨¡å¼å·®å¼‚å¾ˆå¤§
                        positional_anomaly_score += (0.2 - avg_similarity) * 2.0
        
        positional_anomaly_score = min(1.0, positional_anomaly_score)
        
        # === é¢‘è°±åˆ†æå¼‚å¸¸æ£€æµ‹ ===
        spectral_anomaly_score = 0.0
        
        # å¯¹æ¯ä¸ªå°¾æ•°çš„å‡ºç°åºåˆ—è¿›è¡Œé¢‘è°±åˆ†æ
        for tail in range(10):
            binary_sequence = []
            for period in historical_context:
                binary_sequence.append(1 if tail in period.get('tails', []) else 0)
            
            if len(binary_sequence) >= 16:  # éœ€è¦è¶³å¤Ÿé•¿åº¦è¿›è¡ŒFFT
                # ä½¿ç”¨å¿«é€Ÿå‚…é‡Œå¶å˜æ¢æ£€æµ‹å‘¨æœŸæ€§
                fft_result = np.fft.fft(binary_sequence)
                power_spectrum = np.abs(fft_result) ** 2
                
                # æ£€æµ‹æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„å‘¨æœŸæ€§å³°å€¼
                mean_power = np.mean(power_spectrum[1:len(power_spectrum)//2])  # æ’é™¤ç›´æµåˆ†é‡
                max_power = np.max(power_spectrum[1:len(power_spectrum)//2])
                
                if mean_power > 0:
                    power_ratio = max_power / mean_power
                    if power_ratio > 3.0:  # å­˜åœ¨å¼ºå‘¨æœŸæ€§
                        spectral_anomaly_score += min(0.2, (power_ratio - 3.0) / 10.0)
        
        spectral_anomaly_score = min(1.0, spectral_anomaly_score)
        
        # === å¤æ‚æ€§æµ‹åº¦å¼‚å¸¸æ£€æµ‹ ===
        complexity_anomaly_score = 0.0
        
        # Lempel-Zivå¤æ‚åº¦è®¡ç®—
        def lempel_ziv_complexity(sequence):
            if not sequence:
                return 0
            
            i, k, l = 0, 1, 1
            c, k_max = 1, 1
            n = len(sequence)
            
            while k + l <= n:
                if sequence[i + l - 1] == sequence[k + l - 1]:
                    l += 1
                else:
                    if l > k_max:
                        k_max = l
                    i = 0
                    k += 1
                    l = 1
                    c += 1
            
            if l > k_max:
                k_max = l
            
            return c
        
        # è®¡ç®—æ•´ä½“åºåˆ—å¤æ‚åº¦
        overall_sequence = []
        for period in historical_context:
            # å°†æ¯æœŸçš„å°¾æ•°ç»„åˆç¼–ç ä¸ºå•ä¸ªæ•°å­—
            period_code = sum(2**tail for tail in period.get('tails', []))
            overall_sequence.append(period_code)
        
        if len(overall_sequence) >= 10:
            observed_complexity = lempel_ziv_complexity(overall_sequence)
            expected_complexity = len(overall_sequence) * 0.7  # æœŸæœ›å¤æ‚åº¦
            
            complexity_deviation = abs(observed_complexity - expected_complexity) / expected_complexity
            if complexity_deviation > 0.3:  # å¤æ‚åº¦æ˜¾è‘—åç¦»æœŸæœ›
                complexity_anomaly_score += min(0.4, complexity_deviation)
        
        # === ç»¼åˆå¼‚å¸¸è¯„åˆ† ===
        anomaly_components = {
            'entropy_anomaly': (1.0 - entropy_ratio) * 0.25 if entropy_ratio < 0.8 else 0.0,
            'markov_anomaly': markov_anomaly_score * 0.25,
            'long_range_correlation': long_range_correlation * 0.20,
            'positional_anomaly': positional_anomaly_score * 0.15,
            'spectral_anomaly': spectral_anomaly_score * 0.10,
            'complexity_anomaly': complexity_anomaly_score * 0.05
        }
        
        total_anomaly_score = sum(anomaly_components.values())
        
        # åº”ç”¨éçº¿æ€§å˜æ¢
        enhanced_score = total_anomaly_score * (1 + total_anomaly_score * 0.5)
        final_score = min(1.0, max(0.0, enhanced_score))
        
        return {
            'score': final_score,
            'entropy_ratio': entropy_ratio,
            'markov_anomaly': markov_anomaly_score,
            'long_range_correlation': long_range_correlation,
            'positional_anomaly': positional_anomaly_score,
            'spectral_anomaly': spectral_anomaly_score,
            'complexity_anomaly': complexity_anomaly_score,
            'details': 'sequence_anomaly_analysis',
            'anomaly_components': anomaly_components
        }

    def _detect_correlation_breaks(self, period_data: Dict[str, Any], historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç§‘ç ”çº§ç›¸å…³æ€§æ–­è£‚æ£€æµ‹ç®—æ³•
        åŸºäºç½‘ç»œåˆ†æã€äº’ä¿¡æ¯ç†è®ºå’ŒåŠ¨æ€ç›¸å…³æ€§åˆ†æ
        """
        if len(historical_context) < 20:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        # === å°¾æ•°é—´ç›¸å…³æ€§ç½‘ç»œæ„å»º ===
        correlation_matrix = np.zeros((10, 10))
        
        # è®¡ç®—å†å²å°¾æ•°é—´çš„å…±ç°ç›¸å…³æ€§
        for i in range(10):
            for j in range(i, 10):
                if i == j:
                    correlation_matrix[i][j] = 1.0
                else:
                    co_occurrence_count = 0
                    total_periods = len(historical_context)
                    
                    for period in historical_context:
                        period_tails = set(period.get('tails', []))
                        if i in period_tails and j in period_tails:
                            co_occurrence_count += 1
                    
                    # è®¡ç®—äº’ä¿¡æ¯
                    i_count = sum(1 for period in historical_context if i in period.get('tails', []))
                    j_count = sum(1 for period in historical_context if j in period.get('tails', []))
                    
                    if i_count > 0 and j_count > 0:
                        p_i = i_count / total_periods
                        p_j = j_count / total_periods
                        p_ij = co_occurrence_count / total_periods
                        
                        if p_ij > 0:
                            mutual_info = p_ij * math.log2(p_ij / (p_i * p_j))
                            correlation_matrix[i][j] = mutual_info
                            correlation_matrix[j][i] = mutual_info
        
        # === ç›¸å…³æ€§ç½‘ç»œæ‹“æ‰‘åˆ†æ ===
        # ç¡®å®šå¼ºç›¸å…³æ€§é˜ˆå€¼
        strong_correlation_threshold = np.percentile(correlation_matrix.flatten(), 75)
        
        # æ„å»ºç›¸å…³æ€§ç½‘ç»œ
        correlation_network = {}
        for i in range(10):
            correlation_network[i] = []
            for j in range(10):
                if i != j and correlation_matrix[i][j] > strong_correlation_threshold:
                    correlation_network[i].append(j)
        
        # === å½“å‰æœŸç›¸å…³æ€§æ–­è£‚æ£€æµ‹ ===
        correlation_break_score = 0.0
        broken_correlations = []
        
        for tail in current_tails:
            # æ£€æŸ¥ä¸è¯¥å°¾æ•°å¼ºç›¸å…³çš„å…¶ä»–å°¾æ•°
            strongly_correlated = correlation_network.get(tail, [])
            
            for correlated_tail in strongly_correlated:
                # å¦‚æœå¼ºç›¸å…³çš„å°¾æ•°æ²¡æœ‰åŒæ—¶å‡ºç°ï¼Œå¯èƒ½æ˜¯ç›¸å…³æ€§è¢«äººä¸ºæ‰“ç ´
                if correlated_tail not in current_tails:
                    correlation_strength = correlation_matrix[tail][correlated_tail]
                    broken_correlations.append({
                        'tail_pair': (tail, correlated_tail),
                        'correlation_strength': correlation_strength,
                        'break_severity': correlation_strength
                    })
                    correlation_break_score += correlation_strength * 0.3
        
        # === åç›¸å…³æ€§åˆ†æ ===
        anti_correlation_score = 0.0
        
        # å¯»æ‰¾å†å²ä¸Šçš„åç›¸å…³æ€§ï¼ˆä¸€ä¸ªå‡ºç°æ—¶å¦ä¸€ä¸ªå¾ˆå°‘å‡ºç°ï¼‰
        anti_correlations = []
        for i in range(10):
            for j in range(i + 1, 10):
                # è®¡ç®—åç›¸å…³æ€§ï¼šä¸€ä¸ªå‡ºç°æ—¶å¦ä¸€ä¸ªä¸å‡ºç°çš„å€¾å‘
                mutual_exclusion_count = 0
                i_alone_count = 0
                j_alone_count = 0
                
                for period in historical_context:
                    period_tails = set(period.get('tails', []))
                    i_present = i in period_tails
                    j_present = j in period_tails
                    
                    if i_present and not j_present:
                        i_alone_count += 1
                    elif j_present and not i_present:
                        j_alone_count += 1
                    elif not i_present and not j_present:
                        mutual_exclusion_count += 1
                
                # è®¡ç®—åç›¸å…³å¼ºåº¦
                total_periods = len(historical_context)
                anti_correlation_strength = (i_alone_count + j_alone_count) / total_periods
                
                if anti_correlation_strength > 0.6:  # å¼ºåç›¸å…³æ€§
                    anti_correlations.append({
                        'tail_pair': (i, j),
                        'anti_correlation_strength': anti_correlation_strength
                    })
                    
                    # æ£€æŸ¥å½“å‰æœŸæ˜¯å¦è¿åäº†åç›¸å…³æ€§
                    if i in current_tails and j in current_tails:
                        anti_correlation_score += anti_correlation_strength * 0.4
        
        # === åŠ¨æ€ç›¸å…³æ€§å˜åŒ–æ£€æµ‹ ===
        dynamic_correlation_change = 0.0
        
        # åˆ†ææœ€è¿‘å‡ æœŸçš„ç›¸å…³æ€§å˜åŒ–
        if len(historical_context) >= 10:
            recent_periods = historical_context[-5:]
            earlier_periods = historical_context[-10:-5]
            
            # è®¡ç®—æœ€è¿‘æœŸé—´å’Œè¾ƒæ—©æœŸé—´çš„ç›¸å…³æ€§çŸ©é˜µ
            recent_correlation = self._calculate_period_correlation_matrix(recent_periods)
            earlier_correlation = self._calculate_period_correlation_matrix(earlier_periods)
            
            # è®¡ç®—ç›¸å…³æ€§å˜åŒ–
            correlation_change_matrix = np.abs(recent_correlation - earlier_correlation)
            max_change = np.max(correlation_change_matrix)
            avg_change = np.mean(correlation_change_matrix)
            
            if max_change > 0.3:  # ç›¸å…³æ€§å‘ç”Ÿæ˜¾è‘—å˜åŒ–
                dynamic_correlation_change += max_change * 0.5
            
            if avg_change > 0.15:  # æ•´ä½“ç›¸å…³æ€§æ¨¡å¼æ”¹å˜
                dynamic_correlation_change += avg_change * 0.3
        
        dynamic_correlation_change = min(1.0, dynamic_correlation_change)
        
        # === ç»“æ„æ€§ç›¸å…³æ€§æ–­è£‚æ£€æµ‹ ===
        structural_break_score = 0.0
        
        # æ£€æµ‹æ˜¯å¦å­˜åœ¨ç³»ç»Ÿæ€§çš„ç›¸å…³æ€§é‡ç»„
        current_correlation_vector = []
        expected_correlation_vector = []
        
        for tail in current_tails:
            # å½“å‰æœŸè¯¥å°¾æ•°çš„ç›¸å…³æ€§è¡¨ç°
            current_neighbors = len([t for t in current_tails if t != tail and correlation_matrix[tail][t] > strong_correlation_threshold])
            expected_neighbors = len(correlation_network.get(tail, []))
            
            current_correlation_vector.append(current_neighbors)
            expected_correlation_vector.append(expected_neighbors)
        
        if len(current_correlation_vector) > 0:
            correlation_deviation = np.mean(np.abs(np.array(current_correlation_vector) - np.array(expected_correlation_vector)))
            structural_break_score = min(1.0, correlation_deviation / 3.0)
        
        # === ä¿¡æ¯è®ºç›¸å…³æ€§åˆ†æ ===
        information_theory_score = 0.0
        
        # è®¡ç®—å½“å‰æœŸçš„ä¿¡æ¯ç†µä¸å†å²ç›¸å…³æ€§é¢„æœŸçš„åå·®
        if len(current_tails) > 1:
            current_tails_list = list(current_tails)
            
            # åŸºäºå†å²ç›¸å…³æ€§é¢„æµ‹å½“å‰æœŸçš„ä¿¡æ¯ç»“æ„
            predicted_info_content = 0.0
            actual_info_content = math.log2(len(current_tails))
            
            for tail in current_tails_list:
                # è®¡ç®—è¯¥å°¾æ•°å¸¦æ¥çš„é¢„æœŸä¿¡æ¯é‡
                strongly_correlated_count = len(correlation_network.get(tail, []))
                if strongly_correlated_count > 0:
                    # å¦‚æœæœ‰å¼ºç›¸å…³æ€§ï¼Œä¿¡æ¯é‡ä¼šé™ä½
                    predicted_info_reduction = strongly_correlated_count * 0.1
                    predicted_info_content += max(0.1, 1.0 - predicted_info_reduction)
                else:
                    predicted_info_content += 1.0
            
            predicted_info_content = math.log2(max(1, predicted_info_content))
            
            # è®¡ç®—ä¿¡æ¯è®ºåå·®
            info_deviation = abs(actual_info_content - predicted_info_content)
            information_theory_score = min(1.0, info_deviation / 2.0)
        
        # === ç»¼åˆç›¸å…³æ€§æ–­è£‚è¯„åˆ† ===
        break_components = {
            'correlation_breaks': min(1.0, correlation_break_score) * 0.30,
            'anti_correlation_violations': min(1.0, anti_correlation_score) * 0.25,
            'dynamic_changes': dynamic_correlation_change * 0.20,
            'structural_breaks': structural_break_score * 0.15,
            'information_theory_deviation': information_theory_score * 0.10
        }
        
        total_break_score = sum(break_components.values())
        
        # åº”ç”¨ç›¸å…³æ€§æ–­è£‚ç‰¹æœ‰çš„éçº¿æ€§å˜æ¢
        if total_break_score > 0.6:
            enhanced_score = 0.6 + (total_break_score - 0.6) * 2.0
        else:
            enhanced_score = total_break_score
        
        final_score = min(1.0, max(0.0, enhanced_score))
        
        return {
            'score': final_score,
            'broken_correlations': len(broken_correlations),
            'anti_correlation_violations': len([ac for ac in anti_correlations if ac['tail_pair'][0] in current_tails and ac['tail_pair'][1] in current_tails]),
            'dynamic_correlation_change': dynamic_correlation_change,
            'structural_break_score': structural_break_score,
            'information_theory_score': information_theory_score,
            'details': 'correlation_break_analysis',
            'break_components': break_components
        }

    def _calculate_period_correlation_matrix(self, periods: List[Dict]) -> np.ndarray:
        """è®¡ç®—ç‰¹å®šæœŸé—´çš„ç›¸å…³æ€§çŸ©é˜µ"""
        correlation_matrix = np.zeros((10, 10))
        
        for i in range(10):
            for j in range(i, 10):
                if i == j:
                    correlation_matrix[i][j] = 1.0
                else:
                    co_occurrence_count = 0
                    total_periods = len(periods)
                    
                    for period in periods:
                        period_tails = set(period.get('tails', []))
                        if i in period_tails and j in period_tails:
                            co_occurrence_count += 1
                    
                    if total_periods > 0:
                        correlation = co_occurrence_count / total_periods
                        correlation_matrix[i][j] = correlation
                        correlation_matrix[j][i] = correlation
        
        return correlation_matrix

    def _analyze_entropy_deviation(self, period_data: Dict[str, Any], historical_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç§‘ç ”çº§ç†µåå·®åˆ†æç®—æ³•
        åŸºäºä¿¡æ¯è®ºã€çƒ­åŠ›å­¦ç†µå’Œé‡å­ä¿¡æ¯ç†è®ºçš„å¤šç»´åº¦ç†µåˆ†æ
        """
        if len(historical_context) < 12:
            return {'score': 0.0, 'details': 'insufficient_data'}
        
        current_tails = set(period_data.get('tails', []))
        
        # === é¦™å†œä¿¡æ¯ç†µåˆ†æ ===
        def calculate_shannon_entropy(data_set):
            if not data_set:
                return 0.0
            
            counts = defaultdict(int)
            for item in data_set:
                counts[item] += 1
            
            total = len(data_set)
            entropy = 0.0
            
            for count in counts.values():
                if count > 0:
                    probability = count / total
                    entropy -= probability * math.log2(probability)
            
            return entropy
        
        # è®¡ç®—å†å²ç†µåŸºçº¿
        historical_entropy_values = []
        
        for i in range(len(historical_context)):
            # è®¡ç®—æ¯æœŸçš„å±€éƒ¨ç†µ
            period_tails = historical_context[i].get('tails', [])
            if len(period_tails) > 1:
                period_entropy = calculate_shannon_entropy(period_tails)
                historical_entropy_values.append(period_entropy)
        
        if not historical_entropy_values:
            return {'score': 0.0, 'details': 'no_entropy_data'}
        
        # è®¡ç®—å½“å‰æœŸç†µ
        current_entropy = calculate_shannon_entropy(list(current_tails)) if current_tails else 0.0
        
        # å†å²ç†µç»Ÿè®¡
        mean_historical_entropy = np.mean(historical_entropy_values)
        std_historical_entropy = np.std(historical_entropy_values) if len(historical_entropy_values) > 1 else 0.1
        
        # ç†µåå·®Z-score
        entropy_z_score = abs(current_entropy - mean_historical_entropy) / std_historical_entropy if std_historical_entropy > 0 else 0
        
        # === æ¡ä»¶ç†µåˆ†æ ===
        conditional_entropy_deviation = 0.0
        
        # è®¡ç®—ç»™å®šå‰ä¸€æœŸçš„æ¡ä»¶ç†µ
        if len(historical_context) >= 2:
            conditional_entropies = []
            
            for i in range(1, len(historical_context)):
                prev_tails = set(historical_context[i-1].get('tails', []))
                curr_tails = set(historical_context[i].get('tails', []))
                
                # è®¡ç®—æ¡ä»¶ç†µ H(Y|X)
                # ç®€åŒ–è®¡ç®—ï¼šåŸºäºå‰æœŸçŠ¶æ€é¢„æµ‹å½“å‰æœŸçš„ä¿¡æ¯é‡
                if prev_tails:
                    intersection = len(prev_tails.intersection(curr_tails))
                    union = len(prev_tails.union(curr_tails))
                    
                    if union > 0:
                        conditional_info = -math.log2((intersection + 1) / (union + 1))
                        conditional_entropies.append(conditional_info)
            
            if conditional_entropies:
                mean_conditional_entropy = np.mean(conditional_entropies)
                
                # è®¡ç®—å½“å‰æœŸçš„æ¡ä»¶ç†µ
                if len(historical_context) > 0:
                    prev_tails = set(historical_context[-1].get('tails', []))
                    if prev_tails:
                        intersection = len(prev_tails.intersection(current_tails))
                        union = len(prev_tails.union(current_tails))
                        
                        if union > 0:
                            current_conditional_entropy = -math.log2((intersection + 1) / (union + 1))
                            conditional_entropy_deviation = abs(current_conditional_entropy - mean_conditional_entropy)
        
        # === ç›¸å¯¹ç†µï¼ˆKLæ•£åº¦ï¼‰åˆ†æ ===
        kl_divergence_score = 0.0
        
        # æ„å»ºå†å²æ¦‚ç‡åˆ†å¸ƒ
        historical_tail_counts = defaultdict(int)
        total_historical_occurrences = 0
        
        for period in historical_context:
            for tail in period.get('tails', []):
                historical_tail_counts[tail] += 1
                total_historical_occurrences += 1
        
        # å†å²æ¦‚ç‡åˆ†å¸ƒ
        historical_probabilities = {}
        for tail in range(10):
            count = historical_tail_counts.get(tail, 0)
            historical_probabilities[tail] = (count + 1) / (total_historical_occurrences + 10)  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
        
        # å½“å‰æœŸæ¦‚ç‡åˆ†å¸ƒ
        current_probabilities = {}
        current_total = len(current_tails)
        for tail in range(10):
            if tail in current_tails:
                current_probabilities[tail] = 1.0 / current_total if current_total > 0 else 0.1
            else:
                current_probabilities[tail] = 1e-10  # é¿å…log(0)
        
        # è®¡ç®—KLæ•£åº¦
        for tail in range(10):
            p = current_probabilities[tail]
            q = historical_probabilities[tail]
            if p > 0 and q > 0:
                kl_divergence_score += p * math.log2(p / q)
        
        # === äº’ä¿¡æ¯åˆ†æ ===
        mutual_information_deviation = 0.0
        
        # è®¡ç®—å°¾æ•°é—´çš„äº’ä¿¡æ¯
        if len(current_tails) >= 2:
            current_tails_list = list(current_tails)
            
            # å†å²äº’ä¿¡æ¯åŸºçº¿
            historical_mutual_info = 0.0
            pair_count = 0
            
            for i in range(len(current_tails_list)):
                for j in range(i + 1, len(current_tails_list)):
                    tail_i, tail_j = current_tails_list[i], current_tails_list[j]
                    
                    # è®¡ç®—å†å²ä¸Šè¿™ä¸¤ä¸ªå°¾æ•°çš„äº’ä¿¡æ¯
                    joint_count = 0
                    tail_i_count = 0
                    tail_j_count = 0
                    
                    for period in historical_context:
                        period_tails = set(period.get('tails', []))
                        if tail_i in period_tails and tail_j in period_tails:
                            joint_count += 1
                        if tail_i in period_tails:
                            tail_i_count += 1
                        if tail_j in period_tails:
                            tail_j_count += 1
                    
                    total_periods = len(historical_context)
                    if total_periods > 0:
                        p_i = tail_i_count / total_periods
                        p_j = tail_j_count / total_periods
                        p_ij = joint_count / total_periods
                        
                        if p_i > 0 and p_j > 0 and p_ij > 0:
                            mutual_info = p_ij * math.log2(p_ij / (p_i * p_j))
                            historical_mutual_info += mutual_info
                            pair_count += 1
            
            if pair_count > 0:
                avg_historical_mutual_info = historical_mutual_info / pair_count
                
                # å½“å‰æœŸçš„ç†è®ºäº’ä¿¡æ¯ï¼ˆå‡è®¾ç‹¬ç«‹ï¼‰
                theoretical_mutual_info = 0.0  # ç‹¬ç«‹æƒ…å†µä¸‹äº’ä¿¡æ¯ä¸º0
                
                # åå·®è®¡ç®—
                mutual_information_deviation = abs(avg_historical_mutual_info - theoretical_mutual_info)
        
        # === çƒ­åŠ›å­¦ç†µç±»æ¯”åˆ†æ ===
        thermodynamic_entropy_score = 0.0
        
        # å°†å°¾æ•°åˆ†å¸ƒç±»æ¯”ä¸ºç²’å­åˆ†å¸ƒï¼Œè®¡ç®—"æ¸©åº¦"å’Œ"ç†µ"
        if current_tails:
            # è®¡ç®—"èƒ½çº§åˆ†å¸ƒ"ï¼ˆåŸºäºå°¾æ•°å€¼ï¼‰
            energy_levels = {}
            for tail in current_tails:
                energy_levels[tail] = tail  # å°¾æ•°å€¼ä½œä¸ºèƒ½çº§
            
            # è®¡ç®—"é…åˆ†å‡½æ•°"å’Œ"æ¸©åº¦"
            if len(energy_levels) > 1:
                energies = list(energy_levels.values())
                mean_energy = np.mean(energies)
                energy_variance = np.var(energies)
                
                # "æ¸©åº¦"çš„ç±»æ¯”è®¡ç®—
                if energy_variance > 0:
                    effective_temperature = energy_variance / mean_energy if mean_energy > 0 else 1.0
                    
                    # è®¡ç®—çƒ­åŠ›å­¦ç†µ
                    thermodynamic_entropy = math.log(len(current_tails)) + mean_energy / effective_temperature
                    
                    # ä¸å†å²"æ¸©åº¦"å¯¹æ¯”
                    historical_temperatures = []
                    for period in historical_context:
                        period_tails = period.get('tails', [])
                        if len(period_tails) > 1:
                            period_energies = period_tails
                            period_mean_energy = np.mean(period_energies)
                            period_energy_variance = np.var(period_energies)
                            
                            if period_energy_variance > 0 and period_mean_energy > 0:
                                period_temp = period_energy_variance / period_mean_energy
                                historical_temperatures.append(period_temp)
                    
                    if historical_temperatures:
                        mean_historical_temp = np.mean(historical_temperatures)
                        temp_deviation = abs(effective_temperature - mean_historical_temp) / mean_historical_temp
                        thermodynamic_entropy_score = min(1.0, temp_deviation)
        
        # === é‡å­ä¿¡æ¯è®ºåˆ†æ ===
        quantum_entropy_score = 0.0
        
        # å°†å°¾æ•°åˆ†å¸ƒç±»æ¯”ä¸ºé‡å­æ€ï¼Œè®¡ç®—von Neumannç†µ
        if current_tails:
            # æ„å»º"å¯†åº¦çŸ©é˜µ"
            n = len(current_tails)
            density_matrix = np.zeros((n, n))
            
            # ç®€åŒ–çš„å¯†åº¦çŸ©é˜µï¼šå¯¹è§’å…ƒç´ ä¸ºæ¦‚ç‡
            for i in range(n):
                density_matrix[i][i] = 1.0 / n
            
            # è®¡ç®—von Neumannç†µ
            eigenvalues = np.linalg.eigvals(density_matrix)
            von_neumann_entropy = 0.0
            
            for eigenval in eigenvalues:
                if eigenval > 1e-10:
                    von_neumann_entropy -= eigenval * math.log2(eigenval)
            
            # ä¸æœ€å¤§æ··åˆæ€å¯¹æ¯”
            max_von_neumann = math.log2(n)
            if max_von_neumann > 0:
                quantum_purity = von_neumann_entropy / max_von_neumann
                
                # è®¡ç®—ä¸å†å²é‡å­ç†µçš„åå·®
                historical_quantum_entropies = []
                for period in historical_context:
                    period_tails = period.get('tails', [])
                    if len(period_tails) > 1:
                        period_n = len(period_tails)
                        period_max_entropy = math.log2(period_n)
                        historical_quantum_entropies.append(period_max_entropy)
                
                if historical_quantum_entropies:
                    mean_historical_quantum = np.mean(historical_quantum_entropies)
                    quantum_deviation = abs(von_neumann_entropy - mean_historical_quantum)
                    quantum_entropy_score = min(1.0, quantum_deviation / 2.0)
        
        # === ç»¼åˆç†µåå·®è¯„åˆ† ===
        entropy_components = {
            'shannon_entropy_deviation': min(1.0, entropy_z_score / 3.0) * 0.25,
            'conditional_entropy_deviation': min(1.0, conditional_entropy_deviation) * 0.20,
            'kl_divergence': min(1.0, kl_divergence_score / 2.0) * 0.20,
            'mutual_information_deviation': min(1.0, mutual_information_deviation * 5.0) * 0.15,
            'thermodynamic_entropy': thermodynamic_entropy_score * 0.10,
            'quantum_entropy': quantum_entropy_score * 0.10
        }
        
        total_entropy_deviation = sum(entropy_components.values())
        
        # åº”ç”¨ä¿¡æ¯è®ºç‰¹æœ‰çš„éçº¿æ€§å˜æ¢
        if total_entropy_deviation > 0.7:
            # é«˜ç†µåå·®åŒºé—´ï¼šæŒ‡æ•°å¢é•¿
            enhanced_score = 0.7 + (total_entropy_deviation - 0.7) * 3.0
        elif total_entropy_deviation < 0.3:
            # ä½ç†µåå·®åŒºé—´ï¼šæŠ‘åˆ¶å¢é•¿
            enhanced_score = total_entropy_deviation * 0.5
        else:
            # ä¸­ç­‰åŒºé—´ï¼šçº¿æ€§å¢é•¿
            enhanced_score = total_entropy_deviation
        
        final_score = min(1.0, max(0.0, enhanced_score))
        
        return {
            'score': final_score,
            'shannon_entropy': current_entropy,
            'historical_mean_entropy': mean_historical_entropy,
            'entropy_z_score': entropy_z_score,
            'conditional_entropy_deviation': conditional_entropy_deviation,
            'kl_divergence': kl_divergence_score,
            'mutual_information_deviation': mutual_information_deviation,
            'thermodynamic_entropy_score': thermodynamic_entropy_score,
            'quantum_entropy_score': quantum_entropy_score,
            'details': 'entropy_deviation_analysis',
            'entropy_components': entropy_components
        }
    
    def _synthesize_analysis_results(self, period_data: Dict[str, Any], detection_results: Dict[str, Any], 
                                   statistical_anomalies: Dict[str, Any], pattern_matches: Dict[str, Any], 
                                   manipulation_intensity: Any, psychological_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç§‘ç ”çº§åˆ†æç»“æœç»¼åˆå™¨
        åŸºäºå¤šæºè¯æ®èåˆã€è´å¶æ–¯æ¨ç†å’Œæ¨¡ç³Šé€»è¾‘çš„ç»¼åˆè¯„ä¼°
        """
        
        # === è¯æ®æƒé‡è®¡ç®— ===
        evidence_weights = {
            'detection_results': 0.35,
            'statistical_anomalies': 0.25, 
            'pattern_matches': 0.20,
            'psychological_state': 0.15,
            'manipulation_intensity': 0.05
        }
        
        # === è·å–æ£€æµ‹ç»“æœåˆ†æ•° ===
        combined_detection_score = detection_results.get('combined_score', 0.0)
        max_detection_score = detection_results.get('max_score', 0.0)
        detection_consensus = detection_results.get('detection_consensus', 0.0)
        
        # === ç»Ÿè®¡å¼‚å¸¸è¯„åˆ† ===
        statistical_score = 0.0
        if isinstance(statistical_anomalies, dict):
            stat_scores = [
                statistical_anomalies.get('chi_square_test', 0.0),
                statistical_anomalies.get('ks_test', 0.0),
                statistical_anomalies.get('entropy_test', 0.0)
            ]
            statistical_score = np.mean([s for s in stat_scores if s > 0]) if stat_scores else 0.0
        
        # === æ¨¡å¼åŒ¹é…è¯„åˆ† ===
        pattern_score = 0.0
        if isinstance(pattern_matches, dict):
            matched_patterns = pattern_matches.get('matched_patterns', [])
            similarity_scores = pattern_matches.get('similarity_scores', [])
            if similarity_scores:
                pattern_score = np.mean(similarity_scores)
            elif matched_patterns:
                pattern_score = min(1.0, len(matched_patterns) / 5.0)
        
        # === å¿ƒç†çŠ¶æ€è¯„åˆ† ===
        psychological_score = 0.0
        if isinstance(psychological_state, dict):
            psych_factors = [
                psychological_state.get('stress_level', 0.5),
                psychological_state.get('aggressiveness', 0.5),
                1.0 - psychological_state.get('risk_tolerance', 0.5)  # ä½é£é™©å®¹å¿åº¦=é«˜æ“æ§å¯èƒ½
            ]
            psychological_score = np.mean(psych_factors)
        
        # === æ“æ§å¼ºåº¦è¯„åˆ† ===
        intensity_score = 0.0
        if hasattr(manipulation_intensity, 'value'):
            intensity_mapping = {
                0: 0.0,   # NATURAL
                1: 0.2,   # SUBTLE  
                2: 0.5,   # MODERATE
                3: 0.8,   # STRONG
                4: 1.0    # EXTREME
            }
            intensity_score = intensity_mapping.get(manipulation_intensity.value, 0.0)
        
        # === è´å¶æ–¯è¯æ®èåˆ ===
        # ä½¿ç”¨è´å¶æ–¯æ–¹æ³•èåˆå¤šæºè¯æ®
        prior_manipulation_prob = 0.3  # å…ˆéªŒæ“æ§æ¦‚ç‡
        
        # è®¡ç®—å„é¡¹è¯æ®çš„ä¼¼ç„¶æ¯”
        likelihood_ratios = []
        
        # æ£€æµ‹ç»“æœä¼¼ç„¶æ¯”
        if combined_detection_score > 0.7:
            likelihood_ratios.append(4.0)  # å¼ºè¯æ®æ”¯æŒæ“æ§
        elif combined_detection_score > 0.5:
            likelihood_ratios.append(2.0)  # ä¸­ç­‰è¯æ®
        elif combined_detection_score > 0.3:
            likelihood_ratios.append(1.2)  # å¼±è¯æ®
        else:
            likelihood_ratios.append(0.8)  # è¯æ®åå¯¹æ“æ§
        
        # ç»Ÿè®¡å¼‚å¸¸ä¼¼ç„¶æ¯”
        if statistical_score > 0.6:
            likelihood_ratios.append(3.0)
        elif statistical_score > 0.4:
            likelihood_ratios.append(1.5)
        else:
            likelihood_ratios.append(0.9)
        
        # æ¨¡å¼åŒ¹é…ä¼¼ç„¶æ¯”
        if pattern_score > 0.7:
            likelihood_ratios.append(2.5)
        elif pattern_score > 0.4:
            likelihood_ratios.append(1.3)
        else:
            likelihood_ratios.append(0.95)
        
        # å¿ƒç†çŠ¶æ€ä¼¼ç„¶æ¯”
        if psychological_score > 0.8:
            likelihood_ratios.append(2.0)
        elif psychological_score > 0.6:
            likelihood_ratios.append(1.4)
        else:
            likelihood_ratios.append(1.0)
        
        # è´å¶æ–¯æ›´æ–°
        posterior_odds = (prior_manipulation_prob / (1 - prior_manipulation_prob))
        for lr in likelihood_ratios:
            posterior_odds *= lr
        
        bayesian_probability = posterior_odds / (1 + posterior_odds)
        
        # === æ¨¡ç³Šé€»è¾‘è¯„ä¼° ===
        # ä½¿ç”¨æ¨¡ç³Šé€»è¾‘å¤„ç†ä¸ç¡®å®šæ€§
        fuzzy_membership = {
            'definitely_natural': max(0, min(1, (0.2 - combined_detection_score) / 0.2)),
            'possibly_natural': max(0, min(1, (0.4 - combined_detection_score) / 0.2)),
            'uncertain': max(0, min(1, 1 - abs(combined_detection_score - 0.5) / 0.3)),
            'possibly_manipulated': max(0, min(1, (combined_detection_score - 0.6) / 0.2)),
            'definitely_manipulated': max(0, min(1, (combined_detection_score - 0.8) / 0.2))
        }
        
        # è®¡ç®—æ¨¡ç³Šç»¼åˆè¯„ä¼°
        fuzzy_weights = [0.1, 0.2, 0.3, 0.25, 0.15]  # å¯¹åº”ä¸Šè¿°5ä¸ªæ¨¡ç³Šé›†åˆ
        fuzzy_values = list(fuzzy_membership.values())
        fuzzy_score = sum(w * v * i for i, (w, v) in enumerate(zip(fuzzy_weights, fuzzy_values))) / 4.0
        
        # === ç½®ä¿¡åº¦è®¡ç®— ===
        confidence_factors = [
            detection_consensus,  # æ£€æµ‹å™¨ä¸€è‡´æ€§
            min(1.0, len([lr for lr in likelihood_ratios if lr > 1.5]) / len(likelihood_ratios)),  # è¯æ®å¼ºåº¦ä¸€è‡´æ€§
            1.0 - abs(bayesian_probability - combined_detection_score) / 1.0,  # æ–¹æ³•é—´ä¸€è‡´æ€§
            min(1.0, self.total_periods_analyzed / 50.0)  # æ ·æœ¬å……è¶³æ€§
        ]
        
        confidence = np.mean(confidence_factors)
        
        # === æœ€ç»ˆæ¦‚ç‡ç»¼åˆ ===
        # åŠ æƒå¹³å‡å¤šç§æ–¹æ³•çš„ç»“æœ
        method_weights = {
            'detection_score': 0.4,
            'bayesian_probability': 0.35,
            'fuzzy_score': 0.25
        }
        
        final_probability = (
            combined_detection_score * method_weights['detection_score'] +
            bayesian_probability * method_weights['bayesian_probability'] +
            fuzzy_score * method_weights['fuzzy_score']
        )
        
        # === æ“æ§ç±»å‹è¯†åˆ« ===
        manipulation_type = self._identify_manipulation_type(
            detection_results, final_probability, period_data
        )
        
        # === ç›®æ ‡å°¾æ•°è¯†åˆ« ===
        target_tails = self._identify_target_tails(
            period_data, detection_results, final_probability
        )
        
        # === è¯æ®åŒ…æ„å»º ===
        evidence_package = {
            'detection_breakdown': detection_results,
            'statistical_evidence': statistical_anomalies,
            'pattern_evidence': pattern_matches,
            'psychological_evidence': psychological_state,
            'bayesian_analysis': {
                'prior_probability': prior_manipulation_prob,
                'likelihood_ratios': likelihood_ratios,
                'posterior_probability': bayesian_probability
            },
            'fuzzy_analysis': fuzzy_membership,
            'confidence_factors': dict(zip(['consensus', 'evidence_strength', 'method_consistency', 'sample_size'], confidence_factors))
        }
        
        return {
            'manipulation_probability': final_probability,
            'manipulation_type': manipulation_type,
            'confidence': confidence,
            'target_tails': target_tails,
            'evidence': evidence_package,
            'method_scores': {
                'detection_score': combined_detection_score,
                'bayesian_probability': bayesian_probability,
                'fuzzy_score': fuzzy_score,
                'statistical_score': statistical_score,
                'pattern_score': pattern_score,
                'psychological_score': psychological_score
            }
        }

    def _identify_manipulation_type(self, detection_results: Dict[str, Any], 
                                   probability: float, period_data: Dict[str, Any]) -> str:
        """è¯†åˆ«æ“æ§ç±»å‹"""
        
        if probability < 0.3:
            return 'natural_variation'
        elif probability < 0.5:
            return 'subtle_influence'
        elif probability < 0.7:
            return 'moderate_manipulation'
        elif probability < 0.85:
            return 'strong_manipulation'
        else:
            return 'extreme_manipulation'
        
        # å¯ä»¥åŸºäºå…·ä½“çš„æ£€æµ‹ç»“æœè¿›ä¸€æ­¥ç»†åŒ–ç±»å‹
        # ä¾‹å¦‚ï¼šfrequency_manipulation, pattern_manipulation, psychological_manipulationç­‰

    def _identify_target_tails(self, period_data: Dict[str, Any], 
                              detection_results: Dict[str, Any], probability: float) -> List[int]:
        """è¯†åˆ«è¢«æ“æ§çš„ç›®æ ‡å°¾æ•°"""
        
        current_tails = period_data.get('tails', [])
        
        if probability < 0.5:
            return []  # ä½æ“æ§æ¦‚ç‡ï¼Œæ— æ˜ç¡®ç›®æ ‡
        
        # ç®€åŒ–å®ç°ï¼šå¦‚æœæ˜¯é«˜æ“æ§æ¦‚ç‡ï¼Œå½“å‰æœŸçš„æ‰€æœ‰å°¾æ•°éƒ½å¯èƒ½æ˜¯ç›®æ ‡
        if probability > 0.7:
            return current_tails
        else:
            # ä¸­ç­‰æ“æ§æ¦‚ç‡ï¼Œè¿”å›éƒ¨åˆ†å°¾æ•°
            return current_tails[:len(current_tails)//2] if current_tails else []

    def _update_learning_models(self, analysis_result: Dict[str, Any]):
        """æ›´æ–°å­¦ä¹ æ¨¡å‹"""
        
        # è®°å½•åˆ†æç»“æœåˆ°å†å²ä¿¡å·
        if hasattr(self, 'historical_signals'):
            self.historical_signals.append(analysis_result)
        
        # æ›´æ–°æ¨¡å‹ç½®ä¿¡åº¦
        manipulation_prob = analysis_result.get('manipulation_probability', 0.5)
        confidence = analysis_result.get('confidence', 0.5)
        
        # ç®€å•çš„è‡ªé€‚åº”å­¦ä¹ ï¼šæ ¹æ®ç»“æœè°ƒæ•´æ¨¡å‹å‚æ•°
        if manipulation_prob > 0.8 and confidence > 0.7:
            # é«˜ç½®ä¿¡åº¦çš„å¼ºæ“æ§ï¼šå¢å¼ºæ£€æµ‹æ•æ„Ÿæ€§
            if hasattr(self, 'config'):
                current_threshold = self.config.get('manipulation_threshold', 0.65)
                self.config['manipulation_threshold'] = min(0.8, current_threshold + 0.02)
        elif manipulation_prob < 0.2 and confidence > 0.7:
            # é«˜ç½®ä¿¡åº¦çš„è‡ªç„¶å˜åŒ–ï¼šé™ä½æ£€æµ‹æ•æ„Ÿæ€§
            if hasattr(self, 'config'):
                current_threshold = self.config.get('manipulation_threshold', 0.65)
                self.config['manipulation_threshold'] = max(0.5, current_threshold - 0.01)
        
        # æ›´æ–°é¢„æµ‹å‡†ç¡®æ€§ï¼ˆå¦‚æœæœ‰éªŒè¯æ•°æ®ï¼‰
        self.total_periods_analyzed += 1

    def _predict_based_on_patterns(self, current_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åŸºäºå†å²æ“æ§æ¨¡å¼çš„é¢„æµ‹ç®—æ³•
        åˆ©ç”¨æ¨¡å¼è¯†åˆ«å’Œåºåˆ—åˆ†æé¢„æµ‹ä¸‹æœŸæ“æ§è¡Œä¸º
        """
        if len(current_context) < 10:
            return {
                'manipulation_probability': 0.5,
                'predicted_type': 'insufficient_data',
                'confidence': 0.0,
                'reasoning': 'not_enough_historical_data'
            }
        
        # === å†å²æ“æ§æ¨¡å¼æŒ–æ˜ ===
        historical_patterns = []
        
        # åˆ†ææœ€è¿‘20æœŸçš„æ“æ§æ¨¡å¼
        analysis_window = min(20, len(current_context))
        for i in range(analysis_window):
            period_data = current_context[i]
            period_tails = set(period_data.get('tails', []))
            
            # ç®€å•çš„æ“æ§æŒ‡æ ‡è®¡ç®—
            manipulation_indicators = {
                'hot_number_concentration': 0.0,
                'cold_number_comeback': 0.0,
                'pattern_disruption': 0.0,
                'symmetry_level': 0.0
            }
            
            # è®¡ç®—çƒ­é—¨æ•°å­—é›†ä¸­åº¦
            if len(current_context) > i + 5:
                recent_5_periods = current_context[i:i+5]
                tail_counts = defaultdict(int)
                for period in recent_5_periods:
                    for tail in period.get('tails', []):
                        tail_counts[tail] += 1
                
                hot_tails = {tail for tail, count in tail_counts.items() if count >= 3}
                hot_in_current = len(period_tails.intersection(hot_tails))
                manipulation_indicators['hot_number_concentration'] = hot_in_current / max(len(period_tails), 1)
            
            # è®¡ç®—å†·é—¨å¤å‡ºç¨‹åº¦
            if len(current_context) > i + 10:
                cold_analysis_periods = current_context[i+1:i+11]
                cold_tails = set()
                for tail in range(10):
                    if not any(tail in p.get('tails', []) for p in cold_analysis_periods):
                        cold_tails.add(tail)
                
                cold_comebacks = len(period_tails.intersection(cold_tails))
                manipulation_indicators['cold_number_comeback'] = cold_comebacks / max(len(period_tails), 1)
            
            # è®¡ç®—å¯¹ç§°æ€§æ°´å¹³
            symmetry_pairs = [(0, 9), (1, 8), (2, 7), (3, 6), (4, 5)]
            symmetry_count = sum(1 for pair in symmetry_pairs if pair[0] in period_tails and pair[1] in period_tails)
            manipulation_indicators['symmetry_level'] = symmetry_count / len(symmetry_pairs)
            
            # ç»¼åˆæ“æ§å¯èƒ½æ€§è¯„åˆ†
            manipulation_score = (
                manipulation_indicators['hot_number_concentration'] * 0.3 +
                manipulation_indicators['cold_number_comeback'] * 0.3 +
                manipulation_indicators['symmetry_level'] * 0.4
            )
            
            historical_patterns.append({
                'period_index': i,
                'manipulation_score': manipulation_score,
                'indicators': manipulation_indicators,
                'tails': list(period_tails)
            })
        
        # === æ¨¡å¼åºåˆ—åˆ†æ ===
        manipulation_sequence = [p['manipulation_score'] for p in historical_patterns]
        
        # è®¡ç®—æ“æ§å¼ºåº¦çš„è¶‹åŠ¿
        if len(manipulation_sequence) >= 5:
            recent_trend = np.mean(manipulation_sequence[:5])  # æœ€è¿‘5æœŸå¹³å‡
            historical_avg = np.mean(manipulation_sequence[5:]) if len(manipulation_sequence) > 5 else recent_trend
            
            trend_direction = 'increasing' if recent_trend > historical_avg * 1.2 else 'decreasing' if recent_trend < historical_avg * 0.8 else 'stable'
        else:
            trend_direction = 'stable'
            recent_trend = np.mean(manipulation_sequence) if manipulation_sequence else 0.5
        
        # === å‘¨æœŸæ€§æ¨¡å¼æ£€æµ‹ ===
        cycle_prediction = 0.5
        if len(manipulation_sequence) >= 14:
            # æ£€æµ‹7æœŸå‘¨æœŸ
            week_cycle_scores = []
            for offset in range(7):
                cycle_positions = [manipulation_sequence[i] for i in range(offset, len(manipulation_sequence), 7)]
                if len(cycle_positions) >= 2:
                    cycle_variance = np.var(cycle_positions)
                    cycle_mean = np.mean(cycle_positions)
                    week_cycle_scores.append((cycle_mean, cycle_variance))
            
            if week_cycle_scores:
                # æ‰¾åˆ°æ–¹å·®æœ€å°çš„å‘¨æœŸä½ç½®ï¼ˆæœ€è§„å¾‹çš„ï¼‰
                min_variance_idx = min(range(len(week_cycle_scores)), key=lambda i: week_cycle_scores[i][1])
                current_position_in_cycle = len(current_context) % 7
                
                if current_position_in_cycle == min_variance_idx:
                    cycle_prediction = week_cycle_scores[min_variance_idx][0]
        
        # === æ“æ§ç±»å‹æ¨¡å¼è¯†åˆ« ===
        predicted_manipulation_type = 'balanced_manipulation'
        
        if len(historical_patterns) >= 3:
            recent_patterns = historical_patterns[:3]
            
            # åˆ†ææœ€è¿‘çš„ä¸»è¦æ“æ§æ‰‹æ³•
            avg_hot_concentration = np.mean([p['indicators']['hot_number_concentration'] for p in recent_patterns])
            avg_cold_comeback = np.mean([p['indicators']['cold_number_comeback'] for p in recent_patterns])
            avg_symmetry = np.mean([p['indicators']['symmetry_level'] for p in recent_patterns])
            
            dominant_factor = max([
                ('hot_concentration', avg_hot_concentration),
                ('cold_comeback', avg_cold_comeback), 
                ('symmetry_manipulation', avg_symmetry)
            ], key=lambda x: x[1])
            
            predicted_manipulation_type = dominant_factor[0] + '_focused'
        
        # === é¢„æµ‹æ¦‚ç‡è®¡ç®— ===
        # ç»“åˆè¶‹åŠ¿å’Œå‘¨æœŸæ€§é¢„æµ‹
        trend_weight = 0.6
        cycle_weight = 0.4
        
        if trend_direction == 'increasing':
            trend_prediction = min(0.9, recent_trend * 1.3)
        elif trend_direction == 'decreasing':
            trend_prediction = max(0.1, recent_trend * 0.7)
        else:
            trend_prediction = recent_trend
        
        final_manipulation_probability = (
            trend_prediction * trend_weight +
            cycle_prediction * cycle_weight
        )
        
        # === ç½®ä¿¡åº¦è¯„ä¼° ===
        confidence_factors = []
        
        # æ•°æ®å……è¶³æ€§
        data_sufficiency = min(1.0, len(current_context) / 20.0)
        confidence_factors.append(data_sufficiency)
        
        # æ¨¡å¼ä¸€è‡´æ€§
        if len(manipulation_sequence) >= 3:
            pattern_consistency = 1.0 - np.std(manipulation_sequence[:3]) / max(np.mean(manipulation_sequence[:3]), 0.1)
            confidence_factors.append(max(0.0, pattern_consistency))
        
        # è¶‹åŠ¿æ¸…æ™°åº¦
        if trend_direction != 'stable':
            trend_clarity = abs(recent_trend - historical_avg) / max(historical_avg, 0.1) if len(manipulation_sequence) > 5 else 0.5
            confidence_factors.append(min(1.0, trend_clarity))
        else:
            confidence_factors.append(0.3)
        
        overall_confidence = np.mean(confidence_factors) if confidence_factors else 0.5
        
        return {
            'manipulation_probability': final_manipulation_probability,
            'predicted_type': predicted_manipulation_type,
            'confidence': overall_confidence,
            'reasoning': f'Based on {trend_direction} trend analysis and cycle patterns',
            'trend_direction': trend_direction,
            'cycle_prediction': cycle_prediction,
            'pattern_analysis': {
                'historical_patterns_count': len(historical_patterns),
                'trend_strength': abs(recent_trend - historical_avg) if len(manipulation_sequence) > 5 else 0,
                'dominant_manipulation_type': predicted_manipulation_type
            }
        }

    def _predict_based_on_statistics(self, current_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åŸºäºç»Ÿè®¡æ¨¡å‹çš„é¢„æµ‹ç®—æ³•
        ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†æå’Œæ¦‚ç‡ç»Ÿè®¡æ–¹æ³•
        """
        if len(current_context) < 8:
            return {
                'manipulation_probability': 0.5,
                'predicted_type': 'insufficient_data',
                'confidence': 0.0,
                'reasoning': 'not_enough_data_for_statistics'
            }
        
        # === æ¦‚ç‡åˆ†å¸ƒåˆ†æ ===
        # åˆ†ææ¯ä¸ªå°¾æ•°çš„å‡ºç°æ¦‚ç‡åˆ†å¸ƒ
        tail_probabilities = defaultdict(list)
        
        for period in current_context:
            period_tails = set(period.get('tails', []))
            for tail in range(10):
                tail_probabilities[tail].append(1 if tail in period_tails else 0)
        
        # === ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ ===
        statistical_anomaly_score = 0.0
        
        # å¡æ–¹æ‹Ÿåˆä¼˜åº¦æ£€éªŒ
        expected_frequency = len(current_context) * 0.5  # æœŸæœ›æ¯ä¸ªå°¾æ•°å‡ºç°50%çš„æœŸæ•°
        chi_square_statistics = []
        
        for tail in range(10):
            observed_frequency = sum(tail_probabilities[tail])
            if expected_frequency > 0:
                chi_square_stat = ((observed_frequency - expected_frequency) ** 2) / expected_frequency
                chi_square_statistics.append(chi_square_stat)
        
        total_chi_square = sum(chi_square_statistics)
        # è‡ªç”±åº¦ä¸º9ï¼Œæ˜¾è‘—æ€§æ°´å¹³0.05çš„ä¸´ç•Œå€¼çº¦ä¸º16.919
        chi_square_threshold = 16.919
        
        if total_chi_square > chi_square_threshold:
            statistical_anomaly_score += 0.4
        
        # === æ—¶é—´åºåˆ—è‡ªç›¸å…³åˆ†æ ===
        autocorrelation_anomaly = 0.0
        
        # å¯¹æ¯ä¸ªå°¾æ•°çš„å‡ºç°åºåˆ—è®¡ç®—è‡ªç›¸å…³
        for tail in range(10):
            sequence = tail_probabilities[tail]
            if len(sequence) >= 6:
                # è®¡ç®—æ»å1æœŸçš„è‡ªç›¸å…³ç³»æ•°
                lag1_corr = np.corrcoef(sequence[:-1], sequence[1:])[0, 1] if len(sequence) > 1 else 0
                
                if not np.isnan(lag1_corr) and abs(lag1_corr) > 0.4:
                    autocorrelation_anomaly += abs(lag1_corr) * 0.1
        
        autocorrelation_anomaly = min(1.0, autocorrelation_anomaly)
        
        # === æ–¹å·®åˆ†æ ===
        variance_anomaly = 0.0
        
        # åˆ†ææœ€è¿‘å‡ æœŸä¸å†å²æœŸæ•°çš„æ–¹å·®å·®å¼‚
        if len(current_context) >= 10:
            recent_period_sizes = [len(period.get('tails', [])) for period in current_context[:5]]
            historical_period_sizes = [len(period.get('tails', [])) for period in current_context[5:]]
            
            if recent_period_sizes and historical_period_sizes:
                recent_variance = np.var(recent_period_sizes)
                historical_variance = np.var(historical_period_sizes)
                
                if historical_variance > 0:
                    variance_ratio = recent_variance / historical_variance
                    if variance_ratio > 2.0 or variance_ratio < 0.5:  # æ–¹å·®æ˜¾è‘—å˜åŒ–
                        variance_anomaly = min(1.0, abs(math.log(variance_ratio)) / 2.0)
        
        # === ç†µå˜åˆ†æ ===
        entropy_trend_score = 0.0
        
        # è®¡ç®—æ¯æœŸçš„ä¿¡æ¯ç†µå˜åŒ–è¶‹åŠ¿
        period_entropies = []
        for period in current_context:
            period_tails = period.get('tails', [])
            if len(period_tails) > 1:
                # è®¡ç®—è¯¥æœŸå†…å°¾æ•°åˆ†å¸ƒçš„ç†µ
                tail_counts = defaultdict(int)
                for tail in period_tails:
                    tail_counts[tail] += 1
                
                entropy = 0.0
                total_count = len(period_tails)
                for count in tail_counts.values():
                    if count > 0:
                        p = count / total_count
                        entropy -= p * math.log2(p)
                
                period_entropies.append(entropy)
        
        if len(period_entropies) >= 5:
            # åˆ†æç†µçš„è¶‹åŠ¿
            recent_entropy = np.mean(period_entropies[:3])
            historical_entropy = np.mean(period_entropies[3:])
            
            entropy_change_ratio = recent_entropy / historical_entropy if historical_entropy > 0 else 1.0
            if abs(entropy_change_ratio - 1.0) > 0.3:  # ç†µæ˜¾è‘—å˜åŒ–
                entropy_trend_score = min(1.0, abs(entropy_change_ratio - 1.0))
        
        # === è´å¶æ–¯å˜ç‚¹æ£€æµ‹ ===
        change_point_score = 0.0
        
        # ç®€åŒ–çš„å˜ç‚¹æ£€æµ‹ï¼šæ£€æµ‹ç»Ÿè®¡æ€§è´¨çš„çªå˜
        if len(current_context) >= 12:
            # å°†æ•°æ®åˆ†ä¸ºä¸¤æ®µï¼Œæ£€æµ‹å‡å€¼å·®å¼‚
            mid_point = len(current_context) // 2
            first_half = current_context[mid_point:]
            second_half = current_context[:mid_point]
            
            first_half_avg_size = np.mean([len(p.get('tails', [])) for p in first_half])
            second_half_avg_size = np.mean([len(p.get('tails', [])) for p in second_half])
            
            if abs(first_half_avg_size - second_half_avg_size) > 1.0:
                change_point_score = min(1.0, abs(first_half_avg_size - second_half_avg_size) / 3.0)
        
        # === ç»¼åˆç»Ÿè®¡é¢„æµ‹ ===
        statistical_components = {
            'chi_square_anomaly': statistical_anomaly_score * 0.3,
            'autocorrelation_anomaly': autocorrelation_anomaly * 0.25,
            'variance_anomaly': variance_anomaly * 0.2,
            'entropy_trend': entropy_trend_score * 0.15,
            'change_point': change_point_score * 0.1
        }
        
        total_statistical_score = sum(statistical_components.values())
        
        # === é¢„æµ‹ç±»å‹è¯†åˆ« ===
        if statistical_anomaly_score > 0.3:
            predicted_type = 'frequency_manipulation'
        elif autocorrelation_anomaly > 0.4:
            predicted_type = 'temporal_pattern_manipulation'
        elif variance_anomaly > 0.3:
            predicted_type = 'variance_manipulation'
        elif entropy_trend_score > 0.3:
            predicted_type = 'complexity_manipulation'
        else:
            predicted_type = 'statistical_normal'
        
        # === ç½®ä¿¡åº¦è¯„ä¼° ===
        confidence = min(1.0, total_statistical_score * 1.5) if total_statistical_score > 0.3 else max(0.1, total_statistical_score)
        
        return {
            'manipulation_probability': min(1.0, total_statistical_score),
            'predicted_type': predicted_type,
            'confidence': confidence,
            'reasoning': 'Based on statistical anomaly detection and time series analysis',
            'statistical_components': statistical_components,
            'test_results': {
                'chi_square_score': statistical_anomaly_score,
                'autocorrelation_strength': autocorrelation_anomaly,
                'variance_change': variance_anomaly,
                'entropy_trend': entropy_trend_score,
                'change_point_evidence': change_point_score
            }
        }

    def _predict_based_on_psychology(self, current_context: List[Dict[str, Any]], current_state: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŸºäºå¿ƒç†æ¨¡å‹çš„é¢„æµ‹ç®—æ³•
        ç»“åˆåº„å®¶å¿ƒç†çŠ¶æ€å’Œç©å®¶è¡Œä¸ºå¿ƒç†å­¦
        """
        if len(current_context) < 5:
            return {
                'manipulation_probability': 0.5,
                'predicted_type': 'insufficient_data',
                'confidence': 0.0,
                'reasoning': 'insufficient_psychological_data'
            }
        
        # === åº„å®¶å¿ƒç†çŠ¶æ€åˆ†æ ===
        banker_psychology_score = 0.0
        
        # ä»å½“å‰çŠ¶æ€æå–å¿ƒç†æŒ‡æ ‡
        stress_level = current_state.get('stress_level', 0.5)
        aggressiveness = current_state.get('aggressiveness', 0.5)
        risk_tolerance = current_state.get('risk_tolerance', 0.5)
        
        # é«˜å‹åŠ›å’Œé«˜æ”»å‡»æ€§é€šå¸¸å¯¼è‡´æ›´å¤šæ“æ§
        if stress_level > 0.7 and aggressiveness > 0.6:
            banker_psychology_score += 0.4
        elif stress_level > 0.5 or aggressiveness > 0.5:
            banker_psychology_score += 0.2
        
        # ä½é£é™©å®¹å¿åº¦å¯èƒ½å¯¼è‡´ä¿å®ˆæ“æ§
        if risk_tolerance < 0.3:
            banker_psychology_score += 0.3
        elif risk_tolerance < 0.5:
            banker_psychology_score += 0.15
        
        # === ç©å®¶è¡Œä¸ºå¿ƒç†åˆ†æ ===
        player_psychology_exploitation = 0.0
        
        # åˆ†ææœ€è¿‘å‡ æœŸæ˜¯å¦æœ‰æ˜æ˜¾çš„å¿ƒç†é™·é˜±æ¨¡å¼
        recent_periods = current_context[:5]
        
        # æ£€æµ‹"è¿½çƒ­"å¿ƒç†åˆ©ç”¨
        hot_number_trap_evidence = 0.0
        for i, period in enumerate(recent_periods[:-1]):
            current_tails = set(period.get('tails', []))
            next_period_tails = set(recent_periods[i+1].get('tails', []))
            
            # å¦‚æœçƒ­é—¨æ•°å­—åœ¨ä¸‹ä¸€æœŸè¢«"èƒŒå›"
            if len(current_tails.intersection(next_period_tails)) < len(current_tails) * 0.3:
                hot_number_trap_evidence += 0.2
        
        player_psychology_exploitation += min(1.0, hot_number_trap_evidence)
        
        # æ£€æµ‹"è¡¥å¿"å¿ƒç†åˆ©ç”¨
        compensation_psychology = 0.0
        if len(current_context) >= 8:
            # å¯»æ‰¾é•¿æœŸç¼ºå¸­åçªç„¶å‡ºç°çš„æ¨¡å¼
            for tail in range(10):
                recent_appearances = [tail in period.get('tails', []) for period in current_context[:8]]
                
                # å¦‚æœå‰å‡ æœŸé•¿æœŸä¸å‡ºç°ï¼Œæœ€è¿‘çªç„¶å‡ºç°
                if not any(recent_appearances[2:]) and any(recent_appearances[:2]):
                    compensation_psychology += 0.15
        
        player_psychology_exploitation += min(1.0, compensation_psychology)
        
        # === å¸‚åœºæƒ…ç»ªåˆ†æ ===
        market_sentiment_score = 0.0
        
        # åŸºäºæœ€è¿‘æœŸæ•°çš„"éšæœºæ€§"ç¨‹åº¦è¯„ä¼°å¸‚åœºæƒ…ç»ª
        randomness_scores = []
        for period in recent_periods:
            period_tails = period.get('tails', [])
            if len(period_tails) >= 3:
                # è®¡ç®—è¯¥æœŸçš„"éšæœºæ€§"å¾—åˆ†
                sorted_tails = sorted(period_tails)
                gaps = [sorted_tails[i+1] - sorted_tails[i] for i in range(len(sorted_tails)-1)]
                gap_variance = np.var(gaps) if len(gaps) > 1 else 0
                
                # é«˜æ–¹å·®è¡¨ç¤ºæ›´éšæœºï¼Œä½æ–¹å·®è¡¨ç¤ºå¯èƒ½æœ‰è§„å¾‹
                randomness_score = 1.0 - min(1.0, gap_variance / 10.0)
                randomness_scores.append(randomness_score)
        
        if randomness_scores:
            avg_randomness = np.mean(randomness_scores)
            # å¦‚æœéšæœºæ€§è¿‡ä½ï¼Œå¯èƒ½æœ‰äººä¸ºå¹²é¢„
            if avg_randomness > 0.7:
                market_sentiment_score += 0.3
        
        # === æ—¶æœºå¿ƒç†å­¦åˆ†æ ===
        timing_psychology_score = 0.0
        
        # åˆ†ææ˜¯å¦åœ¨ç‰¹å®šæ—¶æœºï¼ˆå¦‚å‘¨æœ«å‰ã€èŠ‚å‡æ—¥ç­‰ï¼‰æœ‰æ“æ§å€¾å‘
        # ç®€åŒ–å®ç°ï¼šåŸºäºæ•°æ®ä½ç½®çš„å‘¨æœŸæ€§
        current_position = len(current_context)
        
        # æ£€æµ‹æ˜¯å¦åœ¨7çš„å€æ•°ä½ç½®ï¼ˆæ¨¡æ‹Ÿå‘¨æœŸæ€§å¹²é¢„ï¼‰
        if current_position % 7 == 0 or current_position % 7 == 6:
            timing_psychology_score += 0.25
        
        # æ£€æµ‹æ˜¯å¦åœ¨"å…³é”®èŠ‚ç‚¹"ï¼ˆæ¯10æœŸï¼‰
        if current_position % 10 == 0:
            timing_psychology_score += 0.2
        
        # === ç»¼åˆå¿ƒç†å­¦é¢„æµ‹ ===
        psychology_components = {
            'banker_psychology': banker_psychology_score * 0.35,
            'player_exploitation': player_psychology_exploitation * 0.3,
            'market_sentiment': market_sentiment_score * 0.2,
            'timing_psychology': timing_psychology_score * 0.15
        }
        
        total_psychology_score = sum(psychology_components.values())
        
        # === å¿ƒç†æ“æ§ç±»å‹è¯†åˆ« ===
        if banker_psychology_score > 0.6:
            predicted_type = 'aggressive_psychological_manipulation'
        elif player_psychology_exploitation > 0.5:
            predicted_type = 'player_bias_exploitation'
        elif market_sentiment_score > 0.4:
            predicted_type = 'market_sentiment_manipulation'
        elif timing_psychology_score > 0.3:
            predicted_type = 'timing_based_manipulation'
        else:
            predicted_type = 'minimal_psychological_influence'
        
        # === ç½®ä¿¡åº¦è¯„ä¼° ===
        confidence_factors = [
            min(1.0, (stress_level + aggressiveness) / 2.0),  # åº„å®¶çŠ¶æ€æ˜ç¡®æ€§
            min(1.0, player_psychology_exploitation),         # ç©å®¶å¿ƒç†è¯æ®å¼ºåº¦
            len(recent_periods) / 10.0                        # æ•°æ®å……è¶³æ€§
        ]
        
        confidence = np.mean(confidence_factors)
        
        return {
            'manipulation_probability': min(1.0, total_psychology_score),
            'predicted_type': predicted_type,
            'confidence': confidence,
            'reasoning': 'Based on banker psychology analysis and player behavior patterns',
            'psychology_components': psychology_components,
            'banker_state_analysis': {
                'stress_level': stress_level,
                'aggressiveness': aggressiveness,
                'risk_tolerance': risk_tolerance,
                'manipulation_propensity': banker_psychology_score
            },
            'player_exploitation_analysis': {
                'hot_number_traps': hot_number_trap_evidence,
                'compensation_psychology': compensation_psychology,
                'total_exploitation': player_psychology_exploitation
            }
        }

    def _fuse_predictions(self, predictions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å¤šæ–¹æ³•é¢„æµ‹èåˆç®—æ³•
        ä½¿ç”¨åŠ æƒå¹³å‡ã€ä¸€è‡´æ€§æ£€éªŒå’Œç½®ä¿¡åº¦è°ƒæ•´çš„ç»¼åˆèåˆ
        """
        if not predictions:
            return {
                'manipulation_probability': 0.5,
                'predicted_type': 'no_predictions',
                'target_tails': [],
                'confidence': 0.0,
                'reasoning': 'no_predictions_to_fuse'
            }
        
        # === æå–å„æ–¹æ³•çš„é¢„æµ‹ç»“æœ ===
        probabilities = []
        confidences = []
        types = []
        
        for pred in predictions:
            if isinstance(pred, dict):
                prob = pred.get('manipulation_probability', 0.5)
                conf = pred.get('confidence', 0.5)
                pred_type = pred.get('predicted_type', 'unknown')
                
                probabilities.append(prob)
                confidences.append(conf)
                types.append(pred_type)
        
        if not probabilities:
            return {
                'manipulation_probability': 0.5,
                'predicted_type': 'fusion_failed',
                'target_tails': [],
                'confidence': 0.0,
                'reasoning': 'no_valid_predictions_to_fuse'
            }
        
        # === åŸºäºç½®ä¿¡åº¦çš„åŠ æƒèåˆ ===
        # ä½¿ç”¨ç½®ä¿¡åº¦ä½œä¸ºæƒé‡
        total_confidence = sum(confidences)
        if total_confidence > 0:
            weights = [conf / total_confidence for conf in confidences]
        else:
            weights = [1.0 / len(probabilities)] * len(probabilities)
        
        # åŠ æƒå¹³å‡æ¦‚ç‡
        weighted_probability = sum(prob * weight for prob, weight in zip(probabilities, weights))
        
        # === ä¸€è‡´æ€§æ£€éªŒ ===
        consistency_score = 0.0
        if len(probabilities) > 1:
            # è®¡ç®—é¢„æµ‹çš„æ ‡å‡†å·®
            prob_std = np.std(probabilities)
            max_possible_std = 0.5  # æœ€å¤§å¯èƒ½çš„æ ‡å‡†å·®ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰
            
            # ä¸€è‡´æ€§å¾—åˆ†ï¼šæ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
            consistency_score = max(0.0, 1.0 - prob_std / max_possible_std)
        else:
            consistency_score = 1.0  # åªæœ‰ä¸€ä¸ªé¢„æµ‹æ—¶è®¤ä¸ºå®Œå…¨ä¸€è‡´
        
        # === é¢„æµ‹ç±»å‹èåˆ ===
        # ç»Ÿè®¡æœ€å¸¸è§çš„é¢„æµ‹ç±»å‹
        type_counts = defaultdict(int)
        for pred_type in types:
            type_counts[pred_type] += 1
        
        if type_counts:
            dominant_type = max(type_counts.items(), key=lambda x: x[1])[0]
        else:
            dominant_type = 'unknown'
        
        # === ç½®ä¿¡åº¦èåˆ ===
        # ç»“åˆåŸå§‹ç½®ä¿¡åº¦å’Œä¸€è‡´æ€§
        average_confidence = np.mean(confidences)
        
        # ä¸€è‡´æ€§è°ƒæ•´ï¼šä¸€è‡´æ€§é«˜æ—¶æå‡ç½®ä¿¡åº¦ï¼Œä¸€è‡´æ€§ä½æ—¶é™ä½ç½®ä¿¡åº¦
        consistency_adjustment = consistency_score * 0.3
        fused_confidence = min(1.0, average_confidence + consistency_adjustment)
        
        # === å¼‚å¸¸å€¼æ£€æµ‹å’Œè°ƒæ•´ ===
        # å¦‚æœæœ‰æç«¯å¼‚å¸¸çš„é¢„æµ‹ï¼Œé™ä½æ€»ä½“ç½®ä¿¡åº¦
        median_prob = np.median(probabilities)
        outlier_penalty = 0.0
        
        for prob in probabilities:
            if abs(prob - median_prob) > 0.4:  # ä¸ä¸­ä½æ•°å·®å¼‚è¶…è¿‡0.4
                outlier_penalty += 0.1
        
        final_confidence = max(0.1, fused_confidence - outlier_penalty)
        
        # === æœ€ç»ˆæ¦‚ç‡è°ƒæ•´ ===
        # æ ¹æ®ä¸€è‡´æ€§å¯¹æœ€ç»ˆæ¦‚ç‡è¿›è¡Œå¾®è°ƒ
        if consistency_score < 0.5:  # ä½ä¸€è‡´æ€§æ—¶å‘ä¸­é—´å€¼é æ‹¢
            adjustment_factor = 0.3 * (0.5 - consistency_score)
            if weighted_probability > 0.5:
                final_probability = weighted_probability - adjustment_factor
            else:
                final_probability = weighted_probability + adjustment_factor
        else:
            final_probability = weighted_probability
        
        final_probability = max(0.0, min(1.0, final_probability))
        
        # === ç”Ÿæˆèåˆæ¨ç†è¯´æ˜ ===
        fusion_reasoning = f"Fused {len(predictions)} predictions with {consistency_score:.2f} consistency"
        
        if consistency_score > 0.8:
            fusion_reasoning += " (high agreement)"
        elif consistency_score > 0.5:
            fusion_reasoning += " (moderate agreement)"
        else:
            fusion_reasoning += " (low agreement)"
        
        return {
            'manipulation_probability': final_probability,
            'predicted_type': dominant_type,
            'target_tails': [],  # å°†åœ¨åç»­æ–¹æ³•ä¸­ç¡®å®š
            'confidence': final_confidence,
            'reasoning': fusion_reasoning,
            'fusion_details': {
                'individual_probabilities': probabilities,
                'individual_confidences': confidences,
                'individual_types': types,
                'weights_used': weights,
                'consistency_score': consistency_score,
                'outlier_penalty': outlier_penalty,
                'dominant_type': dominant_type
            }
        }

    def _generate_anti_manipulation_strategy(self, manipulation_prediction: Dict[str, Any], 
                                           current_context: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆåæ“æ§æŠ•æ³¨ç­–ç•¥
        åŸºäºé¢„æµ‹çš„æ“æ§ç±»å‹å’Œå¼ºåº¦åˆ¶å®šé’ˆå¯¹æ€§ç­–ç•¥
        """
        manipulation_prob = manipulation_prediction.get('manipulation_probability', 0.5)
        predicted_type = manipulation_prediction.get('predicted_type', 'unknown')
        confidence = manipulation_prediction.get('confidence', 0.5)
        
        # === åŸºç¡€ç­–ç•¥å‚æ•° ===
        strategy_config = {
            'risk_level': 'medium',
            'diversification': 'moderate',
            'contrarian_strength': 'balanced'
        }
        
        # === æ ¹æ®æ“æ§æ¦‚ç‡è°ƒæ•´ç­–ç•¥ ===
        if manipulation_prob > 0.8:
            strategy_config['risk_level'] = 'conservative'
            strategy_config['diversification'] = 'high'
            strategy_config['contrarian_strength'] = 'strong'
            strategy_type = 'defensive_anti_manipulation'
        elif manipulation_prob > 0.6:
            strategy_config['risk_level'] = 'moderate'
            strategy_config['diversification'] = 'moderate'
            strategy_config['contrarian_strength'] = 'moderate'
            strategy_type = 'balanced_anti_manipulation'
        elif manipulation_prob > 0.4:
            strategy_config['risk_level'] = 'moderate'
            strategy_config['diversification'] = 'low'
            strategy_config['contrarian_strength'] = 'weak'
            strategy_type = 'cautious_following'
        else:
            strategy_config['risk_level'] = 'aggressive'
            strategy_config['diversification'] = 'low'
            strategy_config['contrarian_strength'] = 'minimal'
            strategy_type = 'trend_following'
        
        # === åŸºäºæ“æ§ç±»å‹çš„ç‰¹å®šç­–ç•¥ ===
        recommended_tails = set()
        avoid_tails = set()
        
        if not current_context:
            return {
                'recommended_tails': [],
                'avoid_tails': [],
                'confidence': 0.0,
                'strategy_type': 'no_data',
                'reasoning': 'insufficient_context_data',
                'risk_assessment': 'unknown'
            }
        
        latest_period = current_context[0]
        latest_tails = set(latest_period.get('tails', []))
        
        # æ ¹æ®é¢„æµ‹ç±»å‹åˆ¶å®šå…·ä½“ç­–ç•¥
        if 'hot' in predicted_type.lower() or 'frequency' in predicted_type.lower():
            # çƒ­é—¨æ•°å­—æ“æ§ï¼šé¿å¼€æœ€è¿‘é¢‘ç¹å‡ºç°çš„æ•°å­—
            if len(current_context) >= 5:
                recent_counts = defaultdict(int)
                for period in current_context[:5]:
                    for tail in period.get('tails', []):
                        recent_counts[tail] += 1
                
                # é¿å¼€å‡ºç°3æ¬¡ä»¥ä¸Šçš„çƒ­é—¨æ•°å­—
                hot_tails = {tail for tail, count in recent_counts.items() if count >= 3}
                avoid_tails.update(hot_tails)
                
                # æ¨èå‡ºç°1-2æ¬¡çš„æ¸©å’Œæ•°å­—
                moderate_tails = {tail for tail, count in recent_counts.items() if 1 <= count <= 2}
                recommended_tails.update(moderate_tails)
        
        elif 'cold' in predicted_type.lower() or 'comeback' in predicted_type.lower():
            # å†·é—¨å¤å‡ºæ“æ§ï¼šé¿å¼€é•¿æœŸç¼ºå¸­çš„æ•°å­—
            if len(current_context) >= 10:
                cold_tails = set()
                for tail in range(10):
                    if not any(tail in period.get('tails', []) for period in current_context[:10]):
                        cold_tails.add(tail)
                
                avoid_tails.update(cold_tails)
                
                # æ¨èæœ€è¿‘æœ‰é€‚åº¦å‡ºç°çš„æ•°å­—
                moderate_activity_tails = set()
                for tail in range(10):
                    recent_appearances = sum(1 for period in current_context[:5] if tail in period.get('tails', []))
                    if 1 <= recent_appearances <= 2:
                        moderate_activity_tails.add(tail)
                
                recommended_tails.update(moderate_activity_tails)
        
        elif 'pattern' in predicted_type.lower() or 'temporal' in predicted_type.lower():
            # æ¨¡å¼æ“æ§ï¼šæ‰“ç ´æ˜æ˜¾çš„æ¨¡å¼
            if len(current_context) >= 3:
                # åˆ†ææœ€è¿‘çš„è¿ç»­æ¨¡å¼
                consecutive_patterns = set()
                for tail in range(10):
                    consecutive_count = 0
                    for period in current_context[:3]:
                        if tail in period.get('tails', []):
                            consecutive_count += 1
                        else:
                            break
                    
                    if consecutive_count >= 2:
                        consecutive_patterns.add(tail)
                
                # é¿å¼€è¿ç»­å‡ºç°çš„æ¨¡å¼
                avoid_tails.update(consecutive_patterns)
                
                # æ¨èæ‰“ç ´æ¨¡å¼çš„æ•°å­—
                pattern_breakers = latest_tails - consecutive_patterns
                recommended_tails.update(pattern_breakers)
        
        elif 'symmetry' in predicted_type.lower():
            # å¯¹ç§°æ€§æ“æ§ï¼šé¿å¼€è¿‡åº¦å¯¹ç§°çš„ç»„åˆ
            symmetry_pairs = [(0, 9), (1, 8), (2, 7), (3, 6), (4, 5)]
            
            # è®¡ç®—å½“å‰æœŸçš„å¯¹ç§°æ€§
            current_symmetry_pairs = []
            for pair in symmetry_pairs:
                if pair[0] in latest_tails and pair[1] in latest_tails:
                    current_symmetry_pairs.append(pair)
            
            if len(current_symmetry_pairs) >= 2:
                # å½“å‰æœŸå¯¹ç§°æ€§è¿‡é«˜ï¼Œé¿å¼€å¯¹ç§°æ•°å­—
                for pair in current_symmetry_pairs:
                    avoid_tails.update(pair)
            
            # æ¨èéå¯¹ç§°æ•°å­—
            non_symmetric_tails = set()
            for tail in latest_tails:
                has_symmetric_pair = any(
                    (tail == pair[0] and pair[1] in latest_tails) or 
                    (tail == pair[1] and pair[0] in latest_tails)
                    for pair in symmetry_pairs
                )
                if not has_symmetric_pair:
                    non_symmetric_tails.add(tail)
            
            recommended_tails.update(non_symmetric_tails)
        
        elif 'psychological' in predicted_type.lower():
            # å¿ƒç†æ“æ§ï¼šé‡‡ç”¨åå¿ƒç†ç­–ç•¥
            # é¿å¼€"å¹¸è¿æ•°å­—"
            lucky_numbers = {6, 8, 9}
            avoid_tails.update(lucky_numbers.intersection(latest_tails))
            
            # æ¨è"éç›´è§‰"æ•°å­—
            non_intuitive_tails = {0, 4, 7}  # é€šå¸¸ä¸è¢«åå¥½çš„æ•°å­—
            recommended_tails.update(non_intuitive_tails.intersection(latest_tails))
        
        else:
            # é»˜è®¤ç­–ç•¥ï¼šå‡è¡¡é€‰æ‹©
            if latest_tails:
                # æ¨èå½“å‰æœŸå‡ºç°çš„æ•°å­—ä¸­çš„ä¸€åŠ
                tails_list = list(latest_tails)
                recommended_count = max(1, len(tails_list) // 2)
                recommended_tails.update(tails_list[:recommended_count])
        
        # === ç­–ç•¥ä¼˜åŒ–å’ŒéªŒè¯ ===
        # ç¡®ä¿æ¨èæ•°å­—ä¸ä¸ºç©º
        if not recommended_tails and latest_tails:
            # å¤‡é€‰ç­–ç•¥ï¼šæ¨èæœ€è¿‘æœŸå‡ºç°çš„æ•°å­—ä¸­é£é™©æœ€ä½çš„
            safe_tails = latest_tails - avoid_tails
            if safe_tails:
                recommended_tails.update(list(safe_tails)[:2])
            else:
                # æœ€åçš„å¤‡é€‰ï¼šæ¨èæœ€æ–°æœŸçš„ä»»æ„æ•°å­—
                recommended_tails.add(list(latest_tails)[0])
        
        # é™åˆ¶æ¨èæ•°é‡
        if len(recommended_tails) > 3:
            recommended_tails = set(list(recommended_tails)[:3])
        
        # === é£é™©è¯„ä¼° ===
        risk_factors = []
        
        # æ“æ§æ¦‚ç‡é£é™©
        risk_factors.append(manipulation_prob)
        
        # ç­–ç•¥å¤æ‚åº¦é£é™©
        strategy_complexity = len(avoid_tails) + len(recommended_tails)
        complexity_risk = min(1.0, strategy_complexity / 8.0)
        risk_factors.append(complexity_risk)
        
        # ç½®ä¿¡åº¦é£é™©ï¼ˆç½®ä¿¡åº¦ä½=é£é™©é«˜ï¼‰
        confidence_risk = 1.0 - confidence
        risk_factors.append(confidence_risk)
        
        overall_risk = np.mean(risk_factors)
        
        if overall_risk > 0.7:
            risk_assessment = 'high'
        elif overall_risk > 0.4:
            risk_assessment = 'medium'
        else:
            risk_assessment = 'low'
        
        # === ç­–ç•¥è¯´æ˜ç”Ÿæˆ ===
        reasoning_parts = []
        reasoning_parts.append(f"Based on {manipulation_prob:.2f} manipulation probability")
        reasoning_parts.append(f"Predicted type: {predicted_type}")
        reasoning_parts.append(f"Strategy: {strategy_type}")
        
        if avoid_tails:
            reasoning_parts.append(f"Avoiding {len(avoid_tails)} potentially manipulated numbers")
        
        if recommended_tails:
            reasoning_parts.append(f"Recommending {len(recommended_tails)} safer alternatives")
        
        full_reasoning = "; ".join(reasoning_parts)
        
        return {
            'recommended_tails': sorted(list(recommended_tails)),
            'avoid_tails': sorted(list(avoid_tails)),
            'confidence': confidence,
            'strategy_type': strategy_type,
            'reasoning': full_reasoning,
            'risk_assessment': risk_assessment,
            'strategy_details': {
                'manipulation_probability': manipulation_prob,
                'predicted_manipulation_type': predicted_type,
                'strategy_config': strategy_config,
                'risk_factors': {
                    'manipulation_risk': manipulation_prob,
                    'complexity_risk': complexity_risk,
                    'confidence_risk': confidence_risk,
                    'overall_risk': overall_risk
                }
            }
        }

# è¾…åŠ©åˆ†æç±»
class StatisticalManipulationAnalyzer:
    """
    ç§‘ç ”çº§ç»Ÿè®¡å­¦æ“æ§åˆ†æå™¨
    åŸºäºé«˜çº§ç»Ÿè®¡å­¦ã€ä¿¡æ¯è®ºã€æ—¶é—´åºåˆ—åˆ†æçš„å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç»Ÿè®¡åˆ†æå™¨"""
        # ç»Ÿè®¡æ£€éªŒå‚æ•°
        self.significance_level = 0.05
        self.critical_values = {
            'chi_square_9df': 16.919,  # 9ä¸ªè‡ªç”±åº¦ï¼ŒÎ±=0.05
            'chi_square_4df': 9.488,   # 4ä¸ªè‡ªç”±åº¦ï¼ŒÎ±=0.05
            'chi_square_1df': 3.841,   # 1ä¸ªè‡ªç”±åº¦ï¼ŒÎ±=0.05
            'kolmogorov_smirnov': 1.36, # KSæ£€éªŒä¸´ç•Œå€¼
            'anderson_darling': 2.502,  # ADæ£€éªŒä¸´ç•Œå€¼
            'shapiro_wilk': 0.05       # SWæ£€éªŒä¸´ç•Œå€¼
        }
        
        # è´å¶æ–¯åˆ†æå‚æ•°
        self.bayesian_priors = {
            'manipulation_prior': 0.15,  # æ“æ§çš„å…ˆéªŒæ¦‚ç‡
            'natural_prior': 0.85,       # è‡ªç„¶çš„å…ˆéªŒæ¦‚ç‡
            'evidence_weight': 0.7       # è¯æ®æƒé‡
        }
        
        # ä¿¡æ¯è®ºå‚æ•°
        self.entropy_thresholds = {
            'min_entropy': 2.8,    # æœ€å°æœŸæœ›ç†µï¼ˆlog2(7)â‰ˆ2.8ï¼‰
            'max_entropy': 3.32,   # æœ€å¤§æœŸæœ›ç†µï¼ˆlog2(10)â‰ˆ3.32ï¼‰
            'suspicious_deviation': 0.5  # å¯ç–‘åå·®é˜ˆå€¼
        }
        
        # æ—¶é—´åºåˆ—å‚æ•°
        self.timeseries_params = {
            'stationarity_window': 20,
            'trend_detection_window': 15,
            'seasonality_periods': [7, 14, 21],
            'change_point_sensitivity': 0.3
        }

    def detect_anomalies(self, period_data: Dict, historical_data: List[Dict]) -> Dict:
        """
        å¤šç»´åº¦ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ä¸»å‡½æ•°
        
        Args:
            period_data: å½“æœŸæ•°æ®
            historical_data: å†å²æ•°æ®
            
        Returns:
            ç»¼åˆå¼‚å¸¸æ£€æµ‹ç»“æœ
        """
        if len(historical_data) < 10:
            return self._insufficient_data_response()
        
        # 1. é¢‘æ•°åˆ†å¸ƒæ£€éªŒ
        frequency_anomalies = self._detect_frequency_anomalies(period_data, historical_data)
        
        # 2. åˆ†å¸ƒæ‹Ÿåˆæ£€éªŒ
        distribution_anomalies = self._detect_distribution_anomalies(period_data, historical_data)
        
        # 3. ä¿¡æ¯è®ºå¼‚å¸¸æ£€æµ‹
        information_anomalies = self._detect_information_anomalies(period_data, historical_data)
        
        # 4. æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
        timeseries_anomalies = self._detect_timeseries_anomalies(period_data, historical_data)
        
        # 5. è´å¶æ–¯å¼‚å¸¸è¯„ä¼°
        bayesian_anomalies = self._bayesian_anomaly_assessment(period_data, historical_data, 
                                                              frequency_anomalies, distribution_anomalies,
                                                              information_anomalies, timeseries_anomalies)
        
        # 6. å¤šå°ºåº¦å¼‚å¸¸åˆ†æ
        multiscale_anomalies = self._multiscale_anomaly_analysis(period_data, historical_data)
        
        # 7. ç»¼åˆå¼‚å¸¸è¯„åˆ†
        composite_score = self._calculate_composite_anomaly_score([
            frequency_anomalies, distribution_anomalies, information_anomalies,
            timeseries_anomalies, bayesian_anomalies, multiscale_anomalies
        ])
        
        return {
            'frequency_anomalies': frequency_anomalies,
            'distribution_anomalies': distribution_anomalies,
            'information_anomalies': information_anomalies,
            'timeseries_anomalies': timeseries_anomalies,
            'bayesian_anomalies': bayesian_anomalies,
            'multiscale_anomalies': multiscale_anomalies,
            'composite_score': composite_score,
            'anomaly_strength': self._classify_anomaly_strength(composite_score),
            'statistical_confidence': self._calculate_statistical_confidence(composite_score),
            'evidence_quality': self._assess_evidence_quality(historical_data),
            'recommendations': self._generate_anomaly_recommendations(composite_score)
        }
    
    def _detect_frequency_anomalies(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        ç§‘ç ”çº§é¢‘ç‡å¼‚å¸¸æ£€æµ‹ç®—æ³•
        åŸºäºå¤šé‡ç»Ÿè®¡æ£€éªŒã€è´å¶æ–¯åˆ†æã€æ—¶é—´åºåˆ—åˆ†è§£å’Œé©¬å°”å¯å¤«é“¾å»ºæ¨¡çš„ç»¼åˆé¢‘ç‡å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
        """
        if len(historical_context) < 30:
            return {'overall_anomaly_score': 0.0, 'details': 'insufficient_data_for_research_grade_analysis'}
        
        current_tails = set(period_data.get('tails', []))
        
        # ===== æ ¸å¿ƒæ•°æ®ç»“æ„æ„å»º =====
        
        # 1. æ„å»ºå¤šç»´åº¦é¢‘ç‡å¼ é‡
        frequency_tensor = self._build_frequency_tensor(historical_context, current_tails)
        
        # 2. æ—¶é—´åºåˆ—åˆ†è§£åˆ†æ
        decomposition_results = self._perform_time_series_decomposition(historical_context)
        
        # 3. é©¬å°”å¯å¤«é“¾çŠ¶æ€åˆ†æ
        markov_analysis = self._analyze_markov_chain_frequencies(historical_context, current_tails)
        
        # ===== å¤šé‡ç»Ÿè®¡æ£€éªŒç³»ç»Ÿ =====
        
        # 1. å¢å¼ºå‹å¡æ–¹æ£€éªŒç³»åˆ—
        enhanced_chi_square_tests = self._enhanced_chi_square_testing_suite(frequency_tensor, current_tails)
        
        # 2. é«˜ç»´åº¦Kolmogorov-Smirnovæ£€éªŒ
        high_dim_ks_tests = self._multidimensional_ks_testing(frequency_tensor, current_tails)
        
        # 3. Anderson-Darlingå¤šå…ƒæ£€éªŒ
        anderson_darling_tests = self._multivariate_anderson_darling_testing(frequency_tensor, current_tails)
        
        # 4. é«˜é˜¶çŸ©ç»Ÿè®¡æ£€éªŒ
        higher_moment_tests = self._higher_moment_statistical_testing(frequency_tensor, current_tails)
        
        # ===== è´å¶æ–¯å¼‚å¸¸æ£€æµ‹ =====
        
        bayesian_anomaly_analysis = self._bayesian_frequency_anomaly_detection(
            frequency_tensor, current_tails, historical_context
        )
        
        # ===== æ—¶é¢‘åŸŸåˆ†æ =====
        
        # 1. å°æ³¢å˜æ¢é¢‘ç‡åˆ†æ
        wavelet_analysis = self._wavelet_frequency_analysis(historical_context, current_tails)
        
        # 2. å‚…é‡Œå¶é¢‘è°±å¼‚å¸¸æ£€æµ‹
        fourier_analysis = self._fourier_spectral_anomaly_detection(historical_context, current_tails)
        
        # 3. å¸Œå°”ä¼¯ç‰¹-é»„å˜æ¢åˆ†æ
        hilbert_huang_analysis = self._hilbert_huang_frequency_analysis(historical_context, current_tails)
        
        # ===== æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹ =====
        
        # 1. å­¤ç«‹æ£®æ—å¼‚å¸¸æ£€æµ‹
        isolation_forest_results = self._isolation_forest_frequency_detection(frequency_tensor, current_tails)
        
        # 2. ä¸€ç±»æ”¯æŒå‘é‡æœºå¼‚å¸¸æ£€æµ‹
        one_class_svm_results = self._one_class_svm_frequency_detection(frequency_tensor, current_tails)
        
        # 3. è‡ªç¼–ç å™¨å¼‚å¸¸æ£€æµ‹
        autoencoder_results = self._autoencoder_frequency_anomaly_detection(frequency_tensor, current_tails)
        
        # ===== ä¿¡æ¯è®ºå¼‚å¸¸æ£€æµ‹ =====
        
        information_theory_analysis = self._information_theoretic_frequency_analysis(
            frequency_tensor, current_tails, historical_context
        )
        
        # ===== å¤æ‚ç½‘ç»œå¼‚å¸¸æ£€æµ‹ =====
        
        network_based_analysis = self._network_based_frequency_anomaly_detection(
            frequency_tensor, current_tails, historical_context
        )
        
        # ===== èµ”ç‡åŠ æƒå¼‚å¸¸æ£€æµ‹ =====
        
        # è€ƒè™‘ä¸åŒå°¾æ•°çš„èµ”ç‡å·®å¼‚è¿›è¡ŒåŠ æƒåˆ†æ
        odds_weighted_analysis = self._odds_weighted_frequency_analysis(
            frequency_tensor, current_tails, {0: 2.0, 1: 1.8, 2: 1.8, 3: 1.8, 4: 1.8, 
                                            5: 1.8, 6: 1.8, 7: 1.8, 8: 1.8, 9: 1.8}
        )
        
        # ===== å¤šå°ºåº¦å¼‚å¸¸æ£€æµ‹ =====
        
        multiscale_analysis = self._multiscale_frequency_anomaly_detection(
            frequency_tensor, current_tails, historical_context
        )
        
        # ===== éå‚æ•°å¼‚å¸¸æ£€æµ‹ =====
        
        nonparametric_analysis = self._nonparametric_frequency_anomaly_detection(
            frequency_tensor, current_tails, historical_context
        )
        
        # ===== ç»¼åˆå¼‚å¸¸è¯„åˆ†è®¡ç®— =====
        
        # ä½¿ç”¨åŠ æƒé›†æˆå­¦ä¹ æ–¹æ³•ç»¼åˆæ‰€æœ‰æ£€æµ‹ç»“æœ
        overall_anomaly_score = self._ensemble_anomaly_scoring([
            enhanced_chi_square_tests,
            high_dim_ks_tests,
            anderson_darling_tests,
            higher_moment_tests,
            bayesian_anomaly_analysis,
            wavelet_analysis,
            fourier_analysis,
            hilbert_huang_analysis,
            isolation_forest_results,
            one_class_svm_results,
            autoencoder_results,
            information_theory_analysis,
            network_based_analysis,
            odds_weighted_analysis,
            multiscale_analysis,
            nonparametric_analysis
        ])
        
        # ===== ç½®ä¿¡åŒºé—´å’Œä¸ç¡®å®šæ€§é‡åŒ– =====
        
        confidence_intervals = self._calculate_anomaly_confidence_intervals(
            overall_anomaly_score, frequency_tensor, len(historical_context)
        )
        
        # ===== ç»“æœè§£é‡Šå’Œå¯è§†åŒ–æ•°æ® =====
        
        interpretation_results = self._generate_anomaly_interpretation(
            overall_anomaly_score, enhanced_chi_square_tests, bayesian_anomaly_analysis,
            current_tails, frequency_tensor
        )
        
        return {
            'overall_anomaly_score': float(overall_anomaly_score),
            'confidence_intervals': confidence_intervals,
            'detailed_test_results': {
                'enhanced_chi_square': enhanced_chi_square_tests,
                'multidimensional_ks': high_dim_ks_tests,
                'anderson_darling': anderson_darling_tests,
                'higher_moments': higher_moment_tests,
                'bayesian_analysis': bayesian_anomaly_analysis,
                'wavelet_analysis': wavelet_analysis,
                'fourier_analysis': fourier_analysis,
                'hilbert_huang': hilbert_huang_analysis,
                'isolation_forest': isolation_forest_results,
                'one_class_svm': one_class_svm_results,
                'autoencoder': autoencoder_results,
                'information_theory': information_theory_analysis,
                'network_based': network_based_analysis,
                'odds_weighted': odds_weighted_analysis,
                'multiscale': multiscale_analysis,
                'nonparametric': nonparametric_analysis
            },
            'interpretation': interpretation_results,
            'anomaly_classification': self._classify_frequency_anomaly_type(overall_anomaly_score, interpretation_results),
            'statistical_significance': self._calculate_statistical_significance(overall_anomaly_score, len(historical_context)),
            'effect_size': self._calculate_effect_size(frequency_tensor, current_tails),
            'power_analysis': self._perform_power_analysis(frequency_tensor, overall_anomaly_score, len(historical_context))
        }
    
    def _detect_distribution_anomalies(self, period_data: Dict, historical_data: List[Dict]) -> Dict:
        """
        åˆ†å¸ƒæ‹Ÿåˆå¼‚å¸¸æ£€æµ‹
        ä½¿ç”¨Kolmogorov-Smirnovæ£€éªŒã€Anderson-Darlingæ£€éªŒã€Shapiro-Wilkæ£€éªŒ
        """
        current_tails = set(period_data.get('tails', []))
        
        # æ„å»ºæ—¶é—´åºåˆ—æ•°æ®
        tail_time_series = {}
        for tail in range(10):
            time_series = []
            for i, period in enumerate(historical_data):
                if tail in period.get('tails', []):
                    time_series.append(i)  # è®°å½•å‡ºç°çš„æ—¶é—´ä½ç½®
            tail_time_series[tail] = time_series
        
        distribution_results = {}
        
        for tail in range(10):
            tail_results = {}
            time_positions = tail_time_series[tail]
            
            if len(time_positions) < 3:
                # æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ
                tail_results = {
                    'ks_test': {'statistic': 0.0, 'p_value': 1.0, 'is_anomalous': False},
                    'ad_test': {'statistic': 0.0, 'p_value': 1.0, 'is_anomalous': False},
                    'uniformity_test': {'is_uniform': True, 'deviation_score': 0.0},
                    'insufficient_data': True
                }
            else:
                # 1. Kolmogorov-Smirnovå‡åŒ€æ€§æ£€éªŒ
                normalized_positions = np.array(time_positions) / len(historical_data)
                uniform_distribution = np.linspace(0, 1, len(time_positions))
                
                try:
                    ks_statistic, ks_p_value = stats.kstest(normalized_positions, 'uniform')
                    ks_anomaly = ks_statistic > self.critical_values['kolmogorov_smirnov'] / math.sqrt(len(time_positions))
                except:
                    ks_statistic, ks_p_value, ks_anomaly = 0.0, 1.0, False
                
                # 2. Anderson-Darlingæ£€éªŒ
                try:
                    ad_statistic, ad_critical_values, ad_significance_levels = stats.anderson(normalized_positions, dist='uniform')
                    ad_p_value = 0.05 if ad_statistic > ad_critical_values[2] else 0.1  # è¿‘ä¼¼på€¼
                    ad_anomaly = ad_statistic > self.critical_values['anderson_darling']
                except:
                    ad_statistic, ad_p_value, ad_anomaly = 0.0, 1.0, False
                
                # 3. é—´éš”åˆ†å¸ƒåˆ†æ
                intervals = np.diff(time_positions) if len(time_positions) > 1 else [0]
                if len(intervals) > 1:
                    # æ£€éªŒé—´éš”æ˜¯å¦ç¬¦åˆæŒ‡æ•°åˆ†å¸ƒï¼ˆæ³Šæ¾è¿‡ç¨‹çš„é—´éš”ï¼‰
                    mean_interval = np.mean(intervals)
                    
                    # æŒ‡æ•°åˆ†å¸ƒçš„KSæ£€éªŒ
                    try:
                        interval_ks_stat, interval_ks_p = stats.kstest(intervals, 
                                                                     lambda x: stats.expon.cdf(x, scale=mean_interval))
                        interval_anomaly = interval_ks_p < 0.05
                    except:
                        interval_ks_stat, interval_ks_p, interval_anomaly = 0.0, 1.0, False
                else:
                    interval_ks_stat, interval_ks_p, interval_anomaly = 0.0, 1.0, False
                
                # 4. å‡åŒ€æ€§åå·®è¯„åˆ†
                expected_positions = np.linspace(0, len(historical_data), len(time_positions), endpoint=False)
                actual_positions = np.array(time_positions)
                uniformity_deviation = np.mean(np.abs(actual_positions - expected_positions)) / len(historical_data)
                
                tail_results = {
                    'ks_test': {
                        'statistic': float(ks_statistic),
                        'p_value': float(ks_p_value),
                        'is_anomalous': ks_anomaly
                    },
                    'ad_test': {
                        'statistic': float(ad_statistic),
                        'p_value': float(ad_p_value),
                        'is_anomalous': ad_anomaly
                    },
                    'interval_test': {
                        'statistic': float(interval_ks_stat),
                        'p_value': float(interval_ks_p),
                        'is_anomalous': interval_anomaly
                    },
                    'uniformity_test': {
                        'deviation_score': float(uniformity_deviation),
                        'is_uniform': uniformity_deviation < 0.1
                    },
                    'insufficient_data': False
                }
            
            distribution_results[tail] = tail_results
        
        # 5. æ•´ä½“åˆ†å¸ƒä¸€è‡´æ€§æ£€éªŒ
        all_positions = []
        all_tails = []
        for tail in range(10):
            positions = tail_time_series[tail]
            all_positions.extend(positions)
            all_tails.extend([tail] * len(positions))
        
        if len(all_positions) > 10:
            # æ£€éªŒå°¾æ•°åˆ†å¸ƒæ˜¯å¦å‡åŒ€
            tail_counts = np.bincount(all_tails, minlength=10)
            expected_count = len(all_positions) / 10.0
            overall_chi_square = np.sum((tail_counts - expected_count) ** 2 / expected_count)
            overall_anomaly = overall_chi_square > self.critical_values['chi_square_9df']
        else:
            overall_chi_square = 0.0
            overall_anomaly = False
        
        return {
            'tail_distribution_tests': distribution_results,
            'overall_distribution_test': {
                'chi_square_statistic': float(overall_chi_square),
                'is_anomalous': overall_anomaly,
                'critical_value': self.critical_values['chi_square_9df']
            },
            'distribution_anomaly_score': self._calculate_distribution_anomaly_score(distribution_results, overall_anomaly)
        }
    
    def _detect_information_anomalies(self, period_data: Dict, historical_data: List[Dict]) -> Dict:
        """
        åŸºäºä¿¡æ¯è®ºçš„å¼‚å¸¸æ£€æµ‹
        ä½¿ç”¨Shannonç†µã€æ¡ä»¶ç†µã€äº’ä¿¡æ¯ã€Kullback-Leibleræ•£åº¦ç­‰
        """
        current_tails = set(period_data.get('tails', []))
        
        # 1. Shannonç†µåˆ†æ
        shannon_entropy_results = self._calculate_shannon_entropy_anomalies(historical_data)
        
        # 2. æ¡ä»¶ç†µåˆ†æ
        conditional_entropy_results = self._calculate_conditional_entropy_anomalies(historical_data)
        
        # 3. äº’ä¿¡æ¯åˆ†æ
        mutual_information_results = self._calculate_mutual_information_anomalies(historical_data)
        
        # 4. KLæ•£åº¦åˆ†æ
        kl_divergence_results = self._calculate_kl_divergence_anomalies(current_tails, historical_data)
        
        # 5. ä¿¡æ¯å¢ç›Šåˆ†æ
        information_gain_results = self._calculate_information_gain_anomalies(historical_data)
        
        # 6. å¤æ‚åº¦åˆ†æ
        complexity_results = self._calculate_complexity_anomalies(historical_data)
        
        return {
            'shannon_entropy': shannon_entropy_results,
            'conditional_entropy': conditional_entropy_results,
            'mutual_information': mutual_information_results,
            'kl_divergence': kl_divergence_results,
            'information_gain': information_gain_results,
            'complexity_analysis': complexity_results,
            'information_anomaly_score': self._calculate_information_anomaly_score([
                shannon_entropy_results, conditional_entropy_results, mutual_information_results,
                kl_divergence_results, information_gain_results, complexity_results
            ])
        }
    
    def _detect_timeseries_anomalies(self, period_data: Dict, historical_data: List[Dict]) -> Dict:
        """
        æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
        ä½¿ç”¨è¶‹åŠ¿æ£€æµ‹ã€å­£èŠ‚æ€§åˆ†æã€å˜ç‚¹æ£€æµ‹ã€è‡ªç›¸å…³åˆ†æ
        """
        # 1. è¶‹åŠ¿æ£€æµ‹
        trend_results = self._detect_trends(historical_data)
        
        # 2. å­£èŠ‚æ€§æ£€æµ‹
        seasonality_results = self._detect_seasonality(historical_data)
        
        # 3. å˜ç‚¹æ£€æµ‹
        changepoint_results = self._detect_changepoints(historical_data)
        
        # 4. è‡ªç›¸å…³åˆ†æ
        autocorr_results = self._analyze_autocorrelation(historical_data)
        
        # 5. å¹³ç¨³æ€§æ£€éªŒ
        stationarity_results = self._test_stationarity(historical_data)
        
        # 6. å¼‚æ–¹å·®æ£€éªŒ
        heteroscedasticity_results = self._test_heteroscedasticity(historical_data)
        
        return {
            'trend_analysis': trend_results,
            'seasonality_analysis': seasonality_results,
            'changepoint_analysis': changepoint_results,
            'autocorrelation_analysis': autocorr_results,
            'stationarity_analysis': stationarity_results,
            'heteroscedasticity_analysis': heteroscedasticity_results,
            'timeseries_anomaly_score': self._calculate_timeseries_anomaly_score([
                trend_results, seasonality_results, changepoint_results,
                autocorr_results, stationarity_results, heteroscedasticity_results
            ])
        }
    
    def _bayesian_anomaly_assessment(self, period_data: Dict, historical_data: List[Dict],
                                   frequency_anomalies: Dict, distribution_anomalies: Dict,
                                   information_anomalies: Dict, timeseries_anomalies: Dict) -> Dict:
        """
        è´å¶æ–¯å¼‚å¸¸è¯„ä¼°
        ç»“åˆå…ˆéªŒä¿¡æ¯å’Œè§‚å¯Ÿè¯æ®è®¡ç®—åéªŒå¼‚å¸¸æ¦‚ç‡
        """
        # æå–è¯æ®å¼ºåº¦
        evidence_scores = []
        
        # é¢‘æ•°å¼‚å¸¸è¯æ®
        if frequency_anomalies.get('overall_frequency_anomaly_score', 0) > 0.5:
            evidence_scores.append(frequency_anomalies['overall_frequency_anomaly_score'])
        
        # åˆ†å¸ƒå¼‚å¸¸è¯æ®
        if distribution_anomalies.get('distribution_anomaly_score', 0) > 0.5:
            evidence_scores.append(distribution_anomalies['distribution_anomaly_score'])
        
        # ä¿¡æ¯è®ºå¼‚å¸¸è¯æ®
        if information_anomalies.get('information_anomaly_score', 0) > 0.5:
            evidence_scores.append(information_anomalies['information_anomaly_score'])
        
        # æ—¶åºå¼‚å¸¸è¯æ®
        if timeseries_anomalies.get('timeseries_anomaly_score', 0) > 0.5:
            evidence_scores.append(timeseries_anomalies['timeseries_anomaly_score'])
        
        # è®¡ç®—è¯æ®å¼ºåº¦
        evidence_strength = np.mean(evidence_scores) if evidence_scores else 0.0
        
        # è´å¶æ–¯æ›´æ–°
        prior_manipulation = self.bayesian_priors['manipulation_prior']
        prior_natural = self.bayesian_priors['natural_prior']
        
        # ä¼¼ç„¶å‡½æ•°ï¼ˆç®€åŒ–æ¨¡å‹ï¼‰
        if evidence_strength > 0.7:
            likelihood_manipulation = 0.8
            likelihood_natural = 0.2
        elif evidence_strength > 0.5:
            likelihood_manipulation = 0.6
            likelihood_natural = 0.4
        else:
            likelihood_manipulation = 0.3
            likelihood_natural = 0.7
        
        # åéªŒæ¦‚ç‡
        marginal_likelihood = (likelihood_manipulation * prior_manipulation + 
                             likelihood_natural * prior_natural)
        
        posterior_manipulation = (likelihood_manipulation * prior_manipulation) / marginal_likelihood
        posterior_natural = (likelihood_natural * prior_natural) / marginal_likelihood
        
        # è´å¶æ–¯å› å­
        bayes_factor = (likelihood_manipulation / likelihood_natural) if likelihood_natural > 0 else float('inf')
        
        return {
            'evidence_strength': float(evidence_strength),
            'prior_manipulation_probability': float(prior_manipulation),
            'posterior_manipulation_probability': float(posterior_manipulation),
            'bayes_factor': float(bayes_factor) if bayes_factor != float('inf') else 999.0,
            'evidence_interpretation': self._interpret_bayes_factor(bayes_factor),
            'confidence_level': float(abs(posterior_manipulation - 0.5) * 2)  # 0-1, 1ä¸ºæœ€é«˜ç½®ä¿¡åº¦
        }
    
    def _multiscale_anomaly_analysis(self, period_data: Dict, historical_data: List[Dict]) -> Dict:
        """
        å¤šå°ºåº¦å¼‚å¸¸åˆ†æ
        åœ¨ä¸åŒæ—¶é—´å°ºåº¦ä¸Šæ£€æµ‹å¼‚å¸¸æ¨¡å¼
        """
        scales = [5, 10, 15, 20, 30]  # ä¸åŒçš„æ—¶é—´çª—å£
        multiscale_results = {}
        
        for scale in scales:
            if len(historical_data) >= scale:
                scale_data = historical_data[:scale]
                
                # åœ¨è¯¥å°ºåº¦ä¸Šçš„ç»Ÿè®¡ç‰¹å¾
                scale_stats = self._calculate_scale_statistics(scale_data)
                
                # å¼‚å¸¸è¯„åˆ†
                scale_anomaly_score = self._calculate_scale_anomaly_score(scale_stats, scale)
                
                multiscale_results[f'scale_{scale}'] = {
                    'statistics': scale_stats,
                    'anomaly_score': float(scale_anomaly_score),
                    'is_anomalous': scale_anomaly_score > 0.6
                }
        
        # è·¨å°ºåº¦ä¸€è‡´æ€§åˆ†æ
        scale_scores = [result['anomaly_score'] for result in multiscale_results.values()]
        if scale_scores:
            consistency_score = 1.0 - np.std(scale_scores) / (np.mean(scale_scores) + 0.1)
            overall_anomaly_score = np.mean(scale_scores)
        else:
            consistency_score = 0.5
            overall_anomaly_score = 0.0
        
        return {
            'scale_results': multiscale_results,
            'cross_scale_consistency': float(consistency_score),
            'overall_multiscale_anomaly_score': float(overall_anomaly_score),
            'scales_analyzed': len(multiscale_results)
        }
    
    # ========== è¾…åŠ©æ–¹æ³•å®ç° ==========
    
    def _insufficient_data_response(self) -> Dict:
        """æ•°æ®ä¸è¶³æ—¶çš„å“åº”"""
        return {
            'error': 'insufficient_data',
            'message': 'Need at least 10 periods of historical data',
            'chi_square_test': 0.0,
            'ks_test': 0.0,
            'entropy_test': 0.0,
            'anomaly_score': 0.0
        }
    
    def _calculate_frequency_anomaly_score(self, chi_square_anomaly: bool, g_anomaly: bool, 
                                         current_anomaly: bool, overdispersion: bool, 
                                         underdispersion: bool) -> float:
        """è®¡ç®—é¢‘æ•°å¼‚å¸¸ç»¼åˆè¯„åˆ†"""
        score = 0.0
        if chi_square_anomaly: score += 0.25
        if g_anomaly: score += 0.25
        if current_anomaly: score += 0.25
        if overdispersion or underdispersion: score += 0.25
        return score
    
    def _calculate_distribution_anomaly_score(self, distribution_results: Dict, overall_anomaly: bool) -> float:
        """è®¡ç®—åˆ†å¸ƒå¼‚å¸¸ç»¼åˆè¯„åˆ†"""
        anomaly_count = 0
        total_tests = 0
        
        for tail_results in distribution_results.values():
            if not tail_results.get('insufficient_data', True):
                total_tests += 3  # KS, AD, interval tests
                if tail_results['ks_test']['is_anomalous']: anomaly_count += 1
                if tail_results['ad_test']['is_anomalous']: anomaly_count += 1
                if tail_results['interval_test']['is_anomalous']: anomaly_count += 1
        
        base_score = anomaly_count / max(1, total_tests)
        overall_bonus = 0.2 if overall_anomaly else 0.0
        
        return min(1.0, base_score + overall_bonus)
    
    def _calculate_information_anomaly_score(self, info_results: List[Dict]) -> float:
        """è®¡ç®—ä¿¡æ¯è®ºå¼‚å¸¸ç»¼åˆè¯„åˆ†"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ›´å¤æ‚
        scores = []
        for result in info_results:
            if isinstance(result, dict) and 'anomaly_score' in result:
                scores.append(result['anomaly_score'])
        
        return np.mean(scores) if scores else 0.0
    
    def _calculate_timeseries_anomaly_score(self, ts_results: List[Dict]) -> float:
        """è®¡ç®—æ—¶é—´åºåˆ—å¼‚å¸¸ç»¼åˆè¯„åˆ†"""
        # ç®€åŒ–å®ç°
        scores = []
        for result in ts_results:
            if isinstance(result, dict) and 'anomaly_score' in result:
                scores.append(result['anomaly_score'])
        
        return np.mean(scores) if scores else 0.0
    
    def _calculate_composite_anomaly_score(self, all_results: List[Dict]) -> float:
        """è®¡ç®—ç»¼åˆå¼‚å¸¸è¯„åˆ†"""
        weights = [0.25, 0.2, 0.2, 0.15, 0.1, 0.1]  # å„ç±»å¼‚å¸¸çš„æƒé‡
        scores = []
        
        for i, result in enumerate(all_results):
            if isinstance(result, dict):
                if 'overall_frequency_anomaly_score' in result:
                    scores.append(result['overall_frequency_anomaly_score'])
                elif 'distribution_anomaly_score' in result:
                    scores.append(result['distribution_anomaly_score'])
                elif 'information_anomaly_score' in result:
                    scores.append(result['information_anomaly_score'])
                elif 'timeseries_anomaly_score' in result:
                    scores.append(result['timeseries_anomaly_score'])
                elif 'confidence_level' in result:
                    scores.append(result['confidence_level'])
                elif 'overall_multiscale_anomaly_score' in result:
                    scores.append(result['overall_multiscale_anomaly_score'])
                else:
                    scores.append(0.0)
            else:
                scores.append(0.0)
        
        # åŠ æƒå¹³å‡
        weighted_score = sum(score * weight for score, weight in zip(scores, weights))
        return min(1.0, max(0.0, weighted_score))
    
    def _classify_anomaly_strength(self, composite_score: float) -> str:
        """åˆ†ç±»å¼‚å¸¸å¼ºåº¦"""
        if composite_score >= 0.8:
            return 'extreme_anomaly'
        elif composite_score >= 0.6:
            return 'strong_anomaly'
        elif composite_score >= 0.4:
            return 'moderate_anomaly'
        elif composite_score >= 0.2:
            return 'weak_anomaly'
        else:
            return 'no_significant_anomaly'
    
    def _calculate_statistical_confidence(self, composite_score: float) -> float:
        """è®¡ç®—ç»Ÿè®¡ç½®ä¿¡åº¦"""
        # åŸºäºå¼‚å¸¸å¼ºåº¦çš„ç½®ä¿¡åº¦æ˜ å°„
        if composite_score >= 0.7:
            return 0.95
        elif composite_score >= 0.5:
            return 0.80
        elif composite_score >= 0.3:
            return 0.65
        else:
            return 0.50
    
    def _assess_evidence_quality(self, historical_data: List[Dict]) -> Dict:
        """è¯„ä¼°è¯æ®è´¨é‡"""
        data_size = len(historical_data)
        
        if data_size >= 100:
            quality = 'high'
            reliability = 0.9
        elif data_size >= 50:
            quality = 'good'
            reliability = 0.8
        elif data_size >= 20:
            quality = 'moderate'
            reliability = 0.65
        else:
            quality = 'low'
            reliability = 0.4
        
        return {
            'quality_level': quality,
            'reliability_score': reliability,
            'sample_size': data_size,
            'adequacy': 'sufficient' if data_size >= 30 else 'insufficient'
        }
    
    def _generate_anomaly_recommendations(self, composite_score: float) -> List[str]:
        """ç”Ÿæˆå¼‚å¸¸å¤„ç†å»ºè®®"""
        recommendations = []
        
        if composite_score >= 0.8:
            recommendations.extend([
                'å¼ºçƒˆå»ºè®®è°ƒæŸ¥æ•°æ®æºçš„å®Œæ•´æ€§',
                'è€ƒè™‘å­˜åœ¨ç³»ç»Ÿæ€§æ“æ§è¡Œä¸º',
                'å»ºè®®æš‚åœç›¸å…³æŠ•èµ„å†³ç­–',
                'éœ€è¦ä¸“ä¸šç»Ÿè®¡å­¦å®¶è¿›ä¸€æ­¥åˆ†æ'
            ])
        elif composite_score >= 0.6:
            recommendations.extend([
                'å»ºè®®è°¨æ…å¯¹å¾…å½“å‰æ•°æ®æ¨¡å¼',
                'è€ƒè™‘å¢åŠ ç›‘æ§é¢‘ç‡',
                'å»ºè®®é™ä½æŠ•èµ„é£é™©æ•å£'
            ])
        elif composite_score >= 0.4:
            recommendations.extend([
                'å»ºè®®æŒç»­ç›‘æ§æ•°æ®è¶‹åŠ¿',
                'å¯è€ƒè™‘é€‚åº¦è°ƒæ•´æŠ•èµ„ç­–ç•¥'
            ])
        else:
            recommendations.extend([
                'å½“å‰ç»Ÿè®¡ç‰¹å¾åœ¨æ­£å¸¸èŒƒå›´å†…',
                'å¯ä»¥æŒ‰æ—¢å®šç­–ç•¥æ‰§è¡Œ'
            ])
        
        return recommendations
    
    # ========== ä¿¡æ¯è®ºæ–¹æ³•å®ç° ==========
    
    def _calculate_shannon_entropy_anomalies(self, historical_data: List[Dict]) -> Dict:
        """è®¡ç®—Shannonç†µå¼‚å¸¸"""
        # è®¡ç®—æ¯æœŸçš„ç†µå€¼
        period_entropies = []
        
        for period in historical_data:
            tails = period.get('tails', [])
            if len(tails) > 1:
                # è®¡ç®—è¯¥æœŸçš„ç†µ
                tail_counts = {}
                for tail in tails:
                    tail_counts[tail] = tail_counts.get(tail, 0) + 1
                
                total_count = len(tails)
                entropy = 0.0
                for count in tail_counts.values():
                    if count > 0:
                        prob = count / total_count
                        entropy -= prob * math.log2(prob)
                
                period_entropies.append(entropy)
        
        if not period_entropies:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        mean_entropy = np.mean(period_entropies)
        std_entropy = np.std(period_entropies)
        
        # ç†µå¼‚å¸¸æ£€æµ‹
        entropy_anomalies = []
        for entropy in period_entropies[-5:]:  # æœ€è¿‘5æœŸ
            if std_entropy > 0:
                z_score = abs(entropy - mean_entropy) / std_entropy
                if z_score > 2.0:  # 2å€æ ‡å‡†å·®
                    entropy_anomalies.append(z_score)
        
        anomaly_score = min(1.0, len(entropy_anomalies) / 5.0)
        
        return {
            'mean_entropy': float(mean_entropy),
            'std_entropy': float(std_entropy),
            'recent_entropy_anomalies': len(entropy_anomalies),
            'anomaly_score': float(anomaly_score),
            'expected_entropy_range': {
                'min': float(self.entropy_thresholds['min_entropy']),
                'max': float(self.entropy_thresholds['max_entropy'])
            }
        }
    
    def _calculate_conditional_entropy_anomalies(self, historical_data: List[Dict]) -> Dict:
        """è®¡ç®—æ¡ä»¶ç†µå¼‚å¸¸"""
        if len(historical_data) < 2:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        # è®¡ç®—H(Y|X)ï¼šç»™å®šå‰ä¸€æœŸçŠ¶æ€çš„æ¡ä»¶ç†µ
        conditional_entropies = []
        
        for i in range(1, len(historical_data)):
            prev_tails = set(historical_data[i-1].get('tails', []))
            curr_tails = set(historical_data[i].get('tails', []))
            
            # ç®€åŒ–çš„æ¡ä»¶ç†µè®¡ç®—
            intersection = len(prev_tails.intersection(curr_tails))
            union = len(prev_tails.union(curr_tails))
            
            if union > 0:
                conditional_prob = intersection / union
                if conditional_prob > 0:
                    conditional_entropy = -conditional_prob * math.log2(conditional_prob) - (1 - conditional_prob) * math.log2(1 - conditional_prob + 1e-10)
                    conditional_entropies.append(conditional_entropy)
        
        if not conditional_entropies:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        mean_cond_entropy = np.mean(conditional_entropies)
        expected_range = [0.8, 1.2]  # æœŸæœ›æ¡ä»¶ç†µèŒƒå›´
        
        anomaly_score = 0.0
        if mean_cond_entropy < expected_range[0] or mean_cond_entropy > expected_range[1]:
            deviation = min(abs(mean_cond_entropy - expected_range[0]), 
                          abs(mean_cond_entropy - expected_range[1]))
            anomaly_score = min(1.0, deviation / 0.5)
        
        return {
            'mean_conditional_entropy': float(mean_cond_entropy),
            'expected_range': expected_range,
            'anomaly_score': float(anomaly_score)
        }
    
    def _calculate_mutual_information_anomalies(self, historical_data: List[Dict]) -> Dict:
        """è®¡ç®—äº’ä¿¡æ¯å¼‚å¸¸"""
        if len(historical_data) < 10:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        # è®¡ç®—å°¾æ•°é—´çš„äº’ä¿¡æ¯
        mutual_infos = []
        
        for tail_i in range(10):
            for tail_j in range(tail_i + 1, 10):
                # ç»Ÿè®¡è”åˆå‡ºç°æƒ…å†µ
                both_appear = 0
                i_appear_j_not = 0
                j_appear_i_not = 0
                neither_appear = 0
                
                for period in historical_data:
                    tails = set(period.get('tails', []))
                    i_in = tail_i in tails
                    j_in = tail_j in tails
                    
                    if i_in and j_in:
                        both_appear += 1
                    elif i_in and not j_in:
                        i_appear_j_not += 1
                    elif not i_in and j_in:
                        j_appear_i_not += 1
                    else:
                        neither_appear += 1
                
                total = len(historical_data)
                
                # è®¡ç®—äº’ä¿¡æ¯
                p_both = both_appear / total
                p_i = (both_appear + i_appear_j_not) / total
                p_j = (both_appear + j_appear_i_not) / total
                
                if p_both > 0 and p_i > 0 and p_j > 0:
                    mi = p_both * math.log2(p_both / (p_i * p_j))
                    mutual_infos.append(abs(mi))
        
        if not mutual_infos:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        mean_mi = np.mean(mutual_infos)
        max_mi = np.max(mutual_infos)
        
        # ç†è®ºä¸Šç‹¬ç«‹å˜é‡çš„äº’ä¿¡æ¯åº”è¯¥æ¥è¿‘0
        anomaly_score = min(1.0, max_mi / 0.5) if max_mi > 0.1 else 0.0
        
        return {
            'mean_mutual_information': float(mean_mi),
            'max_mutual_information': float(max_mi),
            'anomaly_score': float(anomaly_score)
        }
    
    def _calculate_kl_divergence_anomalies(self, current_tails: Set[int], historical_data: List[Dict]) -> Dict:
        """è®¡ç®—KLæ•£åº¦å¼‚å¸¸"""
        # æ„å»ºå†å²åˆ†å¸ƒ
        historical_freq = np.zeros(10)
        total_occurrences = 0
        
        for period in historical_data:
            for tail in period.get('tails', []):
                historical_freq[tail] += 1
                total_occurrences += 1
        
        if total_occurrences == 0:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        historical_prob = historical_freq / np.sum(historical_freq)
        historical_prob = np.maximum(historical_prob, 1e-10)  # é¿å…é›¶æ¦‚ç‡
        
        # æ„å»ºå½“å‰æœŸåˆ†å¸ƒ
        current_prob = np.zeros(10)
        if current_tails:
            for tail in current_tails:
                current_prob[tail] = 1.0 / len(current_tails)
        else:
            current_prob = np.ones(10) / 10.0  # å‡åŒ€åˆ†å¸ƒ
        
        current_prob = np.maximum(current_prob, 1e-10)
        
        # è®¡ç®—KLæ•£åº¦
        kl_div = np.sum(current_prob * np.log(current_prob / historical_prob))
        
        # å¼‚å¸¸è¯„åˆ†ï¼ˆKLæ•£åº¦è¶Šå¤§è¶Šå¼‚å¸¸ï¼‰
        anomaly_score = min(1.0, kl_div / 2.0)  # é™¤ä»¥2è¿›è¡Œå½’ä¸€åŒ–
        
        return {
            'kl_divergence': float(kl_div),
            'anomaly_score': float(anomaly_score),
            'historical_distribution': historical_prob.tolist(),
            'current_distribution': current_prob.tolist()
        }
    
    def _calculate_information_gain_anomalies(self, historical_data: List[Dict]) -> Dict:
        """è®¡ç®—ä¿¡æ¯å¢ç›Šå¼‚å¸¸"""
        # ç®€åŒ–å®ç°ï¼šåˆ†æå†å²ä¿¡æ¯å¢ç›Šæ¨¡å¼
        if len(historical_data) < 5:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        # è®¡ç®—æ¯æœŸç›¸å¯¹äºå‰æœŸçš„ä¿¡æ¯å¢ç›Š
        information_gains = []
        
        for i in range(1, len(historical_data)):
            prev_tails = set(historical_data[i-1].get('tails', []))
            curr_tails = set(historical_data[i].get('tails', []))
            
            # ç®€åŒ–çš„ä¿¡æ¯å¢ç›Šè®¡ç®—
            new_info = len(curr_tails - prev_tails)
            repeated_info = len(curr_tails.intersection(prev_tails))
            
            if len(curr_tails) > 0:
                info_gain = new_info / len(curr_tails)
                information_gains.append(info_gain)
        
        if not information_gains:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        mean_gain = np.mean(information_gains)
        std_gain = np.std(information_gains)
        
        # å¼‚å¸¸æ£€æµ‹ï¼šä¿¡æ¯å¢ç›Šè¿‡é«˜æˆ–è¿‡ä½
        anomaly_score = 0.0
        if std_gain > 0:
            recent_gains = information_gains[-3:]  # æœ€è¿‘3æœŸ
            for gain in recent_gains:
                z_score = abs(gain - mean_gain) / std_gain
                if z_score > 1.5:
                    anomaly_score += 0.33
        
        return {
            'mean_information_gain': float(mean_gain),
            'std_information_gain': float(std_gain),
            'anomaly_score': float(min(1.0, anomaly_score))
        }
    
    def _calculate_complexity_anomalies(self, historical_data: List[Dict]) -> Dict:
        """è®¡ç®—å¤æ‚åº¦å¼‚å¸¸"""
        # Lempel-Zivå¤æ‚åº¦è®¡ç®—
        sequence = []
        for period in historical_data:
            # å°†æ¯æœŸçš„å°¾æ•°ç»„åˆç¼–ç ä¸ºä¸€ä¸ªæ•°å­—
            tails = sorted(period.get('tails', []))
            period_code = sum(2**tail for tail in tails)  # äºŒè¿›åˆ¶ç¼–ç 
            sequence.append(period_code)
        
        if len(sequence) < 10:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        # ç®€åŒ–çš„LZå¤æ‚åº¦
        lz_complexity = self._calculate_lz_complexity(sequence)
        
        # æœŸæœ›å¤æ‚åº¦ï¼ˆç†è®ºå€¼ï¼‰
        expected_complexity = len(sequence) * 0.5
        
        # å¤æ‚åº¦åå·®
        complexity_deviation = abs(lz_complexity - expected_complexity) / expected_complexity
        anomaly_score = min(1.0, complexity_deviation)
        
        return {
            'lz_complexity': float(lz_complexity),
            'expected_complexity': float(expected_complexity),
            'complexity_deviation': float(complexity_deviation),
            'anomaly_score': float(anomaly_score)
        }
    
    def _calculate_lz_complexity(self, sequence: List[int]) -> float:
        """è®¡ç®—Lempel-Zivå¤æ‚åº¦"""
        if not sequence:
            return 0.0
        
        # ç®€åŒ–çš„LZå¤æ‚åº¦ç®—æ³•
        complexity = 1
        i = 0
        
        while i < len(sequence) - 1:
            j = i + 1
            found_match = False
            
            # å¯»æ‰¾æœ€é•¿åŒ¹é…
            for k in range(i):
                if j < len(sequence) and sequence[k] == sequence[j]:
                    # æ‰¾åˆ°åŒ¹é…ï¼Œç»§ç»­å¯»æ‰¾æ›´é•¿çš„åŒ¹é…
                    match_length = 1
                    while (j + match_length < len(sequence) and 
                           k + match_length < i + 1 and
                           sequence[k + match_length] == sequence[j + match_length]):
                        match_length += 1
                    
                    j += match_length
                    found_match = True
                    break
            
            if not found_match:
                j += 1
            
            complexity += 1
            i = j
        
        return float(complexity)
    
    # ========== æ—¶é—´åºåˆ—æ–¹æ³•å®ç° ==========
    
    def _detect_trends(self, historical_data: List[Dict]) -> Dict:
        """è¶‹åŠ¿æ£€æµ‹"""
        if len(historical_data) < self.timeseries_params['trend_detection_window']:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        trend_results = {}
        
        for tail in range(10):
            # æ„å»ºæ—¶é—´åºåˆ—
            time_series = []
            for i, period in enumerate(historical_data):
                value = 1 if tail in period.get('tails', []) else 0
                time_series.append(value)
            
            # Mann-Kendallè¶‹åŠ¿æ£€éªŒ
            mk_stat, mk_p_value = self._mann_kendall_test(time_series)
            
            # çº¿æ€§å›å½’è¶‹åŠ¿
            x = np.arange(len(time_series))
            if len(x) > 1 and np.var(time_series) > 0:
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, time_series)
                trend_strength = abs(slope) * len(time_series)
            else:
                slope, r_value, p_value, trend_strength = 0, 0, 1, 0
            
            trend_results[tail] = {
                'mann_kendall_stat': float(mk_stat),
                'mann_kendall_p_value': float(mk_p_value),
                'linear_slope': float(slope),
                'linear_r_squared': float(r_value**2),
                'linear_p_value': float(p_value),
                'trend_strength': float(trend_strength),
                'has_significant_trend': mk_p_value < 0.05 or p_value < 0.05
            }
        
        # ç»¼åˆè¶‹åŠ¿å¼‚å¸¸è¯„åˆ†
        significant_trends = sum(1 for result in trend_results.values() 
                               if result['has_significant_trend'])
        anomaly_score = min(1.0, significant_trends / 10.0)
        
        return {
            'tail_trends': trend_results,
            'significant_trends_count': significant_trends,
            'anomaly_score': float(anomaly_score)
        }
    
    def _detect_seasonality(self, historical_data: List[Dict]) -> Dict:
        """å­£èŠ‚æ€§æ£€æµ‹"""
        seasonality_results = {}
        
        for period in self.timeseries_params['seasonality_periods']:
            if len(historical_data) >= period * 2:
                # å¯¹æ¯ä¸ªå°¾æ•°æ£€æµ‹å‘¨æœŸæ€§
                tail_seasonality = {}
                
                for tail in range(10):
                    # æ„å»ºæ—¶é—´åºåˆ—
                    time_series = []
                    for period_data in historical_data:
                        value = 1 if tail in period_data.get('tails', []) else 0
                        time_series.append(value)
                    
                    # è‡ªç›¸å…³æ£€éªŒ
                    autocorr = self._calculate_autocorrelation(time_series, period)
                    
                    # å‘¨æœŸæ€§å¼ºåº¦
                    periodicity_strength = abs(autocorr)
                    is_periodic = periodicity_strength > 0.3
                    
                    tail_seasonality[tail] = {
                        'autocorrelation': float(autocorr),
                        'periodicity_strength': float(periodicity_strength),
                        'is_periodic': is_periodic
                    }
                
                # è®¡ç®—è¯¥å‘¨æœŸçš„å¼‚å¸¸è¯„åˆ†
                periodic_tails = sum(1 for result in tail_seasonality.values() 
                                   if result['is_periodic'])
                period_anomaly_score = min(1.0, periodic_tails / 10.0)
                
                seasonality_results[f'period_{period}'] = {
                    'tail_seasonality': tail_seasonality,
                    'periodic_tails_count': periodic_tails,
                    'anomaly_score': float(period_anomaly_score)
                }
        
        # ç»¼åˆå­£èŠ‚æ€§å¼‚å¸¸è¯„åˆ†
        if seasonality_results:
            overall_anomaly_score = np.mean([result['anomaly_score'] 
                                           for result in seasonality_results.values()])
        else:
            overall_anomaly_score = 0.0
        
        return {
            'seasonality_analysis': seasonality_results,
            'anomaly_score': float(overall_anomaly_score)
        }
    
    def _detect_changepoints(self, historical_data: List[Dict]) -> Dict:
        """å˜ç‚¹æ£€æµ‹"""
        if len(historical_data) < 20:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        changepoint_results = {}
        
        for tail in range(10):
            # æ„å»ºæ—¶é—´åºåˆ—
            time_series = []
            for period in historical_data:
                value = 1 if tail in period.get('tails', []) else 0
                time_series.append(value)
            
            # CUSUMå˜ç‚¹æ£€æµ‹
            changepoints = self._cusum_changepoint_detection(time_series)
            
            # æ–¹å·®å˜ç‚¹æ£€æµ‹
            variance_changepoints = self._variance_changepoint_detection(time_series)
            
            changepoint_results[tail] = {
                'cusum_changepoints': changepoints,
                'variance_changepoints': variance_changepoints,
                'total_changepoints': len(changepoints) + len(variance_changepoints)
            }
        
        # ç»¼åˆå˜ç‚¹å¼‚å¸¸è¯„åˆ†
        total_changepoints = sum(result['total_changepoints'] 
                               for result in changepoint_results.values())
        # æœŸæœ›å˜ç‚¹æ•°é‡ï¼ˆåŸºäºæ•°æ®é•¿åº¦ï¼‰
        expected_changepoints = len(historical_data) * 0.05  # 5%çš„æœŸæ•°å¯èƒ½æœ‰å˜ç‚¹
        
        if expected_changepoints > 0:
            anomaly_score = min(1.0, total_changepoints / expected_changepoints)
        else:
            anomaly_score = 0.0
        
        return {
            'changepoint_analysis': changepoint_results,
            'total_changepoints': total_changepoints,
            'expected_changepoints': float(expected_changepoints),
            'anomaly_score': float(anomaly_score)
        }
    
    def _analyze_autocorrelation(self, historical_data: List[Dict]) -> Dict:
        """è‡ªç›¸å…³åˆ†æ"""
        if len(historical_data) < 10:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        autocorr_results = {}
        
        for tail in range(10):
            # æ„å»ºæ—¶é—´åºåˆ—
            time_series = []
            for period in historical_data:
                value = 1 if tail in period.get('tails', []) else 0
                time_series.append(value)
            
            # è®¡ç®—å¤šä¸ªæ»åçš„è‡ªç›¸å…³
            lags = range(1, min(10, len(time_series) // 2))
            autocorrelations = []
            
            for lag in lags:
                autocorr = self._calculate_autocorrelation(time_series, lag)
                autocorrelations.append(autocorr)
            
            # æ£€æµ‹æ˜¾è‘—è‡ªç›¸å…³
            significant_autocorrs = [corr for corr in autocorrelations if abs(corr) > 0.2]
            
            autocorr_results[tail] = {
                'autocorrelations': [float(corr) for corr in autocorrelations],
                'significant_autocorrs_count': len(significant_autocorrs),
                'max_autocorr': float(max(autocorrelations, key=abs) if autocorrelations else 0)
            }
        
        # ç»¼åˆè‡ªç›¸å…³å¼‚å¸¸è¯„åˆ†
        total_significant = sum(result['significant_autocorrs_count'] 
                              for result in autocorr_results.values())
        anomaly_score = min(1.0, total_significant / 20.0)  # 20æ˜¯å¤§è‡´æœŸæœ›å€¼
        
        return {
            'autocorr_analysis': autocorr_results,
            'total_significant_autocorrs': total_significant,
            'anomaly_score': float(anomaly_score)
        }
    
    def _test_stationarity(self, historical_data: List[Dict]) -> Dict:
        """å¹³ç¨³æ€§æ£€éªŒ"""
        if len(historical_data) < self.timeseries_params['stationarity_window']:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        stationarity_results = {}
        
        for tail in range(10):
            # æ„å»ºæ—¶é—´åºåˆ—
            time_series = np.array([1 if tail in period.get('tails', []) else 0 
                                  for period in historical_data])
            
            # å¢å¼ºDickey-Fulleræ£€éªŒï¼ˆç®€åŒ–ç‰ˆï¼‰
            # è®¡ç®—ä¸€é˜¶å·®åˆ†
            diff_series = np.diff(time_series)
            
            # æ–¹å·®é½æ€§æ£€éªŒï¼ˆåˆ†æ®µæ–¹å·®æ¯”è¾ƒï¼‰
            mid_point = len(time_series) // 2
            first_half_var = np.var(time_series[:mid_point])
            second_half_var = np.var(time_series[mid_point:])
            
            if first_half_var > 0 and second_half_var > 0:
                variance_ratio = max(first_half_var, second_half_var) / min(first_half_var, second_half_var)
                is_stationary = variance_ratio < 2.0  # Fæ£€éªŒçš„ç®€åŒ–ç‰ˆ
            else:
                variance_ratio = 1.0
                is_stationary = True
            
            # å‡å€¼å¹³ç¨³æ€§æ£€éªŒ
            first_half_mean = np.mean(time_series[:mid_point])
            second_half_mean = np.mean(time_series[mid_point:])
            mean_difference = abs(first_half_mean - second_half_mean)
            
            stationarity_results[tail] = {
                'variance_ratio': float(variance_ratio),
                'mean_difference': float(mean_difference),
                'is_stationary': is_stationary and mean_difference < 0.1
            }
        
        # ç»¼åˆå¹³ç¨³æ€§å¼‚å¸¸è¯„åˆ†
        non_stationary_count = sum(1 for result in stationarity_results.values() 
                                 if not result['is_stationary'])
        anomaly_score = min(1.0, non_stationary_count / 10.0)
        
        return {
            'stationarity_analysis': stationarity_results,
            'non_stationary_series_count': non_stationary_count,
            'anomaly_score': float(anomaly_score)
        }
    
    def _test_heteroscedasticity(self, historical_data: List[Dict]) -> Dict:
        """å¼‚æ–¹å·®æ£€éªŒ"""
        if len(historical_data) < 15:
            return {'anomaly_score': 0.0, 'insufficient_data': True}
        
        heteroscedasticity_results = {}
        
        for tail in range(10):
            # æ„å»ºæ—¶é—´åºåˆ—
            time_series = np.array([1 if tail in period.get('tails', []) else 0 
                                  for period in historical_data])
            
            # Breusch-Paganæ£€éªŒçš„ç®€åŒ–ç‰ˆ
            # å°†æ•°æ®åˆ†ä¸ºä¸‰æ®µï¼Œæ¯”è¾ƒæ–¹å·®
            n = len(time_series)
            segment_size = n // 3
            
            segment1 = time_series[:segment_size]
            segment2 = time_series[segment_size:2*segment_size]
            segment3 = time_series[2*segment_size:]
            
            variances = []
            if len(segment1) > 1: variances.append(np.var(segment1))
            if len(segment2) > 1: variances.append(np.var(segment2))
            if len(segment3) > 1: variances.append(np.var(segment3))
            
            if len(variances) >= 2:
                variance_range = max(variances) - min(variances)
                mean_variance = np.mean(variances)
                
                if mean_variance > 0:
                    heteroscedasticity_index = variance_range / mean_variance
                    has_heteroscedasticity = heteroscedasticity_index > 1.0
                else:
                    heteroscedasticity_index = 0.0
                    has_heteroscedasticity = False
            else:
                heteroscedasticity_index = 0.0
                has_heteroscedasticity = False
            
            heteroscedasticity_results[tail] = {
                'heteroscedasticity_index': float(heteroscedasticity_index),
                'segment_variances': [float(v) for v in variances],
                'has_heteroscedasticity': has_heteroscedasticity
            }
        
        # ç»¼åˆå¼‚æ–¹å·®å¼‚å¸¸è¯„åˆ†
        heteroscedastic_count = sum(1 for result in heteroscedasticity_results.values() 
                                  if result['has_heteroscedasticity'])
        anomaly_score = min(1.0, heteroscedastic_count / 10.0)
        
        return {
            'heteroscedasticity_analysis': heteroscedasticity_results,
            'heteroscedastic_series_count': heteroscedastic_count,
            'anomaly_score': float(anomaly_score)
        }
    
    # ========== è¾…åŠ©ç»Ÿè®¡å‡½æ•° ==========
    
    def _mann_kendall_test(self, time_series: List[int]) -> Tuple[float, float]:
        """Mann-Kendallè¶‹åŠ¿æ£€éªŒ"""
        n = len(time_series)
        if n < 3:
            return 0.0, 1.0
        
        # è®¡ç®—Sç»Ÿè®¡é‡
        S = 0
        for i in range(n - 1):
            for j in range(i + 1, n):
                if time_series[j] > time_series[i]:
                    S += 1
                elif time_series[j] < time_series[i]:
                    S -= 1
        
        # è®¡ç®—æ–¹å·®
        var_S = n * (n - 1) * (2 * n + 5) / 18
        
        if var_S > 0:
            if S > 0:
                Z = (S - 1) / math.sqrt(var_S)
            elif S < 0:
                Z = (S + 1) / math.sqrt(var_S)
            else:
                Z = 0.0
            
            # è®¡ç®—på€¼ï¼ˆåŒå°¾æ£€éªŒï¼‰
            p_value = 2 * (1 - stats.norm.cdf(abs(Z)))
        else:
            Z = 0.0
            p_value = 1.0
        
        return float(Z), float(p_value)
    
    def _calculate_autocorrelation(self, time_series: List[int], lag: int) -> float:
        """è®¡ç®—è‡ªç›¸å…³ç³»æ•°"""
        if len(time_series) <= lag:
            return 0.0
        
        n = len(time_series)
        mean_val = np.mean(time_series)
        
        # è®¡ç®—è‡ªåæ–¹å·®
        autocovariance = 0.0
        for i in range(n - lag):
            autocovariance += (time_series[i] - mean_val) * (time_series[i + lag] - mean_val)
        autocovariance /= (n - lag)
        
        # è®¡ç®—æ–¹å·®
        variance = np.var(time_series)
        
        if variance > 0:
            autocorr = autocovariance / variance
        else:
            autocorr = 0.0
        
        return float(autocorr)
    
    def _cusum_changepoint_detection(self, time_series: List[int]) -> List[int]:
        """CUSUMå˜ç‚¹æ£€æµ‹"""
        if len(time_series) < 10:
            return []
        
        mean_val = np.mean(time_series)
        std_val = np.std(time_series)
        
        if std_val == 0:
            return []
        
        # CUSUMç»Ÿè®¡é‡
        cusum_pos = 0
        cusum_neg = 0
        threshold = 3 * std_val
        
        changepoints = []
        
        for i, value in enumerate(time_series):
            deviation = value - mean_val
            
            cusum_pos = max(0, cusum_pos + deviation)
            cusum_neg = min(0, cusum_neg + deviation)
            
            if abs(cusum_pos) > threshold or abs(cusum_neg) > threshold:
                changepoints.append(i)
                cusum_pos = 0
                cusum_neg = 0
        
        return changepoints
    
    def _variance_changepoint_detection(self, time_series: List[int]) -> List[int]:
        """æ–¹å·®å˜ç‚¹æ£€æµ‹"""
        if len(time_series) < 20:
            return []
        
        window_size = min(10, len(time_series) // 4)
        changepoints = []
        
        for i in range(window_size, len(time_series) - window_size):
            # å‰çª—å£å’Œåçª—å£çš„æ–¹å·®
            before_window = time_series[i-window_size:i]
            after_window = time_series[i:i+window_size]
            
            var_before = np.var(before_window) if len(before_window) > 1 else 0
            var_after = np.var(after_window) if len(after_window) > 1 else 0
            
            # Fæ£€éªŒçš„ç®€åŒ–ç‰ˆ
            if var_before > 0 and var_after > 0:
                f_ratio = max(var_before, var_after) / min(var_before, var_after)
                if f_ratio > self.timeseries_params['change_point_sensitivity'] * 10:
                    changepoints.append(i)
        
        return changepoints
    
    def _calculate_scale_statistics(self, scale_data: List[Dict]) -> Dict:
        """è®¡ç®—ç‰¹å®šå°ºåº¦çš„ç»Ÿè®¡ç‰¹å¾"""
        # å°¾æ•°é¢‘ç‡ç»Ÿè®¡
        tail_frequencies = np.zeros(10)
        total_occurrences = 0
        
        for period in scale_data:
            for tail in period.get('tails', []):
                tail_frequencies[tail] += 1
                total_occurrences += 1
        
        if total_occurrences > 0:
            tail_probabilities = tail_frequencies / total_occurrences
        else:
            tail_probabilities = np.zeros(10)
        
        # æœŸæ•°ç»Ÿè®¡
        periods_per_tail = tail_frequencies
        mean_periods = np.mean(periods_per_tail)
        std_periods = np.std(periods_per_tail)
        
        # æ¯æœŸå°¾æ•°æ•°é‡ç»Ÿè®¡
        tails_per_period = [len(period.get('tails', [])) for period in scale_data]
        mean_tails_per_period = np.mean(tails_per_period)
        std_tails_per_period = np.std(tails_per_period)
        
        return {
            'tail_probabilities': tail_probabilities.tolist(),
            'total_occurrences': int(total_occurrences),
            'mean_periods_per_tail': float(mean_periods),
            'std_periods_per_tail': float(std_periods),
            'mean_tails_per_period': float(mean_tails_per_period),
            'std_tails_per_period': float(std_tails_per_period),
            'data_points': len(scale_data)
        }
    
    def _calculate_scale_anomaly_score(self, scale_stats: Dict, scale: int) -> float:
        """è®¡ç®—ç‰¹å®šå°ºåº¦çš„å¼‚å¸¸è¯„åˆ†"""
        anomaly_components = []
        
        # 1. å°¾æ•°æ¦‚ç‡åˆ†å¸ƒå¼‚å¸¸
        tail_probs = np.array(scale_stats['tail_probabilities'])
        expected_prob = 1.0 / 10.0
        prob_deviations = np.abs(tail_probs - expected_prob)
        max_prob_deviation = np.max(prob_deviations)
        anomaly_components.append(min(1.0, max_prob_deviation / 0.3))
        
        # 2. æ–¹å·®å¼‚å¸¸
        std_periods = scale_stats['std_periods_per_tail']
        expected_std = math.sqrt(scale * 0.1 * 0.9)  # äºŒé¡¹åˆ†å¸ƒçš„æ ‡å‡†å·®
        if expected_std > 0:
            std_anomaly = abs(std_periods - expected_std) / expected_std
            anomaly_components.append(min(1.0, std_anomaly))
        
        # 3. æ¯æœŸå°¾æ•°æ•°é‡å¼‚å¸¸
        mean_tails = scale_stats['mean_tails_per_period']
        expected_tails = 5.0  # æœŸæœ›æ¯æœŸçº¦5ä¸ªå°¾æ•°
        tails_anomaly = abs(mean_tails - expected_tails) / expected_tails
        anomaly_components.append(min(1.0, tails_anomaly))
        
        return float(np.mean(anomaly_components))
    
    def _interpret_bayes_factor(self, bayes_factor: float) -> str:
        """è§£é‡Šè´å¶æ–¯å› å­"""
        if bayes_factor > 100:
            return 'decisive_evidence_for_manipulation'
        elif bayes_factor > 30:
            return 'very_strong_evidence_for_manipulation'
        elif bayes_factor > 10:
            return 'strong_evidence_for_manipulation'
        elif bayes_factor > 3:
            return 'moderate_evidence_for_manipulation'
        elif bayes_factor > 1:
            return 'weak_evidence_for_manipulation'
        elif bayes_factor > 0.33:
            return 'weak_evidence_for_natural'
        elif bayes_factor > 0.1:
            return 'moderate_evidence_for_natural'
        elif bayes_factor > 0.03:
            return 'strong_evidence_for_natural'
        else:
            return 'very_strong_evidence_for_natural'

    def _build_frequency_tensor(self, historical_context: List[Dict], current_tails: Set[int]) -> np.ndarray:
        """
        æ„å»ºå¤šç»´é¢‘ç‡å¼ é‡
        ç»´åº¦ï¼š[æ—¶é—´, å°¾æ•°, ç‰¹å¾] å…¶ä¸­ç‰¹å¾åŒ…æ‹¬å‡ºç°é¢‘ç‡ã€æ¡ä»¶æ¦‚ç‡ã€è”åˆæ¦‚ç‡ç­‰
        """
        n_periods = len(historical_context)
        n_tails = 10
        n_features = 15  # 15ä¸ªä¸åŒçš„é¢‘ç‡ç‰¹å¾
        
        frequency_tensor = np.zeros((n_periods + 1, n_tails, n_features))
        
        for t, period in enumerate(historical_context):
            period_tails = set(period.get('tails', []))
            
            for tail in range(n_tails):
                # åŸºç¡€ç‰¹å¾
                frequency_tensor[t, tail, 0] = 1.0 if tail in period_tails else 0.0  # å‡ºç°æŒ‡ç¤ºå™¨
                
                # æ¡ä»¶é¢‘ç‡ç‰¹å¾
                if t > 0:
                    prev_period_tails = set(historical_context[t-1].get('tails', []))
                    # ç»™å®šå‰ä¸€æœŸçŠ¶æ€çš„æ¡ä»¶æ¦‚ç‡
                    frequency_tensor[t, tail, 1] = self._calculate_conditional_frequency(tail, period_tails, prev_period_tails)
                
                # ç´¯ç§¯é¢‘ç‡ç‰¹å¾
                cumulative_appearances = sum(1 for i in range(t+1) if tail in historical_context[i].get('tails', []))
                frequency_tensor[t, tail, 2] = cumulative_appearances / (t + 1)
                
                # æ»‘åŠ¨çª—å£é¢‘ç‡ï¼ˆæœ€è¿‘5æœŸï¼‰
                window_start = max(0, t - 4)
                window_appearances = sum(1 for i in range(window_start, t+1) if tail in historical_context[i].get('tails', []))
                frequency_tensor[t, tail, 3] = window_appearances / min(5, t + 1)
                
                # åŠ æƒé¢‘ç‡ï¼ˆè¿‘æœŸæƒé‡æ›´é«˜ï¼‰
                weighted_sum = 0.0
                weight_sum = 0.0
                for i in range(t+1):
                    weight = np.exp(-0.1 * (t - i))  # æŒ‡æ•°è¡°å‡æƒé‡
                    appearance = 1.0 if tail in historical_context[i].get('tails', []) else 0.0
                    weighted_sum += weight * appearance
                    weight_sum += weight
                frequency_tensor[t, tail, 4] = weighted_sum / weight_sum if weight_sum > 0 else 0.0
                
                # å‘¨æœŸæ€§ç‰¹å¾
                for cycle_length in [3, 5, 7]:
                    if t >= cycle_length:
                        cycle_appearances = []
                        for i in range(t - cycle_length + 1, t + 1, cycle_length):
                            if i >= 0:
                                cycle_appearances.append(1.0 if tail in historical_context[i].get('tails', []) else 0.0)
                        feature_idx = 5 + (cycle_length - 3) // 2
                        frequency_tensor[t, tail, feature_idx] = np.mean(cycle_appearances) if cycle_appearances else 0.0
                
                # æ³¢åŠ¨æ€§ç‰¹å¾
                if t >= 9:
                    recent_10_appearances = [1.0 if tail in historical_context[i].get('tails', []) else 0.0 
                                           for i in range(max(0, t-9), t+1)]
                    frequency_tensor[t, tail, 8] = np.std(recent_10_appearances)
                
                # è¶‹åŠ¿ç‰¹å¾
                if t >= 4:
                    recent_5_frequencies = []
                    for i in range(t-4, t+1):
                        window_freq = sum(1 for j in range(max(0, i-2), i+1) 
                                        if tail in historical_context[j].get('tails', [])) / min(3, i+1)
                        recent_5_frequencies.append(window_freq)
                    
                    # çº¿æ€§è¶‹åŠ¿
                    if len(recent_5_frequencies) >= 2:
                        x = np.arange(len(recent_5_frequencies))
                        y = np.array(recent_5_frequencies)
                        if np.var(y) > 0:
                            slope, _, r_value, _, _ = stats.linregress(x, y)
                            frequency_tensor[t, tail, 9] = slope
                            frequency_tensor[t, tail, 10] = r_value ** 2
                
                # å…±ç°ç‰¹å¾
                if len(period_tails) > 1:
                    co_occurrence_score = 0.0
                    for other_tail in period_tails:
                        if other_tail != tail:
                            # è®¡ç®—ä¸å…¶ä»–å°¾æ•°çš„å†å²å…±ç°åº¦
                            co_count = 0
                            for i in range(t+1):
                                hist_tails = set(historical_context[i].get('tails', []))
                                if tail in hist_tails and other_tail in hist_tails:
                                    co_count += 1
                            co_occurrence_score += co_count / (t + 1)
                    frequency_tensor[t, tail, 11] = co_occurrence_score / (len(period_tails) - 1)
                
                # é—´éš”åˆ†å¸ƒç‰¹å¾
                intervals = self._calculate_appearance_intervals(tail, historical_context[:t+1])
                if intervals:
                    frequency_tensor[t, tail, 12] = np.mean(intervals)
                    frequency_tensor[t, tail, 13] = np.std(intervals)
                    frequency_tensor[t, tail, 14] = len(intervals)
        
        # æ·»åŠ å½“å‰æœŸé¢„æµ‹
        current_index = n_periods
        for tail in range(n_tails):
            # åŸºäºå†å²æ•°æ®é¢„æµ‹å½“å‰æœŸç‰¹å¾
            if n_periods > 0:
                # ä½¿ç”¨æœ€è¿‘æœŸçš„ç‰¹å¾ä½œä¸ºåŸºç¡€
                frequency_tensor[current_index, tail, :] = frequency_tensor[current_index-1, tail, :]
                
                # æ›´æ–°åŸºç¡€å‡ºç°æŒ‡ç¤ºå™¨
                frequency_tensor[current_index, tail, 0] = 1.0 if tail in current_tails else 0.0
                
                # é‡æ–°è®¡ç®—ç´¯ç§¯ç‰¹å¾
                total_appearances = sum(1 for period in historical_context if tail in period.get('tails', []))
                if tail in current_tails:
                    total_appearances += 1
                frequency_tensor[current_index, tail, 2] = total_appearances / (n_periods + 1)
        
        return frequency_tensor

    def _calculate_conditional_frequency(self, tail: int, current_tails: Set[int], prev_tails: Set[int]) -> float:
        """è®¡ç®—æ¡ä»¶é¢‘ç‡ P(tail_current | tail_prev_state)"""
        if not prev_tails:
            return 0.5  # æ— æ¡ä»¶æ¦‚ç‡
        
        # åŸºäºå‰ä¸€æœŸçŠ¶æ€è®¡ç®—æ¡ä»¶æ¦‚ç‡
        if tail in current_tails:
            # å½“å‰å°¾æ•°å‡ºç°
            if any(prev_tail in prev_tails for prev_tail in range(10)):
                return 0.8  # æœ‰æ¡ä»¶çš„é«˜æ¦‚ç‡
            else:
                return 0.3  # æ— æ¡ä»¶çš„ä½æ¦‚ç‡
        else:
            # å½“å‰å°¾æ•°æœªå‡ºç°
            return 0.2 if prev_tails else 0.5

    def _calculate_appearance_intervals(self, tail: int, periods: List[Dict]) -> List[int]:
        """è®¡ç®—å°¾æ•°å‡ºç°é—´éš”"""
        appearances = []
        for i, period in enumerate(periods):
            if tail in period.get('tails', []):
                appearances.append(i)
        
        if len(appearances) < 2:
            return []
        
        intervals = [appearances[i+1] - appearances[i] for i in range(len(appearances)-1)]
        return intervals

    def _perform_time_series_decomposition(self, historical_context: List[Dict]) -> Dict:
        """
        æ—¶é—´åºåˆ—åˆ†è§£åˆ†æ
        ä½¿ç”¨STLåˆ†è§£(Seasonal and Trend decomposition using Loess)è¿›è¡Œç§‘ç ”çº§æ—¶é—´åºåˆ—åˆ†è§£
        """
        if len(historical_context) < 20:
            return {'error': 'insufficient_data_for_decomposition'}
        
        decomposition_results = {}
        
        for tail in range(10):
            # æ„å»ºæ—¶é—´åºåˆ—
            time_series = []
            for period in historical_context:
                time_series.append(1.0 if tail in period.get('tails', []) else 0.0)
            
            time_series = np.array(time_series)
            
            # STLåˆ†è§£çš„ç®€åŒ–å®ç°
            # 1. è¶‹åŠ¿æå–ï¼ˆä½¿ç”¨ç§»åŠ¨å¹³å‡ï¼‰
            window_size = min(7, len(time_series) // 3)
            trend = np.convolve(time_series, np.ones(window_size)/window_size, mode='same')
            
            # 2. å»è¶‹åŠ¿
            detrended = time_series - trend
            
            # 3. å­£èŠ‚æ€§æå–ï¼ˆå‡è®¾å‘¨æœŸä¸º7ï¼‰
            if len(time_series) >= 14:
                seasonal_period = 7
                seasonal = np.zeros_like(time_series)
                for i in range(len(time_series)):
                    seasonal_values = []
                    for j in range(i % seasonal_period, len(time_series), seasonal_period):
                        seasonal_values.append(detrended[j])
                    seasonal[i] = np.median(seasonal_values) if seasonal_values else 0.0
            else:
                seasonal = np.zeros_like(time_series)
            
            # 4. æ®‹å·®
            residual = time_series - trend - seasonal
            
            # 5. è®¡ç®—åˆ†è§£è´¨é‡æŒ‡æ ‡
            total_var = np.var(time_series)
            trend_var = np.var(trend)
            seasonal_var = np.var(seasonal)
            residual_var = np.var(residual)
            
            decomposition_results[tail] = {
                'trend': trend.tolist(),
                'seasonal': seasonal.tolist(),
                'residual': residual.tolist(),
                'trend_strength': trend_var / total_var if total_var > 0 else 0,
                'seasonal_strength': seasonal_var / total_var if total_var > 0 else 0,
                'residual_strength': residual_var / total_var if total_var > 0 else 0,
                'decomposition_quality': 1.0 - (residual_var / total_var) if total_var > 0 else 0
            }
        
        return decomposition_results

    def _analyze_markov_chain_frequencies(self, historical_context: List[Dict], current_tails: Set[int]) -> Dict:
        """
        é©¬å°”å¯å¤«é“¾é¢‘ç‡åˆ†æ
        æ„å»ºé«˜é˜¶é©¬å°”å¯å¤«é“¾æ¨¡å‹åˆ†æå°¾æ•°å‡ºç°çš„çŠ¶æ€è½¬ç§»è§„å¾‹
        """
        if len(historical_context) < 10:
            return {'error': 'insufficient_data_for_markov_analysis'}
        
        markov_results = {}
        
        # å¤šé˜¶é©¬å°”å¯å¤«é“¾åˆ†æ
        for order in [1, 2, 3]:
            if len(historical_context) > order:
                transition_analysis = self._build_higher_order_markov_chain(historical_context, order)
                markov_results[f'order_{order}'] = transition_analysis
        
        # å½“å‰çŠ¶æ€å¼‚å¸¸åº¦åˆ†æ
        current_state_analysis = self._analyze_current_state_markov_anomaly(
            historical_context, current_tails, markov_results
        )
        
        markov_results['current_state_anomaly'] = current_state_analysis
        
        return markov_results

    def _build_higher_order_markov_chain(self, historical_context: List[Dict], order: int) -> Dict:
        """æ„å»ºé«˜é˜¶é©¬å°”å¯å¤«é“¾"""
        n_states = 2 ** 10  # 2^10 = 1024 ç§å¯èƒ½çš„å°¾æ•°ç»„åˆçŠ¶æ€
        
        # çŠ¶æ€ç¼–ç ï¼šå°†å°¾æ•°é›†åˆè½¬æ¢ä¸ºäºŒè¿›åˆ¶çŠ¶æ€
        def encode_state(tails_set):
            state = 0
            for tail in tails_set:
                state |= (1 << tail)
            return state
        
        # æ„å»ºçŠ¶æ€åºåˆ—
        state_sequence = []
        for period in historical_context:
            state = encode_state(set(period.get('tails', [])))
            state_sequence.append(state)
        
        # æ„å»ºè½¬ç§»çŸ©é˜µï¼ˆä½¿ç”¨ç¨€ç–è¡¨ç¤ºï¼‰
        transitions = defaultdict(lambda: defaultdict(int))
        
        for i in range(order, len(state_sequence)):
            # æ„å»ºå½“å‰çŠ¶æ€ï¼ˆåŸºäºå‰orderä¸ªçŠ¶æ€ï¼‰
            prev_states = tuple(state_sequence[i-order:i])
            current_state = state_sequence[i]
            
            transitions[prev_states][current_state] += 1
        
        # è½¬æ¢ä¸ºæ¦‚ç‡
        transition_probs = {}
        for prev_states, next_states in transitions.items():
            total = sum(next_states.values())
            if total > 0:
                transition_probs[prev_states] = {
                    next_state: count / total 
                    for next_state, count in next_states.items()
                }
        
        # è®¡ç®—é©¬å°”å¯å¤«é“¾æ€§è´¨
        entropy = self._calculate_markov_chain_entropy(transition_probs)
        mixing_time = self._estimate_mixing_time(transition_probs, n_states)
        stationary_dist = self._compute_stationary_distribution(transitions, n_states)
        
        return {
            'order': order,
            'transition_probabilities': dict(transition_probs),
            'entropy': entropy,
            'mixing_time': mixing_time,
            'stationary_distribution': stationary_dist,
            'num_observed_transitions': len(transitions)
        }

    def _calculate_markov_chain_entropy(self, transition_probs: Dict) -> float:
        """è®¡ç®—é©¬å°”å¯å¤«é“¾ç†µ"""
        total_entropy = 0.0
        total_states = 0
        
        for prev_states, next_probs in transition_probs.items():
            state_entropy = 0.0
            for prob in next_probs.values():
                if prob > 0:
                    state_entropy -= prob * math.log2(prob)
            
            total_entropy += state_entropy
            total_states += 1
        
        return total_entropy / total_states if total_states > 0 else 0.0

    def _estimate_mixing_time(self, transition_probs: Dict, n_states: int) -> float:
        """ä¼°è®¡é©¬å°”å¯å¤«é“¾æ··åˆæ—¶é—´"""
        # ç®€åŒ–ä¼°è®¡ï¼šåŸºäºè½¬ç§»æ¦‚ç‡çš„ä¸å‡åŒ€æ€§
        max_prob_deviation = 0.0
        uniform_prob = 1.0 / n_states
        
        for next_probs in transition_probs.values():
            for prob in next_probs.values():
                deviation = abs(prob - uniform_prob)
                max_prob_deviation = max(max_prob_deviation, deviation)
        
        # æ··åˆæ—¶é—´ä¸æœ€å¤§æ¦‚ç‡åå·®æˆåæ¯”
        if max_prob_deviation > 0:
            estimated_mixing_time = -math.log(0.25) / math.log(1 - max_prob_deviation)
        else:
            estimated_mixing_time = 1.0
        
        return min(100.0, estimated_mixing_time)  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…

    def _compute_stationary_distribution(self, transitions: Dict, n_states: int) -> Dict:
        """è®¡ç®—å¹³ç¨³åˆ†å¸ƒ"""
        # ç®€åŒ–è®¡ç®—ï¼šä½¿ç”¨é•¿æœŸé¢‘ç‡ä½œä¸ºå¹³ç¨³åˆ†å¸ƒçš„è¿‘ä¼¼
        state_counts = defaultdict(int)
        
        for prev_states, next_states in transitions.items():
            for next_state, count in next_states.items():
                state_counts[next_state] += count
        
        total_count = sum(state_counts.values())
        
        if total_count > 0:
            stationary_dist = {
                state: count / total_count 
                for state, count in state_counts.items()
            }
        else:
            stationary_dist = {}
        
        return stationary_dist

    def _analyze_current_state_markov_anomaly(self, historical_context: List[Dict], 
                                            current_tails: Set[int], markov_results: Dict) -> Dict:
        """åˆ†æå½“å‰çŠ¶æ€çš„é©¬å°”å¯å¤«å¼‚å¸¸åº¦"""
        anomaly_scores = []
        
        # ç¼–ç å½“å‰çŠ¶æ€
        current_state = 0
        for tail in current_tails:
            current_state |= (1 << tail)
        
        for order_key, markov_data in markov_results.items():
            if order_key.startswith('order_') and isinstance(markov_data, dict):
                order = int(order_key.split('_')[1])
                transition_probs = markov_data.get('transition_probabilities', {})
                
                if len(historical_context) >= order:
                    # æ„å»ºå‰åºçŠ¶æ€
                    prev_states = []
                    for i in range(order):
                        period = historical_context[-(i+1)]
                        state = 0
                        for tail in period.get('tails', []):
                            state |= (1 << tail)
                        prev_states.append(state)
                    
                    prev_states_tuple = tuple(reversed(prev_states))
                    
                    # æŸ¥æ‰¾è½¬ç§»æ¦‚ç‡
                    if prev_states_tuple in transition_probs:
                        expected_prob = transition_probs[prev_states_tuple].get(current_state, 0.0)
                        
                        # å¼‚å¸¸åº¦ = 1 - è½¬ç§»æ¦‚ç‡
                        anomaly_score = 1.0 - expected_prob
                        anomaly_scores.append({
                            'order': order,
                            'expected_probability': expected_prob,
                            'anomaly_score': anomaly_score
                        })
        
        if anomaly_scores:
            overall_anomaly = np.mean([score['anomaly_score'] for score in anomaly_scores])
        else:
            overall_anomaly = 0.5
        
        return {
            'overall_anomaly_score': overall_anomaly,
            'order_specific_anomalies': anomaly_scores,
            'current_state_encoding': current_state
        }
class BehaviorPatternMatcher:
    """
    ç§‘ç ”çº§è¡Œä¸ºæ¨¡å¼åŒ¹é…å™¨
    åŸºäºå›¾è®ºã€åŠ¨æ€æ—¶é—´è§„æ•´ã€éšé©¬å°”å¯å¤«æ¨¡å‹çš„æ™ºèƒ½æ¨¡å¼è¯†åˆ«ç³»ç»Ÿ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–è¡Œä¸ºæ¨¡å¼åŒ¹é…å™¨"""
        # æ¨¡å¼åŒ¹é…å‚æ•°
        self.matching_params = {
            'dtw_window_size': 0.1,      # DTWçª—å£å¤§å°
            'similarity_threshold': 0.7,  # ç›¸ä¼¼åº¦é˜ˆå€¼
            'pattern_min_length': 3,      # æœ€å°æ¨¡å¼é•¿åº¦
            'pattern_max_length': 15,     # æœ€å¤§æ¨¡å¼é•¿åº¦
            'fuzzy_tolerance': 0.15,      # æ¨¡ç³ŠåŒ¹é…å®¹å¿åº¦
            'temporal_weight': 0.6,       # æ—¶é—´æƒé‡
            'structural_weight': 0.4      # ç»“æ„æƒé‡
        }
        
        # å›¾è®ºå‚æ•°
        self.graph_params = {
            'node_similarity_threshold': 0.8,
            'edge_weight_threshold': 0.5,
            'subgraph_min_size': 3,
            'community_detection_resolution': 1.0
        }
        
        # HMMå‚æ•°
        self.hmm_params = {
            'n_components': 5,        # éšè—çŠ¶æ€æ•°
            'covariance_type': 'full',
            'n_iter': 100,
            'convergence_threshold': 1e-4
        }
        
        # æ¨¡å¼åº“
        self.pattern_library = {
            'sequential_patterns': {},    # åºåˆ—æ¨¡å¼
            'cyclic_patterns': {},       # å¾ªç¯æ¨¡å¼
            'hierarchical_patterns': {}, # å±‚æ¬¡æ¨¡å¼
            'anomaly_patterns': {},      # å¼‚å¸¸æ¨¡å¼
            'transition_patterns': {}    # è½¬ç§»æ¨¡å¼
        }
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'patterns_discovered': 0,
            'successful_matches': 0,
            'failed_matches': 0,
            'pattern_evolution_events': 0,
            'last_update_time': None
        }

    def find_matching_patterns(self, period_data: Dict, behavior_patterns: Dict, 
                             historical_context: List[Dict]) -> Dict:
        """
        ç»¼åˆæ¨¡å¼åŒ¹é…ä¸»å‡½æ•°
        ä½¿ç”¨å¤šç§ç§‘å­¦æ–¹æ³•è¿›è¡Œæ¨¡å¼è¯†åˆ«å’ŒåŒ¹é…
        
        Args:
            period_data: å½“æœŸæ•°æ®
            behavior_patterns: å·²çŸ¥è¡Œä¸ºæ¨¡å¼
            historical_context: å†å²ä¸Šä¸‹æ–‡
            
        Returns:
            ç»¼åˆæ¨¡å¼åŒ¹é…ç»“æœ
        """
        if len(historical_context) < self.matching_params['pattern_min_length']:
            return self._insufficient_data_response()
        
        # 1. åºåˆ—æ¨¡å¼åŒ¹é…
        sequential_matches = self._match_sequential_patterns(period_data, historical_context)
        
        # 2. åŠ¨æ€æ—¶é—´è§„æ•´åŒ¹é…
        dtw_matches = self._dtw_pattern_matching(period_data, historical_context)
        
        # 3. å›¾ç»“æ„æ¨¡å¼åŒ¹é…
        graph_matches = self._graph_pattern_matching(period_data, historical_context)
        
        # 4. éšé©¬å°”å¯å¤«æ¨¡å‹åŒ¹é…
        hmm_matches = self._hmm_pattern_matching(period_data, historical_context)
        
        # 5. æ¨¡ç³Šæ¨¡å¼åŒ¹é…
        fuzzy_matches = self._fuzzy_pattern_matching(period_data, historical_context)
        
        # 6. å±‚æ¬¡æ¨¡å¼åŒ¹é…
        hierarchical_matches = self._hierarchical_pattern_matching(period_data, historical_context)
        
        # 7. é¢‘ç¹å­åºåˆ—æ¨¡å¼åŒ¹é…
        frequent_pattern_matches = self._frequent_subsequence_matching(period_data, historical_context)
        
        # 8. ç»¼åˆç›¸ä¼¼åº¦è®¡ç®—
        similarity_scores = self._calculate_comprehensive_similarity(
            sequential_matches, dtw_matches, graph_matches, hmm_matches,
            fuzzy_matches, hierarchical_matches, frequent_pattern_matches
        )
        
        # 9. æ¨¡å¼ç½®ä¿¡åº¦è¯„ä¼°
        confidence_scores = self._assess_pattern_confidence(
            similarity_scores, historical_context
        )
        
        # 10. è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ 
        learned_patterns = self._adaptive_pattern_learning(
            period_data, historical_context, similarity_scores
        )
        
        return {
            'sequential_matches': sequential_matches,
            'dtw_matches': dtw_matches,
            'graph_matches': graph_matches,
            'hmm_matches': hmm_matches,
            'fuzzy_matches': fuzzy_matches,
            'hierarchical_matches': hierarchical_matches,
            'frequent_pattern_matches': frequent_pattern_matches,
            'similarity_scores': similarity_scores,
            'confidence_scores': confidence_scores,
            'learned_patterns': learned_patterns,
            'matched_patterns': self._extract_best_matches(similarity_scores, confidence_scores),
            'matching_quality': self._assess_matching_quality(similarity_scores, confidence_scores),
            'pattern_insights': self._generate_pattern_insights(similarity_scores, historical_context)
        }
    
    def _match_sequential_patterns(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        åºåˆ—æ¨¡å¼åŒ¹é…
        ä½¿ç”¨åŠ¨æ€è§„åˆ’å’Œæœ€é•¿å…¬å…±å­åºåˆ—ç®—æ³•
        """
        current_tails = set(period_data.get('tails', []))
        sequential_results = {}
        
        # æ„å»ºå°¾æ•°åºåˆ—
        tail_sequences = {}
        for tail in range(10):
            sequence = []
            for period in historical_context:
                sequence.append(1 if tail in period.get('tails', []) else 0)
            tail_sequences[tail] = sequence
        
        # å¯¹æ¯ä¸ªå°¾æ•°è¿›è¡Œåºåˆ—æ¨¡å¼åˆ†æ
        for tail in range(10):
            sequence = tail_sequences[tail]
            
            # 1. æŸ¥æ‰¾é‡å¤å­åºåˆ—
            repeated_patterns = self._find_repeated_subsequences(sequence)
            
            # 2. è®¡ç®—åºåˆ—ç†µå’Œå¤æ‚åº¦
            sequence_entropy = self._calculate_sequence_entropy(sequence)
            sequence_complexity = self._calculate_sequence_complexity(sequence)
            
            # 3. æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼
            periodic_patterns = self._detect_periodic_patterns(sequence)
            
            # 4. åˆ†æçŠ¶æ€è½¬ç§»æ¨¡å¼
            transition_patterns = self._analyze_state_transitions(sequence)
            
            # 5. å½“å‰çŠ¶æ€åŒ¹é…åº¦è¯„ä¼°
            current_state = 1 if tail in current_tails else 0
            state_match_score = self._calculate_state_match_score(
                current_state, sequence, repeated_patterns, periodic_patterns
            )
            
            sequential_results[tail] = {
                'repeated_patterns': repeated_patterns,
                'sequence_entropy': float(sequence_entropy),
                'sequence_complexity': float(sequence_complexity),
                'periodic_patterns': periodic_patterns,
                'transition_patterns': transition_patterns,
                'state_match_score': float(state_match_score),
                'sequence_length': len(sequence)
            }
        
        # ç»¼åˆåºåˆ—åŒ¹é…è¯„åˆ†
        overall_match_score = np.mean([result['state_match_score'] 
                                     for result in sequential_results.values()])
        
        return {
            'tail_sequential_analysis': sequential_results,
            'overall_sequential_match_score': float(overall_match_score),
            'high_confidence_matches': self._identify_high_confidence_sequential_matches(sequential_results)
        }
    
    def _dtw_pattern_matching(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        åŠ¨æ€æ—¶é—´è§„æ•´(DTW)æ¨¡å¼åŒ¹é…
        ç”¨äºå¤„ç†æ—¶åºæ¨¡å¼çš„æ—¶é—´ä¼¸ç¼©å’Œå˜å½¢
        """
        current_tails = set(period_data.get('tails', []))
        dtw_results = {}
        
        # æ„å»ºå¤šç»´æ—¶é—´åºåˆ—
        time_series_matrix = []
        for period in historical_context:
            period_vector = [1 if tail in period.get('tails', []) else 0 for tail in range(10)]
            time_series_matrix.append(period_vector)
        
        time_series_matrix = np.array(time_series_matrix)
        
        if len(time_series_matrix) < 2:
            return {'dtw_matches': [], 'overall_dtw_score': 0.0}
        
        # å½“å‰æœŸå‘é‡
        current_vector = np.array([1 if tail in current_tails else 0 for tail in range(10)])
        
        # æ»‘åŠ¨çª—å£DTWåŒ¹é…
        window_sizes = [5, 8, 12, 15]
        dtw_matches = []
        
        for window_size in window_sizes:
            if len(time_series_matrix) >= window_size:
                # å¯¹æ¯ä¸ªå¯èƒ½çš„çª—å£è¿›è¡ŒDTWè®¡ç®—
                for start_idx in range(len(time_series_matrix) - window_size + 1):
                    window_data = time_series_matrix[start_idx:start_idx + window_size]
                    
                    # è®¡ç®—DTWè·ç¦»
                    dtw_distance = self._calculate_dtw_distance(
                        window_data[-1:],  # çª—å£æœ€åä¸€ä¸ªå‘é‡
                        current_vector.reshape(1, -1)  # å½“å‰å‘é‡
                    )
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
                    similarity = 1.0 / (1.0 + dtw_distance)
                    
                    if similarity > self.matching_params['similarity_threshold']:
                        dtw_matches.append({
                            'window_size': window_size,
                            'start_index': start_idx,
                            'dtw_distance': float(dtw_distance),
                            'similarity': float(similarity),
                            'pattern_data': window_data.tolist(),
                            'match_confidence': self._calculate_dtw_match_confidence(
                                dtw_distance, window_size, similarity
                            )
                        })
        
        # åºåˆ—ç›¸ä¼¼æ€§åŒ¹é…
        sequence_similarities = {}
        for tail in range(10):
            tail_series = time_series_matrix[:, tail]
            
            # å¯»æ‰¾ä¸å½“å‰çŠ¶æ€ç›¸ä¼¼çš„å†å²åºåˆ—æ®µ
            current_tail_state = current_vector[tail]
            similar_segments = self._find_similar_dtw_segments(
                tail_series, current_tail_state, window_sizes
            )
            
            sequence_similarities[tail] = similar_segments
        
        # å¤šå…ƒDTWåˆ†æ
        multivariate_dtw_results = self._multivariate_dtw_analysis(
            time_series_matrix, current_vector
        )
        
        return {
            'dtw_matches': dtw_matches,
            'sequence_similarities': sequence_similarities,
            'multivariate_analysis': multivariate_dtw_results,
            'overall_dtw_score': float(np.mean([match['similarity'] for match in dtw_matches]) if dtw_matches else 0.0),
            'best_dtw_match': max(dtw_matches, key=lambda x: x['similarity']) if dtw_matches else None
        }
    
    def _graph_pattern_matching(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        å›¾ç»“æ„æ¨¡å¼åŒ¹é…
        å°†å°¾æ•°å…³ç³»å»ºæ¨¡ä¸ºå›¾ï¼Œè¿›è¡Œå­å›¾åŒ¹é…å’Œå›¾åŒæ„æ£€æµ‹
        """
        current_tails = set(period_data.get('tails', []))
        
        # 1. æ„å»ºå†å²å…³ç³»å›¾
        historical_graphs = self._build_historical_relationship_graphs(historical_context)
        
        # 2. æ„å»ºå½“å‰æœŸå…³ç³»å›¾
        current_graph = self._build_current_relationship_graph(current_tails, historical_context)
        
        # 3. å­å›¾åŒæ„åŒ¹é…
        isomorphic_matches = self._find_isomorphic_subgraphs(
            current_graph, historical_graphs
        )
        
        # 4. å›¾ç¼–è¾‘è·ç¦»è®¡ç®—
        edit_distances = self._calculate_graph_edit_distances(
            current_graph, historical_graphs
        )
        
        # 5. å›¾æ ¸åŒ¹é…
        graph_kernel_similarities = self._calculate_graph_kernel_similarities(
            current_graph, historical_graphs
        )
        
        # 6. ç¤¾åŒºæ£€æµ‹å’Œæ¨¡å¼è¯†åˆ«
        community_patterns = self._detect_community_patterns(
            historical_graphs, current_graph
        )
        
        # 7. å›¾è°±ç›¸ä¼¼æ€§åˆ†æ
        spectral_similarities = self._calculate_spectral_similarities(
            current_graph, historical_graphs
        )
        
        return {
            'historical_graphs': len(historical_graphs),
            'current_graph_properties': self._calculate_graph_properties(current_graph),
            'isomorphic_matches': isomorphic_matches,
            'edit_distances': edit_distances,
            'kernel_similarities': graph_kernel_similarities,
            'community_patterns': community_patterns,
            'spectral_similarities': spectral_similarities,
            'overall_graph_match_score': self._calculate_overall_graph_match_score(
                isomorphic_matches, edit_distances, graph_kernel_similarities, spectral_similarities
            )
        }
    
    def _hmm_pattern_matching(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        éšé©¬å°”å¯å¤«æ¨¡å‹(HMM)æ¨¡å¼åŒ¹é…
        å»ºæ¨¡éšè—çš„æ“æ§çŠ¶æ€å’Œè§‚å¯Ÿåˆ°çš„å°¾æ•°æ¨¡å¼
        """
        if len(historical_context) < 10:
            return {'hmm_matches': [], 'overall_hmm_score': 0.0}
        
        current_tails = set(period_data.get('tails', []))
        
        # 1. æ„å»ºè§‚å¯Ÿåºåˆ—
        observation_sequences = self._build_hmm_observation_sequences(historical_context)
        
        # 2. çŠ¶æ€ç©ºé—´å®šä¹‰
        hidden_states = self._define_hidden_states()
        
        # 3. HMMå‚æ•°ä¼°è®¡
        hmm_parameters = self._estimate_hmm_parameters(
            observation_sequences, hidden_states
        )
        
        # 4. çŠ¶æ€åºåˆ—é¢„æµ‹
        most_likely_states = self._predict_state_sequence(
            hmm_parameters, observation_sequences
        )
        
        # 5. å½“å‰è§‚å¯Ÿçš„ä¼¼ç„¶æ€§è®¡ç®—
        current_observation = self._encode_current_observation(current_tails)
        observation_likelihood = self._calculate_observation_likelihood(
            current_observation, hmm_parameters, most_likely_states
        )
        
        # 6. çŠ¶æ€è½¬ç§»æ¨¡å¼åˆ†æ
        transition_patterns = self._analyze_hmm_transition_patterns(
            hmm_parameters, most_likely_states
        )
        
        # 7. å¼‚å¸¸çŠ¶æ€æ£€æµ‹
        anomalous_states = self._detect_anomalous_hmm_states(
            most_likely_states, transition_patterns, observation_likelihood
        )
        
        return {
            'hmm_parameters': hmm_parameters,
            'most_likely_states': most_likely_states[-10:] if len(most_likely_states) > 10 else most_likely_states,
            'current_observation_likelihood': float(observation_likelihood),
            'transition_patterns': transition_patterns,
            'anomalous_states': anomalous_states,
            'overall_hmm_score': float(min(1.0, observation_likelihood)),
            'state_prediction_confidence': self._calculate_hmm_prediction_confidence(
                hmm_parameters, most_likely_states, observation_likelihood
            )
        }
    
    def _fuzzy_pattern_matching(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        æ¨¡ç³Šæ¨¡å¼åŒ¹é…
        å¤„ç†ä¸ç¡®å®šæ€§å’Œè¿‘ä¼¼åŒ¹é…
        """
        current_tails = set(period_data.get('tails', []))
        fuzzy_results = {}
        
        # 1. æ„å»ºæ¨¡ç³Šé›†åˆ
        fuzzy_sets = self._build_fuzzy_tail_sets(historical_context)
        
        # 2. æ¨¡ç³Šç›¸ä¼¼åº¦è®¡ç®—
        fuzzy_similarities = {}
        for tail in range(10):
            current_membership = 1.0 if tail in current_tails else 0.0
            historical_membership = fuzzy_sets.get(tail, 0.5)
            
            # ä½¿ç”¨å¤šç§æ¨¡ç³Šç›¸ä¼¼åº¦åº¦é‡
            fuzzy_similarities[tail] = {
                'cosine_similarity': self._fuzzy_cosine_similarity(current_membership, historical_membership),
                'jaccard_similarity': self._fuzzy_jaccard_similarity(current_membership, historical_membership),
                'dice_similarity': self._fuzzy_dice_similarity(current_membership, historical_membership),
                'hamming_distance': self._fuzzy_hamming_distance(current_membership, historical_membership)
            }
        
        # 3. æ¨¡ç³Šè§„åˆ™åŒ¹é…
        fuzzy_rules = self._generate_fuzzy_rules(historical_context)
        rule_matches = self._evaluate_fuzzy_rules(current_tails, fuzzy_rules)
        
        # 4. æ¨¡ç³Šèšç±»åˆ†æ
        fuzzy_clusters = self._fuzzy_clustering_analysis(historical_context, current_tails)
        
        # 5. æ¨¡ç³Šæ—¶é—´åºåˆ—åŒ¹é…
        fuzzy_timeseries_matches = self._fuzzy_timeseries_matching(
            historical_context, current_tails
        )
        
        return {
            'fuzzy_sets': {str(k): float(v) for k, v in fuzzy_sets.items()},
            'fuzzy_similarities': fuzzy_similarities,
            'rule_matches': rule_matches,
            'fuzzy_clusters': fuzzy_clusters,
            'timeseries_matches': fuzzy_timeseries_matches,
            'overall_fuzzy_score': self._calculate_overall_fuzzy_score(
                fuzzy_similarities, rule_matches, fuzzy_clusters
            )
        }
    
    def _hierarchical_pattern_matching(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        å±‚æ¬¡æ¨¡å¼åŒ¹é…
        è¯†åˆ«å¤šå±‚æ¬¡çš„æ¨¡å¼ç»“æ„
        """
        current_tails = set(period_data.get('tails', []))
        
        # 1. æ„å»ºæ¨¡å¼å±‚æ¬¡ç»“æ„
        pattern_hierarchy = self._build_pattern_hierarchy(historical_context)
        
        # 2. å¤šå°ºåº¦æ¨¡å¼åŒ¹é…
        multiscale_matches = {}
        scales = [3, 5, 8, 12, 20]
        
        for scale in scales:
            if len(historical_context) >= scale:
                scale_patterns = self._extract_scale_patterns(historical_context, scale)
                scale_match = self._match_patterns_at_scale(current_tails, scale_patterns, scale)
                multiscale_matches[f'scale_{scale}'] = scale_match
        
        # 3. å±‚æ¬¡èšç±»åˆ†æ
        hierarchical_clusters = self._hierarchical_clustering_analysis(
            historical_context, current_tails
        )
        
        # 4. æ ‘ç»“æ„æ¨¡å¼åŒ¹é…
        tree_structure_matches = self._tree_structure_pattern_matching(
            pattern_hierarchy, current_tails
        )
        
        # 5. åˆ†å½¢æ¨¡å¼æ£€æµ‹
        fractal_patterns = self._detect_fractal_patterns(historical_context, current_tails)
        
        return {
            'pattern_hierarchy': pattern_hierarchy,
            'multiscale_matches': multiscale_matches,
            'hierarchical_clusters': hierarchical_clusters,
            'tree_structure_matches': tree_structure_matches,
            'fractal_patterns': fractal_patterns,
            'overall_hierarchical_score': self._calculate_hierarchical_match_score(
                multiscale_matches, hierarchical_clusters, tree_structure_matches
            )
        }
    
    def _frequent_subsequence_matching(self, period_data: Dict, historical_context: List[Dict]) -> Dict:
        """
        é¢‘ç¹å­åºåˆ—æ¨¡å¼åŒ¹é…
        åŸºäºAprioriç®—æ³•å’Œåºåˆ—æŒ–æ˜
        """
        current_tails = set(period_data.get('tails', []))
        
        # 1. æ„å»ºäº‹åŠ¡æ•°æ®åº“
        transaction_database = []
        for period in historical_context:
            transaction = sorted(period.get('tails', []))
            transaction_database.append(transaction)
        
        # 2. é¢‘ç¹é¡¹é›†æŒ–æ˜
        frequent_itemsets = self._mine_frequent_itemsets(
            transaction_database, min_support=0.1
        )
        
        # 3. å…³è”è§„åˆ™æŒ–æ˜
        association_rules = self._mine_association_rules(
            frequent_itemsets, min_confidence=0.6
        )
        
        # 4. åºåˆ—æ¨¡å¼æŒ–æ˜
        sequential_patterns = self._mine_sequential_patterns(
            transaction_database, min_support=0.15
        )
        
        # 5. å½“å‰æœŸä¸é¢‘ç¹æ¨¡å¼çš„åŒ¹é…åº¦
        itemset_matches = self._match_with_frequent_itemsets(current_tails, frequent_itemsets)
        rule_matches = self._match_with_association_rules(current_tails, association_rules)
        sequence_matches = self._match_with_sequential_patterns(current_tails, sequential_patterns)
        
        # 6. å¼‚å¸¸é¢‘ç¹æ¨¡å¼æ£€æµ‹
        anomalous_patterns = self._detect_anomalous_frequent_patterns(
            current_tails, frequent_itemsets, association_rules
        )
        
        return {
            'frequent_itemsets': frequent_itemsets,
            'association_rules': association_rules,
            'sequential_patterns': sequential_patterns,
            'itemset_matches': itemset_matches,
            'rule_matches': rule_matches,
            'sequence_matches': sequence_matches,
            'anomalous_patterns': anomalous_patterns,
            'overall_frequent_pattern_score': self._calculate_frequent_pattern_score(
                itemset_matches, rule_matches, sequence_matches
            )
        }
    
    # ========== æ ¸å¿ƒç®—æ³•å®ç° ==========
    
    def _find_repeated_subsequences(self, sequence: List[int]) -> List[Dict]:
        """æŸ¥æ‰¾é‡å¤å­åºåˆ—"""
        if len(sequence) < 6:
            return []
        
        repeated_patterns = []
        min_length = self.matching_params['pattern_min_length']
        max_length = min(self.matching_params['pattern_max_length'], len(sequence) // 2)
        
        for length in range(min_length, max_length + 1):
            pattern_counts = {}
            
            # æ»‘åŠ¨çª—å£æå–æ‰€æœ‰å¯èƒ½çš„å­åºåˆ—
            for i in range(len(sequence) - length + 1):
                pattern = tuple(sequence[i:i + length])
                if pattern not in pattern_counts:
                    pattern_counts[pattern] = []
                pattern_counts[pattern].append(i)
            
            # æ‰¾åˆ°é‡å¤çš„æ¨¡å¼
            for pattern, positions in pattern_counts.items():
                if len(positions) >= 2:  # è‡³å°‘é‡å¤ä¸€æ¬¡
                    # è®¡ç®—æ¨¡å¼è´¨é‡æŒ‡æ ‡
                    pattern_strength = len(positions) / (len(sequence) - length + 1)
                    pattern_regularity = self._calculate_pattern_regularity(positions)
                    
                    repeated_patterns.append({
                        'pattern': list(pattern),
                        'length': length,
                        'occurrences': len(positions),
                        'positions': positions,
                        'strength': float(pattern_strength),
                        'regularity': float(pattern_regularity),
                        'quality_score': float(pattern_strength * pattern_regularity)
                    })
        
        # æŒ‰è´¨é‡è¯„åˆ†æ’åº
        repeated_patterns.sort(key=lambda x: x['quality_score'], reverse=True)
        return repeated_patterns[:10]  # è¿”å›å‰10ä¸ªæœ€ä½³æ¨¡å¼
    
    def _calculate_sequence_entropy(self, sequence: List[int]) -> float:
        """è®¡ç®—åºåˆ—ç†µ"""
        if not sequence:
            return 0.0
        
        # è®¡ç®—å„å€¼çš„é¢‘ç‡
        from collections import Counter
        counts = Counter(sequence)
        total = len(sequence)
        
        entropy = 0.0
        for count in counts.values():
            if count > 0:
                probability = count / total
                entropy -= probability * math.log2(probability)
        
        return entropy
    
    def _calculate_sequence_complexity(self, sequence: List[int]) -> float:
        """è®¡ç®—åºåˆ—å¤æ‚åº¦ï¼ˆåŸºäºLempel-Zivï¼‰"""
        if len(sequence) < 2:
            return 0.0
        
        # ç®€åŒ–çš„LZå¤æ‚åº¦è®¡ç®—
        complexity = 0
        i = 0
        
        while i < len(sequence):
            j = 1
            # å¯»æ‰¾æœ€é•¿çš„æ–°å­ä¸²
            while i + j <= len(sequence):
                substring = sequence[i:i+j]
                if substring not in [sequence[k:k+j] for k in range(i)]:
                    j += 1
                else:
                    break
            
            complexity += 1
            i += max(1, j - 1)
        
        # å½’ä¸€åŒ–
        max_complexity = len(sequence)
        return complexity / max_complexity if max_complexity > 0 else 0.0
    
    def _detect_periodic_patterns(self, sequence: List[int]) -> List[Dict]:
        """æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼"""
        if len(sequence) < 6:
            return []
        
        periodic_patterns = []
        max_period = min(len(sequence) // 3, 20)  # æœ€å¤§å‘¨æœŸé•¿åº¦
        
        for period in range(2, max_period + 1):
            # æ£€æŸ¥è¯¥å‘¨æœŸçš„è§„å¾‹æ€§
            period_matches = 0
            total_checks = len(sequence) - period
            
            if total_checks <= 0:
                continue
            
            for i in range(total_checks):
                if sequence[i] == sequence[i + period]:
                    period_matches += 1
            
            # è®¡ç®—å‘¨æœŸæ€§å¼ºåº¦
            periodicity_strength = period_matches / total_checks
            
            if periodicity_strength > 0.6:  # è‡³å°‘60%åŒ¹é…
                # æå–å‘¨æœŸæ¨¡å¼
                pattern = sequence[:period]
                
                # è®¡ç®—æ¨¡å¼åœ¨æ•´ä¸ªåºåˆ—ä¸­çš„ä¸€è‡´æ€§
                consistency_score = self._calculate_period_consistency(sequence, pattern, period)
                
                periodic_patterns.append({
                    'period': period,
                    'pattern': pattern,
                    'strength': float(periodicity_strength),
                    'consistency': float(consistency_score),
                    'quality_score': float(periodicity_strength * consistency_score)
                })
        
        # æŒ‰è´¨é‡è¯„åˆ†æ’åº
        periodic_patterns.sort(key=lambda x: x['quality_score'], reverse=True)
        return periodic_patterns[:5]  # è¿”å›å‰5ä¸ªæœ€ä½³å‘¨æœŸæ¨¡å¼
    
    def _analyze_state_transitions(self, sequence: List[int]) -> Dict:
        """åˆ†æçŠ¶æ€è½¬ç§»æ¨¡å¼"""
        if len(sequence) < 2:
            return {'transition_matrix': [], 'transition_entropy': 0.0}
        
        # æ„å»ºçŠ¶æ€è½¬ç§»çŸ©é˜µ
        states = sorted(set(sequence))
        n_states = len(states)
        state_to_idx = {state: idx for idx, state in enumerate(states)}
        
        transition_matrix = np.zeros((n_states, n_states))
        
        for i in range(len(sequence) - 1):
            current_state = state_to_idx[sequence[i]]
            next_state = state_to_idx[sequence[i + 1]]
            transition_matrix[current_state][next_state] += 1
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡çŸ©é˜µ
        row_sums = transition_matrix.sum(axis=1)
        for i in range(n_states):
            if row_sums[i] > 0:
                transition_matrix[i] = transition_matrix[i] / row_sums[i]
        
        # è®¡ç®—è½¬ç§»ç†µ
        transition_entropy = 0.0
        for i in range(n_states):
            for j in range(n_states):
                prob = transition_matrix[i][j]
                if prob > 0:
                    transition_entropy -= prob * math.log2(prob)
        
        # åˆ†æè½¬ç§»æ¨¡å¼ç‰¹å¾
        self_transition_prob = np.diag(transition_matrix).sum() / n_states
        max_transition_prob = np.max(transition_matrix)
        
        return {
            'transition_matrix': transition_matrix.tolist(),
            'transition_entropy': float(transition_entropy),
            'self_transition_probability': float(self_transition_prob),
            'max_transition_probability': float(max_transition_prob),
            'states': states,
            'n_transitions': int(len(sequence) - 1)
        }
    
    def _calculate_state_match_score(self, current_state: int, sequence: List[int],
                                   repeated_patterns: List[Dict], periodic_patterns: List[Dict]) -> float:
        """è®¡ç®—å½“å‰çŠ¶æ€çš„åŒ¹é…è¯„åˆ†"""
        if not sequence:
            return 0.0
        
        match_components = []
        
        # 1. åŸºäºé‡å¤æ¨¡å¼çš„é¢„æµ‹
        if repeated_patterns:
            pattern_predictions = []
            for pattern in repeated_patterns[:3]:  # ä½¿ç”¨å‰3ä¸ªæœ€ä½³æ¨¡å¼
                pattern_data = pattern['pattern']
                pattern_length = pattern['length']
                
                # æ£€æŸ¥å½“å‰ä½ç½®æ˜¯å¦ç¬¦åˆè¯¥æ¨¡å¼
                if len(sequence) >= pattern_length:
                    recent_segment = sequence[-pattern_length:]
                    pattern_match = sum(1 for i, val in enumerate(pattern_data) 
                                      if i < len(recent_segment) and recent_segment[i] == val)
                    pattern_score = pattern_match / pattern_length
                    pattern_predictions.append(pattern_score * pattern['quality_score'])
            
            if pattern_predictions:
                match_components.append(np.mean(pattern_predictions))
        
        # 2. åŸºäºå‘¨æœŸæ¨¡å¼çš„é¢„æµ‹
        if periodic_patterns:
            periodic_predictions = []
            for pattern in periodic_patterns[:2]:  # ä½¿ç”¨å‰2ä¸ªæœ€ä½³å‘¨æœŸ
                period = pattern['period']
                pattern_data = pattern['pattern']
                
                # æ ¹æ®å‘¨æœŸé¢„æµ‹å½“å‰ä½ç½®çš„å€¼
                position_in_cycle = len(sequence) % period
                if position_in_cycle < len(pattern_data):
                    expected_value = pattern_data[position_in_cycle]
                    prediction_score = 1.0 if current_state == expected_value else 0.0
                    periodic_predictions.append(prediction_score * pattern['quality_score'])
            
            if periodic_predictions:
                match_components.append(np.mean(periodic_predictions))
        
        # 3. åŸºäºæœ€è¿‘è¶‹åŠ¿çš„é¢„æµ‹
        if len(sequence) >= 5:
            recent_trend = sequence[-5:]
            trend_score = sum(1 for val in recent_trend if val == current_state) / 5.0
            match_components.append(trend_score)
        
        # 4. åŸºäºæ•´ä½“é¢‘ç‡çš„é¢„æµ‹
        if sequence:
            overall_frequency = sequence.count(current_state) / len(sequence)
            match_components.append(overall_frequency)
        
        # ç»¼åˆè¯„åˆ†
        if match_components:
            return float(np.mean(match_components))
        else:
            return 0.5  # é»˜è®¤è¯„åˆ†
    
    def _calculate_dtw_distance(self, series1: np.ndarray, series2: np.ndarray) -> float:
        """è®¡ç®—åŠ¨æ€æ—¶é—´è§„æ•´è·ç¦»"""
        if len(series1) == 0 or len(series2) == 0:
            return float('inf')
        
        n, m = len(series1), len(series2)
        
        # åˆ›å»ºè·ç¦»çŸ©é˜µ
        dtw_matrix = np.full((n + 1, m + 1), float('inf'))
        dtw_matrix[0, 0] = 0
        
        for i in range(1, n + 1):
            for j in range(1, m + 1):
                cost = np.linalg.norm(series1[i-1] - series2[j-1])
                dtw_matrix[i, j] = cost + min(
                    dtw_matrix[i-1, j],     # insertion
                    dtw_matrix[i, j-1],     # deletion
                    dtw_matrix[i-1, j-1]    # match
                )
        
        return float(dtw_matrix[n, m])
    
    def _calculate_dtw_match_confidence(self, dtw_distance: float, window_size: int, 
                                      similarity: float) -> float:
        """è®¡ç®—DTWåŒ¹é…ç½®ä¿¡åº¦"""
        # åŸºäºè·ç¦»ã€çª—å£å¤§å°å’Œç›¸ä¼¼åº¦è®¡ç®—ç½®ä¿¡åº¦
        distance_factor = 1.0 / (1.0 + dtw_distance)
        size_factor = min(1.0, window_size / 15.0)  # çª—å£å¤§å°æ ‡å‡†åŒ–
        
        confidence = (distance_factor * 0.4 + similarity * 0.4 + size_factor * 0.2)
        return float(min(1.0, max(0.0, confidence)))
    
    def _find_similar_dtw_segments(self, tail_series: np.ndarray, current_state: int, 
                                 window_sizes: List[int]) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„DTWæ®µ"""
        similar_segments = []
        
        for window_size in window_sizes:
            if len(tail_series) >= window_size:
                for start_idx in range(len(tail_series) - window_size + 1):
                    segment = tail_series[start_idx:start_idx + window_size]
                    
                    # è®¡ç®—æ®µä¸å½“å‰çŠ¶æ€çš„ç›¸ä¼¼æ€§
                    segment_mean = np.mean(segment)
                    segment_std = np.std(segment)
                    
                    # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
                    if segment_std > 0:
                        z_score = abs(current_state - segment_mean) / segment_std
                        similarity = 1.0 / (1.0 + z_score)
                    else:
                        similarity = 1.0 if abs(current_state - segment_mean) < 0.1 else 0.0
                    
                    if similarity > 0.7:
                        similar_segments.append({
                            'start_index': start_idx,
                            'window_size': window_size,
                            'similarity': float(similarity),
                            'segment_data': segment.tolist()
                        })
        
        return sorted(similar_segments, key=lambda x: x['similarity'], reverse=True)[:5]
    
    def _multivariate_dtw_analysis(self, time_series_matrix: np.ndarray, 
                                 current_vector: np.ndarray) -> Dict:
        """å¤šå…ƒDTWåˆ†æ"""
        if len(time_series_matrix) < 3:
            return {'multivariate_similarity': 0.0, 'best_match_index': -1}
        
        similarities = []
        
        for i in range(len(time_series_matrix)):
            historical_vector = time_series_matrix[i:i+1]
            dtw_distance = self._calculate_dtw_distance(historical_vector, current_vector.reshape(1, -1))
            similarity = 1.0 / (1.0 + dtw_distance)
            similarities.append(similarity)
        
        best_match_index = np.argmax(similarities)
        avg_similarity = np.mean(similarities)
        
        return {
            'similarities': [float(s) for s in similarities],
            'best_match_index': int(best_match_index),
            'best_match_similarity': float(similarities[best_match_index]),
            'average_similarity': float(avg_similarity),
            'similarity_variance': float(np.var(similarities))
        }
    
    # ========== è¾…åŠ©æ–¹æ³•å®ç° ==========
    
    def _insufficient_data_response(self) -> Dict:
        """æ•°æ®ä¸è¶³æ—¶çš„å“åº”"""
        return {
            'error': 'insufficient_data',
            'message': 'Need at least {} periods of data for pattern matching'.format(
                self.matching_params['pattern_min_length']
            ),
            'matched_patterns': [],
            'similarity_scores': []
        }
    
    def _identify_high_confidence_sequential_matches(self, sequential_results: Dict) -> List[Dict]:
        """è¯†åˆ«é«˜ç½®ä¿¡åº¦çš„åºåˆ—åŒ¹é…"""
        high_confidence_matches = []
        
        for tail, result in sequential_results.items():
            if result['state_match_score'] > 0.75:
                high_confidence_matches.append({
                    'tail': tail,
                    'match_score': result['state_match_score'],
                    'evidence': {
                        'repeated_patterns_count': len(result['repeated_patterns']),
                        'periodic_patterns_count': len(result['periodic_patterns']),
                        'sequence_entropy': result['sequence_entropy'],
                        'sequence_complexity': result['sequence_complexity']
                    }
                })
        
        return sorted(high_confidence_matches, key=lambda x: x['match_score'], reverse=True)
    
    def _calculate_pattern_regularity(self, positions: List[int]) -> float:
        """è®¡ç®—æ¨¡å¼è§„å¾‹æ€§"""
        if len(positions) < 2:
            return 0.0
        
        intervals = [positions[i+1] - positions[i] for i in range(len(positions) - 1)]
        
        if not intervals:
            return 0.0
        
        mean_interval = np.mean(intervals)
        std_interval = np.std(intervals)
        
        # è§„å¾‹æ€§ = 1 - (æ ‡å‡†å·® / å‡å€¼)ï¼Œæ ‡å‡†å·®è¶Šå°è¶Šè§„å¾‹
        if mean_interval > 0:
            regularity = max(0.0, 1.0 - (std_interval / mean_interval))
        else:
            regularity = 0.0
        
        return regularity
    
    def _calculate_period_consistency(self, sequence: List[int], pattern: List[int], period: int) -> float:
        """è®¡ç®—å‘¨æœŸä¸€è‡´æ€§"""
        if period <= 0 or len(pattern) != period:
            return 0.0
        
        total_checks = 0
        consistent_checks = 0
        
        for i in range(len(sequence)):
            pattern_index = i % period
            if pattern_index < len(pattern):
                total_checks += 1
                if sequence[i] == pattern[pattern_index]:
                    consistent_checks += 1
        
        return consistent_checks / total_checks if total_checks > 0 else 0.0
    
    def _calculate_comprehensive_similarity(self, *match_results) -> Dict:
        """è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦"""
        similarity_components = []
        
        # æå–å„ç§åŒ¹é…æ–¹æ³•çš„è¯„åˆ†
        for result in match_results:
            if isinstance(result, dict):
                if 'overall_sequential_match_score' in result:
                    similarity_components.append(result['overall_sequential_match_score'])
                elif 'overall_dtw_score' in result:
                    similarity_components.append(result['overall_dtw_score'])
                elif 'overall_graph_match_score' in result:
                    similarity_components.append(result['overall_graph_match_score'])
                elif 'overall_hmm_score' in result:
                    similarity_components.append(result['overall_hmm_score'])
                elif 'overall_fuzzy_score' in result:
                    similarity_components.append(result['overall_fuzzy_score'])
                elif 'overall_hierarchical_score' in result:
                    similarity_components.append(result['overall_hierarchical_score'])
                elif 'overall_frequent_pattern_score' in result:
                    similarity_components.append(result['overall_frequent_pattern_score'])
        
        if similarity_components:
            overall_similarity = np.mean(similarity_components)
            max_similarity = np.max(similarity_components)
            min_similarity = np.min(similarity_components)
            similarity_variance = np.var(similarity_components)
        else:
            overall_similarity = 0.0
            max_similarity = 0.0
            min_similarity = 0.0
            similarity_variance = 0.0
        
        return {
            'overall_similarity': float(overall_similarity),
            'max_similarity': float(max_similarity),
            'min_similarity': float(min_similarity),
            'similarity_variance': float(similarity_variance),
            'component_scores': [float(s) for s in similarity_components],
            'consistency_score': 1.0 - float(similarity_variance)  # ä½æ–¹å·® = é«˜ä¸€è‡´æ€§
        }
    
    def _assess_pattern_confidence(self, similarity_scores: Dict, historical_context: List[Dict]) -> Dict:
        """è¯„ä¼°æ¨¡å¼ç½®ä¿¡åº¦"""
        data_size = len(historical_context)
        overall_similarity = similarity_scores.get('overall_similarity', 0.0)
        consistency = similarity_scores.get('consistency_score', 0.0)
        
        # æ•°æ®è´¨é‡å› å­
        if data_size >= 100:
            data_quality_factor = 1.0
        elif data_size >= 50:
            data_quality_factor = 0.9
        elif data_size >= 20:
            data_quality_factor = 0.7
        else:
            data_quality_factor = 0.5
        
        # ç›¸ä¼¼åº¦ç½®ä¿¡åº¦
        similarity_confidence = min(1.0, overall_similarity * 1.2)
        
        # ä¸€è‡´æ€§ç½®ä¿¡åº¦
        consistency_confidence = consistency
        
        # ç»¼åˆç½®ä¿¡åº¦
        overall_confidence = (
            similarity_confidence * 0.4 +
            consistency_confidence * 0.3 +
            data_quality_factor * 0.3
        )
        
        return {
            'overall_confidence': float(overall_confidence),
            'similarity_confidence': float(similarity_confidence),
            'consistency_confidence': float(consistency_confidence),
            'data_quality_factor': float(data_quality_factor),
            'data_size': data_size,
            'confidence_level': self._classify_confidence_level(overall_confidence)
        }
    
    def _classify_confidence_level(self, confidence: float) -> str:
        """åˆ†ç±»ç½®ä¿¡åº¦ç­‰çº§"""
        if confidence >= 0.9:
            return 'very_high'
        elif confidence >= 0.7:
            return 'high'
        elif confidence >= 0.5:
            return 'moderate'
        elif confidence >= 0.3:
            return 'low'
        else:
            return 'very_low'
    
    def _adaptive_pattern_learning(self, period_data: Dict, historical_context: List[Dict], 
                                 similarity_scores: Dict) -> Dict:
        """è‡ªé€‚åº”æ¨¡å¼å­¦ä¹ """
        current_tails = set(period_data.get('tails', []))
        
        # å­¦ä¹ æ–°æ¨¡å¼
        if similarity_scores.get('overall_similarity', 0.0) < 0.3:
            # ç›¸ä¼¼åº¦ä½ï¼Œå¯èƒ½æ˜¯æ–°æ¨¡å¼
            new_pattern = self._extract_emerging_pattern(current_tails, historical_context)
            if new_pattern:
                self.pattern_library['sequential_patterns'][f'pattern_{len(self.pattern_library["sequential_patterns"])}'] = new_pattern
                self.learning_stats['patterns_discovered'] += 1
        
        # æ›´æ–°ç°æœ‰æ¨¡å¼
        pattern_updates = []
        for pattern_id, pattern in self.pattern_library['sequential_patterns'].items():
            updated_pattern = self._update_pattern_with_new_data(pattern, current_tails, historical_context)
            if updated_pattern != pattern:
                pattern_updates.append({
                    'pattern_id': pattern_id,
                    'old_pattern': pattern,
                    'new_pattern': updated_pattern
                })
        
        return {
            'new_patterns_discovered': 1 if similarity_scores.get('overall_similarity', 0.0) < 0.3 else 0,
            'patterns_updated': len(pattern_updates),
            'total_patterns_in_library': len(self.pattern_library['sequential_patterns']),
            'learning_statistics': self.learning_stats
        }
    
    def _extract_best_matches(self, similarity_scores: Dict, confidence_scores: Dict) -> List[Dict]:
        """æå–æœ€ä½³åŒ¹é…"""
        matches = []
        
        overall_similarity = similarity_scores.get('overall_similarity', 0.0)
        overall_confidence = confidence_scores.get('overall_confidence', 0.0)
        
        if overall_similarity > 0.5 and overall_confidence > 0.5:
            matches.append({
                'match_type': 'comprehensive',
                'similarity': float(overall_similarity),
                'confidence': float(overall_confidence),
                'quality_score': float(overall_similarity * overall_confidence)
            })
        
        return sorted(matches, key=lambda x: x['quality_score'], reverse=True)
    
    def _assess_matching_quality(self, similarity_scores: Dict, confidence_scores: Dict) -> Dict:
        """è¯„ä¼°åŒ¹é…è´¨é‡"""
        quality_metrics = {
            'similarity_quality': similarity_scores.get('overall_similarity', 0.0),
            'confidence_quality': confidence_scores.get('overall_confidence', 0.0),
            'consistency_quality': similarity_scores.get('consistency_score', 0.0)
        }
        
        overall_quality = np.mean(list(quality_metrics.values()))
        
        return {
            **quality_metrics,
            'overall_quality': float(overall_quality),
            'quality_grade': self._grade_quality(overall_quality)
        }
    
    def _grade_quality(self, quality: float) -> str:
        """è´¨é‡ç­‰çº§è¯„å®š"""
        if quality >= 0.9:
            return 'excellent'
        elif quality >= 0.7:
            return 'good'
        elif quality >= 0.5:
            return 'fair'
        elif quality >= 0.3:
            return 'poor'
        else:
            return 'very_poor'
    
    def _generate_pattern_insights(self, similarity_scores: Dict, historical_context: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ¨¡å¼æ´å¯Ÿ"""
        insights = []
        
        overall_similarity = similarity_scores.get('overall_similarity', 0.0)
        consistency = similarity_scores.get('consistency_score', 0.0)
        
        if overall_similarity > 0.8:
            insights.append("å‘ç°å¼ºçƒˆçš„æ¨¡å¼ç›¸ä¼¼æ€§ï¼Œå½“å‰æœŸä¸å†å²æ¨¡å¼é«˜åº¦ä¸€è‡´")
        elif overall_similarity > 0.6:
            insights.append("å‘ç°ä¸­ç­‰ç¨‹åº¦çš„æ¨¡å¼ç›¸ä¼¼æ€§ï¼Œå­˜åœ¨å¯è¯†åˆ«çš„æ¨¡å¼åŒ¹é…")
        elif overall_similarity < 0.3:
            insights.append("æ¨¡å¼ç›¸ä¼¼æ€§è¾ƒä½ï¼Œå¯èƒ½å‡ºç°æ–°çš„è¡Œä¸ºæ¨¡å¼æˆ–å¼‚å¸¸æƒ…å†µ")
        
        if consistency > 0.8:
            insights.append("å„ç§åŒ¹é…æ–¹æ³•ç»“æœé«˜åº¦ä¸€è‡´ï¼Œæé«˜äº†åˆ†æçš„å¯é æ€§")
        elif consistency < 0.4:
            insights.append("ä¸åŒåŒ¹é…æ–¹æ³•ç»“æœå­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æ")
        
        data_size = len(historical_context)
        if data_size < 30:
            insights.append("å†å²æ•°æ®é‡è¾ƒå°‘ï¼Œå¯èƒ½å½±å“æ¨¡å¼åŒ¹é…çš„å‡†ç¡®æ€§")
        elif data_size > 200:
            insights.append("å†å²æ•°æ®é‡å……è¶³ï¼Œæ”¯æŒå¯é çš„æ¨¡å¼åˆ†æ")
        
        return insights
    
    # è¿™é‡Œçœç•¥äº†ä¸€äº›å¤æ‚æ–¹æ³•çš„å®Œæ•´å®ç°
    # å®é™…å®ç°ä¸­éœ€è¦åŒ…å«æ‰€æœ‰æ–¹æ³•çš„å®Œæ•´ç§‘ç ”çº§ç®—æ³•
    
    def _build_historical_relationship_graphs(self, historical_context: List[Dict]) -> List[Dict]:
        """
        æ„å»ºå†å²å…³ç³»å›¾çš„å®Œæ•´å®ç°
        åŸºäºå°¾æ•°å…±ç°å…³ç³»ã€æ—¶åºå…³ç³»å’Œç»Ÿè®¡å…³ç³»æ„å»ºå¤æ‚ç½‘ç»œ
        """
        historical_graphs = []
        window_size = 5  # æ»‘åŠ¨çª—å£å¤§å°
        
        for i in range(len(historical_context) - window_size + 1):
            window_data = historical_context[i:i + window_size]
            
            # æ„å»ºå›¾çš„èŠ‚ç‚¹å’Œè¾¹
            nodes = set()
            edges = {}
            node_properties = {}
            
            # 1. æ·»åŠ å°¾æ•°èŠ‚ç‚¹
            for tail in range(10):
                nodes.add(tail)
                
                # è®¡ç®—èŠ‚ç‚¹å±æ€§
                appearances_in_window = sum(1 for period in window_data if tail in period.get('tails', []))
                frequency = appearances_in_window / len(window_data)
                
                # è®¡ç®—èŠ‚ç‚¹ä¸­å¿ƒæ€§åº¦é‡
                degree_centrality = 0
                betweenness_centrality = 0
                clustering_coefficient = 0
                
                node_properties[tail] = {
                    'frequency': frequency,
                    'appearances': appearances_in_window,
                    'degree_centrality': degree_centrality,
                    'betweenness_centrality': betweenness_centrality,
                    'clustering_coefficient': clustering_coefficient
                }
        
            # 2. æ„å»ºè¾¹å…³ç³»
            # å…±ç°å…³ç³»è¾¹
            co_occurrence_matrix = np.zeros((10, 10))
            temporal_correlation_matrix = np.zeros((10, 10))
            
            for period in window_data:
                period_tails = period.get('tails', [])
                # å…±ç°å…³ç³»
                for i, tail_i in enumerate(period_tails):
                    for j, tail_j in enumerate(period_tails):
                        if i != j:
                            co_occurrence_matrix[tail_i][tail_j] += 1
            
            # æ—¶åºç›¸å…³å…³ç³»
            for k in range(len(window_data) - 1):
                current_tails = set(window_data[k].get('tails', []))
                next_tails = set(window_data[k + 1].get('tails', []))
                
                for tail_i in current_tails:
                    for tail_j in next_tails:
                        temporal_correlation_matrix[tail_i][tail_j] += 1
            
            # 3. è®¡ç®—è¾¹æƒé‡å’Œå±æ€§
            for tail_i in range(10):
                for tail_j in range(10):
                    if tail_i != tail_j:
                        # å…±ç°å¼ºåº¦
                        co_occurrence_strength = co_occurrence_matrix[tail_i][tail_j] / len(window_data)
                        
                        # æ—¶åºç›¸å…³å¼ºåº¦
                        temporal_strength = temporal_correlation_matrix[tail_i][tail_j] / (len(window_data) - 1)
                        
                        # ç»Ÿè®¡ç›¸å…³æ€§
                        series_i = [1 if tail_i in period.get('tails', []) else 0 for period in window_data]
                        series_j = [1 if tail_j in period.get('tails', []) else 0 for period in window_data]
                    
                        if np.var(series_i) > 0 and np.var(series_j) > 0:
                            correlation_coefficient = np.corrcoef(series_i, series_j)[0, 1]
                            if np.isnan(correlation_coefficient):
                                correlation_coefficient = 0.0
                        else:
                            correlation_coefficient = 0.0
                        
                        # ç»¼åˆè¾¹æƒé‡
                        edge_weight = (
                            co_occurrence_strength * 0.4 +
                            temporal_strength * 0.3 +
                            abs(correlation_coefficient) * 0.3
                        )
                        
                        if edge_weight > self.graph_params['edge_weight_threshold']:
                            edge_key = (min(tail_i, tail_j), max(tail_i, tail_j))
                            if edge_key not in edges:
                                edges[edge_key] = {
                                    'weight': edge_weight,
                                    'co_occurrence_strength': co_occurrence_strength,
                                    'temporal_strength': temporal_strength,
                                    'correlation_coefficient': correlation_coefficient,
                                    'edge_type': self._classify_edge_type(co_occurrence_strength, 
                                                                    temporal_strength, 
                                                                    correlation_coefficient)
                                }
            
            # 4. è®¡ç®—å›¾çš„å…¨å±€å±æ€§
            total_edges = len(edges)
            total_possible_edges = 10 * 9 // 2  # å®Œå…¨å›¾çš„è¾¹æ•°
            graph_density = total_edges / total_possible_edges if total_possible_edges > 0 else 0
            
            # è®¡ç®—è¿é€šæ€§
            adjacency_matrix = self._build_adjacency_matrix(edges, nodes)
            connected_components = self._find_connected_components(adjacency_matrix)
        
            # è®¡ç®—é›†èšç³»æ•°
            global_clustering_coefficient = self._calculate_global_clustering_coefficient(adjacency_matrix)
            
            # è®¡ç®—å›¾çš„ç›´å¾„å’Œå¹³å‡è·¯å¾„é•¿åº¦
            diameter, avg_path_length = self._calculate_graph_metrics(adjacency_matrix)
            
            # è®¡ç®—åº¦åˆ†å¸ƒ
            degree_sequence = self._calculate_degree_sequence(adjacency_matrix)
            
            # 5. æ›´æ–°èŠ‚ç‚¹ä¸­å¿ƒæ€§åº¦é‡
            centrality_measures = self._calculate_centrality_measures(adjacency_matrix)
            for tail in range(10):
                node_properties[tail].update(centrality_measures[tail])
            
            # 6. åˆ›å»ºå›¾å¯¹è±¡
            graph_obj = {
                'window_start': i,
                'window_size': window_size,
                'nodes': list(nodes),
                'edges': edges,
                'node_properties': node_properties,
                'graph_properties': {
                    'density': graph_density,
                    'num_edges': total_edges,
                    'num_nodes': len(nodes),
                    'connected_components': len(connected_components),
                    'largest_component_size': max(len(comp) for comp in connected_components) if connected_components else 0,
                    'global_clustering_coefficient': global_clustering_coefficient,
                    'diameter': diameter,
                    'average_path_length': avg_path_length,
                    'degree_sequence': degree_sequence,
                    'degree_distribution': self._calculate_degree_distribution(degree_sequence)
                },
                'adjacency_matrix': adjacency_matrix.tolist(),
                'co_occurrence_matrix': co_occurrence_matrix.tolist(),
                'temporal_correlation_matrix': temporal_correlation_matrix.tolist()
            }
        
            historical_graphs.append(graph_obj)
    
        return historical_graphs

    def _build_current_relationship_graph(self, current_tails: Set[int], historical_context: List[Dict]) -> Dict:
        """
        æ„å»ºå½“å‰æœŸå…³ç³»å›¾çš„å®Œæ•´å®ç°
        """
        # ä½¿ç”¨æœ€è¿‘çš„å†å²æ•°æ®æ¥æ¨æ–­å½“å‰çš„å…³ç³»ç»“æ„
        recent_history = historical_context[:min(20, len(historical_context))]
        
        nodes = set(range(10))  # æ‰€æœ‰å¯èƒ½çš„å°¾æ•°èŠ‚ç‚¹
        edges = {}
        node_properties = {}
        
        # 1. è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å±æ€§
        for tail in range(10):
            # å†å²é¢‘ç‡
            historical_frequency = sum(1 for period in recent_history if tail in period.get('tails', [])) / len(recent_history)
            
            # å½“å‰çŠ¶æ€
            is_present = tail in current_tails
            
            # æœ€è¿‘è¶‹åŠ¿
            recent_appearances = []
            for period in recent_history[:5]:  # æœ€è¿‘5æœŸ
                recent_appearances.append(1 if tail in period.get('tails', []) else 0)
            
            recent_trend = np.mean(recent_appearances) if recent_appearances else 0
            
            # å˜åŒ–ç‡
            if len(recent_history) >= 10:
                earlier_frequency = sum(1 for period in recent_history[5:10] if tail in period.get('tails', [])) / 5
                later_frequency = sum(1 for period in recent_history[:5] if tail in period.get('tails', [])) / 5
                change_rate = later_frequency - earlier_frequency
            else:
                change_rate = 0
        
            # ç¨³å®šæ€§æŒ‡æ ‡
            if len(recent_appearances) > 1:
                stability = 1.0 - np.std(recent_appearances)
            else:
                stability = 0.5
            
            node_properties[tail] = {
                'historical_frequency': historical_frequency,
                'is_present': is_present,
                'recent_trend': recent_trend,
                'change_rate': change_rate,
                'stability': stability,
                'activation_level': self._calculate_node_activation(tail, current_tails, recent_history)
            }
    
        # 2. æ„å»ºè¾¹å…³ç³»
        # åŸºäºå†å²å…±ç°æ¨¡å¼é¢„æµ‹å½“å‰å…³ç³»
        for tail_i in range(10):
            for tail_j in range(tail_i + 1, 10):
                # å†å²å…±ç°ç»Ÿè®¡
                co_occurrence_count = 0
                total_possible_co_occurrences = 0
                
                for period in recent_history:
                    period_tails = period.get('tails', [])
                    if tail_i in period_tails or tail_j in period_tails:
                        total_possible_co_occurrences += 1
                        if tail_i in period_tails and tail_j in period_tails:
                            co_occurrence_count += 1
                
                # å…±ç°æ¦‚ç‡
                if total_possible_co_occurrences > 0:
                    co_occurrence_probability = co_occurrence_count / total_possible_co_occurrences
                else:
                    co_occurrence_probability = 0
                
                # æ¡ä»¶æ¦‚ç‡ P(tail_j | tail_i)
                tail_i_appearances = sum(1 for period in recent_history if tail_i in period.get('tails', []))
                if tail_i_appearances > 0:
                    conditional_prob_j_given_i = co_occurrence_count / tail_i_appearances
                else:
                    conditional_prob_j_given_i = 0
                
                # æ¡ä»¶æ¦‚ç‡ P(tail_i | tail_j)
                tail_j_appearances = sum(1 for period in recent_history if tail_j in period.get('tails', []))
                if tail_j_appearances > 0:
                    conditional_prob_i_given_j = co_occurrence_count / tail_j_appearances
                else:
                    conditional_prob_i_given_j = 0
            
                # äº’ä¿¡æ¯
                if (tail_i_appearances > 0 and tail_j_appearances > 0 and 
                    co_occurrence_count > 0 and len(recent_history) > 0):
                    
                    p_i = tail_i_appearances / len(recent_history)
                    p_j = tail_j_appearances / len(recent_history)
                    p_ij = co_occurrence_count / len(recent_history)
                    
                    if p_i > 0 and p_j > 0 and p_ij > 0:
                        mutual_information = p_ij * math.log2(p_ij / (p_i * p_j))
                    else:
                        mutual_information = 0
                else:
                    mutual_information = 0
                
                # å½“å‰æœŸè¿æ¥å¼ºåº¦é¢„æµ‹
                current_connection_strength = self._predict_current_connection_strength(
                    tail_i, tail_j, current_tails, co_occurrence_probability,
                    conditional_prob_j_given_i, conditional_prob_i_given_j, mutual_information
                )
                
                # å¦‚æœè¿æ¥å¼ºåº¦è¶³å¤Ÿé«˜ï¼Œæ·»åŠ è¾¹
                if current_connection_strength > self.graph_params['edge_weight_threshold']:
                    edges[(tail_i, tail_j)] = {
                        'weight': current_connection_strength,
                        'co_occurrence_probability': co_occurrence_probability,
                        'conditional_prob_j_given_i': conditional_prob_j_given_i,
                        'conditional_prob_i_given_j': conditional_prob_i_given_j,
                        'mutual_information': mutual_information,
                        'edge_type': self._classify_edge_type(co_occurrence_probability,
                                                            conditional_prob_j_given_i,
                                                            mutual_information),
                        'prediction_confidence': self._calculate_edge_prediction_confidence(
                            co_occurrence_count, total_possible_co_occurrences, len(recent_history)
                        )
                    }
    
        # 3. è®¡ç®—å½“å‰å›¾çš„å…¨å±€å±æ€§
        adjacency_matrix = self._build_adjacency_matrix(edges, nodes)
        connected_components = self._find_connected_components(adjacency_matrix)
        global_clustering_coefficient = self._calculate_global_clustering_coefficient(adjacency_matrix)
        diameter, avg_path_length = self._calculate_graph_metrics(adjacency_matrix)
        degree_sequence = self._calculate_degree_sequence(adjacency_matrix)
        centrality_measures = self._calculate_centrality_measures(adjacency_matrix)
        
        # æ›´æ–°èŠ‚ç‚¹ä¸­å¿ƒæ€§åº¦é‡
        for tail in range(10):
            node_properties[tail].update(centrality_measures[tail])
        
        # 4. ç‰¹æ®Šåˆ†æï¼šå½“å‰æœŸæ¿€æ´»æ¨¡å¼
        activation_patterns = self._analyze_current_activation_patterns(current_tails, node_properties, edges)
        
        return {
            'nodes': list(nodes),
            'active_nodes': list(current_tails),
            'edges': edges,
            'node_properties': node_properties,
            'graph_properties': {
                'density': len(edges) / (10 * 9 // 2),
                'num_edges': len(edges),
                'num_nodes': len(nodes),
                'num_active_nodes': len(current_tails),
                'connected_components': len(connected_components),
                'largest_component_size': max(len(comp) for comp in connected_components) if connected_components else 0,
                'global_clustering_coefficient': global_clustering_coefficient,
                'diameter': diameter,
                'average_path_length': avg_path_length,
                'degree_sequence': degree_sequence,
                'degree_distribution': self._calculate_degree_distribution(degree_sequence)
            },
            'adjacency_matrix': adjacency_matrix.tolist(),
            'activation_patterns': activation_patterns,
            'graph_signature': self._calculate_graph_signature(adjacency_matrix, node_properties)
        }

    def _find_isomorphic_subgraphs(self, current_graph: Dict, historical_graphs: List[Dict]) -> List[Dict]:
        """
        æŸ¥æ‰¾åŒæ„å­å›¾çš„å®Œæ•´å®ç°
        ä½¿ç”¨VF2ç®—æ³•å’Œå›¾ä¸å˜é‡è¿›è¡ŒåŒæ„æ£€æµ‹
        """
        isomorphic_matches = []
        
        current_adj_matrix = np.array(current_graph['adjacency_matrix'])
        current_signature = current_graph['graph_signature']
        
        for i, hist_graph in enumerate(historical_graphs):
            hist_adj_matrix = np.array(hist_graph['adjacency_matrix'])
            hist_signature = hist_graph['graph_signature']
            
            # 1. å¿«é€Ÿé¢„ç­›é€‰ï¼šåŸºäºå›¾ä¸å˜é‡
            if not self._graphs_potentially_isomorphic(current_signature, hist_signature):
                continue
            
            # 2. å¯»æ‰¾æ‰€æœ‰å¯èƒ½çš„å­å›¾åŒæ„
            subgraph_matches = []
            
            # æå–æ‰€æœ‰è¿é€šå­å›¾
            current_subgraphs = self._extract_connected_subgraphs(current_adj_matrix, min_size=3)
            hist_subgraphs = self._extract_connected_subgraphs(hist_adj_matrix, min_size=3)
            
            for curr_subgraph in current_subgraphs:
                for hist_subgraph in hist_subgraphs:
                    # æ£€æŸ¥å­å›¾åŒæ„
                    isomorphism_mapping = self._vf2_subgraph_isomorphism(
                        curr_subgraph['adj_matrix'], 
                        hist_subgraph['adj_matrix'],
                        curr_subgraph['nodes'],
                        hist_subgraph['nodes']
                    )
                    
                    if isomorphism_mapping:
                        # è®¡ç®—åŒæ„è´¨é‡
                        isomorphism_quality = self._calculate_isomorphism_quality(
                            curr_subgraph, hist_subgraph, isomorphism_mapping, 
                            current_graph, hist_graph
                        )
                        
                        subgraph_matches.append({
                            'current_subgraph': curr_subgraph,
                            'historical_subgraph': hist_subgraph,
                            'mapping': isomorphism_mapping,
                            'quality': isomorphism_quality,
                            'size': len(curr_subgraph['nodes']),
                            'edge_preservation_ratio': self._calculate_edge_preservation_ratio(
                                curr_subgraph['adj_matrix'], hist_subgraph['adj_matrix'], isomorphism_mapping
                            )
                        })
            
            if subgraph_matches:
                # è®¡ç®—æ•´ä½“åŒæ„åˆ†æ•°
                overall_isomorphism_score = self._calculate_overall_isomorphism_score(subgraph_matches)
                
                isomorphic_matches.append({
                    'historical_graph_index': i,
                    'subgraph_matches': sorted(subgraph_matches, key=lambda x: x['quality'], reverse=True),
                    'overall_score': overall_isomorphism_score,
                    'num_isomorphic_subgraphs': len(subgraph_matches),
                    'largest_isomorphic_subgraph_size': max(match['size'] for match in subgraph_matches),
                    'average_quality': np.mean([match['quality'] for match in subgraph_matches]),
                    'graph_similarity_metrics': self._calculate_graph_similarity_metrics(
                        current_graph, hist_graph
                    )
                })
        
        return sorted(isomorphic_matches, key=lambda x: x['overall_score'], reverse=True)

    def _vf2_subgraph_isomorphism(self, subgraph1_adj: np.ndarray, subgraph2_adj: np.ndarray,
                                nodes1: List[int], nodes2: List[int]) -> Optional[Dict[int, int]]:
        """
        VF2å­å›¾åŒæ„ç®—æ³•çš„å®Œæ•´ç§‘ç ”çº§å®ç°
        åŸºäºCordellaç­‰äºº2004å¹´çš„ç»å…¸VF2ç®—æ³•
        
        Args:
            subgraph1_adj: å­å›¾1çš„é‚»æ¥çŸ©é˜µ
            subgraph2_adj: å­å›¾2çš„é‚»æ¥çŸ©é˜µ  
            nodes1: å­å›¾1çš„èŠ‚ç‚¹åˆ—è¡¨
            nodes2: å­å›¾2çš„èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            åŒæ„æ˜ å°„å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åŒæ„åˆ™è¿”å›None
        """
        if len(nodes1) != len(nodes2):
            return None
            
        if len(nodes1) == 0:
            return {}
            
        # VF2ç®—æ³•çŠ¶æ€ç±»
        class VF2State:
            def __init__(self, adj1: np.ndarray, adj2: np.ndarray, nodes1: List[int], nodes2: List[int]):
                self.adj1 = adj1
                self.adj2 = adj2
                self.nodes1 = nodes1
                self.nodes2 = nodes2
                self.n1 = len(nodes1)
                self.n2 = len(nodes2)
                
                # æ ¸å¿ƒæ˜ å°„ï¼šå·²ç¡®å®šçš„èŠ‚ç‚¹å¯¹åº”å…³ç³»
                self.core_1 = {}  # å›¾1èŠ‚ç‚¹ -> å›¾2èŠ‚ç‚¹
                self.core_2 = {}  # å›¾2èŠ‚ç‚¹ -> å›¾1èŠ‚ç‚¹
                
                # ç»ˆç«¯é›†åˆï¼šä¸å·²æ˜ å°„èŠ‚ç‚¹ç›¸é‚»ä½†æœªæ˜ å°„çš„èŠ‚ç‚¹
                self.in_1 = set()   # å›¾1çš„inç»ˆç«¯é›†
                self.in_2 = set()   # å›¾2çš„inç»ˆç«¯é›†
                self.out_1 = set()  # å›¾1çš„outç»ˆç«¯é›†
                self.out_2 = set()  # å›¾2çš„outç»ˆç«¯é›†
                
            def add_pair(self, n1: int, n2: int):
                """æ·»åŠ æ–°çš„æ˜ å°„å¯¹"""
                self.core_1[n1] = n2
                self.core_2[n2] = n1
                
                # æ›´æ–°ç»ˆç«¯é›†åˆ
                self._update_terminal_sets(n1, n2)
                
            def remove_pair(self, n1: int, n2: int):
                """ç§»é™¤æ˜ å°„å¯¹"""
                del self.core_1[n1]
                del self.core_2[n2]
                
                # é‡æ–°è®¡ç®—ç»ˆç«¯é›†åˆ
                self._recompute_terminal_sets()
                
            def _update_terminal_sets(self, n1: int, n2: int):
                """æ›´æ–°ç»ˆç«¯é›†åˆ"""
                # æ›´æ–°inç»ˆç«¯é›†
                for i in range(self.n1):
                    if i not in self.core_1 and self.adj1[i][self.nodes1.index(n1)] > 0:
                        self.in_1.add(i)
                        
                for i in range(self.n2):
                    if i not in self.core_2 and self.adj2[i][self.nodes2.index(n2)] > 0:
                        self.in_2.add(i)
                        
                # æ›´æ–°outç»ˆç«¯é›†  
                for i in range(self.n1):
                    if i not in self.core_1 and self.adj1[self.nodes1.index(n1)][i] > 0:
                        self.out_1.add(i)
                        
                for i in range(self.n2):
                    if i not in self.core_2 and self.adj2[self.nodes2.index(n2)][i] > 0:
                        self.out_2.add(i)
                        
            def _recompute_terminal_sets(self):
                """é‡æ–°è®¡ç®—ç»ˆç«¯é›†åˆ"""
                self.in_1.clear()
                self.in_2.clear()
                self.out_1.clear()
                self.out_2.clear()
                
                for n1, n2 in self.core_1.items():
                    self._update_terminal_sets(n1, n2)
                    
            def get_candidate_pairs(self):
                """è·å–å€™é€‰èŠ‚ç‚¹å¯¹"""
                if self.out_1 and self.out_2:
                    # ä»outç»ˆç«¯é›†é€‰æ‹©
                    return [(n1, n2) for n1 in self.out_1 for n2 in self.out_2]
                elif self.in_1 and self.in_2:
                    # ä»inç»ˆç«¯é›†é€‰æ‹©
                    return [(n1, n2) for n1 in self.in_1 for n2 in self.in_2]
                else:
                    # ä»å‰©ä½™æœªæ˜ å°„èŠ‚ç‚¹é€‰æ‹©
                    remaining_1 = [i for i in range(self.n1) if i not in self.core_1]
                    remaining_2 = [i for i in range(self.n2) if i not in self.core_2]
                    if remaining_1 and remaining_2:
                        return [(remaining_1[0], remaining_2[j]) for j in range(len(remaining_2))]
                return []
                
            def is_feasible(self, n1: int, n2: int) -> bool:
                """æ£€æŸ¥èŠ‚ç‚¹å¯¹æ˜¯å¦å¯è¡Œ"""
                # è¯­æ³•å¯è¡Œæ€§æ£€æŸ¥
                if not self._syntax_feasible(n1, n2):
                    return False
                    
                # è¯­ä¹‰å¯è¡Œæ€§æ£€æŸ¥  
                if not self._semantic_feasible(n1, n2):
                    return False
                    
                return True
                
            def _syntax_feasible(self, n1: int, n2: int) -> bool:
                """è¯­æ³•å¯è¡Œæ€§æ£€æŸ¥"""
                # æ£€æŸ¥å·²æ˜ å°„çš„é‚»æ¥å…³ç³»ä¸€è‡´æ€§
                for mapped_n1, mapped_n2 in self.core_1.items():
                    # æ£€æŸ¥è¾¹çš„å­˜åœ¨æ€§ä¸€è‡´
                    edge_1_exists = self.adj1[n1][mapped_n1] > 0
                    edge_2_exists = self.adj2[n2][mapped_n2] > 0
                    
                    if edge_1_exists != edge_2_exists:
                        return False
                        
                    # æ£€æŸ¥åå‘è¾¹
                    edge_1_rev_exists = self.adj1[mapped_n1][n1] > 0
                    edge_2_rev_exists = self.adj2[mapped_n2][n2] > 0
                    
                    if edge_1_rev_exists != edge_2_rev_exists:
                        return False
                        
                return True
                
            def _semantic_feasible(self, n1: int, n2: int) -> bool:
                """è¯­ä¹‰å¯è¡Œæ€§æ£€æŸ¥"""
                # ç»ˆç«¯é›†å¤§å°çº¦æŸ
                # Pred(n1) âˆ© T1^{in} çš„å¤§å°åº”è¯¥ç­‰äº Pred(n2) âˆ© T2^{in} çš„å¤§å°
                pred_n1_in_t1 = self._count_predecessors_in_terminal(n1, self.in_1, self.adj1, 1)
                pred_n2_in_t2 = self._count_predecessors_in_terminal(n2, self.in_2, self.adj2, 1)
                
                if pred_n1_in_t1 != pred_n2_in_t2:
                    return False
                    
                # Succ(n1) âˆ© T1^{out} çš„å¤§å°åº”è¯¥ç­‰äº Succ(n2) âˆ© T2^{out} çš„å¤§å°
                succ_n1_out_t1 = self._count_successors_in_terminal(n1, self.out_1, self.adj1, 1)
                succ_n2_out_t2 = self._count_successors_in_terminal(n2, self.out_2, self.adj2, 1)
                
                if succ_n1_out_t1 != succ_n2_out_t2:
                    return False
                    
                # å‰ç»è§„åˆ™ï¼šæ£€æŸ¥æœªæ¥å¯èƒ½çš„æ˜ å°„æ•°é‡
                pred_n1_new = self._count_new_predecessors(n1, self.adj1)
                pred_n2_new = self._count_new_predecessors(n2, self.adj2)
                
                if pred_n1_new < pred_n2_new:  # å›¾1çš„å¯æ‰©å±•æ€§ä¸èƒ½å°‘äºå›¾2
                    return False
                    
                succ_n1_new = self._count_new_successors(n1, self.adj1)
                succ_n2_new = self._count_new_successors(n2, self.adj2)
                
                if succ_n1_new < succ_n2_new:
                    return False
                    
                return True
                
            def _count_predecessors_in_terminal(self, node: int, terminal_set: set, 
                                             adj: np.ndarray, direction: int) -> int:
                """è®¡ç®—èŠ‚ç‚¹åœ¨ç»ˆç«¯é›†ä¸­çš„å‰é©±æ•°é‡"""
                count = 0
                for terminal_node in terminal_set:
                    if direction == 1:  # inæ–¹å‘
                        if adj[terminal_node][node] > 0:
                            count += 1
                    else:  # outæ–¹å‘
                        if adj[node][terminal_node] > 0:
                            count += 1
                return count
                
            def _count_successors_in_terminal(self, node: int, terminal_set: set,
                                            adj: np.ndarray, direction: int) -> int:
                """è®¡ç®—èŠ‚ç‚¹åœ¨ç»ˆç«¯é›†ä¸­çš„åç»§æ•°é‡"""
                count = 0
                for terminal_node in terminal_set:
                    if direction == 1:  # outæ–¹å‘
                        if adj[node][terminal_node] > 0:
                            count += 1
                    else:  # inæ–¹å‘
                        if adj[terminal_node][node] > 0:
                            count += 1
                return count
                
            def _count_new_predecessors(self, node: int, adj: np.ndarray) -> int:
                """è®¡ç®—æ–°å‰é©±èŠ‚ç‚¹æ•°é‡"""
                count = 0
                n = len(adj)
                for i in range(n):
                    if (i not in self.core_1 and i not in self.in_1 and 
                        i not in self.out_1 and adj[i][node] > 0):
                        count += 1
                return count
                
            def _count_new_successors(self, node: int, adj: np.ndarray) -> int:
                """è®¡ç®—æ–°åç»§èŠ‚ç‚¹æ•°é‡"""
                count = 0
                n = len(adj)
                for i in range(n):
                    if (i not in self.core_1 and i not in self.in_1 and 
                        i not in self.out_1 and adj[node][i] > 0):
                        count += 1
                return count
                
            def is_goal(self) -> bool:
                """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡çŠ¶æ€"""
                return len(self.core_1) == self.n1 and len(self.core_1) == self.n2
                
            def copy(self):
                """å¤åˆ¶çŠ¶æ€"""
                new_state = VF2State(self.adj1, self.adj2, self.nodes1, self.nodes2)
                new_state.core_1 = self.core_1.copy()
                new_state.core_2 = self.core_2.copy()
                new_state.in_1 = self.in_1.copy()
                new_state.in_2 = self.in_2.copy()
                new_state.out_1 = self.out_1.copy()
                new_state.out_2 = self.out_2.copy()
                return new_state
        
        # VF2ä¸»ç®—æ³•
        def vf2_recursive(state: VF2State) -> Optional[Dict[int, int]]:
            """VF2é€’å½’åŒ¹é…ç®—æ³•"""
            if state.is_goal():
                # è½¬æ¢èŠ‚ç‚¹ç´¢å¼•ä¸ºå®é™…èŠ‚ç‚¹ID
                mapping = {}
                for n1_idx, n2_idx in state.core_1.items():
                    mapping[nodes1[n1_idx]] = nodes2[n2_idx]
                return mapping
                
            # è·å–å€™é€‰èŠ‚ç‚¹å¯¹
            candidates = state.get_candidate_pairs()
            
            for n1, n2 in candidates:
                if state.is_feasible(n1, n2):
                    # åˆ›å»ºæ–°çŠ¶æ€
                    new_state = state.copy()
                    new_state.add_pair(n1, n2)
                    
                    # é€’å½’æœç´¢
                    result = vf2_recursive(new_state)
                    if result is not None:
                        return result
                        
            return None
            
        # æ‰§è¡ŒVF2ç®—æ³•
        initial_state = VF2State(subgraph1_adj, subgraph2_adj, nodes1, nodes2)
        return vf2_recursive(initial_state)
    
    def _calculate_isomorphism_quality(self, curr_subgraph: Dict, hist_subgraph: Dict,
                                     mapping: Dict[int, int], current_graph: Dict, hist_graph: Dict) -> float:
        """
        è®¡ç®—åŒæ„è´¨é‡çš„ç§‘ç ”çº§å®ç°
        ç»¼åˆè€ƒè™‘ç»“æ„ç›¸ä¼¼æ€§ã€å±æ€§ä¸€è‡´æ€§å’Œæ‹“æ‰‘è´¨é‡
        """
        quality_components = []
        
        # 1. ç»“æ„ä¸€è‡´æ€§è¯„åˆ†
        structural_quality = self._calculate_structural_consistency(
            curr_subgraph, hist_subgraph, mapping
        )
        quality_components.append(('structural', structural_quality, 0.35))
        
        # 2. èŠ‚ç‚¹å±æ€§ä¸€è‡´æ€§è¯„åˆ†
        attribute_quality = self._calculate_node_attribute_consistency(
            mapping, current_graph, hist_graph
        )
        quality_components.append(('attribute', attribute_quality, 0.25))
        
        # 3. æ‹“æ‰‘ç‰¹å¾ç›¸ä¼¼æ€§è¯„åˆ†
        topological_quality = self._calculate_topological_similarity(
            curr_subgraph, hist_subgraph
        )
        quality_components.append(('topological', topological_quality, 0.25))
        
        # 4. è¾¹æƒé‡ä¸€è‡´æ€§è¯„åˆ†
        edge_weight_quality = self._calculate_edge_weight_consistency(
            curr_subgraph, hist_subgraph, mapping
        )
        quality_components.append(('edge_weight', edge_weight_quality, 0.15))
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_quality = sum(score * weight for name, score, weight in quality_components)
        
        # åº”ç”¨è´¨é‡å¢å¼ºå‡½æ•°
        enhanced_quality = self._apply_quality_enhancement(total_quality, curr_subgraph['size'])
        
        return float(min(1.0, max(0.0, enhanced_quality)))
    
    def _calculate_structural_consistency(self, curr_subgraph: Dict, hist_subgraph: Dict,
                                        mapping: Dict[int, int]) -> float:
        """è®¡ç®—ç»“æ„ä¸€è‡´æ€§"""
        curr_adj = curr_subgraph['adj_matrix']
        hist_adj = hist_subgraph['adj_matrix']
        curr_nodes = curr_subgraph['nodes']
        hist_nodes = hist_subgraph['nodes']
        
        if len(curr_nodes) != len(hist_nodes):
            return 0.0
        
        # åˆ›å»ºç´¢å¼•æ˜ å°„
        curr_to_idx = {node: i for i, node in enumerate(curr_nodes)}
        hist_to_idx = {node: i for i, node in enumerate(hist_nodes)}
        
        consistent_edges = 0
        total_edges = 0
        
        for curr_node in curr_nodes:
            if curr_node in mapping:
                hist_node = mapping[curr_node]
                curr_idx = curr_to_idx[curr_node]
                hist_idx = hist_to_idx[hist_node]
                
                for curr_neighbor in curr_nodes:
                    if curr_neighbor in mapping and curr_neighbor != curr_node:
                        hist_neighbor = mapping[curr_neighbor]
                        curr_neighbor_idx = curr_to_idx[curr_neighbor]
                        hist_neighbor_idx = hist_to_idx[hist_neighbor]
                        
                        curr_edge_exists = curr_adj[curr_idx][curr_neighbor_idx] > 0
                        hist_edge_exists = hist_adj[hist_idx][hist_neighbor_idx] > 0
                        
                        if curr_edge_exists == hist_edge_exists:
                            consistent_edges += 1
                        total_edges += 1
        
        return consistent_edges / total_edges if total_edges > 0 else 1.0
    
    def _calculate_node_attribute_consistency(self, mapping: Dict[int, int],
                                            current_graph: Dict, hist_graph: Dict) -> float:
        """è®¡ç®—èŠ‚ç‚¹å±æ€§ä¸€è‡´æ€§"""
        if not mapping:
            return 0.0
        
        curr_node_props = current_graph.get('node_properties', {})
        hist_node_props = hist_graph.get('node_properties', {})
        
        consistency_scores = []
        
        for curr_node, hist_node in mapping.items():
            curr_attrs = curr_node_props.get(curr_node, {})
            hist_attrs = hist_node_props.get(hist_node, {})
            
            # æ¯”è¾ƒå…³é”®å±æ€§
            attr_similarities = []
            
            key_attributes = ['frequency', 'recent_trend', 'stability', 'activation_level']
            for attr in key_attributes:
                curr_val = curr_attrs.get(attr, 0.0)
                hist_val = hist_attrs.get(attr, 0.0)
                
                # è®¡ç®—å½’ä¸€åŒ–ç›¸ä¼¼åº¦
                max_val = max(abs(curr_val), abs(hist_val), 1e-10)
                similarity = 1.0 - abs(curr_val - hist_val) / max_val
                attr_similarities.append(similarity)
            
            if attr_similarities:
                node_consistency = np.mean(attr_similarities)
                consistency_scores.append(node_consistency)
        
        return float(np.mean(consistency_scores)) if consistency_scores else 0.0
    
    def _calculate_topological_similarity(self, curr_subgraph: Dict, hist_subgraph: Dict) -> float:
        """è®¡ç®—æ‹“æ‰‘ç‰¹å¾ç›¸ä¼¼æ€§"""
        curr_signature = curr_subgraph.get('topology_signature', [])
        hist_signature = hist_subgraph.get('topology_signature', [])
        
        if not curr_signature or not hist_signature:
            return 0.5  # é»˜è®¤ä¸­ç­‰ç›¸ä¼¼åº¦
        
        # ç¡®ä¿ç­¾åé•¿åº¦ä¸€è‡´
        min_len = min(len(curr_signature), len(hist_signature))
        if min_len == 0:
            return 0.5
        
        curr_sig = curr_signature[:min_len]
        hist_sig = hist_signature[:min_len]
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        curr_norm = np.linalg.norm(curr_sig)
        hist_norm = np.linalg.norm(hist_sig)
        
        if curr_norm > 0 and hist_norm > 0:
            cosine_sim = np.dot(curr_sig, hist_sig) / (curr_norm * hist_norm)
            return float(max(0.0, cosine_sim))
        else:
            return 1.0 if curr_norm == hist_norm == 0 else 0.0
    
    def _calculate_edge_weight_consistency(self, curr_subgraph: Dict, hist_subgraph: Dict,
                                         mapping: Dict[int, int]) -> float:
        """è®¡ç®—è¾¹æƒé‡ä¸€è‡´æ€§"""
        curr_adj = curr_subgraph['adj_matrix']
        hist_adj = hist_subgraph['adj_matrix']
        curr_nodes = curr_subgraph['nodes']
        hist_nodes = hist_subgraph['nodes']
        
        if len(curr_nodes) != len(hist_nodes):
            return 0.0
        
        curr_to_idx = {node: i for i, node in enumerate(curr_nodes)}
        hist_to_idx = {node: i for i, node in enumerate(hist_nodes)}
        
        weight_differences = []
        
        for curr_node in curr_nodes:
            if curr_node in mapping:
                hist_node = mapping[curr_node]
                curr_idx = curr_to_idx[curr_node]
                hist_idx = hist_to_idx[hist_node]
                
                for curr_neighbor in curr_nodes:
                    if curr_neighbor in mapping and curr_neighbor != curr_node:
                        hist_neighbor = mapping[curr_neighbor]
                        curr_neighbor_idx = curr_to_idx[curr_neighbor]
                        hist_neighbor_idx = hist_to_idx[hist_neighbor]
                        
                        curr_weight = curr_adj[curr_idx][curr_neighbor_idx]
                        hist_weight = hist_adj[hist_idx][hist_neighbor_idx]
                        
                        if curr_weight > 0 or hist_weight > 0:
                            max_weight = max(curr_weight, hist_weight, 1e-10)
                            weight_diff = abs(curr_weight - hist_weight) / max_weight
                            weight_differences.append(1.0 - weight_diff)
        
        return float(np.mean(weight_differences)) if weight_differences else 1.0
    
    def _apply_quality_enhancement(self, base_quality: float, subgraph_size: int) -> float:
        """åº”ç”¨è´¨é‡å¢å¼ºå‡½æ•°"""
        # å¤§å°å¥–åŠ±ï¼šè¾ƒå¤§çš„å­å›¾åŒæ„æ›´æœ‰ä»·å€¼
        size_bonus = min(0.1, (subgraph_size - 3) * 0.02)
        
        # éçº¿æ€§å¢å¼º
        if base_quality > 0.8:
            enhanced = base_quality + size_bonus + (base_quality - 0.8) * 0.5
        elif base_quality < 0.3:
            enhanced = base_quality * 0.8  # é™ä½ä½è´¨é‡åˆ†æ•°
        else:
            enhanced = base_quality + size_bonus
        
        return enhanced
    
    def _calculate_overall_isomorphism_score(self, subgraph_matches: List[Dict]) -> float:
        """è®¡ç®—æ€»ä½“åŒæ„åˆ†æ•°"""
        if not subgraph_matches:
            return 0.0
        
        # å¤šå› å­è¯„åˆ†æ¨¡å‹
        quality_scores = [match['quality'] for match in subgraph_matches]
        size_scores = [match['size'] / 10.0 for match in subgraph_matches]  # å½’ä¸€åŒ–å¤§å°åˆ†æ•°
        edge_preservation_scores = [match.get('edge_preservation_ratio', 0.5) for match in subgraph_matches]
        
        # åŠ æƒç»„åˆ
        component_scores = []
        for i in range(len(subgraph_matches)):
            weighted_score = (
                quality_scores[i] * 0.5 +
                size_scores[i] * 0.3 +
                edge_preservation_scores[i] * 0.2
            )
            component_scores.append(weighted_score)
        
        # ä½¿ç”¨å¯¹æ•°å¹³å‡é¿å…å•ä¸€é«˜åˆ†é¡¹ä¸»å¯¼
        if component_scores:
            # å–å‰5ä¸ªæœ€å¥½çš„åŒ¹é…
            top_scores = sorted(component_scores, reverse=True)[:5]
            overall_score = np.mean(top_scores)
            
            # è€ƒè™‘åŒ¹é…æ•°é‡çš„å¥–åŠ±
            quantity_bonus = min(0.1, len(subgraph_matches) * 0.01)
            final_score = overall_score + quantity_bonus
            
            return float(min(1.0, final_score))
        else:
            return 0.0

    def _calculate_edge_preservation_ratio(self, curr_adj: np.ndarray, hist_adj: np.ndarray,
                                         mapping: Dict[int, int]) -> float:
        """è®¡ç®—è¾¹ä¿æŒæ¯”ç‡"""
        if not mapping or len(curr_adj) == 0 or len(hist_adj) == 0:
            return 0.0
        
        preserved_edges = 0
        total_edges = 0
        
        n = len(curr_adj)
        for i in range(n):
            for j in range(i + 1, n):
                curr_edge_exists = curr_adj[i][j] > 0
                hist_edge_exists = hist_adj[i][j] > 0
                
                total_edges += 1
                if curr_edge_exists == hist_edge_exists:
                    preserved_edges += 1
        
        return preserved_edges / total_edges if total_edges > 0 else 1.0
    
    def _calculate_graph_edit_distances(self, current_graph: Dict, historical_graphs: List[Dict]) -> List[float]:
        """
        è®¡ç®—å›¾ç¼–è¾‘è·ç¦»çš„å®Œæ•´å®ç°
        å®ç°ç²¾ç¡®çš„å›¾ç¼–è¾‘è·ç¦»ç®—æ³•
        """
        edit_distances = []
        
        current_adj_matrix = np.array(current_graph['adjacency_matrix'])
        current_node_props = current_graph['node_properties']
        
        for hist_graph in historical_graphs:
            hist_adj_matrix = np.array(hist_graph['adjacency_matrix'])
            hist_node_props = hist_graph['node_properties']
            
            # è®¡ç®—ç»¼åˆå›¾ç¼–è¾‘è·ç¦»
            edit_distance = self._comprehensive_graph_edit_distance(
                current_adj_matrix, hist_adj_matrix,
                current_node_props, hist_node_props
            )
            
            edit_distances.append(edit_distance)
        
        return edit_distances

    def _comprehensive_graph_edit_distance(self, adj1: np.ndarray, adj2: np.ndarray,
                                        props1: Dict, props2: Dict) -> float:
        """è®¡ç®—ç»¼åˆå›¾ç¼–è¾‘è·ç¦»"""
        n = len(adj1)
        
        # 1. ç»“æ„ç¼–è¾‘è·ç¦»
        structural_distance = 0.0
        
        # è¾¹çš„å¢åŠ /åˆ é™¤æˆæœ¬
        for i in range(n):
            for j in range(i + 1, n):
                edge1_exists = adj1[i][j] > 0
                edge2_exists = adj2[i][j] > 0
                
                if edge1_exists != edge2_exists:
                    structural_distance += 1.0  # è¾¹çš„æ’å…¥/åˆ é™¤æˆæœ¬
                elif edge1_exists and edge2_exists:
                    # è¾¹æƒé‡å·®å¼‚æˆæœ¬
                    weight_diff = abs(adj1[i][j] - adj2[i][j])
                    structural_distance += weight_diff * 0.5
    
        # 2. èŠ‚ç‚¹å±æ€§ç¼–è¾‘è·ç¦»
        node_distance = 0.0
        
        for node in range(n):
            prop1 = props1.get(node, {})
            prop2 = props2.get(node, {})
            
            # æ¯”è¾ƒå…³é”®å±æ€§
            key_attributes = ['frequency', 'is_present', 'recent_trend', 'stability']
            
            for attr in key_attributes:
                val1 = prop1.get(attr, 0.0)
                val2 = prop2.get(attr, 0.0)
                node_distance += abs(val1 - val2)
        
        # 3. å…¨å±€å›¾å±æ€§è·ç¦»
        global_distance = 0.0
        
        # å¯†åº¦å·®å¼‚
        density1 = np.sum(adj1 > 0) / (n * (n - 1))
        density2 = np.sum(adj2 > 0) / (n * (n - 1))
        global_distance += abs(density1 - density2) * 10  # æ”¾å¤§å¯†åº¦å·®å¼‚çš„å½±å“
        
        # è¿é€šæ€§å·®å¼‚
        components1 = len(self._find_connected_components(adj1))
        components2 = len(self._find_connected_components(adj2))
        global_distance += abs(components1 - components2) * 2
        
        # ç»¼åˆç¼–è¾‘è·ç¦»
        total_distance = (
            structural_distance * 0.5 +
            node_distance * 0.3 +
            global_distance * 0.2
        )
    
        # å½’ä¸€åŒ–åˆ°[0, 1]
        max_possible_distance = n * (n - 1) / 2 + n * len(key_attributes) + 20
        normalized_distance = total_distance / max_possible_distance
        
        return min(1.0, normalized_distance)

    def _calculate_graph_kernel_similarities(self, current_graph: Dict, historical_graphs: List[Dict]) -> List[float]:
        """
        è®¡ç®—å›¾æ ¸ç›¸ä¼¼åº¦çš„å®Œæ•´å®ç°
        å®ç°å¤šç§å›¾æ ¸å‡½æ•°
        """
        kernel_similarities = []
        
        current_adj_matrix = np.array(current_graph['adjacency_matrix'])
        
        for hist_graph in historical_graphs:
            hist_adj_matrix = np.array(hist_graph['adjacency_matrix'])
            
            # 1. éšæœºæ¸¸èµ°æ ¸
            rw_kernel_sim = self._random_walk_kernel(current_adj_matrix, hist_adj_matrix)
            
            # 2. æœ€çŸ­è·¯å¾„æ ¸
            sp_kernel_sim = self._shortest_path_kernel(current_adj_matrix, hist_adj_matrix)
            
            # 3. å­å›¾æ ¸
            subgraph_kernel_sim = self._subgraph_kernel(current_adj_matrix, hist_adj_matrix)
            
            # 4. Weisfeiler-Lehmanæ ¸
            wl_kernel_sim = self._weisfeiler_lehman_kernel(current_adj_matrix, hist_adj_matrix)
            
            # ç»¼åˆæ ¸ç›¸ä¼¼åº¦
            combined_similarity = (
                rw_kernel_sim * 0.3 +
                sp_kernel_sim * 0.25 +
                subgraph_kernel_sim * 0.25 +
                wl_kernel_sim * 0.2
            )
        
            kernel_similarities.append(combined_similarity)
        
        return kernel_similarities

    def _random_walk_kernel(self, adj1: np.ndarray, adj2: np.ndarray, 
                        max_steps: int = 10, lambda_param: float = 0.01) -> float:
        """éšæœºæ¸¸èµ°æ ¸å®ç°"""
        n = len(adj1)
        
        # æ„å»ºè½¬ç§»æ¦‚ç‡çŸ©é˜µ
        def normalize_adjacency(adj):
            row_sums = np.sum(adj, axis=1)
            row_sums[row_sums == 0] = 1  # é¿å…é™¤é›¶
            return adj / row_sums[:, np.newaxis]
        
        P1 = normalize_adjacency(adj1)
        P2 = normalize_adjacency(adj2)
        
        # è®¡ç®—éšæœºæ¸¸èµ°æ ¸
        kernel_value = 0.0
        
        # åˆå§‹åˆ†å¸ƒï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        q = np.ones(n) / n
        
        for step in range(max_steps):
            # è®¡ç®—æ­¥é•¿ä¸ºstepçš„æ¸¸èµ°æ¦‚ç‡
            if step == 0:
                P1_step = np.eye(n)
                P2_step = np.eye(n)
            else:
                P1_step = np.linalg.matrix_power(P1, step)
                P2_step = np.linalg.matrix_power(P2, step)
        
            # è®¡ç®—æ ¸è´¡çŒ®
            step_contribution = 0.0
            for i in range(n):
                for j in range(n):
                    step_contribution += q[i] * P1_step[i, j] * P2_step[i, j] * q[j]
            
            # åŠ æƒç´¯åŠ 
            kernel_value += (lambda_param ** step) * step_contribution
        
        return kernel_value

    def _shortest_path_kernel(self, adj1: np.ndarray, adj2: np.ndarray) -> float:
        """æœ€çŸ­è·¯å¾„æ ¸å®ç°"""
        # è®¡ç®—æ‰€æœ‰ç‚¹å¯¹æœ€çŸ­è·¯å¾„
        def floyd_warshall(adj):
            n = len(adj)
            dist = np.full((n, n), np.inf)
            
            # åˆå§‹åŒ–
            for i in range(n):
                for j in range(n):
                    if i == j:
                        dist[i][j] = 0
                    elif adj[i][j] > 0:
                        dist[i][j] = 1  # ç®€åŒ–ï¼šæ‰€æœ‰è¾¹æƒé‡ä¸º1
            
            # Floyd-Warshallç®—æ³•
            for k in range(n):
                for i in range(n):
                    for j in range(n):
                        if dist[i][k] + dist[k][j] < dist[i][j]:
                            dist[i][j] = dist[i][k] + dist[k][j]
            
            return dist
    
        dist1 = floyd_warshall(adj1)
        dist2 = floyd_warshall(adj2)
        
        # è®¡ç®—æœ€çŸ­è·¯å¾„åˆ†å¸ƒçš„ç›¸ä¼¼åº¦
        max_dist = max(np.max(dist1[dist1 != np.inf]), np.max(dist2[dist2 != np.inf]))
        if max_dist == 0:
            return 1.0
        
        # ç»Ÿè®¡è·¯å¾„é•¿åº¦åˆ†å¸ƒ
        hist1 = np.zeros(int(max_dist) + 1)
        hist2 = np.zeros(int(max_dist) + 1)
        
        n = len(adj1)
        for i in range(n):
            for j in range(n):
                if dist1[i][j] != np.inf:
                    hist1[int(dist1[i][j])] += 1
                if dist2[i][j] != np.inf:
                    hist2[int(dist2[i][j])] += 1
        
        # å½’ä¸€åŒ–
        hist1 = hist1 / np.sum(hist1) if np.sum(hist1) > 0 else hist1
        hist2 = hist2 / np.sum(hist2) if np.sum(hist2) > 0 else hist2
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(hist1, hist2)
        norm1 = np.linalg.norm(hist1)
        norm2 = np.linalg.norm(hist2)
        
        if norm1 > 0 and norm2 > 0:
            similarity = dot_product / (norm1 * norm2)
        else:
            similarity = 0.0
        
        return similarity
    
    def _subgraph_kernel(self, adj1: np.ndarray, adj2: np.ndarray) -> float:
        """å­å›¾æ ¸å®ç°"""
        # æå–æ‰€æœ‰å¤§å°ä¸º3çš„å­å›¾
        def extract_subgraphs_of_size(adj, size):
            n = len(adj)
            subgraphs = []
        
            from itertools import combinations
            for nodes in combinations(range(n), size):
                subgraph_adj = adj[np.ix_(nodes, nodes)]
                # å°†å­å›¾æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨è§„èŒƒæ ‡è®°ï¼‰
                canonical_form = self._canonicalize_subgraph(subgraph_adj)
                subgraphs.append(canonical_form)
            
            return subgraphs
        
        # æå–å¤§å°ä¸º3çš„å­å›¾
        subgraphs1 = extract_subgraphs_of_size(adj1, 3)
        subgraphs2 = extract_subgraphs_of_size(adj2, 3)
        
        # ç»Ÿè®¡å­å›¾ç±»å‹
        from collections import Counter
        counter1 = Counter(subgraphs1)
        counter2 = Counter(subgraphs2)
        
        # è®¡ç®—äº¤é›†
        all_subgraph_types = set(counter1.keys()) | set(counter2.keys())
        
        # æ„å»ºç‰¹å¾å‘é‡
        vec1 = np.array([counter1.get(sg_type, 0) for sg_type in all_subgraph_types])
        vec2 = np.array([counter2.get(sg_type, 0) for sg_type in all_subgraph_types])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        if np.linalg.norm(vec1) > 0 and np.linalg.norm(vec2) > 0:
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        else:
            similarity = 0.0
        
        return similarity

    def _canonicalize_subgraph(self, subgraph_adj: np.ndarray) -> str:
        """å­å›¾è§„èŒƒåŒ–"""
        # ç®€å•çš„è§„èŒƒåŒ–ï¼šå°†é‚»æ¥çŸ©é˜µè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤º
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„è§„èŒƒåŒ–ç®—æ³•
        return str(subgraph_adj.flatten().tolist())

    def _weisfeiler_lehman_kernel(self, adj1: np.ndarray, adj2: np.ndarray, 
                                max_iterations: int = 5) -> float:
        """Weisfeiler-Lehmanæ ¸å®ç°"""
        n = len(adj1)
        
        # åˆå§‹åŒ–èŠ‚ç‚¹æ ‡ç­¾
        labels1 = {i: str(i) for i in range(n)}
        labels2 = {i: str(i) for i in range(n)}
        
        # å­˜å‚¨æ¯æ¬¡è¿­ä»£çš„æ ‡ç­¾åˆ†å¸ƒ
        all_labels1 = []
        all_labels2 = []
    
        for iteration in range(max_iterations):
            # è®°å½•å½“å‰æ ‡ç­¾åˆ†å¸ƒ
            from collections import Counter
            all_labels1.extend(list(labels1.values()))
            all_labels2.extend(list(labels2.values()))
            
            # æ›´æ–°æ ‡ç­¾
            new_labels1 = {}
            new_labels2 = {}
            
            for node in range(n):
                # æ”¶é›†é‚»å±…æ ‡ç­¾
                neighbors1 = [labels1[j] for j in range(n) if adj1[node][j] > 0]
                neighbors2 = [labels2[j] for j in range(n) if adj2[node][j] > 0]
                
                # æ’åºå¹¶è¿æ¥
                neighbors1.sort()
                neighbors2.sort()
                
                # åˆ›å»ºæ–°æ ‡ç­¾
                new_labels1[node] = labels1[node] + ''.join(neighbors1)
                new_labels2[node] = labels2[node] + ''.join(neighbors2)
        
            labels1 = new_labels1
            labels2 = new_labels2
        
        # æ·»åŠ æœ€åä¸€æ¬¡è¿­ä»£çš„æ ‡ç­¾
        all_labels1.extend(list(labels1.values()))
        all_labels2.extend(list(labels2.values()))
        
        # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒç›¸ä¼¼åº¦
        counter1 = Counter(all_labels1)
        counter2 = Counter(all_labels2)
        
        all_label_types = set(counter1.keys()) | set(counter2.keys())
        
        vec1 = np.array([counter1.get(label, 0) for label in all_label_types])
        vec2 = np.array([counter2.get(label, 0) for label in all_label_types])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        if np.linalg.norm(vec1) > 0 and np.linalg.norm(vec2) > 0:
            similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        else:
            similarity = 0.0
    
        return similarity

    def _detect_community_patterns(self, historical_graphs: List[Dict], current_graph: Dict) -> Dict:
        """
        æ£€æµ‹ç¤¾åŒºæ¨¡å¼çš„å®Œæ•´å®ç°
        ä½¿ç”¨å¤šç§ç¤¾åŒºæ£€æµ‹ç®—æ³•
        """
        # 1. å¯¹å†å²å›¾è¿›è¡Œç¤¾åŒºæ£€æµ‹
        historical_communities = []
        
        for graph in historical_graphs:
            adj_matrix = np.array(graph['adjacency_matrix'])
            
            # Louvainç®—æ³•çš„ç®€åŒ–ç‰ˆæœ¬
            communities = self._louvain_community_detection(adj_matrix)
            
            # è®¡ç®—ç¤¾åŒºè´¨é‡æŒ‡æ ‡
            modularity = self._calculate_modularity(adj_matrix, communities)
            
            historical_communities.append({
                'communities': communities,
                'modularity': modularity,
                'num_communities': len(communities),
                'community_sizes': [len(comm) for comm in communities],
                'largest_community_size': max(len(comm) for comm in communities) if communities else 0
            })
        
        # 2. å¯¹å½“å‰å›¾è¿›è¡Œç¤¾åŒºæ£€æµ‹
        current_adj_matrix = np.array(current_graph['adjacency_matrix'])
        current_communities = self._louvain_community_detection(current_adj_matrix)
        current_modularity = self._calculate_modularity(current_adj_matrix, current_communities)
        
        current_community_info = {
            'communities': current_communities,
            'modularity': current_modularity,
            'num_communities': len(current_communities),
            'community_sizes': [len(comm) for comm in current_communities],
            'largest_community_size': max(len(comm) for comm in current_communities) if current_communities else 0
        }
    
        # 3. åˆ†æç¤¾åŒºæ¨¡å¼çš„æ¼”åŒ–
        pattern_evolution = self._analyze_community_evolution(historical_communities, current_community_info)
        
        # 4. è¯†åˆ«ç¨³å®šçš„ç¤¾åŒºç»“æ„
        stable_communities = self._identify_stable_communities(historical_communities, current_community_info)
        
        return {
            'historical_communities': historical_communities,
            'current_communities': current_community_info,
            'pattern_evolution': pattern_evolution,
            'stable_communities': stable_communities,
            'community_stability_score': self._calculate_community_stability_score(historical_communities, current_community_info),
            'community_anomaly_score': self._calculate_community_anomaly_score(historical_communities, current_community_info)
        }

    def _louvain_community_detection(self, adj_matrix: np.ndarray, resolution: float = 1.0) -> List[List[int]]:
        """
        Louvainç¤¾åŒºæ£€æµ‹ç®—æ³•çš„å®Œæ•´å®ç°
        """
        n = len(adj_matrix)
        
        # åˆå§‹åŒ–ï¼šæ¯ä¸ªèŠ‚ç‚¹ä¸ºä¸€ä¸ªç¤¾åŒº
        communities = {i: [i] for i in range(n)}
        node_to_community = {i: i for i in range(n)}
        
        # è®¡ç®—æ€»è¾¹æƒé‡
        total_weight = np.sum(adj_matrix) / 2  # æ— å‘å›¾
        
        improved = True
        iteration = 0
        max_iterations = 100
        
        while improved and iteration < max_iterations:
            improved = False
            iteration += 1
        
            for node in range(n):
                current_community = node_to_community[node]
                current_modularity_gain = 0
                
                # å°è¯•å°†èŠ‚ç‚¹ç§»åŠ¨åˆ°é‚»å±…çš„ç¤¾åŒº
                neighbor_communities = set()
                for neighbor in range(n):
                    if adj_matrix[node][neighbor] > 0 and neighbor != node:
                        neighbor_communities.add(node_to_community[neighbor])
            
                best_community = current_community
                best_gain = 0
            
                for target_community in neighbor_communities:
                    if target_community != current_community:
                        # è®¡ç®—æ¨¡å—åº¦å¢ç›Š
                        gain = self._calculate_modularity_gain(
                            node, current_community, target_community,
                            adj_matrix, communities, total_weight, resolution
                        )
                    
                        if gain > best_gain:
                            best_gain = gain
                            best_community = target_community
            
                # å¦‚æœæœ‰æ”¹è¿›ï¼Œç§»åŠ¨èŠ‚ç‚¹
                if best_gain > 0:
                    # ä»å½“å‰ç¤¾åŒºç§»é™¤
                    communities[current_community].remove(node)
                    if not communities[current_community]:
                        del communities[current_community]
                
                    # æ·»åŠ åˆ°æ–°ç¤¾åŒº
                    if best_community not in communities:
                        communities[best_community] = []
                    communities[best_community].append(node)
                    node_to_community[node] = best_community
                
                    improved = True
    
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        return [comm for comm in communities.values() if comm]

    def _analyze_community_evolution(self, historical_communities: List[Dict], 
                                   current_community_info: Dict) -> Dict:
        """
        ç¤¾åŒºæ¼”åŒ–åˆ†æçš„å®Œæ•´ç§‘ç ”çº§å®ç°
        åŸºäºåŠ¨æ€ç½‘ç»œåˆ†æå’Œç¤¾åŒºè¿½è¸ªç®—æ³•
        """
        if not historical_communities:
            return {
                'evolution_type': 'insufficient_data',
                'stability_score': 0.0,
                'change_events': [],
                'evolution_trajectory': []
            }
        
        # 1. ç¤¾åŒºå˜åŒ–äº‹ä»¶æ£€æµ‹
        change_events = self._detect_community_change_events(historical_communities, current_community_info)
        
        # 2. ç¤¾åŒºç¨³å®šæ€§åˆ†æ
        stability_analysis = self._analyze_community_stability(historical_communities, current_community_info)
        
        # 3. æ¼”åŒ–è½¨è¿¹æ„å»º
        evolution_trajectory = self._construct_evolution_trajectory(historical_communities, current_community_info)
        
        # 4. æ¼”åŒ–æ¨¡å¼åˆ†ç±»
        evolution_type = self._classify_evolution_pattern(change_events, stability_analysis, evolution_trajectory)
        
        # 5. æ¼”åŒ–é©±åŠ¨åŠ›åˆ†æ
        driving_forces = self._analyze_evolution_driving_forces(historical_communities, current_community_info)
        
        return {
            'evolution_type': evolution_type,
            'stability_score': stability_analysis['overall_stability'],
            'change_events': change_events,
            'evolution_trajectory': evolution_trajectory,
            'driving_forces': driving_forces,
            'community_lifecycle': self._analyze_community_lifecycle(historical_communities, current_community_info),
            'prediction_confidence': self._calculate_evolution_prediction_confidence(stability_analysis, change_events)
        }
    
    def _detect_community_change_events(self, historical_communities: List[Dict], 
                                      current_community_info: Dict) -> List[Dict]:
        """æ£€æµ‹ç¤¾åŒºå˜åŒ–äº‹ä»¶"""
        change_events = []
        
        if not historical_communities:
            return change_events
        
        # è·å–æœ€è¿‘çš„å†å²ç¤¾åŒºå’Œå½“å‰ç¤¾åŒº
        recent_communities = historical_communities[-1]['communities'] if historical_communities else []
        current_communities = current_community_info['communities']
        
        # æ„å»ºç¤¾åŒºåŒ¹é…çŸ©é˜µ
        community_matches = self._build_community_matching_matrix(recent_communities, current_communities)
        
        # æ£€æµ‹å„ç§å˜åŒ–äº‹ä»¶
        # 1. ç¤¾åŒºåˆ†è£‚äº‹ä»¶
        split_events = self._detect_community_splits(recent_communities, current_communities, community_matches)
        change_events.extend(split_events)
        
        # 2. ç¤¾åŒºåˆå¹¶äº‹ä»¶
        merge_events = self._detect_community_merges(recent_communities, current_communities, community_matches)
        change_events.extend(merge_events)
        
        # 3. ç¤¾åŒºå‡ºç°äº‹ä»¶
        birth_events = self._detect_community_births(recent_communities, current_communities, community_matches)
        change_events.extend(birth_events)
        
        # 4. ç¤¾åŒºæ¶ˆå¤±äº‹ä»¶
        death_events = self._detect_community_deaths(recent_communities, current_communities, community_matches)
        change_events.extend(death_events)
        
        # 5. ç¤¾åŒºæ¼”åŒ–äº‹ä»¶
        evolution_events = self._detect_community_evolutions(recent_communities, current_communities, community_matches)
        change_events.extend(evolution_events)
        
        # æŒ‰é‡è¦æ€§æ’åº
        change_events.sort(key=lambda x: x.get('significance', 0.0), reverse=True)
        
        return change_events
    
    def _build_community_matching_matrix(self, communities1: List[List[int]], 
                                       communities2: List[List[int]]) -> np.ndarray:
        """æ„å»ºç¤¾åŒºåŒ¹é…çŸ©é˜µ"""
        n1, n2 = len(communities1), len(communities2)
        if n1 == 0 or n2 == 0:
            return np.zeros((max(1, n1), max(1, n2)))
        
        matching_matrix = np.zeros((n1, n2))
        
        for i, comm1 in enumerate(communities1):
            set1 = set(comm1)
            for j, comm2 in enumerate(communities2):
                set2 = set(comm2)
                
                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                intersection = len(set1.intersection(set2))
                union = len(set1.union(set2))
                
                if union > 0:
                    jaccard_sim = intersection / union
                    matching_matrix[i][j] = jaccard_sim
        
        return matching_matrix
    
    def _detect_community_splits(self, old_communities: List[List[int]], 
                                new_communities: List[List[int]], 
                                matching_matrix: np.ndarray) -> List[Dict]:
        """æ£€æµ‹ç¤¾åŒºåˆ†è£‚äº‹ä»¶"""
        split_events = []
        
        if len(old_communities) == 0:
            return split_events
        
        for i, old_comm in enumerate(old_communities):
            # æ‰¾åˆ°ä¸æ—§ç¤¾åŒºåŒ¹é…åº¦æœ€é«˜çš„æ–°ç¤¾åŒºä»¬
            if i < len(matching_matrix):
                matches = [(j, matching_matrix[i][j]) for j in range(len(new_communities)) 
                          if matching_matrix[i][j] > 0.3]  # é˜ˆå€¼
                
                if len(matches) > 1:  # ä¸€ä¸ªæ—§ç¤¾åŒºå¯¹åº”å¤šä¸ªæ–°ç¤¾åŒº
                    # éªŒè¯æ˜¯å¦çœŸçš„æ˜¯åˆ†è£‚ï¼ˆæ€»è¦†ç›–ç‡è¦é«˜ï¼‰
                    total_coverage = sum(similarity for _, similarity in matches)
                    
                    if total_coverage > 0.7:  # é«˜è¦†ç›–ç‡è¡¨ç¤ºåˆ†è£‚
                        split_events.append({
                            'type': 'split',
                            'source_community': old_comm,
                            'target_communities': [new_communities[j] for j, _ in matches],
                            'split_ratio': len(matches),
                            'coverage_ratio': total_coverage,
                            'significance': total_coverage * len(matches) * 0.2,
                            'timestamp': len(old_communities)  # ç®€åŒ–çš„æ—¶é—´æˆ³
                        })
        
        return split_events
    
    def _detect_community_merges(self, old_communities: List[List[int]], 
                                new_communities: List[List[int]], 
                                matching_matrix: np.ndarray) -> List[Dict]:
        """æ£€æµ‹ç¤¾åŒºåˆå¹¶äº‹ä»¶"""
        merge_events = []
        
        if len(new_communities) == 0:
            return merge_events
        
        for j, new_comm in enumerate(new_communities):
            # æ‰¾åˆ°ä¸æ–°ç¤¾åŒºåŒ¹é…çš„æ—§ç¤¾åŒºä»¬
            matches = []
            for i in range(len(old_communities)):
                if i < len(matching_matrix) and j < matching_matrix.shape[1]:
                    if matching_matrix[i][j] > 0.3:
                        matches.append((i, matching_matrix[i][j]))
            
            if len(matches) > 1:  # å¤šä¸ªæ—§ç¤¾åŒºå¯¹åº”ä¸€ä¸ªæ–°ç¤¾åŒº
                # éªŒè¯åˆå¹¶çš„æœ‰æ•ˆæ€§
                total_coverage = sum(similarity for _, similarity in matches)
                
                if total_coverage > 0.7:
                    merge_events.append({
                        'type': 'merge',
                        'source_communities': [old_communities[i] for i, _ in matches],
                        'target_community': new_comm,
                        'merge_ratio': len(matches),
                        'coverage_ratio': total_coverage,
                        'significance': total_coverage * len(matches) * 0.25,
                        'timestamp': len(old_communities)
                    })
        
        return merge_events
    
    def _detect_community_births(self, old_communities: List[List[int]], 
                                new_communities: List[List[int]], 
                                matching_matrix: np.ndarray) -> List[Dict]:
        """æ£€æµ‹ç¤¾åŒºå‡ºç°äº‹ä»¶"""
        birth_events = []
        
        for j, new_comm in enumerate(new_communities):
            # æ£€æŸ¥æ–°ç¤¾åŒºæ˜¯å¦ä¸ä»»ä½•æ—§ç¤¾åŒºæœ‰æ˜¾è‘—é‡å 
            has_significant_overlap = False
            
            for i in range(len(old_communities)):
                if (i < len(matching_matrix) and j < matching_matrix.shape[1] and 
                    matching_matrix[i][j] > 0.5):
                    has_significant_overlap = True
                    break
            
            if not has_significant_overlap:
                # è¿™æ˜¯ä¸€ä¸ªæ–°å‡ºç°çš„ç¤¾åŒº
                birth_events.append({
                    'type': 'birth',
                    'new_community': new_comm,
                    'community_size': len(new_comm),
                    'novelty_score': 1.0,  # å®Œå…¨æ–°é¢–
                    'significance': len(new_comm) * 0.15,
                    'timestamp': len(old_communities)
                })
        
        return birth_events
    
    def _detect_community_deaths(self, old_communities: List[List[int]], 
                                new_communities: List[List[int]], 
                                matching_matrix: np.ndarray) -> List[Dict]:
        """æ£€æµ‹ç¤¾åŒºæ¶ˆå¤±äº‹ä»¶"""
        death_events = []
        
        for i, old_comm in enumerate(old_communities):
            # æ£€æŸ¥æ—§ç¤¾åŒºæ˜¯å¦åœ¨æ–°ç¤¾åŒºä¸­æœ‰å»¶ç»­
            has_continuation = False
            
            if i < len(matching_matrix):
                for j in range(len(new_communities)):
                    if j < matching_matrix.shape[1] and matching_matrix[i][j] > 0.5:
                        has_continuation = True
                        break
            
            if not has_continuation:
                # è¿™ä¸ªç¤¾åŒºæ¶ˆå¤±äº†
                death_events.append({
                    'type': 'death',
                    'dead_community': old_comm,
                    'community_size': len(old_comm),
                    'dissolution_completeness': 1.0,
                    'significance': len(old_comm) * 0.18,
                    'timestamp': len(old_communities)
                })
        
        return death_events
    
    def _detect_community_evolutions(self, old_communities: List[List[int]], 
                                   new_communities: List[List[int]], 
                                   matching_matrix: np.ndarray) -> List[Dict]:
        """æ£€æµ‹ç¤¾åŒºæ¼”åŒ–äº‹ä»¶ï¼ˆç¨³å®šæ¼”åŒ–ï¼‰"""
        evolution_events = []
        
        if len(old_communities) == 0 or len(new_communities) == 0:
            return evolution_events
        
        for i, old_comm in enumerate(old_communities):
            if i >= len(matching_matrix):
                continue
                
            # æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„æ–°ç¤¾åŒº
            best_match_j = -1
            best_similarity = 0.0
            
            for j in range(len(new_communities)):
                if j < matching_matrix.shape[1] and matching_matrix[i][j] > best_similarity:
                    best_similarity = matching_matrix[i][j]
                    best_match_j = j
            
            # å¦‚æœæœ‰åˆç†çš„åŒ¹é…ä½†ä¸æ˜¯å®Œç¾åŒ¹é…ï¼Œè®¤ä¸ºæ˜¯æ¼”åŒ–
            if 0.5 <= best_similarity < 0.95 and best_match_j != -1:
                new_comm = new_communities[best_match_j]
                
                # åˆ†ææ¼”åŒ–ç‰¹å¾
                old_set = set(old_comm)
                new_set = set(new_comm)
                
                gained_nodes = new_set - old_set
                lost_nodes = old_set - new_set
                stable_nodes = old_set.intersection(new_set)
                
                evolution_events.append({
                    'type': 'evolution',
                    'old_community': old_comm,
                    'new_community': new_comm,
                    'similarity': best_similarity,
                    'gained_nodes': list(gained_nodes),
                    'lost_nodes': list(lost_nodes),
                    'stable_nodes': list(stable_nodes),
                    'growth_rate': (len(new_comm) - len(old_comm)) / len(old_comm) if old_comm else 0,
                    'stability_ratio': len(stable_nodes) / len(old_comm) if old_comm else 0,
                    'significance': best_similarity * max(len(old_comm), len(new_comm)) * 0.1,
                    'timestamp': len(old_communities)
                })
        
        return evolution_events
    
    def _analyze_community_stability(self, historical_communities: List[Dict], 
                                   current_community_info: Dict) -> Dict:
        """åˆ†æç¤¾åŒºç¨³å®šæ€§"""
        if len(historical_communities) < 2:
            return {
                'overall_stability': 0.5,
                'individual_stabilities': [],
                'stability_trend': 'insufficient_data'
            }
        
        # è®¡ç®—è¿ç»­æ—¶é—´çª—å£çš„ç¤¾åŒºç¨³å®šæ€§
        stability_scores = []
        
        for i in range(len(historical_communities) - 1):
            comm1 = historical_communities[i]['communities']
            comm2 = historical_communities[i + 1]['communities']
            
            stability = self._calculate_community_stability_between_snapshots(comm1, comm2)
            stability_scores.append(stability)
        
        # è®¡ç®—ä¸å½“å‰çŠ¶æ€çš„ç¨³å®šæ€§
        if historical_communities:
            last_historical = historical_communities[-1]['communities']
            current_communities = current_community_info['communities']
            current_stability = self._calculate_community_stability_between_snapshots(
                last_historical, current_communities
            )
            stability_scores.append(current_stability)
        
        # åˆ†æç¨³å®šæ€§è¶‹åŠ¿
        if len(stability_scores) >= 3:
            recent_trend = np.mean(stability_scores[-3:])
            earlier_trend = np.mean(stability_scores[:-3]) if len(stability_scores) > 3 else recent_trend
            
            if recent_trend > earlier_trend + 0.1:
                trend = 'increasing'
            elif recent_trend < earlier_trend - 0.1:
                trend = 'decreasing'
            else:
                trend = 'stable'
        else:
            trend = 'insufficient_data'
        
        return {
            'overall_stability': float(np.mean(stability_scores)) if stability_scores else 0.5,
            'individual_stabilities': [float(s) for s in stability_scores],
            'stability_trend': trend,
            'stability_variance': float(np.var(stability_scores)) if len(stability_scores) > 1 else 0.0,
            'min_stability': float(min(stability_scores)) if stability_scores else 0.0,
            'max_stability': float(max(stability_scores)) if stability_scores else 0.0
        }
    
    def _calculate_community_stability_between_snapshots(self, communities1: List[List[int]], 
                                                       communities2: List[List[int]]) -> float:
        """è®¡ç®—ä¸¤ä¸ªç¤¾åŒºå¿«ç…§ä¹‹é—´çš„ç¨³å®šæ€§"""
        if not communities1 or not communities2:
            return 0.0
        
        # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ‰¾åˆ°æœ€ä¼˜åŒ¹é…
        matching_matrix = self._build_community_matching_matrix(communities1, communities2)
        
        if matching_matrix.size == 0:
            return 0.0
        
        # è®¡ç®—æœ€å¤§æƒé‡äºŒåˆ†åŒ¹é…
        optimal_matching = self._hungarian_matching(matching_matrix)
        
        # è®¡ç®—åŸºäºæœ€ä¼˜åŒ¹é…çš„ç¨³å®šæ€§åˆ†æ•°
        stability_components = []
        
        for i, j in optimal_matching:
            if i < len(communities1) and j < len(communities2):
                similarity = matching_matrix[i][j]
                weight = (len(communities1[i]) + len(communities2[j])) / 2
                stability_components.append(similarity * weight)
        
        # è€ƒè™‘æœªåŒ¹é…çš„ç¤¾åŒºï¼ˆé™ä½ç¨³å®šæ€§ï¼‰
        total_communities = len(communities1) + len(communities2)
        matched_communities = len(optimal_matching) * 2
        
        if total_communities > 0:
            match_ratio = matched_communities / total_communities
            base_stability = np.sum(stability_components) / len(stability_components) if stability_components else 0
            
            # åŠ æƒç¨³å®šæ€§åˆ†æ•°
            weighted_stability = base_stability * match_ratio
            return min(1.0, weighted_stability)
        else:
            return 1.0
    
    def _hungarian_matching(self, cost_matrix: np.ndarray) -> List[Tuple[int, int]]:
        """åŒˆç‰™åˆ©ç®—æ³•çš„ç®€åŒ–å®ç°"""
        if cost_matrix.size == 0:
            return []
        
        # è½¬æ¢ä¸ºæœ€å°åŒ–é—®é¢˜ï¼ˆåŒˆç‰™åˆ©ç®—æ³•æ±‚æœ€å°å€¼ï¼‰
        max_cost = np.max(cost_matrix)
        min_cost_matrix = max_cost - cost_matrix
        
        # ç®€åŒ–ç‰ˆåŒˆç‰™åˆ©ç®—æ³•
        n_rows, n_cols = cost_matrix.shape
        
        # è´ªå©ªåŒ¹é…ä½œä¸ºè¿‘ä¼¼è§£
        used_rows = set()
        used_cols = set()
        matching = []
        
        # åˆ›å»ºæˆæœ¬-æ”¶ç›Šå¯¹åˆ—è¡¨å¹¶æ’åº
        candidates = []
        for i in range(n_rows):
            for j in range(n_cols):
                candidates.append((cost_matrix[i][j], i, j))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        candidates.sort(reverse=True)
        
        # è´ªå©ªé€‰æ‹©
        for similarity, i, j in candidates:
            if i not in used_rows and j not in used_cols and similarity > 0.3:  # é˜ˆå€¼
                matching.append((i, j))
                used_rows.add(i)
                used_cols.add(j)
        
        return matching
    
    def _construct_evolution_trajectory(self, historical_communities: List[Dict], 
                                      current_community_info: Dict) -> List[Dict]:
        """æ„å»ºæ¼”åŒ–è½¨è¿¹"""
        trajectory = []
        
        all_snapshots = historical_communities + [current_community_info]
        
        for i, snapshot in enumerate(all_snapshots):
            communities = snapshot['communities']
            
            trajectory_point = {
                'timestamp': i,
                'num_communities': len(communities),
                'modularity': snapshot.get('modularity', 0.0),
                'largest_community_size': snapshot.get('largest_community_size', 0),
                'community_sizes': snapshot.get('community_sizes', []),
                'size_distribution': self._analyze_size_distribution(snapshot.get('community_sizes', [])),
                'structural_features': self._extract_structural_features(communities)
            }
            
            trajectory.append(trajectory_point)
        
        return trajectory
    
    def _analyze_size_distribution(self, community_sizes: List[int]) -> Dict:
        """åˆ†æç¤¾åŒºå¤§å°åˆ†å¸ƒ"""
        if not community_sizes:
            return {'mean': 0, 'std': 0, 'entropy': 0, 'gini': 0}
        
        sizes = np.array(community_sizes)
        
        # åŸºæœ¬ç»Ÿè®¡
        mean_size = float(np.mean(sizes))
        std_size = float(np.std(sizes))
        
        # è®¡ç®—ç†µ
        total_nodes = np.sum(sizes)
        if total_nodes > 0:
            probabilities = sizes / total_nodes
            entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
        else:
            entropy = 0.0
        
        # è®¡ç®—åŸºå°¼ç³»æ•°
        gini = self._calculate_gini_coefficient(sizes)
        
        return {
            'mean': mean_size,
            'std': std_size,
            'entropy': float(entropy),
            'gini': float(gini),
            'max_size': int(np.max(sizes)),
            'min_size': int(np.min(sizes))
        }
    
    def _calculate_gini_coefficient(self, sizes: np.ndarray) -> float:
        """è®¡ç®—åŸºå°¼ç³»æ•°"""
        if len(sizes) == 0:
            return 0.0
        
        sorted_sizes = np.sort(sizes)
        n = len(sorted_sizes)
        
        if np.sum(sorted_sizes) == 0:
            return 0.0
        
        cumsum = np.cumsum(sorted_sizes)
        gini = (2 * np.sum((np.arange(1, n + 1) * sorted_sizes))) / (n * np.sum(sorted_sizes)) - (n + 1) / n
        
        return max(0.0, gini)
    
    def _extract_structural_features(self, communities: List[List[int]]) -> Dict:
        """æå–ç»“æ„ç‰¹å¾"""
        if not communities:
            return {'heterogeneity': 0, 'fragmentation': 0, 'concentration': 0}
        
        sizes = [len(comm) for comm in communities]
        total_nodes = sum(sizes)
        
        # å¼‚è´¨æ€§ï¼šå¤§å°æ–¹å·®
        heterogeneity = float(np.var(sizes) / np.mean(sizes)) if np.mean(sizes) > 0 else 0
        
        # ç¢ç‰‡åŒ–ï¼šç¤¾åŒºæ•°é‡ä¸èŠ‚ç‚¹æ•°çš„æ¯”ç‡
        fragmentation = len(communities) / max(1, total_nodes)
        
        # é›†ä¸­åº¦ï¼šæœ€å¤§ç¤¾åŒºçš„ç›¸å¯¹å¤§å°
        concentration = max(sizes) / max(1, total_nodes) if sizes else 0
        
        return {
            'heterogeneity': heterogeneity,
            'fragmentation': float(fragmentation),
            'concentration': float(concentration)
        }
    
    def _classify_evolution_pattern(self, change_events: List[Dict], 
                                  stability_analysis: Dict, evolution_trajectory: List[Dict]) -> str:
        """åˆ†ç±»æ¼”åŒ–æ¨¡å¼"""
        stability_score = stability_analysis['overall_stability']
        trend = stability_analysis['stability_trend']
        
        # ç»Ÿè®¡å˜åŒ–äº‹ä»¶ç±»å‹
        event_types = [event['type'] for event in change_events]
        event_counts = {
            'split': event_types.count('split'),
            'merge': event_types.count('merge'),
            'birth': event_types.count('birth'),
            'death': event_types.count('death'),
            'evolution': event_types.count('evolution')
        }
        
        total_events = sum(event_counts.values())
        
        # åŸºäºç¨³å®šæ€§å’Œäº‹ä»¶æ¨¡å¼åˆ†ç±»
        if stability_score > 0.8:
            if total_events == 0:
                return 'stable'
            elif event_counts['evolution'] > total_events * 0.7:
                return 'stable_evolution'
            else:
                return 'stable_with_minor_changes'
        
        elif stability_score > 0.6:
            if event_counts['merge'] > event_counts['split']:
                return 'consolidating'
            elif event_counts['split'] > event_counts['merge']:
                return 'fragmenting'
            elif event_counts['birth'] > event_counts['death']:
                return 'growing'
            elif event_counts['death'] > event_counts['birth']:
                return 'shrinking'
            else:
                return 'moderate_evolution'
        
        elif stability_score > 0.4:
            if trend == 'decreasing':
                return 'destabilizing'
            elif event_counts['split'] + event_counts['death'] > total_events * 0.6:
                return 'fragmenting_rapidly'
            elif event_counts['merge'] + event_counts['birth'] > total_events * 0.6:
                return 'restructuring'
            else:
                return 'volatile_evolution'
        
        else:
            return 'highly_unstable'
    
    def _analyze_evolution_driving_forces(self, historical_communities: List[Dict], 
                                        current_community_info: Dict) -> Dict:
        """åˆ†ææ¼”åŒ–é©±åŠ¨åŠ›"""
        driving_forces = {
            'internal_dynamics': 0.0,
            'external_pressure': 0.0,
            'network_growth': 0.0,
            'structural_optimization': 0.0
        }
        
        if len(historical_communities) < 2:
            return driving_forces
        
        # åˆ†æå†…éƒ¨åŠ¨åŠ›å­¦
        modularity_changes = []
        for i in range(len(historical_communities) - 1):
            mod1 = historical_communities[i].get('modularity', 0)
            mod2 = historical_communities[i + 1].get('modularity', 0)
            modularity_changes.append(mod2 - mod1)
        
        # æ·»åŠ å½“å‰ä¸æœ€åå†å²çš„æ¯”è¾ƒ
        current_mod = current_community_info.get('modularity', 0)
        last_historical_mod = historical_communities[-1].get('modularity', 0)
        modularity_changes.append(current_mod - last_historical_mod)
        
        if modularity_changes:
            avg_modularity_change = np.mean(modularity_changes)
            if avg_modularity_change > 0.05:
                driving_forces['structural_optimization'] = 0.8
            elif avg_modularity_change < -0.05:
                driving_forces['internal_dynamics'] = 0.7
        
        # åˆ†æç½‘ç»œå¢é•¿
        size_changes = []
        for i in range(len(historical_communities) - 1):
            size1 = sum(historical_communities[i].get('community_sizes', []))
            size2 = sum(historical_communities[i + 1].get('community_sizes', []))
            if size1 > 0:
                size_changes.append((size2 - size1) / size1)
        
        # æ·»åŠ å½“å‰å¤§å°å˜åŒ–
        current_size = sum(current_community_info.get('community_sizes', []))
        last_size = sum(historical_communities[-1].get('community_sizes', []))
        if last_size > 0:
            size_changes.append((current_size - last_size) / last_size)
        
        if size_changes:
            avg_growth = np.mean(size_changes)
            if avg_growth > 0.1:
                driving_forces['network_growth'] = min(1.0, avg_growth * 2)
            elif avg_growth < -0.1:
                driving_forces['external_pressure'] = min(1.0, abs(avg_growth) * 2)
        
        return driving_forces
    
    def _analyze_community_lifecycle(self, historical_communities: List[Dict], 
                                   current_community_info: Dict) -> Dict:
        """åˆ†æç¤¾åŒºç”Ÿå‘½å‘¨æœŸ"""
        lifecycle_phases = {
            'formation': 0.0,
            'growth': 0.0,
            'maturity': 0.0,
            'decline': 0.0
        }
        
        if not historical_communities:
            lifecycle_phases['formation'] = 1.0
            return lifecycle_phases
        
        # åˆ†æç¤¾åŒºæ•°é‡å˜åŒ–è¶‹åŠ¿
        community_counts = [len(hc['communities']) for hc in historical_communities]
        community_counts.append(len(current_community_info['communities']))
        
        if len(community_counts) >= 3:
            recent_trend = np.polyfit(range(len(community_counts)), community_counts, 1)[0]
            
            if recent_trend > 0.5:
                lifecycle_phases['formation'] = 0.7
                lifecycle_phases['growth'] = 0.3
            elif recent_trend > 0:
                lifecycle_phases['growth'] = 0.8
                lifecycle_phases['maturity'] = 0.2
            elif recent_trend > -0.5:
                lifecycle_phases['maturity'] = 0.9
                lifecycle_phases['decline'] = 0.1
            else:
                lifecycle_phases['decline'] = 0.8
                lifecycle_phases['maturity'] = 0.2
        
        return lifecycle_phases
    
    def _calculate_evolution_prediction_confidence(self, stability_analysis: Dict, 
                                                 change_events: List[Dict]) -> float:
        """è®¡ç®—æ¼”åŒ–é¢„æµ‹ç½®ä¿¡åº¦"""
        stability_score = stability_analysis['overall_stability']
        stability_variance = stability_analysis.get('stability_variance', 0.5)
        
        # åŸºäºç¨³å®šæ€§çš„ç½®ä¿¡åº¦
        stability_confidence = stability_score * (1 - stability_variance)
        
        # åŸºäºäº‹ä»¶ä¸€è‡´æ€§çš„ç½®ä¿¡åº¦
        if change_events:
            event_significance = [event.get('significance', 0.0) for event in change_events]
            avg_significance = np.mean(event_significance)
            event_confidence = min(1.0, avg_significance)
        else:
            event_confidence = 0.8  # æ— äº‹ä»¶é€šå¸¸è¡¨ç¤ºç¨³å®š
        
        # ç»¼åˆç½®ä¿¡åº¦
        overall_confidence = (stability_confidence * 0.6 + event_confidence * 0.4)
        
        return float(min(1.0, max(0.1, overall_confidence)))
    
    def _identify_stable_communities(self, historical_communities: List[Dict], 
                                   current_community_info: Dict) -> List[Dict]:
        """è¯†åˆ«ç¨³å®šç¤¾åŒº"""
        stable_communities = []
        
        if not historical_communities:
            return stable_communities
        
        # è¿½è¸ªç¤¾åŒºåœ¨æ—¶é—´ä¸Šçš„è¿ç»­æ€§
        community_tracks = self._track_communities_over_time(historical_communities, current_community_info)
        
        # è¯„ä¼°æ¯ä¸ªè¿½è¸ªçš„ç¨³å®šæ€§
        for track in community_tracks:
            stability_metrics = self._evaluate_community_track_stability(track)
            
            if stability_metrics['stability_score'] > 0.7:  # ç¨³å®šæ€§é˜ˆå€¼
                stable_communities.append({
                    'community_track': track,
                    'stability_metrics': stability_metrics,
                    'persistence_duration': len(track['snapshots']),
                    'average_size': np.mean([snapshot['size'] for snapshot in track['snapshots']]),
                    'size_variance': np.var([snapshot['size'] for snapshot in track['snapshots']]),
                    'core_members': self._identify_core_members(track),
                    'stability_classification': self._classify_stability_type(stability_metrics)
                })
        
        # æŒ‰ç¨³å®šæ€§æ’åº
        stable_communities.sort(key=lambda x: x['stability_metrics']['stability_score'], reverse=True)
        
        return stable_communities
    
    def _track_communities_over_time(self, historical_communities: List[Dict], 
                                   current_community_info: Dict) -> List[Dict]:
        """è¿½è¸ªç¤¾åŒºéšæ—¶é—´çš„æ¼”åŒ–"""
        all_snapshots = historical_communities + [current_community_info]
        community_tracks = []
        
        if len(all_snapshots) < 2:
            return community_tracks
        
        # åˆå§‹åŒ–è¿½è¸ªï¼Œä»ç¬¬ä¸€ä¸ªå¿«ç…§å¼€å§‹
        for i, initial_community in enumerate(all_snapshots[0]['communities']):
            track = {
                'track_id': i,
                'snapshots': [{
                    'timestamp': 0,
                    'community': initial_community,
                    'size': len(initial_community),
                    'nodes': set(initial_community)
                }],
                'active': True
            }
            community_tracks.append(track)
        
        # è¿½è¸ªåç»­å¿«ç…§ä¸­çš„ç¤¾åŒº
        for t in range(1, len(all_snapshots)):
            current_snapshot_communities = all_snapshots[t]['communities']
            
            # ä¸ºæ¯ä¸ªæ´»è·ƒçš„è¿½è¸ªå¯»æ‰¾æœ€ä½³åŒ¹é…
            for track in community_tracks:
                if track['active']:
                    last_community = track['snapshots'][-1]['nodes']
                    
                    best_match = None
                    best_similarity = 0.0
                    
                    for current_community in current_snapshot_communities:
                        current_nodes = set(current_community)
                        similarity = self._calculate_community_similarity(last_community, current_nodes)
                        
                        if similarity > best_similarity and similarity > 0.4:  # æœ€å°ç›¸ä¼¼æ€§é˜ˆå€¼
                            best_similarity = similarity
                            best_match = current_community
                    
                    if best_match is not None:
                        # ç»§ç»­è¿½è¸ª
                        track['snapshots'].append({
                            'timestamp': t,
                            'community': best_match,
                            'size': len(best_match),
                            'nodes': set(best_match),
                            'similarity_to_previous': best_similarity
                        })
                    else:
                        # è¿½è¸ªç»“æŸ
                        track['active'] = False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ç¤¾åŒºå‡ºç°
            for current_community in current_snapshot_communities:
                current_nodes = set(current_community)
                
                # æ£€æŸ¥è¿™ä¸ªç¤¾åŒºæ˜¯å¦å·²ç»è¢«æŸä¸ªè¿½è¸ªè¦†ç›–
                is_covered = False
                for track in community_tracks:
                    if (track['active'] and len(track['snapshots']) > 0 and 
                        track['snapshots'][-1]['timestamp'] == t):
                        if self._calculate_community_similarity(track['snapshots'][-1]['nodes'], current_nodes) > 0.4:
                            is_covered = True
                            break
                
                if not is_covered:
                    # åˆ›å»ºæ–°çš„è¿½è¸ª
                    new_track = {
                        'track_id': len(community_tracks),
                        'snapshots': [{
                            'timestamp': t,
                            'community': current_community,
                            'size': len(current_community),
                            'nodes': current_nodes
                        }],
                        'active': True
                    }
                    community_tracks.append(new_track)
        
        # åªè¿”å›æœ‰è¶³å¤Ÿé•¿åº¦çš„è¿½è¸ª
        return [track for track in community_tracks if len(track['snapshots']) >= 2]
    
    def _calculate_community_similarity(self, nodes1: set, nodes2: set) -> float:
        """è®¡ç®—ç¤¾åŒºç›¸ä¼¼åº¦"""
        intersection = len(nodes1.intersection(nodes2))
        union = len(nodes1.union(nodes2))
        
        if union == 0:
            return 1.0 if len(nodes1) == len(nodes2) == 0 else 0.0
        
        return intersection / union
    
    def _evaluate_community_track_stability(self, track: Dict) -> Dict:
        """è¯„ä¼°ç¤¾åŒºè¿½è¸ªçš„ç¨³å®šæ€§"""
        snapshots = track['snapshots']
        
        if len(snapshots) < 2:
            return {'stability_score': 0.0, 'size_stability': 0.0, 'composition_stability': 0.0}
        
        # 1. å¤§å°ç¨³å®šæ€§
        sizes = [snapshot['size'] for snapshot in snapshots]
        size_cv = np.std(sizes) / np.mean(sizes) if np.mean(sizes) > 0 else 1.0
        size_stability = max(0.0, 1.0 - size_cv)
        
        # 2. ç»„æˆç¨³å®šæ€§
        composition_similarities = []
        for i in range(len(snapshots) - 1):
            sim = snapshots[i + 1].get('similarity_to_previous', 
                                      self._calculate_community_similarity(snapshots[i]['nodes'], snapshots[i + 1]['nodes']))
            composition_similarities.append(sim)
        
        composition_stability = np.mean(composition_similarities) if composition_similarities else 0.0
        
        # 3. æŒç»­æ€§ç¨³å®šæ€§
        duration = len(snapshots)
        max_possible_duration = len(snapshots)  # ç®€åŒ–
        persistence_stability = duration / max_possible_duration
        
        # 4. ç»¼åˆç¨³å®šæ€§
        overall_stability = (
            size_stability * 0.3 +
            composition_stability * 0.5 +
            persistence_stability * 0.2
        )
        
        return {
            'stability_score': float(overall_stability),
            'size_stability': float(size_stability),
            'composition_stability': float(composition_stability),
            'persistence_stability': float(persistence_stability),
            'duration': duration,
            'avg_size': float(np.mean(sizes)),
            'size_variance': float(np.var(sizes))
        }
    
    def _identify_core_members(self, track: Dict) -> List[int]:
        """è¯†åˆ«æ ¸å¿ƒæˆå‘˜"""
        all_appearances = defaultdict(int)
        total_snapshots = len(track['snapshots'])
        
        # ç»Ÿè®¡æ¯ä¸ªèŠ‚ç‚¹çš„å‡ºç°æ¬¡æ•°
        for snapshot in track['snapshots']:
            for node in snapshot['nodes']:
                all_appearances[node] += 1
        
        # æ ¸å¿ƒæˆå‘˜ï¼šå‡ºç°é¢‘ç‡è¶…è¿‡é˜ˆå€¼çš„èŠ‚ç‚¹
        threshold = total_snapshots * 0.7  # 70%ä»¥ä¸Šå‡ºç°ç‡
        core_members = [node for node, count in all_appearances.items() if count >= threshold]
        
        return core_members
    
    def _classify_stability_type(self, stability_metrics: Dict) -> str:
        """åˆ†ç±»ç¨³å®šæ€§ç±»å‹"""
        size_stability = stability_metrics['size_stability']
        composition_stability = stability_metrics['composition_stability']
        
        if size_stability > 0.8 and composition_stability > 0.8:
            return 'highly_stable'
        elif size_stability > 0.6 and composition_stability > 0.6:
            return 'moderately_stable'
        elif size_stability > 0.8:
            return 'size_stable'
        elif composition_stability > 0.8:
            return 'composition_stable'
        else:
            return 'weakly_stable'

    def _calculate_community_stability_score(self, historical_communities: List[Dict], 
                                           current_community_info: Dict) -> float:
        """è®¡ç®—ç¤¾åŒºç¨³å®šæ€§åˆ†æ•°"""
        if not historical_communities:
            return 0.5
        
        stability_analysis = self._analyze_community_stability(historical_communities, current_community_info)
        return stability_analysis['overall_stability']

    def _calculate_community_anomaly_score(self, historical_communities: List[Dict], 
                                         current_community_info: Dict) -> float:
        """è®¡ç®—ç¤¾åŒºå¼‚å¸¸åˆ†æ•°"""
        if not historical_communities:
            return 0.0
        
        # åŸºäºç¤¾åŒºæ•°é‡çš„å¼‚å¸¸
        historical_counts = [len(hc['communities']) for hc in historical_communities]
        current_count = len(current_community_info['communities'])
        
        if historical_counts:
            mean_count = np.mean(historical_counts)
            std_count = np.std(historical_counts)
            
            if std_count > 0:
                count_anomaly = abs(current_count - mean_count) / std_count
                count_anomaly_score = min(1.0, count_anomaly / 3.0)  # 3-sigmaè§„åˆ™
            else:
                count_anomaly_score = 0.0 if current_count == mean_count else 1.0
        else:
            count_anomaly_score = 0.0
        
        # åŸºäºæ¨¡å—åº¦çš„å¼‚å¸¸
        historical_modularities = [hc.get('modularity', 0) for hc in historical_communities]
        current_modularity = current_community_info.get('modularity', 0)
        
        if historical_modularities:
            mean_mod = np.mean(historical_modularities)
            std_mod = np.std(historical_modularities)
            
            if std_mod > 0:
                mod_anomaly = abs(current_modularity - mean_mod) / std_mod
                mod_anomaly_score = min(1.0, mod_anomaly / 2.0)
            else:
                mod_anomaly_score = 0.0 if current_modularity == mean_mod else 0.5
        else:
            mod_anomaly_score = 0.0
        
        # ç»¼åˆå¼‚å¸¸åˆ†æ•°
        overall_anomaly = (count_anomaly_score * 0.6 + mod_anomaly_score * 0.4)
        
        return float(overall_anomaly)
    
    def _calculate_modularity_gain(self, node: int, current_comm: int, target_comm: int,
                                adj_matrix: np.ndarray, communities: Dict[int, List[int]],
                                total_weight: float, resolution: float) -> float:
        """è®¡ç®—æ¨¡å—åº¦å¢ç›Š"""
        if total_weight == 0:
            return 0
        
        # èŠ‚ç‚¹çš„åº¦
        node_degree = np.sum(adj_matrix[node])
        
        # å½“å‰ç¤¾åŒºçš„å†…éƒ¨æƒé‡å’Œæ€»åº¦
        current_comm_nodes = communities.get(current_comm, [])
        current_internal_weight = 0
        current_total_degree = 0
        
        for n1 in current_comm_nodes:
            current_total_degree += np.sum(adj_matrix[n1])
            for n2 in current_comm_nodes:
                if n1 < n2:  # é¿å…é‡å¤è®¡ç®—
                    current_internal_weight += adj_matrix[n1][n2]
        
        # ç›®æ ‡ç¤¾åŒºçš„å†…éƒ¨æƒé‡å’Œæ€»åº¦
        target_comm_nodes = communities.get(target_comm, [])
        target_internal_weight = 0
        target_total_degree = 0
        
        for n1 in target_comm_nodes:
            target_total_degree += np.sum(adj_matrix[n1])
            for n2 in target_comm_nodes:
                if n1 < n2:
                    target_internal_weight += adj_matrix[n1][n2]
    
        # èŠ‚ç‚¹ä¸ç›®æ ‡ç¤¾åŒºçš„è¿æ¥æƒé‡
        node_to_target_weight = 0
        for target_node in target_comm_nodes:
            node_to_target_weight += adj_matrix[node][target_node]
        
        # èŠ‚ç‚¹ä¸å½“å‰ç¤¾åŒºçš„è¿æ¥æƒé‡
        node_to_current_weight = 0
        for current_node in current_comm_nodes:
            if current_node != node:
                node_to_current_weight += adj_matrix[node][current_node]
        
        # è®¡ç®—æ¨¡å—åº¦å¢ç›Š
        delta_q = (node_to_target_weight - node_to_current_weight) / (2 * total_weight) - \
                resolution * node_degree * (target_total_degree - current_total_degree + node_degree) / (4 * total_weight * total_weight)
    
        return delta_q

    def _calculate_modularity(self, adj_matrix: np.ndarray, communities: List[List[int]]) -> float:
        """è®¡ç®—æ¨¡å—åº¦"""
        if not communities:
            return 0.0
        
        n = len(adj_matrix)
        total_weight = np.sum(adj_matrix) / 2  # æ— å‘å›¾
        
        if total_weight == 0:
            return 0.0
        
        modularity = 0.0
        
        for community in communities:
            for i in community:
                for j in community:
                    if i <= j:  # é¿å…é‡å¤è®¡ç®—
                        # å®é™…è¾¹æƒé‡
                        actual_weight = adj_matrix[i][j]
                        
                        # æœŸæœ›è¾¹æƒé‡
                        degree_i = np.sum(adj_matrix[i])
                        degree_j = np.sum(adj_matrix[j])
                        expected_weight = (degree_i * degree_j) / (2 * total_weight)
                        
                        # è´¡çŒ®åˆ°æ¨¡å—åº¦
                        if i == j:
                            modularity += (actual_weight - expected_weight) / (2 * total_weight)
                        else:
                            modularity += 2 * (actual_weight - expected_weight) / (2 * total_weight)
        
        return modularity

    def _calculate_spectral_similarities(self, current_graph: Dict, historical_graphs: List[Dict]) -> List[float]:
        """
        è®¡ç®—è°±ç›¸ä¼¼åº¦çš„å®Œæ•´å®ç°
        åŸºäºå›¾çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        """
        spectral_similarities = []
        
        current_adj_matrix = np.array(current_graph['adjacency_matrix'])
        current_laplacian = self._calculate_laplacian_matrix(current_adj_matrix)
        current_eigenvalues, current_eigenvectors = self._calculate_graph_spectrum(current_laplacian)
        
        for hist_graph in historical_graphs:
            hist_adj_matrix = np.array(hist_graph['adjacency_matrix'])
            hist_laplacian = self._calculate_laplacian_matrix(hist_adj_matrix)
            hist_eigenvalues, hist_eigenvectors = self._calculate_graph_spectrum(hist_laplacian)
            
            # 1. ç‰¹å¾å€¼åˆ†å¸ƒç›¸ä¼¼åº¦
            eigenvalue_similarity = self._compare_eigenvalue_distributions(current_eigenvalues, hist_eigenvalues)
            
            # 2. ç‰¹å¾å‘é‡ç›¸ä¼¼åº¦
            eigenvector_similarity = self._compare_eigenvector_spaces(current_eigenvectors, hist_eigenvectors)
            
            # 3. è°±åŠå¾„ç›¸ä¼¼åº¦
            spectral_radius_sim = self._compare_spectral_radii(current_eigenvalues, hist_eigenvalues)
            
            # 4. ä»£æ•°è¿é€šåº¦ç›¸ä¼¼åº¦
            algebraic_connectivity_sim = self._compare_algebraic_connectivity(current_eigenvalues, hist_eigenvalues)
            
            # ç»¼åˆè°±ç›¸ä¼¼åº¦
            combined_similarity = (
                eigenvalue_similarity * 0.4 +
                eigenvector_similarity * 0.3 +
                spectral_radius_sim * 0.2 +
                algebraic_connectivity_sim * 0.1
            )
        
            spectral_similarities.append(combined_similarity)
        
        return spectral_similarities

    def _calculate_laplacian_matrix(self, adj_matrix: np.ndarray) -> np.ndarray:
        """è®¡ç®—æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ"""
        degree_matrix = np.diag(np.sum(adj_matrix, axis=1))
        laplacian = degree_matrix - adj_matrix
        return laplacian

    def _calculate_graph_spectrum(self, laplacian: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """è®¡ç®—å›¾çš„è°±"""
        eigenvalues, eigenvectors = np.linalg.eigh(laplacian)
        # æ’åºç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        idx = eigenvalues.argsort()
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        return eigenvalues, eigenvectors

    def _compare_eigenvalue_distributions(self, eigenvals1: np.ndarray, eigenvals2: np.ndarray) -> float:
        """æ¯”è¾ƒç‰¹å¾å€¼åˆ†å¸ƒ"""
        # å½’ä¸€åŒ–ç‰¹å¾å€¼
        if len(eigenvals1) > 0 and np.max(eigenvals1) > 0:
            norm_eigenvals1 = eigenvals1 / np.max(eigenvals1)
        else:
            norm_eigenvals1 = eigenvals1
        
        if len(eigenvals2) > 0 and np.max(eigenvals2) > 0:
            norm_eigenvals2 = eigenvals2 / np.max(eigenvals2)
        else:
            norm_eigenvals2 = eigenvals2
        
        # ä½¿ç”¨Wassersteinè·ç¦»æˆ–è€…ç®€å•çš„L2è·ç¦»
        min_len = min(len(norm_eigenvals1), len(norm_eigenvals2))
        if min_len == 0:
            return 0.0
        
        # æˆªå–ç›¸åŒé•¿åº¦
        eigenvals1_trunc = norm_eigenvals1[:min_len]
        eigenvals2_trunc = norm_eigenvals2[:min_len]
        
        # è®¡ç®—L2è·ç¦»å¹¶è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        l2_distance = np.linalg.norm(eigenvals1_trunc - eigenvals2_trunc)
        similarity = 1.0 / (1.0 + l2_distance)
        
        return similarity

    def _compare_eigenvector_spaces(self, eigenvecs1: np.ndarray, eigenvecs2: np.ndarray) -> float:
        """æ¯”è¾ƒç‰¹å¾å‘é‡ç©ºé—´"""
        # é€‰æ‹©å‰å‡ ä¸ªæœ€é‡è¦çš„ç‰¹å¾å‘é‡
        num_vecs = min(3, eigenvecs1.shape[1], eigenvecs2.shape[1])
        
        if num_vecs == 0:
            return 0.0
        
        # æå–å‰num_vecsä¸ªç‰¹å¾å‘é‡
        vecs1 = eigenvecs1[:, :num_vecs]
        vecs2 = eigenvecs2[:, :num_vecs]
        
        # è®¡ç®—å­ç©ºé—´è§’åº¦ï¼ˆä½¿ç”¨ä¸»è§’åº¦ï¼‰
        try:
            # è®¡ç®—ä¸¤ä¸ªå­ç©ºé—´ä¹‹é—´çš„ä¸»è§’åº¦
            U1, _, _ = np.linalg.svd(vecs1, full_matrices=False)
            U2, _, _ = np.linalg.svd(vecs2, full_matrices=False)
            
            # è®¡ç®—æŠ•å½±çŸ©é˜µçš„å¥‡å¼‚å€¼
            M = U1.T @ U2
            singular_values = np.linalg.svd(M, compute_uv=False)
        
            # ä¸»è§’åº¦
            angles = np.arccos(np.clip(singular_values, 0, 1))
            avg_angle = np.mean(angles)
        
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            similarity = np.cos(avg_angle)
        
        except:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–çš„ç›¸ä¼¼åº¦
            dot_products = []
            for i in range(num_vecs):
                dot_product = abs(np.dot(vecs1[:, i], vecs2[:, i]))
                dot_products.append(dot_product)
        
            similarity = np.mean(dot_products)
    
        return similarity

    def _compare_spectral_radii(self, eigenvals1: np.ndarray, eigenvals2: np.ndarray) -> float:
        """æ¯”è¾ƒè°±åŠå¾„"""
        if len(eigenvals1) == 0 or len(eigenvals2) == 0:
            return 0.0
        
        spectral_radius1 = np.max(np.abs(eigenvals1))
        spectral_radius2 = np.max(np.abs(eigenvals2))
        
        if spectral_radius1 == 0 and spectral_radius2 == 0:
            return 1.0
        
        # ç›¸å¯¹å·®å¼‚
        max_radius = max(spectral_radius1, spectral_radius2)
        if max_radius > 0:
            relative_diff = abs(spectral_radius1 - spectral_radius2) / max_radius
            similarity = 1.0 - relative_diff
        else:
            similarity = 1.0
    
        return similarity

    def _compare_algebraic_connectivity(self, eigenvals1: np.ndarray, eigenvals2: np.ndarray) -> float:
        """æ¯”è¾ƒä»£æ•°è¿é€šåº¦"""
        # ä»£æ•°è¿é€šåº¦æ˜¯ç¬¬äºŒå°çš„ç‰¹å¾å€¼
        if len(eigenvals1) < 2 or len(eigenvals2) < 2:
            return 0.0
        
        # æ’åºå¹¶å–ç¬¬äºŒå°çš„ç‰¹å¾å€¼
        sorted_eigenvals1 = np.sort(eigenvals1)
        sorted_eigenvals2 = np.sort(eigenvals2)
        
        algebraic_conn1 = sorted_eigenvals1[1]
        algebraic_conn2 = sorted_eigenvals2[1]
        
        if algebraic_conn1 == 0 and algebraic_conn2 == 0:
            return 1.0
        
        # ç›¸å¯¹å·®å¼‚
        max_conn = max(algebraic_conn1, algebraic_conn2)
        if max_conn > 0:
            relative_diff = abs(algebraic_conn1 - algebraic_conn2) / max_conn
            similarity = 1.0 - relative_diff
        else:
            similarity = 1.0
    
        return similarity
    
    def _calculate_overall_graph_match_score(self, isomorphic_matches: List[Dict], 
                                           edit_distances: List[float], 
                                           kernel_similarities: List[float], 
                                           spectral_similarities: List[float]) -> float:
        """è®¡ç®—æ€»ä½“å›¾åŒ¹é…åˆ†æ•°"""
        scores = []
        if isomorphic_matches:
            scores.append(np.mean([match['similarity'] for match in isomorphic_matches]))
        if edit_distances:
            scores.append(1.0 - np.mean(edit_distances))  # è·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
        if kernel_similarities:
            scores.append(np.mean(kernel_similarities))
        if spectral_similarities:
            scores.append(np.mean(spectral_similarities))
        
        return float(np.mean(scores)) if scores else 0.0
    
    def _build_hmm_observation_sequences(self, historical_context: List[Dict]) -> List[List[int]]:
        """
        æ„å»ºHMMè§‚å¯Ÿåºåˆ—çš„å®Œæ•´å®ç°
        """
        observation_sequences = []
        
        # 1. åŸºäºå°¾æ•°æ•°é‡çš„è§‚å¯Ÿåºåˆ—
        tail_count_sequence = []
        for period in historical_context:
            tail_count = len(period.get('tails', []))
            tail_count_sequence.append(tail_count)
        observation_sequences.append(tail_count_sequence)
        
        # 2. åŸºäºç‰¹å®šå°¾æ•°å‡ºç°æ¨¡å¼çš„è§‚å¯Ÿåºåˆ—
        for tail in range(10):
            tail_sequence = []
            for period in historical_context:
                if tail in period.get('tails', []):
                    tail_sequence.append(1)
                else:
                    tail_sequence.append(0)
            observation_sequences.append(tail_sequence)
    
        # 3. åŸºäºå°¾æ•°åˆ†å¸ƒæ¨¡å¼çš„è§‚å¯Ÿåºåˆ—
        distribution_sequence = []
        for period in historical_context:
            period_tails = period.get('tails', [])
            if not period_tails:
                distribution_code = 0
            else:
                # ç¼–ç åˆ†å¸ƒæ¨¡å¼
                # 0: ä½å€¼é›†ä¸­ (0-3), 1: ä¸­å€¼é›†ä¸­ (4-6), 2: é«˜å€¼é›†ä¸­ (7-9), 3: åˆ†æ•£åˆ†å¸ƒ
                low_count = sum(1 for tail in period_tails if tail <= 3)
                mid_count = sum(1 for tail in period_tails if 4 <= tail <= 6)
                high_count = sum(1 for tail in period_tails if tail >= 7)
                
                total = len(period_tails)
                if low_count / total > 0.6:
                    distribution_code = 0
                elif mid_count / total > 0.6:
                    distribution_code = 1
                elif high_count / total > 0.6:
                    distribution_code = 2
                else:
                    distribution_code = 3
            
            distribution_sequence.append(distribution_code)
        observation_sequences.append(distribution_sequence)
    
        # 4. åŸºäºè¿ç»­æ€§æ¨¡å¼çš„è§‚å¯Ÿåºåˆ—
        continuity_sequence = []
        for i, period in enumerate(historical_context):
            if i == 0:
                continuity_score = 0
            else:
                prev_tails = set(historical_context[i-1].get('tails', []))
                curr_tails = set(period.get('tails', []))
                
                if not prev_tails and not curr_tails:
                    continuity_score = 2  # éƒ½ä¸ºç©º
                elif not prev_tails or not curr_tails:
                    continuity_score = 0  # ä¸€ä¸ªä¸ºç©º
                else:
                    overlap = len(prev_tails.intersection(curr_tails))
                    union = len(prev_tails.union(curr_tails))
                    continuity_ratio = overlap / union if union > 0 else 0
                    
                    if continuity_ratio > 0.7:
                        continuity_score = 3  # é«˜è¿ç»­æ€§
                    elif continuity_ratio > 0.4:
                        continuity_score = 2  # ä¸­ç­‰è¿ç»­æ€§
                    elif continuity_ratio > 0.1:
                        continuity_score = 1  # ä½è¿ç»­æ€§
                    else:
                        continuity_score = 0  # æ— è¿ç»­æ€§
            
            continuity_sequence.append(continuity_score)
        observation_sequences.append(continuity_sequence)
        
        return observation_sequences

    def _define_hidden_states(self) -> List[str]:
        """
        å®šä¹‰éšè—çŠ¶æ€çš„å®Œæ•´å®ç°
        """
        return [
            'natural_random',      # è‡ªç„¶éšæœºçŠ¶æ€
            'subtle_pattern',      # å¾®å¦™æ¨¡å¼çŠ¶æ€
            'moderate_control',    # ä¸­ç­‰æ§åˆ¶çŠ¶æ€
            'strong_manipulation', # å¼ºæ“æ§çŠ¶æ€
            'extreme_intervention' # æç«¯å¹²é¢„çŠ¶æ€
        ]

    def _estimate_hmm_parameters(self, observation_sequences: List[List[int]], 
                            hidden_states: List[str]) -> Dict:
        """
        ä¼°è®¡HMMå‚æ•°çš„å®Œæ•´å®ç°
        ä½¿ç”¨Baum-Welchç®—æ³•
        """
        n_states = len(hidden_states)
        n_sequences = len(observation_sequences)
        
        if n_sequences == 0 or not observation_sequences[0]:
            return {
                'initial_probs': [1.0/n_states] * n_states,
                'transition_probs': [[1.0/n_states] * n_states for _ in range(n_states)],
                'emission_probs': {},
                'log_likelihood': float('-inf')
            }
        
        # ç¡®å®šè§‚å¯Ÿç¬¦å·çš„èŒƒå›´
        all_observations = set()
        for sequence in observation_sequences:
            all_observations.update(sequence)
        
        observation_symbols = sorted(list(all_observations))
        n_symbols = len(observation_symbols)
        symbol_to_idx = {symbol: idx for idx, symbol in enumerate(observation_symbols)}
        
        # åˆå§‹åŒ–å‚æ•°
        # åˆå§‹æ¦‚ç‡ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        initial_probs = np.ones(n_states) / n_states
        
        # è½¬ç§»æ¦‚ç‡ï¼ˆæ·»åŠ å°çš„éšæœºæ‰°åŠ¨ï¼‰
        transition_probs = np.ones((n_states, n_states)) / n_states
        noise = np.random.random((n_states, n_states)) * 0.1
        transition_probs += noise
        transition_probs = transition_probs / transition_probs.sum(axis=1, keepdims=True)
        
        # å‘å°„æ¦‚ç‡
        emission_probs = np.ones((n_states, n_symbols)) / n_symbols
        noise = np.random.random((n_states, n_symbols)) * 0.1
        emission_probs += noise
        emission_probs = emission_probs / emission_probs.sum(axis=1, keepdims=True)
        
        # Baum-Welchç®—æ³•
        max_iterations = 50
        convergence_threshold = 1e-4
        prev_log_likelihood = float('-inf')
        
        for iteration in range(max_iterations):
            # Eæ­¥ï¼šå‰å‘-åå‘ç®—æ³•
            total_log_likelihood = 0
            all_gammas = []
            all_xis = []
            
            for sequence in observation_sequences:
                if not sequence:
                    continue
                    
                T = len(sequence)
            
                # å‰å‘ç®—æ³•
                alpha = self._forward_algorithm(sequence, initial_probs, transition_probs, 
                                            emission_probs, symbol_to_idx)
                
                # åå‘ç®—æ³•
                beta = self._backward_algorithm(sequence, transition_probs, emission_probs, 
                                            symbol_to_idx)
                
                # è®¡ç®—ä¼¼ç„¶
                sequence_log_likelihood = np.log(np.sum(alpha[-1]))
                total_log_likelihood += sequence_log_likelihood
                
                # è®¡ç®—gammaå’Œxi
                gamma = self._compute_gamma(alpha, beta)
                xi = self._compute_xi(alpha, beta, sequence, transition_probs, 
                                    emission_probs, symbol_to_idx)
                
                all_gammas.append(gamma)
                all_xis.append(xi)
            
            # æ£€æŸ¥æ”¶æ•›
            if abs(total_log_likelihood - prev_log_likelihood) < convergence_threshold:
                break
            prev_log_likelihood = total_log_likelihood
            
            # Mæ­¥ï¼šæ›´æ–°å‚æ•°
            if all_gammas and all_xis:
                initial_probs, transition_probs, emission_probs = self._update_hmm_parameters(
                    all_gammas, all_xis, observation_sequences, n_states, n_symbols, symbol_to_idx
                )
        
        return {
            'initial_probs': initial_probs.tolist(),
            'transition_probs': transition_probs.tolist(),
            'emission_probs': emission_probs.tolist(),
            'observation_symbols': observation_symbols,
            'hidden_states': hidden_states,
            'log_likelihood': total_log_likelihood,
            'n_iterations': iteration + 1
        }

    def _forward_algorithm(self, sequence: List[int], initial_probs: np.ndarray,
                        transition_probs: np.ndarray, emission_probs: np.ndarray,
                        symbol_to_idx: Dict[int, int]) -> np.ndarray:
        """å‰å‘ç®—æ³•å®ç°"""
        T = len(sequence)
        n_states = len(initial_probs)
        
        alpha = np.zeros((T, n_states))
        
        # åˆå§‹åŒ–
        obs_idx = symbol_to_idx.get(sequence[0], 0)
        alpha[0] = initial_probs * emission_probs[:, obs_idx]
        
        # é€’æ¨
        for t in range(1, T):
            obs_idx = symbol_to_idx.get(sequence[t], 0)
            for j in range(n_states):
                alpha[t, j] = np.sum(alpha[t-1] * transition_probs[:, j]) * emission_probs[j, obs_idx]
            
            # æ•°å€¼ç¨³å®šæ€§ï¼šå½’ä¸€åŒ–
            alpha[t] = alpha[t] / (np.sum(alpha[t]) + 1e-10)
        
        return alpha

    def _backward_algorithm(self, sequence: List[int], transition_probs: np.ndarray,
                        emission_probs: np.ndarray, symbol_to_idx: Dict[int, int]) -> np.ndarray:
        """åå‘ç®—æ³•å®ç°"""
        T = len(sequence)
        n_states = transition_probs.shape[0]
        
        beta = np.zeros((T, n_states))
        
        # åˆå§‹åŒ–
        beta[T-1] = 1.0
        
        # é€’æ¨
        for t in range(T-2, -1, -1):
            obs_idx = symbol_to_idx.get(sequence[t+1], 0)
            for i in range(n_states):
                beta[t, i] = np.sum(transition_probs[i] * emission_probs[:, obs_idx] * beta[t+1])
            
            # æ•°å€¼ç¨³å®šæ€§ï¼šå½’ä¸€åŒ–
            beta[t] = beta[t] / (np.sum(beta[t]) + 1e-10)
        
        return beta

    def _compute_gamma(self, alpha: np.ndarray, beta: np.ndarray) -> np.ndarray:
        """è®¡ç®—gammaï¼ˆçŠ¶æ€åéªŒæ¦‚ç‡ï¼‰"""
        gamma = alpha * beta
        gamma = gamma / (np.sum(gamma, axis=1, keepdims=True) + 1e-10)
        return gamma

    def _compute_xi(self, alpha: np.ndarray, beta: np.ndarray, sequence: List[int],
                transition_probs: np.ndarray, emission_probs: np.ndarray,
                symbol_to_idx: Dict[int, int]) -> np.ndarray:
        """è®¡ç®—xiï¼ˆè½¬ç§»åéªŒæ¦‚ç‡ï¼‰"""
        T = len(sequence)
        n_states = alpha.shape[1]
        
        xi = np.zeros((T-1, n_states, n_states))
        
        for t in range(T-1):
            obs_idx = symbol_to_idx.get(sequence[t+1], 0)
            denominator = 0
            
            for i in range(n_states):
                for j in range(n_states):
                    xi[t, i, j] = (alpha[t, i] * transition_probs[i, j] * 
                                emission_probs[j, obs_idx] * beta[t+1, j])
                    denominator += xi[t, i, j]
            
            # å½’ä¸€åŒ–
            if denominator > 0:
                xi[t] = xi[t] / denominator
        
        return xi

    def _update_hmm_parameters(self, all_gammas: List[np.ndarray], all_xis: List[np.ndarray],
                            observation_sequences: List[List[int]], n_states: int, n_symbols: int,
                            symbol_to_idx: Dict[int, int]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """æ›´æ–°HMMå‚æ•°"""
        # æ›´æ–°åˆå§‹æ¦‚ç‡
        initial_probs = np.zeros(n_states)
        for gamma in all_gammas:
            if len(gamma) > 0:
                initial_probs += gamma[0]
        initial_probs = initial_probs / (len(all_gammas) + 1e-10)
        
        # æ›´æ–°è½¬ç§»æ¦‚ç‡
        transition_probs = np.zeros((n_states, n_states))
        for xi in all_xis:
            if len(xi) > 0:
                transition_probs += np.sum(xi, axis=0)
        
        # å½’ä¸€åŒ–è½¬ç§»æ¦‚ç‡
        for i in range(n_states):
            row_sum = np.sum(transition_probs[i])
            if row_sum > 0:
                transition_probs[i] = transition_probs[i] / row_sum
            else:
                transition_probs[i] = np.ones(n_states) / n_states
        
        # æ›´æ–°å‘å°„æ¦‚ç‡
        emission_probs = np.zeros((n_states, n_symbols))
        
        for seq_idx, sequence in enumerate(observation_sequences):
            if seq_idx >= len(all_gammas) or not sequence:
                continue
                
            gamma = all_gammas[seq_idx]
            
            for t, obs in enumerate(sequence):
                if t < len(gamma):
                    obs_idx = symbol_to_idx.get(obs, 0)
                    emission_probs[:, obs_idx] += gamma[t]
        
        # å½’ä¸€åŒ–å‘å°„æ¦‚ç‡
        for i in range(n_states):
            row_sum = np.sum(emission_probs[i])
            if row_sum > 0:
                emission_probs[i] = emission_probs[i] / row_sum
            else:
                emission_probs[i] = np.ones(n_symbols) / n_symbols
        
        return initial_probs, transition_probs, emission_probs
    
    def _predict_state_sequence(self, hmm_parameters: Dict, observation_sequences: List[List[int]]) -> List[int]:
        """
        é¢„æµ‹çŠ¶æ€åºåˆ—çš„å®Œæ•´å®ç° - Viterbiç®—æ³•
        """
        if not observation_sequences or not observation_sequences[0]:
            return []
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§‚å¯Ÿåºåˆ—è¿›è¡Œé¢„æµ‹
        sequence = observation_sequences[0]
        
        initial_probs = np.array(hmm_parameters['initial_probs'])
        transition_probs = np.array(hmm_parameters['transition_probs'])
        emission_probs = np.array(hmm_parameters['emission_probs'])
        observation_symbols = hmm_parameters['observation_symbols']
        
        symbol_to_idx = {symbol: idx for idx, symbol in enumerate(observation_symbols)}
        
        T = len(sequence)
        n_states = len(initial_probs)
        
        # Viterbiç®—æ³•
        # delta[t][i] = åˆ°æ—¶åˆ»tçŠ¶æ€içš„æœ€å¤§æ¦‚ç‡
        delta = np.zeros((T, n_states))
        # psi[t][i] = åˆ°æ—¶åˆ»tçŠ¶æ€içš„æœ€ä¼˜å‰ä¸€çŠ¶æ€
        psi = np.zeros((T, n_states), dtype=int)
        
        # åˆå§‹åŒ–
        obs_idx = symbol_to_idx.get(sequence[0], 0)
        delta[0] = initial_probs * emission_probs[:, obs_idx]
        psi[0] = 0
        
        # é€’æ¨
        for t in range(1, T):
            obs_idx = symbol_to_idx.get(sequence[t], 0)
            for j in range(n_states):
                # æ‰¾åˆ°æœ€å¤§æ¦‚ç‡è·¯å¾„
                prob_candidates = delta[t-1] * transition_probs[:, j]
                psi[t, j] = np.argmax(prob_candidates)
                delta[t, j] = np.max(prob_candidates) * emission_probs[j, obs_idx]
            
            # æ•°å€¼ç¨³å®šæ€§
            if np.sum(delta[t]) > 0:
                delta[t] = delta[t] / np.sum(delta[t])
        
        # å›æº¯æ‰¾åˆ°æœ€ä¼˜è·¯å¾„
        states = np.zeros(T, dtype=int)
        states[T-1] = np.argmax(delta[T-1])
        
        for t in range(T-2, -1, -1):
            states[t] = psi[t+1, states[t+1]]
        
        return states.tolist()

    def _encode_current_observation(self, current_tails: Set[int]) -> List[int]:
        """
        ç¼–ç å½“å‰è§‚å¯Ÿçš„å®Œæ•´å®ç°
        """
        observations = []
        
        # 1. å°¾æ•°æ•°é‡è§‚å¯Ÿ
        tail_count = len(current_tails)
        observations.append(tail_count)
        
        # 2. å„å°¾æ•°çš„äºŒå…ƒè§‚å¯Ÿ
        for tail in range(10):
            observations.append(1 if tail in current_tails else 0)
        
        # 3. åˆ†å¸ƒæ¨¡å¼è§‚å¯Ÿ
        if not current_tails:
            distribution_code = 0
        else:
            low_count = sum(1 for tail in current_tails if tail <= 3)
            mid_count = sum(1 for tail in current_tails if 4 <= tail <= 6)
            high_count = sum(1 for tail in current_tails if tail >= 7)
            
            total = len(current_tails)
            if low_count / total > 0.6:
                distribution_code = 0
            elif mid_count / total > 0.6:
                distribution_code = 1
            elif high_count / total > 0.6:
                distribution_code = 2
            else:
                distribution_code = 3
        
        observations.append(distribution_code)
        
        return observations

    def _calculate_observation_likelihood(self, current_observation: List[int], 
                                        hmm_parameters: Dict, most_likely_states: List[int]) -> float:
        """
        è®¡ç®—è§‚å¯Ÿä¼¼ç„¶æ€§çš„å®Œæ•´å®ç°
        """
        if not current_observation or not most_likely_states:
            return 0.0
        
        emission_probs = np.array(hmm_parameters['emission_probs'])
        observation_symbols = hmm_parameters['observation_symbols']
        
        # è®¡ç®—æ¯ä¸ªè§‚å¯Ÿç»´åº¦çš„ä¼¼ç„¶æ€§
        likelihoods = []
        
        for obs_idx, obs_value in enumerate(current_observation):
            if obs_idx < len(observation_symbols):
                symbol_idx = observation_symbols.index(obs_value) if obs_value in observation_symbols else 0
                
                # ä½¿ç”¨æœ€å¯èƒ½çš„å½“å‰çŠ¶æ€
                current_state = most_likely_states[-1] if most_likely_states else 0
                current_state = min(current_state, len(emission_probs) - 1)
                
                # è·å–è¯¥çŠ¶æ€ä¸‹è§‚å¯Ÿè¯¥ç¬¦å·çš„æ¦‚ç‡
                if symbol_idx < emission_probs.shape[1]:
                    likelihood = emission_probs[current_state, symbol_idx]
                    likelihoods.append(likelihood)
        
        # è®¡ç®—å¹³å‡ä¼¼ç„¶æ€§
        if likelihoods:
            return float(np.mean(likelihoods))
        else:
            return 0.5  # é»˜è®¤å€¼

    def _analyze_hmm_transition_patterns(self, hmm_parameters: Dict, most_likely_states: List[int]) -> Dict:
        """
        åˆ†æHMMè½¬ç§»æ¨¡å¼çš„å®Œæ•´å®ç°
        """
        if not most_likely_states or len(most_likely_states) < 2:
            return {
                'transition_entropy': 0.0,
                'state_persistence': 0.0,
                'dominant_transitions': [],
                'anomalous_transitions': []
            }
        
        transition_probs = np.array(hmm_parameters['transition_probs'])
        hidden_states = hmm_parameters['hidden_states']
        
        # 1. è®¡ç®—è½¬ç§»ç†µ
        transition_entropy = 0.0
        for i in range(len(transition_probs)):
            for j in range(len(transition_probs[i])):
                prob = transition_probs[i][j]
                if prob > 0:
                    transition_entropy -= prob * math.log2(prob)
        
        # 2. è®¡ç®—çŠ¶æ€æŒç»­æ€§
        state_changes = 0
        total_transitions = len(most_likely_states) - 1
        
        for i in range(total_transitions):
            if most_likely_states[i] != most_likely_states[i + 1]:
                state_changes += 1
        
        state_persistence = 1.0 - (state_changes / total_transitions) if total_transitions > 0 else 1.0
        
        # 3. è¯†åˆ«ä¸»å¯¼è½¬ç§»
        dominant_transitions = []
        for i in range(len(transition_probs)):
            for j in range(len(transition_probs[i])):
                if transition_probs[i][j] > 0.3:  # é˜ˆå€¼
                    dominant_transitions.append({
                        'from_state': hidden_states[i],
                        'to_state': hidden_states[j],
                        'probability': float(transition_probs[i][j])
                    })
        
        dominant_transitions.sort(key=lambda x: x['probability'], reverse=True)
        
        # 4. è¯†åˆ«å¼‚å¸¸è½¬ç§»
        # åŸºäºè§‚å¯Ÿåˆ°çš„è½¬ç§»ä¸ç†è®ºè½¬ç§»çš„å·®å¼‚
        observed_transitions = {}
        for i in range(len(most_likely_states) - 1):
            from_state = most_likely_states[i]
            to_state = most_likely_states[i + 1]
            key = (from_state, to_state)
            observed_transitions[key] = observed_transitions.get(key, 0) + 1
        
        # å½’ä¸€åŒ–è§‚å¯Ÿè½¬ç§»
        total_observed = sum(observed_transitions.values())
        if total_observed > 0:
            for key in observed_transitions:
                observed_transitions[key] /= total_observed
    
        anomalous_transitions = []
        for (from_state, to_state), observed_prob in observed_transitions.items():
            if (from_state < len(transition_probs) and to_state < len(transition_probs[0])):
                expected_prob = transition_probs[from_state][to_state]
                deviation = abs(observed_prob - expected_prob)
                
                if deviation > 0.2:  # é˜ˆå€¼
                    anomalous_transitions.append({
                        'from_state': hidden_states[from_state] if from_state < len(hidden_states) else f'state_{from_state}',
                        'to_state': hidden_states[to_state] if to_state < len(hidden_states) else f'state_{to_state}',
                        'observed_probability': float(observed_prob),
                        'expected_probability': float(expected_prob),
                        'deviation': float(deviation)
                    })
        
        anomalous_transitions.sort(key=lambda x: x['deviation'], reverse=True)
        
        return {
            'transition_entropy': float(transition_entropy),
            'state_persistence': float(state_persistence),
            'dominant_transitions': dominant_transitions[:5],  # å‰5ä¸ª
            'anomalous_transitions': anomalous_transitions[:3],  # å‰3ä¸ª
            'state_distribution': self._calculate_state_distribution(most_likely_states, len(hidden_states))
        }

    def _detect_anomalous_hmm_states(self, most_likely_states: List[int], 
                                transition_patterns: Dict, observation_likelihood: float) -> List[int]:
        """
        æ£€æµ‹å¼‚å¸¸HMMçŠ¶æ€çš„å®Œæ•´å®ç°
        """
        anomalous_states = []
        
        if not most_likely_states:
            return anomalous_states
    
        # 1. åŸºäºçŠ¶æ€å€¼æ£€æµ‹å¼‚å¸¸ï¼ˆé«˜æ•°å€¼çŠ¶æ€é€šå¸¸è¡¨ç¤ºå¼‚å¸¸ï¼‰
        high_state_threshold = 2  # çŠ¶æ€å€¼é˜ˆå€¼
        for i, state in enumerate(most_likely_states):
            if state >= high_state_threshold:
                anomalous_states.append(i)
        
        # 2. åŸºäºçŠ¶æ€è½¬ç§»å¼‚å¸¸æ£€æµ‹
        anomalous_transitions = transition_patterns.get('anomalous_transitions', [])
        if anomalous_transitions:
            # æ‰¾åˆ°å¼‚å¸¸è½¬ç§»å‘ç”Ÿçš„æ—¶é—´ç‚¹
            for i in range(len(most_likely_states) - 1):
                from_state = most_likely_states[i]
                to_state = most_likely_states[i + 1]
                
                # æ£€æŸ¥è¿™ä¸ªè½¬ç§»æ˜¯å¦åœ¨å¼‚å¸¸åˆ—è¡¨ä¸­
                for anom_trans in anomalous_transitions:
                    if (anom_trans['from_state'].endswith(str(from_state)) and 
                        anom_trans['to_state'].endswith(str(to_state))):
                        anomalous_states.append(i + 1)
                        break
        
        # 3. åŸºäºè§‚å¯Ÿä¼¼ç„¶æ€§æ£€æµ‹å¼‚å¸¸
        if observation_likelihood < 0.3:  # ä½ä¼¼ç„¶æ€§é˜ˆå€¼
            # æœ€è¿‘çš„çŠ¶æ€å¯èƒ½æ˜¯å¼‚å¸¸çš„
            if most_likely_states:
                anomalous_states.append(len(most_likely_states) - 1)
        
        # 4. åŸºäºçŠ¶æ€æŒç»­æ€§æ£€æµ‹å¼‚å¸¸
        state_persistence = transition_patterns.get('state_persistence', 1.0)
        if state_persistence < 0.3:  # çŠ¶æ€å˜åŒ–è¿‡äºé¢‘ç¹
            # æ ‡è®°çŠ¶æ€å˜åŒ–ç‚¹
            for i in range(len(most_likely_states) - 1):
                if most_likely_states[i] != most_likely_states[i + 1]:
                    anomalous_states.append(i + 1)
        
        # å»é‡å¹¶æ’åº
        anomalous_states = sorted(list(set(anomalous_states)))
        
        return anomalous_states

    def _calculate_hmm_prediction_confidence(self, hmm_parameters: Dict, 
                                        most_likely_states: List[int], observation_likelihood: float) -> float:
        """
        è®¡ç®—HMMé¢„æµ‹ç½®ä¿¡åº¦çš„å®Œæ•´å®ç°
        """
        confidence_factors = []
        
        # 1. åŸºäºè§‚å¯Ÿä¼¼ç„¶æ€§çš„ç½®ä¿¡åº¦
        likelihood_confidence = min(1.0, observation_likelihood * 2.0)
        confidence_factors.append(likelihood_confidence)
        
        # 2. åŸºäºæ¨¡å‹è®­ç»ƒè´¨é‡çš„ç½®ä¿¡åº¦
        log_likelihood = hmm_parameters.get('log_likelihood', float('-inf'))
        if log_likelihood > float('-inf'):
            # å°†å¯¹æ•°ä¼¼ç„¶è½¬æ¢ä¸ºç½®ä¿¡åº¦
            training_confidence = min(1.0, max(0.0, (log_likelihood + 100) / 100))  # å¯å‘å¼è½¬æ¢
        else:
            training_confidence = 0.5
        confidence_factors.append(training_confidence)
        
        # 3. åŸºäºçŠ¶æ€åºåˆ—ç¨³å®šæ€§çš„ç½®ä¿¡åº¦
        if len(most_likely_states) >= 5:
            # æ£€æŸ¥æœ€è¿‘çŠ¶æ€çš„ç¨³å®šæ€§
            recent_states = most_likely_states[-5:]
            state_changes = sum(1 for i in range(len(recent_states)-1) 
                            if recent_states[i] != recent_states[i+1])
            stability_confidence = 1.0 - (state_changes / 4.0)  # 4æ˜¯æœ€å¤§å¯èƒ½å˜åŒ–æ•°
        else:
            stability_confidence = 0.5
        confidence_factors.append(stability_confidence)
        
        # 4. åŸºäºè½¬ç§»æ¦‚ç‡çš„ç½®ä¿¡åº¦
        transition_probs = np.array(hmm_parameters['transition_probs'])
        if len(most_likely_states) >= 2:
            current_state = most_likely_states[-2]
            next_state = most_likely_states[-1]
            
            if (current_state < len(transition_probs) and next_state < len(transition_probs[0])):
                transition_confidence = transition_probs[current_state][next_state]
            else:
                transition_confidence = 0.5
        else:
            transition_confidence = 0.5
        confidence_factors.append(transition_confidence)
        
        # ç»¼åˆç½®ä¿¡åº¦
        overall_confidence = np.mean(confidence_factors)
        
        return float(overall_confidence)
        
    # ç»§ç»­ç®€åŒ–å®ç°å…¶ä»–å¤æ‚æ–¹æ³•...
    def _build_fuzzy_tail_sets(self, historical_context: List[Dict]) -> Dict[int, float]:
        """æ„å»ºæ¨¡ç³Šå°¾æ•°é›†åˆ"""
        fuzzy_sets = {}
        total_periods = len(historical_context)
            
        for tail in range(10):
            appearances = sum(1 for period in historical_context if tail in period.get('tails', []))
            membership = appearances / total_periods if total_periods > 0 else 0.1
            fuzzy_sets[tail] = membership
            
        return fuzzy_sets
    
    def _fuzzy_cosine_similarity(self, membership1: float, membership2: float) -> float:
        """æ¨¡ç³Šä½™å¼¦ç›¸ä¼¼åº¦"""
        norm1 = math.sqrt(membership1**2)
        norm2 = math.sqrt(membership2**2)
        if norm1 > 0 and norm2 > 0:
            return (membership1 * membership2) / (norm1 * norm2)
        return 0.0
    
    def _fuzzy_jaccard_similarity(self, membership1: float, membership2: float) -> float:
        """æ¨¡ç³ŠJaccardç›¸ä¼¼åº¦"""
        intersection = min(membership1, membership2)
        union = max(membership1, membership2)
        return intersection / union if union > 0 else 0.0
    
    def _fuzzy_dice_similarity(self, membership1: float, membership2: float) -> float:
        """æ¨¡ç³ŠDiceç›¸ä¼¼åº¦"""
        intersection = min(membership1, membership2)
        return (2 * intersection) / (membership1 + membership2) if (membership1 + membership2) > 0 else 0.0
    
    def _fuzzy_hamming_distance(self, membership1: float, membership2: float) -> float:
        """æ¨¡ç³ŠHammingè·ç¦»"""
        return abs(membership1 - membership2)
    
    def _predict_state_sequence(self, hmm_parameters: Dict, observation_sequences: List[List[int]]) -> List[int]:
        """
        é¢„æµ‹çŠ¶æ€åºåˆ—çš„å®Œæ•´å®ç° - Viterbiç®—æ³•
        """
        if not observation_sequences or not observation_sequences[0]:
            return []
    
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§‚å¯Ÿåºåˆ—è¿›è¡Œé¢„æµ‹
        sequence = observation_sequences[0]
    
        initial_probs = np.array(hmm_parameters['initial_probs'])
        transition_probs = np.array(hmm_parameters['transition_probs'])
        emission_probs = np.array(hmm_parameters['emission_probs'])
        observation_symbols = hmm_parameters['observation_symbols']
    
        symbol_to_idx = {symbol: idx for idx, symbol in enumerate(observation_symbols)}
    
        T = len(sequence)
        n_states = len(initial_probs)
    
        # Viterbiç®—æ³•
        # delta[t][i] = åˆ°æ—¶åˆ»tçŠ¶æ€içš„æœ€å¤§æ¦‚ç‡
        delta = np.zeros((T, n_states))
        # psi[t][i] = åˆ°æ—¶åˆ»tçŠ¶æ€içš„æœ€ä¼˜å‰ä¸€çŠ¶æ€
        psi = np.zeros((T, n_states), dtype=int)
    
        # åˆå§‹åŒ–
        obs_idx = symbol_to_idx.get(sequence[0], 0)
        delta[0] = initial_probs * emission_probs[:, obs_idx]
        psi[0] = 0
    
        # é€’æ¨
        for t in range(1, T):
            obs_idx = symbol_to_idx.get(sequence[t], 0)
            for j in range(n_states):
                # æ‰¾åˆ°æœ€å¤§æ¦‚ç‡è·¯å¾„
                prob_candidates = delta[t-1] * transition_probs[:, j]
                psi[t, j] = np.argmax(prob_candidates)
                delta[t, j] = np.max(prob_candidates) * emission_probs[j, obs_idx]
            
            # æ•°å€¼ç¨³å®šæ€§
            if np.sum(delta[t]) > 0:
                delta[t] = delta[t] / np.sum(delta[t])
        
        # å›æº¯æ‰¾åˆ°æœ€ä¼˜è·¯å¾„
        states = np.zeros(T, dtype=int)
        states[T-1] = np.argmax(delta[T-1])
        
        for t in range(T-2, -1, -1):
            states[t] = psi[t+1, states[t+1]]
        
        return states.tolist()

    def _encode_current_observation(self, current_tails: Set[int]) -> List[int]:
        """
        ç¼–ç å½“å‰è§‚å¯Ÿçš„å®Œæ•´å®ç°
        """
        observations = []
        
        # 1. å°¾æ•°æ•°é‡è§‚å¯Ÿ
        tail_count = len(current_tails)
        observations.append(tail_count)
        
        # 2. å„å°¾æ•°çš„äºŒå…ƒè§‚å¯Ÿ
        for tail in range(10):
            observations.append(1 if tail in current_tails else 0)
        
        # 3. åˆ†å¸ƒæ¨¡å¼è§‚å¯Ÿ
        if not current_tails:
            distribution_code = 0
        else:
            low_count = sum(1 for tail in current_tails if tail <= 3)
            mid_count = sum(1 for tail in current_tails if 4 <= tail <= 6)
            high_count = sum(1 for tail in current_tails if tail >= 7)
            
            total = len(current_tails)
            if low_count / total > 0.6:
                distribution_code = 0
            elif mid_count / total > 0.6:
                distribution_code = 1
            elif high_count / total > 0.6:
                distribution_code = 2
            else:
                distribution_code = 3
        
        observations.append(distribution_code)
        
        return observations

    def _calculate_observation_likelihood(self, current_observation: List[int], 
                                        hmm_parameters: Dict, most_likely_states: List[int]) -> float:
        """
        è®¡ç®—è§‚å¯Ÿä¼¼ç„¶æ€§çš„å®Œæ•´å®ç°
        """
        if not current_observation or not most_likely_states:
            return 0.0
        
        emission_probs = np.array(hmm_parameters['emission_probs'])
        observation_symbols = hmm_parameters['observation_symbols']
        
        # è®¡ç®—æ¯ä¸ªè§‚å¯Ÿç»´åº¦çš„ä¼¼ç„¶æ€§
        likelihoods = []
        
        for obs_idx, obs_value in enumerate(current_observation):
            if obs_idx < len(observation_symbols):
                symbol_idx = observation_symbols.index(obs_value) if obs_value in observation_symbols else 0
                
                # ä½¿ç”¨æœ€å¯èƒ½çš„å½“å‰çŠ¶æ€
                current_state = most_likely_states[-1] if most_likely_states else 0
                current_state = min(current_state, len(emission_probs) - 1)
                
                # è·å–è¯¥çŠ¶æ€ä¸‹è§‚å¯Ÿè¯¥ç¬¦å·çš„æ¦‚ç‡
                if symbol_idx < emission_probs.shape[1]:
                    likelihood = emission_probs[current_state, symbol_idx]
                    likelihoods.append(likelihood)
        
        # è®¡ç®—å¹³å‡ä¼¼ç„¶æ€§
        if likelihoods:
            return float(np.mean(likelihoods))
        else:
            return 0.5  # é»˜è®¤å€¼

    def _analyze_hmm_transition_patterns(self, hmm_parameters: Dict, most_likely_states: List[int]) -> Dict:
        """
        åˆ†æHMMè½¬ç§»æ¨¡å¼çš„å®Œæ•´å®ç°
        """
        if not most_likely_states or len(most_likely_states) < 2:
            return {
                'transition_entropy': 0.0,
                'state_persistence': 0.0,
                'dominant_transitions': [],
                'anomalous_transitions': []
            }
        
        transition_probs = np.array(hmm_parameters['transition_probs'])
        hidden_states = hmm_parameters['hidden_states']
        
        # 1. è®¡ç®—è½¬ç§»ç†µ
        transition_entropy = 0.0
        for i in range(len(transition_probs)):
            for j in range(len(transition_probs[i])):
                prob = transition_probs[i][j]
                if prob > 0:
                    transition_entropy -= prob * math.log2(prob)
        
        # 2. è®¡ç®—çŠ¶æ€æŒç»­æ€§
        state_changes = 0
        total_transitions = len(most_likely_states) - 1
        
        for i in range(total_transitions):
            if most_likely_states[i] != most_likely_states[i + 1]:
                state_changes += 1
        
        state_persistence = 1.0 - (state_changes / total_transitions) if total_transitions > 0 else 1.0
        
        # 3. è¯†åˆ«ä¸»å¯¼è½¬ç§»
        dominant_transitions = []
        for i in range(len(transition_probs)):
            for j in range(len(transition_probs[i])):
                if transition_probs[i][j] > 0.3:  # é˜ˆå€¼
                    dominant_transitions.append({
                        'from_state': hidden_states[i],
                        'to_state': hidden_states[j],
                        'probability': float(transition_probs[i][j])
                    })
        
        dominant_transitions.sort(key=lambda x: x['probability'], reverse=True)
        
        # 4. è¯†åˆ«å¼‚å¸¸è½¬ç§»
        # åŸºäºè§‚å¯Ÿåˆ°çš„è½¬ç§»ä¸ç†è®ºè½¬ç§»çš„å·®å¼‚
        observed_transitions = {}
        for i in range(len(most_likely_states) - 1):
            from_state = most_likely_states[i]
            to_state = most_likely_states[i + 1]
            key = (from_state, to_state)
            observed_transitions[key] = observed_transitions.get(key, 0) + 1
        
        # å½’ä¸€åŒ–è§‚å¯Ÿè½¬ç§»
        total_observed = sum(observed_transitions.values())
        if total_observed > 0:
            for key in observed_transitions:
                observed_transitions[key] /= total_observed
    
        anomalous_transitions = []
        for (from_state, to_state), observed_prob in observed_transitions.items():
            if (from_state < len(transition_probs) and to_state < len(transition_probs[0])):
                expected_prob = transition_probs[from_state][to_state]
                deviation = abs(observed_prob - expected_prob)
                
                if deviation > 0.2:  # é˜ˆå€¼
                    anomalous_transitions.append({
                        'from_state': hidden_states[from_state] if from_state < len(hidden_states) else f'state_{from_state}',
                        'to_state': hidden_states[to_state] if to_state < len(hidden_states) else f'state_{to_state}',
                        'observed_probability': float(observed_prob),
                        'expected_probability': float(expected_prob),
                        'deviation': float(deviation)
                    })
        
        anomalous_transitions.sort(key=lambda x: x['deviation'], reverse=True)
        
        return {
            'transition_entropy': float(transition_entropy),
            'state_persistence': float(state_persistence),
            'dominant_transitions': dominant_transitions[:5],  # å‰5ä¸ª
            'anomalous_transitions': anomalous_transitions[:3],  # å‰3ä¸ª
            'state_distribution': self._calculate_state_distribution(most_likely_states, len(hidden_states))
        }

    def _detect_anomalous_hmm_states(self, most_likely_states: List[int], 
                                transition_patterns: Dict, observation_likelihood: float) -> List[int]:
        """
        æ£€æµ‹å¼‚å¸¸HMMçŠ¶æ€çš„å®Œæ•´å®ç°
        """
        anomalous_states = []
        
        if not most_likely_states:
            return anomalous_states
    
        # 1. åŸºäºçŠ¶æ€å€¼æ£€æµ‹å¼‚å¸¸ï¼ˆé«˜æ•°å€¼çŠ¶æ€é€šå¸¸è¡¨ç¤ºå¼‚å¸¸ï¼‰
        high_state_threshold = 2  # çŠ¶æ€å€¼é˜ˆå€¼
        for i, state in enumerate(most_likely_states):
            if state >= high_state_threshold:
                anomalous_states.append(i)
        
        # 2. åŸºäºçŠ¶æ€è½¬ç§»å¼‚å¸¸æ£€æµ‹
        anomalous_transitions = transition_patterns.get('anomalous_transitions', [])
        if anomalous_transitions:
            # æ‰¾åˆ°å¼‚å¸¸è½¬ç§»å‘ç”Ÿçš„æ—¶é—´ç‚¹
            for i in range(len(most_likely_states) - 1):
                from_state = most_likely_states[i]
                to_state = most_likely_states[i + 1]
                
                # æ£€æŸ¥è¿™ä¸ªè½¬ç§»æ˜¯å¦åœ¨å¼‚å¸¸åˆ—è¡¨ä¸­
                for anom_trans in anomalous_transitions:
                    if (anom_trans['from_state'].endswith(str(from_state)) and 
                        anom_trans['to_state'].endswith(str(to_state))):
                        anomalous_states.append(i + 1)
                        break
        
        # 3. åŸºäºè§‚å¯Ÿä¼¼ç„¶æ€§æ£€æµ‹å¼‚å¸¸
        if observation_likelihood < 0.3:  # ä½ä¼¼ç„¶æ€§é˜ˆå€¼
            # æœ€è¿‘çš„çŠ¶æ€å¯èƒ½æ˜¯å¼‚å¸¸çš„
            if most_likely_states:
                anomalous_states.append(len(most_likely_states) - 1)
        
        # 4. åŸºäºçŠ¶æ€æŒç»­æ€§æ£€æµ‹å¼‚å¸¸
        state_persistence = transition_patterns.get('state_persistence', 1.0)
        if state_persistence < 0.3:  # çŠ¶æ€å˜åŒ–è¿‡äºé¢‘ç¹
            # æ ‡è®°çŠ¶æ€å˜åŒ–ç‚¹
            for i in range(len(most_likely_states) - 1):
                if most_likely_states[i] != most_likely_states[i + 1]:
                    anomalous_states.append(i + 1)
        
        # å»é‡å¹¶æ’åº
        anomalous_states = sorted(list(set(anomalous_states)))
        
        return anomalous_states

    def _calculate_hmm_prediction_confidence(self, hmm_parameters: Dict, 
                                        most_likely_states: List[int], observation_likelihood: float) -> float:
        """
        è®¡ç®—HMMé¢„æµ‹ç½®ä¿¡åº¦çš„å®Œæ•´å®ç°
        """
        confidence_factors = []
        
        # 1. åŸºäºè§‚å¯Ÿä¼¼ç„¶æ€§çš„ç½®ä¿¡åº¦
        likelihood_confidence = min(1.0, observation_likelihood * 2.0)
        confidence_factors.append(likelihood_confidence)
        
        # 2. åŸºäºæ¨¡å‹è®­ç»ƒè´¨é‡çš„ç½®ä¿¡åº¦
        log_likelihood = hmm_parameters.get('log_likelihood', float('-inf'))
        if log_likelihood > float('-inf'):
            # å°†å¯¹æ•°ä¼¼ç„¶è½¬æ¢ä¸ºç½®ä¿¡åº¦
            training_confidence = min(1.0, max(0.0, (log_likelihood + 100) / 100))  # å¯å‘å¼è½¬æ¢
        else:
            training_confidence = 0.5
        confidence_factors.append(training_confidence)
        
        # 3. åŸºäºçŠ¶æ€åºåˆ—ç¨³å®šæ€§çš„ç½®ä¿¡åº¦
        if len(most_likely_states) >= 5:
            # æ£€æŸ¥æœ€è¿‘çŠ¶æ€çš„ç¨³å®šæ€§
            recent_states = most_likely_states[-5:]
            state_changes = sum(1 for i in range(len(recent_states)-1) 
                            if recent_states[i] != recent_states[i+1])
            stability_confidence = 1.0 - (state_changes / 4.0)  # 4æ˜¯æœ€å¤§å¯èƒ½å˜åŒ–æ•°
        else:
            stability_confidence = 0.5
        confidence_factors.append(stability_confidence)
        
        # 4. åŸºäºè½¬ç§»æ¦‚ç‡çš„ç½®ä¿¡åº¦
        transition_probs = np.array(hmm_parameters['transition_probs'])
        if len(most_likely_states) >= 2:
            current_state = most_likely_states[-2]
            next_state = most_likely_states[-1]
            
            if (current_state < len(transition_probs) and next_state < len(transition_probs[0])):
                transition_confidence = transition_probs[current_state][next_state]
            else:
                transition_confidence = 0.5
        else:
            transition_confidence = 0.5
        confidence_factors.append(transition_confidence)
        
        # ç»¼åˆç½®ä¿¡åº¦
        overall_confidence = np.mean(confidence_factors)
        
        return float(overall_confidence)

# ==================== æ¨¡ç³Šé€»è¾‘æ–¹æ³•çš„å®Œæ•´å®ç° ====================

    def _generate_fuzzy_rules(self, historical_context: List[Dict]) -> List[Dict]:
        """
        ç”Ÿæˆæ¨¡ç³Šè§„åˆ™çš„å®Œæ•´å®ç°
        åŸºäºå†å²æ•°æ®æŒ–æ˜æ¨¡ç³Šå…³è”è§„åˆ™
        """
        fuzzy_rules = []
        
        if len(historical_context) < 10:
            return fuzzy_rules
    
        # 1. å®šä¹‰æ¨¡ç³Šé›†åˆ
        fuzzy_sets = {
            'frequency': {
                'low': lambda x: max(0, min(1, (0.3 - x) / 0.3)),
                'medium': lambda x: max(0, min((x - 0.1) / 0.2, (0.7 - x) / 0.2)),
                'high': lambda x: max(0, min(1, (x - 0.5) / 0.3))
            },
            'recency': {
                'recent': lambda x: max(0, min(1, (3 - x) / 3)),
                'moderate': lambda x: max(0, min((x - 1) / 2, (7 - x) / 2)),
                'old': lambda x: max(0, min(1, (x - 5) / 5))
            },
            'clustering': {
                'scattered': lambda x: max(0, min(1, (0.4 - x) / 0.4)),
                'moderate': lambda x: max(0, min((x - 0.2) / 0.3, (0.8 - x) / 0.3)),
                'clustered': lambda x: max(0, min(1, (x - 0.6) / 0.4))
            }
        }
        
        # 2. ä¸ºæ¯ä¸ªå°¾æ•°è®¡ç®—æ¨¡ç³Šå±æ€§
        tail_fuzzy_attributes = {}
        for tail in range(10):
            # è®¡ç®—é¢‘ç‡
            appearances = sum(1 for period in historical_context if tail in period.get('tails', []))
            frequency = appearances / len(historical_context)
            
            # è®¡ç®—æœ€è¿‘æ€§ï¼ˆè·ç¦»ä¸Šæ¬¡å‡ºç°çš„æœŸæ•°ï¼‰
            recency = 0
            for i, period in enumerate(historical_context):
                if tail in period.get('tails', []):
                    recency = i
                    break
        
            # è®¡ç®—èšé›†æ€§ï¼ˆä¸å…¶ä»–å°¾æ•°çš„å…±ç°ç¨‹åº¦ï¼‰
            clustering = 0
            co_occurrences = 0
            total_appearances = 0
            
            for period in historical_context:
                if tail in period.get('tails', []):
                    total_appearances += 1
                    period_tails = period.get('tails', [])
                    co_occurrences += len([t for t in period_tails if t != tail])
            
            if total_appearances > 0:
                clustering = co_occurrences / (total_appearances * 9)  # 9æ˜¯å…¶ä»–å°¾æ•°çš„æœ€å¤§æ•°é‡
            
            tail_fuzzy_attributes[tail] = {
                'frequency': frequency,
                'recency': recency,
                'clustering': clustering
            }
    
        # 3. ç”Ÿæˆè§„åˆ™
        # è§„åˆ™1ï¼šé¢‘ç‡è§„åˆ™
        for tail in range(10):
            attrs = tail_fuzzy_attributes[tail]
            freq = attrs['frequency']
            
            # é«˜é¢‘ç‡ -> å¯èƒ½å‡ºç°
            high_freq_membership = fuzzy_sets['frequency']['high'](freq)
            if high_freq_membership > 0.5:
                fuzzy_rules.append({
                    'rule_id': f'freq_high_{tail}',
                    'antecedent': {'tail': tail, 'condition': 'high_frequency'},
                    'consequent': {'action': 'likely_appear', 'tail': tail},
                    'confidence': high_freq_membership,
                    'support': freq,
                    'rule_type': 'frequency'
                })
            
            # ä½é¢‘ç‡ -> å¯èƒ½ä¸å‡ºç°
            low_freq_membership = fuzzy_sets['frequency']['low'](freq)
            if low_freq_membership > 0.5:
                fuzzy_rules.append({
                    'rule_id': f'freq_low_{tail}',
                    'antecedent': {'tail': tail, 'condition': 'low_frequency'},
                    'consequent': {'action': 'unlikely_appear', 'tail': tail},
                    'confidence': low_freq_membership,
                    'support': 1 - freq,
                    'rule_type': 'frequency'
                })
        
        # è§„åˆ™2ï¼šæœ€è¿‘æ€§è§„åˆ™
        for tail in range(10):
            attrs = tail_fuzzy_attributes[tail]
            recency = attrs['recency']
            
            # æœ€è¿‘å‡ºç° -> å¯èƒ½ç»§ç»­å‡ºç°
            recent_membership = fuzzy_sets['recency']['recent'](recency)
            if recent_membership > 0.5:
                fuzzy_rules.append({
                    'rule_id': f'recency_recent_{tail}',
                    'antecedent': {'tail': tail, 'condition': 'recently_appeared'},
                    'consequent': {'action': 'likely_continue', 'tail': tail},
                    'confidence': recent_membership,
                    'support': recent_membership * attrs['frequency'],
                    'rule_type': 'recency'
                })
            
            # é•¿æ—¶é—´æœªå‡ºç° -> å¯èƒ½å›å½’
            old_membership = fuzzy_sets['recency']['old'](recency)
            if old_membership > 0.5:
                fuzzy_rules.append({
                    'rule_id': f'recency_old_{tail}',
                    'antecedent': {'tail': tail, 'condition': 'long_absent'},
                    'consequent': {'action': 'likely_return', 'tail': tail},
                    'confidence': old_membership,
                    'support': old_membership * (1 - attrs['frequency']),
                    'rule_type': 'recency'
                })
        
        # è§„åˆ™3ï¼šèšé›†æ€§è§„åˆ™
        for tail in range(10):
            attrs = tail_fuzzy_attributes[tail]
            clustering = attrs['clustering']
            
            # é«˜èšé›† -> ä¸å…¶ä»–å°¾æ•°ä¸€èµ·å‡ºç°
            clustered_membership = fuzzy_sets['clustering']['clustered'](clustering)
            if clustered_membership > 0.5:
                fuzzy_rules.append({
                    'rule_id': f'clustering_high_{tail}',
                    'antecedent': {'tail': tail, 'condition': 'highly_clustered'},
                    'consequent': {'action': 'appear_with_others', 'tail': tail},
                    'confidence': clustered_membership,
                    'support': clustered_membership * attrs['frequency'],
                    'rule_type': 'clustering'
                })
        
        # è§„åˆ™4ï¼šç»„åˆè§„åˆ™
        for tail in range(10):
            attrs = tail_fuzzy_attributes[tail]
            
            # é«˜é¢‘ç‡ AND æœ€è¿‘å‡ºç° -> å¾ˆå¯èƒ½å‡ºç°
            high_freq = fuzzy_sets['frequency']['high'](attrs['frequency'])
            recent = fuzzy_sets['recency']['recent'](attrs['recency'])
            
            combined_membership = min(high_freq, recent)  # æ¨¡ç³ŠANDæ“ä½œ
            if combined_membership > 0.6:
                fuzzy_rules.append({
                    'rule_id': f'combined_high_freq_recent_{tail}',
                    'antecedent': {'tail': tail, 'condition': 'high_frequency_and_recent'},
                    'consequent': {'action': 'very_likely_appear', 'tail': tail},
                    'confidence': combined_membership,
                    'support': combined_membership * attrs['frequency'],
                    'rule_type': 'combined'
                })
            
            # ä½é¢‘ç‡ AND é•¿æ—¶é—´æœªå‡ºç° -> å¾ˆå¯èƒ½ä¸å‡ºç°
            low_freq = fuzzy_sets['frequency']['low'](attrs['frequency'])
            old = fuzzy_sets['recency']['old'](attrs['recency'])
            
            combined_membership = min(low_freq, old)
            if combined_membership > 0.6:
                fuzzy_rules.append({
                    'rule_id': f'combined_low_freq_old_{tail}',
                    'antecedent': {'tail': tail, 'condition': 'low_frequency_and_old'},
                    'consequent': {'action': 'very_unlikely_appear', 'tail': tail},
                    'confidence': combined_membership,
                    'support': combined_membership * (1 - attrs['frequency']),
                    'rule_type': 'combined'
                })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        fuzzy_rules.sort(key=lambda x: x['confidence'], reverse=True)
        
        return fuzzy_rules

    def _evaluate_fuzzy_rules(self, current_tails: Set[int], fuzzy_rules: List[Dict]) -> List[Dict]:
        """
        è¯„ä¼°æ¨¡ç³Šè§„åˆ™çš„å®Œæ•´å®ç°
        """
        rule_evaluations = []
        
        for rule in fuzzy_rules:
            evaluation = {
                'rule_id': rule['rule_id'],
                'rule_type': rule['rule_type'],
                'rule_confidence': rule['confidence'],
                'match_degree': 0.0,
                'firing_strength': 0.0,
                'prediction_accuracy': 0.0
            }
            
            tail = rule['antecedent']['tail']
            condition = rule['antecedent']['condition']
            action = rule['consequent']['action']
            
            # è®¡ç®—å‰ä»¶åŒ¹é…åº¦
            if condition == 'high_frequency':
                # éœ€è¦ä»å†å²æ•°æ®é‡æ–°è®¡ç®—å½“å‰çš„æ¨¡ç³Šéš¶å±åº¦
                match_degree = rule['confidence']  # ç®€åŒ–ï¼šä½¿ç”¨è§„åˆ™ç½®ä¿¡åº¦
            elif condition == 'low_frequency':
                match_degree = rule['confidence']
            elif condition == 'recently_appeared':
                # æ£€æŸ¥è¯¥å°¾æ•°æ˜¯å¦åœ¨å½“å‰æœŸå‡ºç°
                match_degree = 1.0 if tail in current_tails else 0.0
            elif condition == 'long_absent':
                # æ£€æŸ¥è¯¥å°¾æ•°æ˜¯å¦åœ¨å½“å‰æœŸç¼ºå¸­
                match_degree = 0.0 if tail in current_tails else 1.0
            elif condition == 'highly_clustered':
                # æ£€æŸ¥è¯¥å°¾æ•°æ˜¯å¦ä¸å…¶ä»–å°¾æ•°ä¸€èµ·å‡ºç°
                if tail in current_tails:
                    other_tails_count = len(current_tails) - 1
                    match_degree = min(1.0, other_tails_count / 3.0)  # æ ‡å‡†åŒ–åˆ°[0,1]
                else:
                    match_degree = 0.0
            elif condition in ['high_frequency_and_recent', 'low_frequency_and_old']:
                match_degree = rule['confidence']  # ç®€åŒ–å¤„ç†
            else:
                match_degree = 0.5  # é»˜è®¤å€¼
            
            evaluation['match_degree'] = match_degree
        
            # è®¡ç®—æ¿€å‘å¼ºåº¦
            firing_strength = match_degree * rule['confidence']
            evaluation['firing_strength'] = firing_strength
        
            # è¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§
            if action == 'likely_appear' or action == 'very_likely_appear':
                prediction_accuracy = 1.0 if tail in current_tails else 0.0
            elif action == 'unlikely_appear' or action == 'very_unlikely_appear':
                prediction_accuracy = 0.0 if tail in current_tails else 1.0
            elif action == 'likely_continue':
                prediction_accuracy = 1.0 if tail in current_tails else 0.0
            elif action == 'likely_return':
                prediction_accuracy = 1.0 if tail in current_tails else 0.0
            elif action == 'appear_with_others':
                if tail in current_tails:
                    prediction_accuracy = min(1.0, (len(current_tails) - 1) / 2.0)
                else:
                    prediction_accuracy = 0.0
            else:
                prediction_accuracy = 0.5
            
            evaluation['prediction_accuracy'] = prediction_accuracy
        
            rule_evaluations.append(evaluation)
    
        return rule_evaluations

    def _fuzzy_clustering_analysis(self, historical_context: List[Dict], current_tails: Set[int]) -> Dict:
        """
        æ¨¡ç³Šèšç±»åˆ†æçš„å®Œæ•´å®ç°
        ä½¿ç”¨æ¨¡ç³ŠC-å‡å€¼èšç±»ç®—æ³•
        """
        if len(historical_context) < 10:
            return {
                'clusters': [],
                'membership_matrix': [],
                'cluster_centers': [],
                'current_membership': []
            }
        
        # 1. æ„å»ºæ•°æ®çŸ©é˜µ
        data_matrix = []
        for period in historical_context:
            period_vector = [1 if tail in period.get('tails', []) else 0 for tail in range(10)]
            data_matrix.append(period_vector)
        
        data_matrix = np.array(data_matrix)
        
        # 2. è®¾ç½®èšç±»å‚æ•°
        n_clusters = min(4, len(historical_context) // 3)  # èšç±»æ•°
        max_iterations = 100
        tolerance = 1e-4
        fuzziness = 2.0  # æ¨¡ç³ŠæŒ‡æ•°
        
        # 3. åˆå§‹åŒ–éš¶å±åº¦çŸ©é˜µ
        n_samples, n_features = data_matrix.shape
        membership_matrix = np.random.random((n_samples, n_clusters))
    
        # å½’ä¸€åŒ–éš¶å±åº¦çŸ©é˜µ
        membership_matrix = membership_matrix / membership_matrix.sum(axis=1, keepdims=True)
        
        # 4. æ¨¡ç³ŠC-å‡å€¼è¿­ä»£
        for iteration in range(max_iterations):
            # è®¡ç®—èšç±»ä¸­å¿ƒ
            cluster_centers = self._compute_fuzzy_cluster_centers(
                data_matrix, membership_matrix, fuzziness
            )
        
            # æ›´æ–°éš¶å±åº¦çŸ©é˜µ
            new_membership_matrix = self._update_fuzzy_membership_matrix(
                data_matrix, cluster_centers, fuzziness
            )
        
            # æ£€æŸ¥æ”¶æ•›
            change = np.max(np.abs(membership_matrix - new_membership_matrix))
            if change < tolerance:
                break
        
            membership_matrix = new_membership_matrix
    
        # 5. åˆ†æèšç±»ç»“æœ
        # ç¡®å®šæ¯ä¸ªæ ·æœ¬çš„ä¸»è¦å½’å±
        primary_clusters = np.argmax(membership_matrix, axis=1)
    
        # æ„å»ºèšç±»ä¿¡æ¯
        clusters = []
        for cluster_id in range(n_clusters):
            cluster_members = np.where(primary_clusters == cluster_id)[0]
        
            if len(cluster_members) > 0:
                cluster_info = {
                    'cluster_id': cluster_id,
                    'members': cluster_members.tolist(),
                    'size': len(cluster_members),
                    'center': cluster_centers[cluster_id].tolist(),
                    'avg_membership': float(np.mean(membership_matrix[cluster_members, cluster_id])),
                    'characteristic_tails': self._identify_cluster_characteristic_tails(
                        cluster_centers[cluster_id]
                    )
                }
                clusters.append(cluster_info)
    
        # 6. è®¡ç®—å½“å‰æœŸçš„èšç±»éš¶å±åº¦
        current_vector = np.array([1 if tail in current_tails else 0 for tail in range(10)])
        current_membership = self._calculate_sample_membership(
            current_vector, cluster_centers, fuzziness
        )
    
        # 7. èšç±»è´¨é‡è¯„ä¼°
        cluster_quality = self._evaluate_fuzzy_clustering_quality(
         data_matrix, cluster_centers, membership_matrix
        )
    
        return {
            'n_clusters': n_clusters,
            'clusters': clusters,
            'membership_matrix': membership_matrix.tolist(),
            'cluster_centers': cluster_centers.tolist(),
            'current_membership': current_membership.tolist(),
            'primary_cluster': int(np.argmax(current_membership)),
            'cluster_quality': cluster_quality,
            'iterations_completed': iteration + 1
        }

    def _compute_fuzzy_cluster_centers(self, data: np.ndarray, membership: np.ndarray, 
                                    fuzziness: float) -> np.ndarray:
        """è®¡ç®—æ¨¡ç³Šèšç±»ä¸­å¿ƒ"""
        n_clusters = membership.shape[1]
        n_features = data.shape[1]
        
        centers = np.zeros((n_clusters, n_features))
        
        for cluster_id in range(n_clusters):
            # è®¡ç®—åŠ æƒå¹³å‡
            weights = membership[:, cluster_id] ** fuzziness
            weight_sum = np.sum(weights)
        
            if weight_sum > 0:
                centers[cluster_id] = np.sum(data * weights[:, np.newaxis], axis=0) / weight_sum
            else:
                # å¦‚æœæƒé‡å’Œä¸º0ï¼Œä½¿ç”¨æ•°æ®çš„å‡å€¼
                centers[cluster_id] = np.mean(data, axis=0)
    
        return centers

    def _update_fuzzy_membership_matrix(self, data: np.ndarray, centers: np.ndarray, 
                                    fuzziness: float) -> np.ndarray:
        """æ›´æ–°æ¨¡ç³Šéš¶å±åº¦çŸ©é˜µ"""
        n_samples = data.shape[0]
        n_clusters = centers.shape[0]
        
        membership = np.zeros((n_samples, n_clusters))
        
        for i in range(n_samples):
            for j in range(n_clusters):
                # è®¡ç®—æ ·æœ¬åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
                distances = []
                for k in range(n_clusters):
                    distance = np.linalg.norm(data[i] - centers[k])
                    distances.append(max(distance, 1e-10))  # é¿å…é™¤é›¶
                
                # è®¡ç®—éš¶å±åº¦
                distance_j = distances[j]
                membership_sum = 0.0
                
                for k in range(n_clusters):
                    ratio = distance_j / distances[k]
                    membership_sum += ratio ** (2 / (fuzziness - 1))
                
                membership[i, j] = 1.0 / membership_sum
    
        return membership

    def _calculate_sample_membership(self, sample: np.ndarray, centers: np.ndarray, 
                                fuzziness: float) -> np.ndarray:
        """è®¡ç®—å•ä¸ªæ ·æœ¬çš„èšç±»éš¶å±åº¦"""
        n_clusters = centers.shape[0]
        membership = np.zeros(n_clusters)
        
        # è®¡ç®—åˆ°å„èšç±»ä¸­å¿ƒçš„è·ç¦»
        distances = []
        for center in centers:
            distance = np.linalg.norm(sample - center)
            distances.append(max(distance, 1e-10))
        
        # è®¡ç®—éš¶å±åº¦
        for j in range(n_clusters):
            distance_j = distances[j]
            membership_sum = 0.0
            
            for k in range(n_clusters):
                ratio = distance_j / distances[k]
                membership_sum += ratio ** (2 / (fuzziness - 1))
            
            membership[j] = 1.0 / membership_sum
    
        return membership

    def _identify_cluster_characteristic_tails(self, cluster_center: np.ndarray) -> List[int]:
        """è¯†åˆ«èšç±»çš„ç‰¹å¾å°¾æ•°"""
        # æ‰¾å‡ºèšç±»ä¸­å¿ƒä¸­å€¼æœ€é«˜çš„å°¾æ•°
        threshold = 0.5  # é˜ˆå€¼
        characteristic_tails = []
        
        for tail in range(len(cluster_center)):
            if cluster_center[tail] > threshold:
                characteristic_tails.append(tail)
        
        # å¦‚æœæ²¡æœ‰å°¾æ•°è¶…è¿‡é˜ˆå€¼ï¼Œé€‰æ‹©æœ€å¤§çš„å‡ ä¸ª
        if not characteristic_tails:
            sorted_indices = np.argsort(cluster_center)[::-1]
            characteristic_tails = sorted_indices[:3].tolist()  # å–å‰3ä¸ª
        
        return characteristic_tails

    def _evaluate_fuzzy_clustering_quality(self, data: np.ndarray, centers: np.ndarray, 
                                        membership: np.ndarray) -> Dict:
        """è¯„ä¼°æ¨¡ç³Šèšç±»è´¨é‡"""
        # 1. è®¡ç®—æ¨¡ç³Šåˆ†å‰²ç³»æ•° (Fuzzy Partition Coefficient, FPC)
        fpc = np.sum(membership ** 2) / data.shape[0]
        
        # 2. è®¡ç®—æ¨¡ç³Šåˆ†å‰²ç†µ (Fuzzy Partition Entropy, FPE)
        # é¿å…log(0)
        membership_safe = np.clip(membership, 1e-10, 1.0)
        fpe = -np.sum(membership * np.log(membership_safe)) / data.shape[0]
        
        # 3. è®¡ç®—ç±»å†…å¹³æ–¹å’Œ
        within_cluster_sum_squares = 0.0
        for i in range(data.shape[0]):
            for j in range(centers.shape[0]):
                distance_sq = np.sum((data[i] - centers[j]) ** 2)
                within_cluster_sum_squares += membership[i, j] * distance_sq
    
        return {
            'fuzzy_partition_coefficient': float(fpc),
            'fuzzy_partition_entropy': float(fpe),
            'within_cluster_sum_squares': float(within_cluster_sum_squares),
            'quality_score': float(fpc - fpe / 10)  # ç»¼åˆè´¨é‡åˆ†æ•°
        }

    def _fuzzy_timeseries_matching(self, historical_context: List[Dict], current_tails: Set[int]) -> Dict:
        """
        æ¨¡ç³Šæ—¶é—´åºåˆ—åŒ¹é…çš„å®Œæ•´å®ç°
        """
        if len(historical_context) < 5:
            return {
                'match_score': 0.0,
                'fuzzy_trend': 'insufficient_data',
                'similar_periods': [],
                'trend_strength': 0.0
            }
        
        # 1. æ„å»ºæ¨¡ç³Šæ—¶é—´åºåˆ—
        fuzzy_series = []
        
        for period in historical_context:
            period_tails = period.get('tails', [])
            
            # ä¸ºæ¯ä¸ªå°¾æ•°è®¡ç®—æ¨¡ç³Šå€¼
            fuzzy_vector = []
            for tail in range(10):
                if tail in period_tails:
                    # åŸºäºå°¾æ•°åœ¨è¯¥æœŸä¸­çš„"é‡è¦æ€§"è®¡ç®—æ¨¡ç³Šå€¼
                    tail_importance = 1.0  # ç®€åŒ–ï¼šæ‰€æœ‰å‡ºç°çš„å°¾æ•°é‡è¦æ€§ç›¸åŒ
                    fuzzy_vector.append(tail_importance)
                else:
                    fuzzy_vector.append(0.0)
            
            fuzzy_series.append(fuzzy_vector)
        
        fuzzy_series = np.array(fuzzy_series)
        
        # 2. å½“å‰æœŸçš„æ¨¡ç³Šå‘é‡
        current_fuzzy_vector = np.array([1.0 if tail in current_tails else 0.0 for tail in range(10)])
        
        # 3. è®¡ç®—ä¸å†å²å„æœŸçš„æ¨¡ç³Šç›¸ä¼¼åº¦
        similarities = []
        for i, historical_vector in enumerate(fuzzy_series):
            similarity = self._calculate_fuzzy_vector_similarity(current_fuzzy_vector, historical_vector)
            similarities.append({
                'period_index': i,
                'similarity': similarity,
                'historical_tails': [tail for tail in range(10) if historical_vector[tail] > 0]
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        # 4. åˆ†ææ¨¡ç³Šè¶‹åŠ¿
        fuzzy_trend, trend_strength = self._analyze_fuzzy_trend(fuzzy_series, current_fuzzy_vector)
        
        # 5. è®¡ç®—æ•´ä½“åŒ¹é…åˆ†æ•°
        if similarities:
            match_score = np.mean([sim['similarity'] for sim in similarities[:5]])  # å‰5ä¸ªæœ€ç›¸ä¼¼çš„
        else:
            match_score = 0.0
        
        return {
            'match_score': float(match_score),
            'fuzzy_trend': fuzzy_trend,
            'trend_strength': float(trend_strength),
            'similar_periods': similarities[:3],  # å‰3ä¸ªæœ€ç›¸ä¼¼çš„æœŸæ•°
            'avg_similarity_top5': float(np.mean([sim['similarity'] for sim in similarities[:5]])) if len(similarities) >= 5 else match_score,
            'similarity_variance': float(np.var([sim['similarity'] for sim in similarities])) if similarities else 0.0
        }

    def _calculate_fuzzy_vector_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ¨¡ç³Šå‘é‡çš„ç›¸ä¼¼åº¦"""
        # ä½¿ç”¨å¤šç§ç›¸ä¼¼åº¦åº¦é‡çš„åŠ æƒç»„åˆ
        
        # 1. ä½™å¼¦ç›¸ä¼¼åº¦
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 > 0 and norm2 > 0:
            cosine_sim = np.dot(vec1, vec2) / (norm1 * norm2)
        else:
            cosine_sim = 0.0
        
        # 2. æ¨¡ç³ŠJaccardç›¸ä¼¼åº¦
        intersection = np.sum(np.minimum(vec1, vec2))
        union = np.sum(np.maximum(vec1, vec2))
        if union > 0:
            jaccard_sim = intersection / union
        else:
            jaccard_sim = 0.0
    
        # 3. æ¨¡ç³ŠDiceç›¸ä¼¼åº¦
        sum_mins = np.sum(np.minimum(vec1, vec2))
        sum_both = np.sum(vec1) + np.sum(vec2)
        if sum_both > 0:
            dice_sim = (2 * sum_mins) / sum_both
        else:
            dice_sim = 0.0
    
        # 4. ç»¼åˆç›¸ä¼¼åº¦
        combined_similarity = (cosine_sim * 0.4 + jaccard_sim * 0.3 + dice_sim * 0.3)
    
        return combined_similarity

    def _analyze_fuzzy_trend(self, fuzzy_series: np.ndarray, current_vector: np.ndarray) -> Tuple[str, float]:
        """åˆ†ææ¨¡ç³Šè¶‹åŠ¿"""
        if len(fuzzy_series) < 3:
            return 'insufficient_data', 0.0
        
        # è®¡ç®—æœ€è¿‘å‡ æœŸçš„è¶‹åŠ¿
        recent_series = fuzzy_series[-5:] if len(fuzzy_series) >= 5 else fuzzy_series
        
        # è®¡ç®—æ¯æœŸçš„"æ´»è·ƒåº¦"ï¼ˆæ‰€æœ‰å°¾æ•°æ¨¡ç³Šå€¼çš„å’Œï¼‰
        activity_levels = [np.sum(period) for period in recent_series]
        current_activity = np.sum(current_vector)
        
        # åˆ†æè¶‹åŠ¿æ–¹å‘
        if len(activity_levels) >= 2:
            # ä½¿ç”¨çº¿æ€§å›å½’åˆ†æè¶‹åŠ¿
            x = np.arange(len(activity_levels))
            y = np.array(activity_levels)
            
            if len(x) > 1 and np.var(y) > 0:
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
                trend_strength = abs(r_value)  # ä½¿ç”¨ç›¸å…³ç³»æ•°ä½œä¸ºè¶‹åŠ¿å¼ºåº¦
                
                # é¢„æµ‹å½“å‰æœŸçš„æ´»è·ƒåº¦
                predicted_activity = slope * len(activity_levels) + intercept
                activity_deviation = abs(current_activity - predicted_activity)
                
                # åˆ¤æ–­è¶‹åŠ¿æ–¹å‘
                if slope > 0.1:
                    if current_activity > predicted_activity * 1.1:
                        trend = 'strongly_increasing'
                    else:
                        trend = 'increasing'
                elif slope < -0.1:
                    if current_activity < predicted_activity * 0.9:
                        trend = 'strongly_decreasing'
                    else:
                        trend = 'decreasing'
                else:
                    if activity_deviation > np.std(activity_levels):
                        trend = 'volatile'
                    else:
                        trend = 'stable'
            else:
                trend = 'stable'
                trend_strength = 0.0
        else:
            trend = 'insufficient_data'
            trend_strength = 0.0
        
        return trend, trend_strength
    
    def _build_adjacency_matrix(self, edges: Dict, nodes: Set) -> np.ndarray:
        """æ„å»ºé‚»æ¥çŸ©é˜µ"""
        n = len(nodes)
        node_list = sorted(list(nodes))
        node_to_idx = {node: idx for idx, node in enumerate(node_list)}
    
        adjacency = np.zeros((n, n))
    
        for (node1, node2), edge_info in edges.items():
            idx1 = node_to_idx[node1]
            idx2 = node_to_idx[node2]
            weight = edge_info['weight']
        
            adjacency[idx1][idx2] = weight
            adjacency[idx2][idx1] = weight  # æ— å‘å›¾
    
        return adjacency
    
    def _find_connected_components(self, adjacency_matrix: np.ndarray) -> List[List[int]]:
        """æŸ¥æ‰¾è¿é€šåˆ†é‡"""
        n = len(adjacency_matrix)
        visited = [False] * n
        components = []
    
        def dfs(node, component):
            visited[node] = True
            component.append(node)
            
            for neighbor in range(n):
                if adjacency_matrix[node][neighbor] > 0 and not visited[neighbor]:
                    dfs(neighbor, component)
        
        for i in range(n):
            if not visited[i]:
                component = []
                dfs(i, component)
                components.append(component)
        
        return components

    def _calculate_global_clustering_coefficient(self, adjacency_matrix: np.ndarray) -> float:
        """è®¡ç®—å…¨å±€èšé›†ç³»æ•°"""
        n = len(adjacency_matrix)
        total_triangles = 0
        total_triplets = 0
        
        for i in range(n):
            neighbors_i = [j for j in range(n) if adjacency_matrix[i][j] > 0]
            degree_i = len(neighbors_i)
            
            if degree_i >= 2:
                # è®¡ç®—å¯èƒ½çš„ä¸‰å…ƒç»„æ•°é‡
                possible_triplets = degree_i * (degree_i - 1) // 2
                total_triplets += possible_triplets
            
                # è®¡ç®—å®é™…çš„ä¸‰è§’å½¢æ•°é‡
                triangles = 0
                for j in range(len(neighbors_i)):
                    for k in range(j + 1, len(neighbors_i)):
                        if adjacency_matrix[neighbors_i[j]][neighbors_i[k]] > 0:
                            triangles += 1
                
                total_triangles += triangles
    
        if total_triplets > 0:
            return total_triangles / total_triplets
        else:
            return 0.0

    def _calculate_graph_metrics(self, adjacency_matrix: np.ndarray) -> Tuple[float, float]:
        """è®¡ç®—å›¾çš„ç›´å¾„å’Œå¹³å‡è·¯å¾„é•¿åº¦"""
        n = len(adjacency_matrix)
        
        # ä½¿ç”¨Floyd-Warshallç®—æ³•è®¡ç®—æ‰€æœ‰å¯¹æœ€çŸ­è·¯å¾„
        dist = np.full((n, n), float('inf'))
        
        # åˆå§‹åŒ–
        for i in range(n):
            for j in range(n):
                if i == j:
                    dist[i][j] = 0
                elif adjacency_matrix[i][j] > 0:
                    dist[i][j] = 1  # ç®€åŒ–ï¼šæ‰€æœ‰è¾¹æƒé‡ä¸º1
        
        # Floyd-Warshallç®—æ³•
        for k in range(n):
            for i in range(n):
                for j in range(n):
                    if dist[i][k] + dist[k][j] < dist[i][j]:
                        dist[i][j] = dist[i][k] + dist[k][j]
        
        # è®¡ç®—ç›´å¾„å’Œå¹³å‡è·¯å¾„é•¿åº¦
        finite_distances = []
        for i in range(n):
            for j in range(i + 1, n):
                if dist[i][j] != float('inf'):
                    finite_distances.append(dist[i][j])
        
        if finite_distances:
            diameter = max(finite_distances)
            avg_path_length = np.mean(finite_distances)
        else:
            diameter = 0
            avg_path_length = 0
        
        return diameter, avg_path_length

    def _calculate_degree_sequence(self, adjacency_matrix: np.ndarray) -> List[int]:
        """è®¡ç®—åº¦åºåˆ—"""
        degrees = []
        for i in range(len(adjacency_matrix)):
            degree = sum(1 for j in range(len(adjacency_matrix)) if adjacency_matrix[i][j] > 0)
            degrees.append(degree)
        
        return sorted(degrees, reverse=True)

    def _calculate_degree_distribution(self, degree_sequence: List[int]) -> Dict[int, float]:
        """è®¡ç®—åº¦åˆ†å¸ƒ"""
        if not degree_sequence:
            return {}
        
        from collections import Counter
        degree_counts = Counter(degree_sequence)
        total_nodes = len(degree_sequence)
    
        distribution = {}
        for degree, count in degree_counts.items():
            distribution[degree] = count / total_nodes
    
        return distribution
    
    def _classify_edge_type(self, co_occurrence_strength: float, temporal_strength: float, 
                        correlation_coefficient: float) -> str:
        """åˆ†ç±»è¾¹çš„ç±»å‹"""
        if co_occurrence_strength > 0.7:
            return 'strong_co_occurrence'
        elif temporal_strength > 0.7:
            return 'strong_temporal'
        elif abs(correlation_coefficient) > 0.7:
            return 'strong_correlation'
        elif co_occurrence_strength > 0.3 or temporal_strength > 0.3:
            return 'moderate_relationship'
        else:
            return 'weak_relationship'

    def _calculate_node_activation(self, tail: int, current_tails: Set[int], recent_history: List[Dict]) -> float:
        """è®¡ç®—èŠ‚ç‚¹æ¿€æ´»æ°´å¹³"""
        # å½“å‰æ¿€æ´»
        current_activation = 1.0 if tail in current_tails else 0.0
        
        # å†å²æ¿€æ´»è¡°å‡
        historical_activation = 0.0
        decay_factor = 0.9
        
        for i, period in enumerate(recent_history[:10]):  # æœ€è¿‘10æœŸ
            if tail in period.get('tails', []):
                historical_activation += (decay_factor ** i)
        
        # ç»¼åˆæ¿€æ´»æ°´å¹³
        total_activation = current_activation + historical_activation * 0.1
        
        return min(1.0, total_activation)

    def _predict_current_connection_strength(self, tail_i: int, tail_j: int, current_tails: Set[int],
                                        co_occurrence_prob: float, cond_prob_j_i: float,
                                        cond_prob_i_j: float, mutual_info: float) -> float:
        """é¢„æµ‹å½“å‰è¿æ¥å¼ºåº¦"""
        # åŸºäºå½“å‰çŠ¶æ€çš„è¿æ¥å¼ºåº¦
        both_present = tail_i in current_tails and tail_j in current_tails
        
        if both_present:
            # ä¸¤ä¸ªå°¾æ•°éƒ½å‡ºç°ï¼Œè¿æ¥å¼ºåº¦é«˜
            base_strength = 0.8
        elif tail_i in current_tails or tail_j in current_tails:
            # åªæœ‰ä¸€ä¸ªå‡ºç°ï¼ŒåŸºäºæ¡ä»¶æ¦‚ç‡
            if tail_i in current_tails:
                base_strength = cond_prob_j_i
            else:
                base_strength = cond_prob_i_j
        else:
            # ä¸¤ä¸ªéƒ½ä¸å‡ºç°ï¼Œè¿æ¥å¼ºåº¦ä½
            base_strength = 0.1
    
        # ç»“åˆå†å²ä¿¡æ¯è°ƒæ•´
        historical_factor = (co_occurrence_prob * 0.4 + 
                            max(cond_prob_j_i, cond_prob_i_j) * 0.4 + 
                            min(1.0, abs(mutual_info) * 2) * 0.2)
    
        final_strength = base_strength * 0.6 + historical_factor * 0.4
    
        return final_strength

    def _calculate_edge_prediction_confidence(self, co_occurrence_count: int, 
                                            total_possible: int, history_length: int) -> float:
        """è®¡ç®—è¾¹é¢„æµ‹ç½®ä¿¡åº¦"""
        if total_possible == 0:
            return 0.0
        
        # åŸºäºæ ·æœ¬é‡çš„ç½®ä¿¡åº¦
        sample_confidence = min(1.0, history_length / 50.0)
        
        # åŸºäºå…±ç°é¢‘ç‡çš„ç½®ä¿¡åº¦
        frequency_confidence = co_occurrence_count / total_possible
        
        # ç»¼åˆç½®ä¿¡åº¦
        overall_confidence = (sample_confidence * 0.6 + frequency_confidence * 0.4)
        
        return overall_confidence

    def _analyze_current_activation_patterns(self, current_tails: Set[int], 
                                        node_properties: Dict, edges: Dict) -> Dict:
        """åˆ†æå½“å‰æ¿€æ´»æ¨¡å¼"""
        active_nodes = list(current_tails)
        
        # è®¡ç®—æ¿€æ´»å¯†åº¦
        if len(active_nodes) > 1:
            total_possible_edges = len(active_nodes) * (len(active_nodes) - 1) // 2
            actual_active_edges = 0
            
            for i, node1 in enumerate(active_nodes):
                for j, node2 in enumerate(active_nodes[i+1:], i+1):
                    edge_key = (min(node1, node2), max(node1, node2))
                    if edge_key in edges:
                        actual_active_edges += 1
            
            activation_density = actual_active_edges / total_possible_edges
        else:
            activation_density = 0.0
    
        # åˆ†ææ¿€æ´»èšé›†æ€§
        activation_clustering = 0.0
        if len(active_nodes) >= 3:
            triangles = 0
            for i, node1 in enumerate(active_nodes):
                for j, node2 in enumerate(active_nodes[i+1:], i+1):
                    for k, node3 in enumerate(active_nodes[j+1:], j+1):
                        # æ£€æŸ¥æ˜¯å¦å½¢æˆä¸‰è§’å½¢
                        edge1 = (min(node1, node2), max(node1, node2)) in edges
                        edge2 = (min(node2, node3), max(node2, node3)) in edges
                        edge3 = (min(node1, node3), max(node1, node3)) in edges
                    
                        if edge1 and edge2 and edge3:
                            triangles += 1
        
            max_triangles = len(active_nodes) * (len(active_nodes) - 1) * (len(active_nodes) - 2) // 6
            activation_clustering = triangles / max_triangles if max_triangles > 0 else 0.0
    
        return {
            'active_nodes': active_nodes,
            'activation_density': activation_density,
            'activation_clustering': activation_clustering,
            'num_active_nodes': len(active_nodes),
            'avg_node_activation': np.mean([node_properties[node]['activation_level'] 
                                      for node in active_nodes]) if active_nodes else 0.0
        }

    def _calculate_graph_signature(self, adjacency_matrix: np.ndarray, node_properties: Dict) -> Dict:
        """è®¡ç®—å›¾ç­¾å"""
        n = len(adjacency_matrix)
        
        # åŸºæœ¬å›¾ç»Ÿè®¡
        num_edges = np.sum(adjacency_matrix > 0) // 2  # æ— å‘å›¾
        density = num_edges / (n * (n - 1) // 2) if n > 1 else 0
        
        # åº¦åˆ†å¸ƒç‰¹å¾
        degree_sequence = self._calculate_degree_sequence(adjacency_matrix)
        max_degree = max(degree_sequence) if degree_sequence else 0
        avg_degree = np.mean(degree_sequence) if degree_sequence else 0
        
        # è¿é€šæ€§ç‰¹å¾
        connected_components = self._find_connected_components(adjacency_matrix)
        num_components = len(connected_components)
        largest_component_size = max(len(comp) for comp in connected_components) if connected_components else 0
        
        return {
            'num_nodes': n,
            'num_edges': num_edges,
            'density': density,
            'max_degree': max_degree,
            'avg_degree': avg_degree,
            'num_components': num_components,
            'largest_component_size': largest_component_size
        }

    def _graphs_potentially_isomorphic(self, signature1: Dict, signature2: Dict) -> bool:
        """æ£€æŸ¥å›¾æ˜¯å¦å¯èƒ½åŒæ„"""
        # åŸºæœ¬ä¸å˜é‡æ£€æŸ¥
        if signature1['num_nodes'] != signature2['num_nodes']:
            return False
        if signature1['num_edges'] != signature2['num_edges']:
            return False
        if signature1['num_components'] != signature2['num_components']:
            return False
        
        # åº¦åºåˆ—æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        if abs(signature1['max_degree'] - signature2['max_degree']) > 0:
            return False
        
        return True

    def _calculate_centrality_measures(self, adjacency_matrix: np.ndarray) -> Dict[int, Dict]:
        """è®¡ç®—ä¸­å¿ƒæ€§åº¦é‡"""
        n = len(adjacency_matrix)
        centrality_measures = {}
        
        for i in range(n):
            # åº¦ä¸­å¿ƒæ€§
            degree = sum(1 for j in range(n) if adjacency_matrix[i][j] > 0)
            degree_centrality = degree / (n - 1) if n > 1 else 0
            
            # ç®€åŒ–çš„ä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆè®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œè¿™é‡Œä½¿ç”¨è¿‘ä¼¼ï¼‰
            betweenness_centrality = degree / max(1, n - 2) if n > 2 else 0
            
            # èšé›†ç³»æ•°
            neighbors = [j for j in range(n) if adjacency_matrix[i][j] > 0]
            if len(neighbors) >= 2:
                possible_edges = len(neighbors) * (len(neighbors) - 1) // 2
                actual_edges = 0
                for j in range(len(neighbors)):
                    for k in range(j + 1, len(neighbors)):
                        if adjacency_matrix[neighbors[j]][neighbors[k]] > 0:
                            actual_edges += 1
                clustering_coefficient = actual_edges / possible_edges
            else:
                clustering_coefficient = 0.0
        
            centrality_measures[i] = {
                'degree_centrality': degree_centrality,
                'betweenness_centrality': betweenness_centrality,
                'clustering_coefficient': clustering_coefficient
            }
    
        return centrality_measures

    def _calculate_state_distribution(self, state_sequence: List[int], n_states: int) -> Dict[int, float]:
        """è®¡ç®—çŠ¶æ€åˆ†å¸ƒ"""
        from collections import Counter
        state_counts = Counter(state_sequence)
        total_states = len(state_sequence)
        
        distribution = {}
        for state in range(n_states):
            distribution[state] = state_counts.get(state, 0) / total_states if total_states > 0 else 0.0
        
        return distribution

    def _calculate_overall_fuzzy_score(self, fuzzy_similarities: Dict, rule_matches: List[Dict], 
                                     fuzzy_clusters: Dict) -> float:
        """è®¡ç®—æ€»ä½“æ¨¡ç³Šè¯„åˆ†"""
        # ç®€åŒ–å®ç°
        similarity_scores = []
        for tail_sims in fuzzy_similarities.values():
            similarity_scores.extend(tail_sims.values())
        
        avg_similarity = np.mean(similarity_scores) if similarity_scores else 0.0
        rule_score = np.mean([match['match_degree'] for match in rule_matches]) if rule_matches else 0.0
        cluster_score = max(fuzzy_clusters.get('membership_degrees', [0.0]))
        
        return float((avg_similarity * 0.5 + rule_score * 0.3 + cluster_score * 0.2))
    
    # å…¶ä»–æ–¹æ³•ç»§ç»­ç®€åŒ–å®ç°...
    # (ä¸ºäº†ç¯‡å¹…é™åˆ¶ï¼Œè¿™é‡Œä¸å±•ç¤ºæ‰€æœ‰æ–¹æ³•çš„å®Œæ•´å®ç°)
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦å®ç°æ‰€æœ‰æ–¹æ³•çš„å®Œæ•´ç§‘ç ”çº§ç®—æ³•
    
    def _extract_emerging_pattern(self, current_tails: Set[int], historical_context: List[Dict]) -> Dict:
        """æå–æ–°å…´æ¨¡å¼"""
        return {'pattern_type': 'emerging', 'confidence': 0.6}
    
    def _update_pattern_with_new_data(self, pattern: Dict, current_tails: Set[int], 
                                    historical_context: List[Dict]) -> Dict:
        """ç”¨æ–°æ•°æ®æ›´æ–°æ¨¡å¼"""
        return pattern  # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ›´æ–°æ¨¡å¼å‚æ•°
    
    def _calculate_graph_properties(self, graph: Dict) -> Dict:
        """è®¡ç®—å›¾å±æ€§"""
        return {'density': 0.6, 'clustering_coefficient': 0.4, 'diameter': 3}
    
    # æ›´å¤šæ–¹æ³•çš„ç®€åŒ–å®ç°...
    def _build_pattern_hierarchy(self, historical_context: List[Dict]) -> Dict:
        """æ„å»ºæ¨¡å¼å±‚æ¬¡ç»“æ„"""
        return {'levels': 3, 'root_patterns': 5}
    
    def _extract_scale_patterns(self, historical_context: List[Dict], scale: int) -> List[Dict]:
        """æå–å°ºåº¦æ¨¡å¼"""
        return [{'pattern_id': i, 'scale': scale} for i in range(3)]
    
    def _match_patterns_at_scale(self, current_tails: Set[int], scale_patterns: List[Dict], scale: int) -> Dict:
        """åœ¨ç‰¹å®šå°ºåº¦åŒ¹é…æ¨¡å¼"""
        return {'match_score': 0.7, 'matched_patterns': len(scale_patterns)}
    
    def _hierarchical_clustering_analysis(self, historical_context: List[Dict], current_tails: Set[int]) -> Dict:
        """å±‚æ¬¡èšç±»åˆ†æ"""
        return {'clusters': 4, 'linkage_matrix': [[1, 2, 0.5, 2]]}
    
    def _tree_structure_pattern_matching(self, pattern_hierarchy: Dict, current_tails: Set[int]) -> Dict:
        """æ ‘ç»“æ„æ¨¡å¼åŒ¹é…"""
        return {'tree_similarity': 0.65, 'matched_nodes': 8}
    
    def _detect_fractal_patterns(self, historical_context: List[Dict], current_tails: Set[int]) -> Dict:
        """æ£€æµ‹åˆ†å½¢æ¨¡å¼"""
        return {'fractal_dimension': 1.6, 'self_similarity': 0.7}
    
    def _calculate_hierarchical_match_score(self, multiscale_matches: Dict, 
                                          hierarchical_clusters: Dict, tree_structure_matches: Dict) -> float:
        """è®¡ç®—å±‚æ¬¡åŒ¹é…åˆ†æ•°"""
        scores = []
        for match in multiscale_matches.values():
            if isinstance(match, dict) and 'match_score' in match:
                scores.append(match['match_score'])
        
        if tree_structure_matches and 'tree_similarity' in tree_structure_matches:
            scores.append(tree_structure_matches['tree_similarity'])
        
        return float(np.mean(scores)) if scores else 0.0
    
    def _mine_frequent_itemsets(self, transaction_database: List[List[int]], min_support: float) -> List[Dict]:
        """æŒ–æ˜é¢‘ç¹é¡¹é›†"""
        # ç®€åŒ–çš„Aprioriç®—æ³•å®ç°
        frequent_itemsets = []
        
        # è®¡ç®—å•é¡¹é¢‘ç¹é›†
        item_counts = {}
        total_transactions = len(transaction_database)
        
        for transaction in transaction_database:
            for item in transaction:
                item_counts[item] = item_counts.get(item, 0) + 1
        
        # æ‰¾åˆ°é¢‘ç¹å•é¡¹
        frequent_1_itemsets = []
        for item, count in item_counts.items():
            support = count / total_transactions
            if support >= min_support:
                frequent_1_itemsets.append({
                    'itemset': [item],
                    'support': support,
                    'count': count
                })
        
        frequent_itemsets.extend(frequent_1_itemsets)
        
        # ç”Ÿæˆæ›´å¤§çš„é¢‘ç¹é¡¹é›†ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if len(frequent_1_itemsets) > 1:
            for i in range(len(frequent_1_itemsets)):
                for j in range(i + 1, len(frequent_1_itemsets)):
                    candidate_itemset = sorted(list(set(frequent_1_itemsets[i]['itemset'] + frequent_1_itemsets[j]['itemset'])))
                    
                    # è®¡ç®—æ”¯æŒåº¦
                    count = 0
                    for transaction in transaction_database:
                        if all(item in transaction for item in candidate_itemset):
                            count += 1
                    
                    support = count / total_transactions
                    if support >= min_support:
                        frequent_itemsets.append({
                            'itemset': candidate_itemset,
                            'support': support,
                            'count': count
                        })
        
        return frequent_itemsets
    
    def _mine_association_rules(self, frequent_itemsets: List[Dict], min_confidence: float) -> List[Dict]:
        """æŒ–æ˜å…³è”è§„åˆ™"""
        association_rules = []
        
        # ä»é¢‘ç¹é¡¹é›†ç”Ÿæˆå…³è”è§„åˆ™
        for itemset_info in frequent_itemsets:
            itemset = itemset_info['itemset']
            if len(itemset) >= 2:
                # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è§„åˆ™
                for i in range(len(itemset)):
                    antecedent = [itemset[i]]
                    consequent = [item for item in itemset if item != itemset[i]]
                    
                    if consequent:
                        # è®¡ç®—ç½®ä¿¡åº¦
                        antecedent_support = self._calculate_itemset_support(antecedent, frequent_itemsets)
                        rule_support = itemset_info['support']
                        
                        if antecedent_support > 0:
                            confidence = rule_support / antecedent_support
                            if confidence >= min_confidence:
                                association_rules.append({
                                    'antecedent': antecedent,
                                    'consequent': consequent,
                                    'support': rule_support,
                                    'confidence': confidence,
                                    'lift': confidence / self._calculate_itemset_support(consequent, frequent_itemsets) if self._calculate_itemset_support(consequent, frequent_itemsets) > 0 else 0
                                })
        
        return association_rules
    
    def _calculate_itemset_support(self, itemset: List[int], frequent_itemsets: List[Dict]) -> float:
        """è®¡ç®—é¡¹é›†æ”¯æŒåº¦"""
        for itemset_info in frequent_itemsets:
            if sorted(itemset_info['itemset']) == sorted(itemset):
                return itemset_info['support']
        return 0.0
    
    def _mine_sequential_patterns(self, transaction_database: List[List[int]], min_support: float) -> List[Dict]:
        """æŒ–æ˜åºåˆ—æ¨¡å¼"""
        # ç®€åŒ–çš„åºåˆ—æ¨¡å¼æŒ–æ˜
        sequential_patterns = []
        
        # å¯»æ‰¾é•¿åº¦ä¸º2çš„åºåˆ—æ¨¡å¼
        pattern_counts = {}
        total_sequences = len(transaction_database) - 1
        
        for i in range(len(transaction_database) - 1):
            current_transaction = set(transaction_database[i])
            next_transaction = set(transaction_database[i + 1])
            
            for current_item in current_transaction:
                for next_item in next_transaction:
                    pattern = (current_item, next_item)
                    pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        # è¿‡æ»¤é¢‘ç¹åºåˆ—æ¨¡å¼
        for pattern, count in pattern_counts.items():
            support = count / total_sequences if total_sequences > 0 else 0
            if support >= min_support:
                sequential_patterns.append({
                    'pattern': list(pattern),
                    'support': support,
                    'count': count
                })
        
        return sequential_patterns
    
    def _match_with_frequent_itemsets(self, current_tails: Set[int], frequent_itemsets: List[Dict]) -> List[Dict]:
        """ä¸é¢‘ç¹é¡¹é›†åŒ¹é…"""
        matches = []
        
        for itemset_info in frequent_itemsets:
            itemset = set(itemset_info['itemset'])
            intersection = current_tails.intersection(itemset)
            
            if intersection:
                match_ratio = len(intersection) / len(itemset)
                matches.append({
                    'itemset': itemset_info['itemset'],
                    'match_ratio': match_ratio,
                    'support': itemset_info['support'],
                    'matched_items': list(intersection)
                })
        
        return sorted(matches, key=lambda x: x['match_ratio'] * x['support'], reverse=True)
    
    def _match_with_association_rules(self, current_tails: Set[int], association_rules: List[Dict]) -> List[Dict]:
        """ä¸å…³è”è§„åˆ™åŒ¹é…"""
        matches = []
        
        for rule in association_rules:
            antecedent = set(rule['antecedent'])
            consequent = set(rule['consequent'])
            
            # æ£€æŸ¥å‰ä»¶æ˜¯å¦æ»¡è¶³
            if antecedent.issubset(current_tails):
                consequent_match = len(current_tails.intersection(consequent)) / len(consequent)
                matches.append({
                    'rule': rule,
                    'antecedent_satisfied': True,
                    'consequent_match_ratio': consequent_match,
                    'confidence': rule['confidence']
                })
        
        return sorted(matches, key=lambda x: x['consequent_match_ratio'] * x['confidence'], reverse=True)
    
    def _match_with_sequential_patterns(self, current_tails: Set[int], sequential_patterns: List[Dict]) -> List[Dict]:
        """ä¸åºåˆ—æ¨¡å¼åŒ¹é…"""
        matches = []
        
        for pattern_info in sequential_patterns:
            pattern = pattern_info['pattern']
            if len(pattern) >= 2:
                # æ£€æŸ¥æ¨¡å¼çš„æœ€åä¸€ä¸ªå…ƒç´ æ˜¯å¦åœ¨å½“å‰å°¾æ•°ä¸­
                if pattern[-1] in current_tails:
                    matches.append({
                        'pattern': pattern,
                        'support': pattern_info['support'],
                        'match_position': 'end',
                        'match_confidence': pattern_info['support']
                    })
        
        return sorted(matches, key=lambda x: x['support'], reverse=True)
    
    def _detect_anomalous_frequent_patterns(self, current_tails: Set[int], frequent_itemsets: List[Dict], 
                                          association_rules: List[Dict]) -> List[Dict]:
        """æ£€æµ‹å¼‚å¸¸é¢‘ç¹æ¨¡å¼"""
        anomalies = []
        
        # æ£€æµ‹é¢‘ç¹é¡¹é›†ä¸­çš„å¼‚å¸¸
        for itemset_info in frequent_itemsets:
            itemset = set(itemset_info['itemset'])
            if itemset.issubset(current_tails):
                # å¦‚æœä¸€ä¸ªé«˜æ”¯æŒåº¦çš„é¡¹é›†å®Œå…¨å‡ºç°ï¼Œå¯èƒ½æ˜¯å¼‚å¸¸
                if itemset_info['support'] > 0.8:
                    anomalies.append({
                        'type': 'high_support_itemset_complete_match',
                        'itemset': list(itemset),
                        'support': itemset_info['support'],
                        'anomaly_score': itemset_info['support']
                    })
        
        # æ£€æµ‹å…³è”è§„åˆ™ä¸­çš„å¼‚å¸¸
        for rule in association_rules:
            antecedent = set(rule['antecedent'])
            consequent = set(rule['consequent'])
            
            if antecedent.issubset(current_tails) and not consequent.intersection(current_tails):
                # å‰ä»¶æ»¡è¶³ä½†åä»¶ä¸æ»¡è¶³ï¼Œå¯èƒ½æ˜¯è§„åˆ™è¢«ç ´å
                if rule['confidence'] > 0.8:
                    anomalies.append({
                        'type': 'association_rule_violation',
                        'rule': rule,
                        'anomaly_score': rule['confidence']
                    })
        
        return sorted(anomalies, key=lambda x: x['anomaly_score'], reverse=True)
    
    def _calculate_frequent_pattern_score(self, itemset_matches: List[Dict], rule_matches: List[Dict], 
                                        sequence_matches: List[Dict]) -> float:
        """è®¡ç®—é¢‘ç¹æ¨¡å¼åˆ†æ•°"""
        scores = []
        
        if itemset_matches:
            itemset_score = np.mean([match['match_ratio'] * match['support'] for match in itemset_matches[:3]])
            scores.append(itemset_score)
        
        if rule_matches:
            rule_score = np.mean([match['consequent_match_ratio'] * match['confidence'] for match in rule_matches[:3]])
            scores.append(rule_score)
        
        if sequence_matches:
            sequence_score = np.mean([match['support'] for match in sequence_matches[:3]])
            scores.append(sequence_score)
        
        return float(np.mean(scores)) if scores else 0.0

class ManipulationIntensityCalculator:
    """
    ç§‘ç ”çº§æ“æ§å¼ºåº¦è®¡ç®—å™¨
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å¤šç»´è¯æ®èåˆç®—æ³•
    2. åŠ¨æ€é˜ˆå€¼è‡ªé€‚åº”ç³»ç»Ÿ
    3. è´å¶æ–¯è¯æ®æ›´æ–°æœºåˆ¶
    4. æ—¶é—´åºåˆ—æ“æ§å¼ºåº¦åˆ†æ
    5. èµ”ç‡æ•æ„Ÿæ€§åˆ†æ
    6. æ“æ§ç±»å‹æƒé‡è¯„ä¼°
    7. ç½®ä¿¡åº¦é‡åŒ–ç³»ç»Ÿ
    8. å†å²å¼ºåº¦è¿½è¸ªä¸å­¦ä¹ 
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–æ“æ§å¼ºåº¦è®¡ç®—å™¨"""
        self.config = config or self._get_default_config()
        
        # å†å²æ“æ§å¼ºåº¦è®°å½•
        self.intensity_history = deque(maxlen=self.config['max_history_length'])
        self.intensity_statistics = {
            'total_calculations': 0,
            'intensity_distribution': defaultdict(int),
            'accuracy_tracking': [],
            'confidence_history': deque(maxlen=100)
        }
        
        # åŠ¨æ€é˜ˆå€¼ç³»ç»Ÿ
        self.dynamic_thresholds = {
            'subtle_threshold': 0.25,
            'moderate_threshold': 0.45,
            'strong_threshold': 0.65,
            'extreme_threshold': 0.85
        }
        
        # è¯æ®æƒé‡çŸ©é˜µï¼ˆè‡ªé€‚åº”ï¼‰
        self.evidence_weights = {
            'detection_results': {
                'base_weight': 0.40,
                'adaptive_factor': 1.0,
                'reliability_score': 0.8
            },
            'statistical_anomalies': {
                'base_weight': 0.30,
                'adaptive_factor': 1.0,
                'reliability_score': 0.9
            },
            'pattern_matches': {
                'base_weight': 0.30,
                'adaptive_factor': 1.0,
                'reliability_score': 0.7
            }
        }
        
        # èµ”ç‡å½±å“ç³»æ•°
        self.odds_sensitivity_matrix = {
            0: 2.0,   # 0å°¾èµ”ç‡2å€ï¼Œæ“æ§å½±å“æ›´å¤§
            1: 1.8, 2: 1.8, 3: 1.8, 4: 1.8, 5: 1.8,
            6: 1.8, 7: 1.8, 8: 1.8, 9: 1.8   # 1-9å°¾èµ”ç‡1.8å€
        }
        
        # è´å¶æ–¯å…ˆéªŒåˆ†å¸ƒ
        self.bayesian_priors = {
            ManipulationIntensity.NATURAL: 0.4,
            ManipulationIntensity.SUBTLE: 0.25,
            ManipulationIntensity.MODERATE: 0.2,
            ManipulationIntensity.STRONG: 0.1,
            ManipulationIntensity.EXTREME: 0.05
        }
        
        # æ—¶é—´è¡°å‡å› å­
        self.temporal_weights = self._initialize_temporal_weights()
        
        print(f"ğŸ§® ç§‘ç ”çº§æ“æ§å¼ºåº¦è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - åŠ¨æ€é˜ˆå€¼ç³»ç»Ÿ: {len(self.dynamic_thresholds)}ä¸ªé˜ˆå€¼")
        print(f"   - è¯æ®æƒé‡ç»´åº¦: {len(self.evidence_weights)}ä¸ª")
        print(f"   - èµ”ç‡æ•æ„Ÿæ€§çŸ©é˜µ: 10ä¸ªå°¾æ•°å·®å¼‚åŒ–åˆ†æ")
        print(f"   - è´å¶æ–¯å…ˆéªŒåˆ†å¸ƒå·²åŠ è½½")

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'max_history_length': 200,
            'threshold_adaptation_rate': 0.05,
            'evidence_weight_learning_rate': 0.03,
            'temporal_decay_factor': 0.95,
            'confidence_calculation_window': 20,
            'bayesian_update_frequency': 10,
            'outlier_detection_sensitivity': 2.0,
            'intensity_smoothing_factor': 0.8,
            'cross_validation_window': 50,
            'minimum_evidence_threshold': 0.1
        }

    def _initialize_temporal_weights(self) -> Dict[str, float]:
        """åˆå§‹åŒ–æ—¶é—´æƒé‡ç³»ç»Ÿ"""
        return {
            'immediate': 1.0,      # å½“å‰æœŸæƒé‡
            'recent': 0.8,         # æœ€è¿‘3æœŸæƒé‡
            'short_term': 0.6,     # æœ€è¿‘10æœŸæƒé‡
            'medium_term': 0.4,    # æœ€è¿‘30æœŸæƒé‡
            'long_term': 0.2       # å†å²æƒé‡
        }

    def calculate_intensity(self, detection_results: Dict, statistical_anomalies: Dict, pattern_matches: Dict) -> ManipulationIntensity:
        """
        ç§‘ç ”çº§æ“æ§å¼ºåº¦è®¡ç®—ä¸»æ–¹æ³•
        
        Args:
            detection_results: å¤šç»´åº¦æ£€æµ‹ç»“æœ
            statistical_anomalies: ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ç»“æœ  
            pattern_matches: æ¨¡å¼åŒ¹é…ç»“æœ
            
        Returns:
            ManipulationIntensity: æ“æ§å¼ºåº¦ç­‰çº§
        """
        calculation_start_time = datetime.now()
        
        try:
            # === ç¬¬ä¸€é˜¶æ®µï¼šè¯æ®é¢„å¤„ç†ä¸éªŒè¯ ===
            validated_evidence = self._validate_and_preprocess_evidence(
                detection_results, statistical_anomalies, pattern_matches
            )
            
            if not validated_evidence['is_valid']:
                return self._handle_insufficient_evidence(validated_evidence)
            
            # === ç¬¬äºŒé˜¶æ®µï¼šå¤šç»´åº¦è¯æ®èåˆ ===
            fusion_result = self._multi_dimensional_evidence_fusion(
                validated_evidence['detection_results'],
                validated_evidence['statistical_anomalies'], 
                validated_evidence['pattern_matches']
            )
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šè´å¶æ–¯åéªŒæ¦‚ç‡è®¡ç®— ===
            bayesian_posterior = self._calculate_bayesian_posterior(
                fusion_result, validated_evidence
            )
            
            # === ç¬¬å››é˜¶æ®µï¼šæ—¶é—´åºåˆ—å¼ºåº¦åˆ†æ ===
            temporal_analysis = self._temporal_intensity_analysis(
                fusion_result, bayesian_posterior
            )
            
            # === ç¬¬äº”é˜¶æ®µï¼šèµ”ç‡æ•æ„Ÿæ€§ä¿®æ­£ ===
            odds_corrected_intensity = self._apply_odds_sensitivity_correction(
                temporal_analysis, validated_evidence
            )
            
            # === ç¬¬å…­é˜¶æ®µï¼šåŠ¨æ€é˜ˆå€¼åˆ†ç±» ===
            intensity_classification = self._dynamic_threshold_classification(
                odds_corrected_intensity
            )
            
            # === ç¬¬ä¸ƒé˜¶æ®µï¼šç½®ä¿¡åº¦é‡åŒ–ä¸éªŒè¯ ===
            final_result = self._calculate_confidence_and_validate(
                intensity_classification, fusion_result, bayesian_posterior
            )
            
            # === ç¬¬å…«é˜¶æ®µï¼šå­¦ä¹ ä¸è‡ªé€‚åº”æ›´æ–° ===
            self._update_learning_systems(final_result, validated_evidence)
            
            # è®°å½•è®¡ç®—ç»Ÿè®¡
            calculation_duration = (datetime.now() - calculation_start_time).total_seconds()
            self._record_calculation_statistics(final_result, calculation_duration)
            
            return final_result['intensity']
            
        except Exception as e:
            print(f"âŒ æ“æ§å¼ºåº¦è®¡ç®—å¤±è´¥: {e}")
            return ManipulationIntensity.NATURAL  # å®‰å…¨é»˜è®¤å€¼

    def _validate_and_preprocess_evidence(self, detection_results: Dict, statistical_anomalies: Dict, pattern_matches: Dict) -> Dict[str, Any]:
        """
        ç§‘ç ”çº§è¯æ®éªŒè¯ä¸é¢„å¤„ç†ç®—æ³•
        ç¡®ä¿è¾“å…¥æ•°æ®çš„è´¨é‡å’Œä¸€è‡´æ€§
        """
        validation_result = {
            'is_valid': True,
            'validation_score': 0.0,
            'quality_metrics': {},
            'detection_results': {},
            'statistical_anomalies': {},
            'pattern_matches': {}
        }
        
        # === æ£€æµ‹ç»“æœéªŒè¯ ===
        detection_quality = self._validate_detection_results(detection_results)
        validation_result['detection_results'] = detection_quality['processed_data']
        validation_result['quality_metrics']['detection_quality'] = detection_quality['quality_score']
        
        # === ç»Ÿè®¡å¼‚å¸¸éªŒè¯ ===
        statistical_quality = self._validate_statistical_anomalies(statistical_anomalies)
        validation_result['statistical_anomalies'] = statistical_quality['processed_data']
        validation_result['quality_metrics']['statistical_quality'] = statistical_quality['quality_score']
        
        # === æ¨¡å¼åŒ¹é…éªŒè¯ ===
        pattern_quality = self._validate_pattern_matches(pattern_matches)
        validation_result['pattern_matches'] = pattern_quality['processed_data']
        validation_result['quality_metrics']['pattern_quality'] = pattern_quality['quality_score']
        
        # === ç»¼åˆè´¨é‡è¯„ä¼° ===
        overall_quality = np.mean([
            detection_quality['quality_score'],
            statistical_quality['quality_score'],
            pattern_quality['quality_score']
        ])
        
        validation_result['validation_score'] = overall_quality
        validation_result['is_valid'] = overall_quality >= self.config['minimum_evidence_threshold']
        
        return validation_result

    def _validate_detection_results(self, detection_results: Dict) -> Dict[str, Any]:
        """éªŒè¯æ£€æµ‹ç»“æœæ•°æ®è´¨é‡"""
        quality_score = 0.0
        processed_data = {}
        
        try:
            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ['combined_score', 'max_score', 'detection_consensus']
            field_scores = []
            
            for field in required_fields:
                if field in detection_results:
                    value = detection_results[field]
                    if isinstance(value, (int, float)) and 0 <= value <= 1:
                        field_scores.append(1.0)
                        processed_data[field] = float(value)
                    else:
                        field_scores.append(0.5)
                        processed_data[field] = 0.5  # é»˜è®¤å€¼
                else:
                    field_scores.append(0.0)
                    processed_data[field] = 0.5
            
            # éªŒè¯è¯¦ç»†æ£€æµ‹ç»“æœ
            detailed_results = {}
            if isinstance(detection_results, dict):
                for key, value in detection_results.items():
                    if key not in required_fields and isinstance(value, dict):
                        score = value.get('score', 0.0)
                        if isinstance(score, (int, float)) and 0 <= score <= 1:
                            detailed_results[key] = {
                                'score': float(score),
                                'details': value.get('details', 'processed'),
                                'evidence': value.get('evidence', {})
                            }
            
            processed_data['detailed_results'] = detailed_results
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = np.mean(field_scores) if field_scores else 0.0
            
            # ä¸€è‡´æ€§æ£€éªŒ
            if 'combined_score' in processed_data and 'max_score' in processed_data:
                consistency = 1.0 - abs(processed_data['combined_score'] - processed_data['max_score'] * 0.7)
                quality_score = (quality_score + max(0, consistency)) / 2.0
            
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹ç»“æœéªŒè¯å¤±è´¥: {e}")
            quality_score = 0.1
            processed_data = {
                'combined_score': 0.5,
                'max_score': 0.5,
                'detection_consensus': 0.5,
                'detailed_results': {}
            }
        
        return {
            'quality_score': quality_score,
            'processed_data': processed_data
        }

    def _validate_statistical_anomalies(self, statistical_anomalies: Dict) -> Dict[str, Any]:
        """éªŒè¯ç»Ÿè®¡å¼‚å¸¸æ•°æ®è´¨é‡"""
        quality_score = 0.0
        processed_data = {}
        
        try:
            # ç»Ÿè®¡å¼‚å¸¸å¯èƒ½çš„å­—æ®µ
            possible_fields = ['chi_square_test', 'ks_test', 'entropy_test', 'outlier_score', 'anomaly_score']
            valid_scores = []
            
            for field in possible_fields:
                if field in statistical_anomalies:
                    value = statistical_anomalies[field]
                    if isinstance(value, (int, float)) and 0 <= value <= 1:
                        processed_data[field] = float(value)
                        valid_scores.append(value)
                    elif isinstance(value, dict) and 'score' in value:
                        score = value['score']
                        if isinstance(score, (int, float)) and 0 <= score <= 1:
                            processed_data[field] = float(score)
                            valid_scores.append(score)
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå°è¯•ä»å…¶ä»–å¯èƒ½çš„ç»“æ„ä¸­æå–
            if not valid_scores and isinstance(statistical_anomalies, dict):
                for key, value in statistical_anomalies.items():
                    if isinstance(value, (int, float)) and 0 <= value <= 1:
                        processed_data[key] = float(value)
                        valid_scores.append(value)
            
            # è®¡ç®—ç»¼åˆå¼‚å¸¸åˆ†æ•°
            if valid_scores:
                processed_data['combined_anomaly_score'] = np.mean(valid_scores)
                processed_data['max_anomaly_score'] = max(valid_scores)
                quality_score = 1.0
            else:
                processed_data['combined_anomaly_score'] = 0.3
                processed_data['max_anomaly_score'] = 0.3
                quality_score = 0.2
            
        except Exception as e:
            print(f"âš ï¸ ç»Ÿè®¡å¼‚å¸¸éªŒè¯å¤±è´¥: {e}")
            quality_score = 0.1
            processed_data = {
                'combined_anomaly_score': 0.3,
                'max_anomaly_score': 0.3
            }
        
        return {
            'quality_score': quality_score,
            'processed_data': processed_data
        }

    def _validate_pattern_matches(self, pattern_matches: Dict) -> Dict[str, Any]:
        """éªŒè¯æ¨¡å¼åŒ¹é…æ•°æ®è´¨é‡"""
        quality_score = 0.0
        processed_data = {}
        
        try:
            # æå–æ¨¡å¼åŒ¹é…ä¿¡æ¯
            matched_patterns = pattern_matches.get('matched_patterns', [])
            similarity_scores = pattern_matches.get('similarity_scores', [])
            
            if similarity_scores and all(isinstance(s, (int, float)) for s in similarity_scores):
                # æœ‰æ•ˆçš„ç›¸ä¼¼åº¦åˆ†æ•°
                valid_scores = [s for s in similarity_scores if 0 <= s <= 1]
                if valid_scores:
                    processed_data['average_similarity'] = np.mean(valid_scores)
                    processed_data['max_similarity'] = max(valid_scores)
                    processed_data['pattern_count'] = len(matched_patterns)
                    quality_score = 1.0
                else:
                    processed_data['average_similarity'] = 0.3
                    processed_data['max_similarity'] = 0.3
                    processed_data['pattern_count'] = 0
                    quality_score = 0.3
            elif matched_patterns:
                # åªæœ‰æ¨¡å¼åˆ—è¡¨ï¼Œæ²¡æœ‰åˆ†æ•°
                processed_data['pattern_count'] = len(matched_patterns)
                processed_data['average_similarity'] = min(1.0, len(matched_patterns) / 5.0)
                processed_data['max_similarity'] = min(1.0, len(matched_patterns) / 3.0)
                quality_score = 0.7
            else:
                # æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å¼åŒ¹é…æ•°æ®
                processed_data['pattern_count'] = 0
                processed_data['average_similarity'] = 0.2
                processed_data['max_similarity'] = 0.2
                quality_score = 0.2
            
            # æ¨¡å¼ç±»å‹åˆ†æ
            if isinstance(pattern_matches, dict):
                pattern_types = [k for k, v in pattern_matches.items() if k not in ['matched_patterns', 'similarity_scores']]
                processed_data['pattern_types'] = pattern_types
                processed_data['pattern_diversity'] = len(pattern_types) / 10.0  # å‡è®¾æœ€å¤š10ç§ç±»å‹
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å¼åŒ¹é…éªŒè¯å¤±è´¥: {e}")
            quality_score = 0.1
            processed_data = {
                'pattern_count': 0,
                'average_similarity': 0.2,
                'max_similarity': 0.2,
                'pattern_diversity': 0.0
            }
        
        return {
            'quality_score': quality_score,
            'processed_data': processed_data
        }

    def _multi_dimensional_evidence_fusion(self, detection_results: Dict, statistical_anomalies: Dict, pattern_matches: Dict) -> Dict[str, Any]:
        """
        å¤šç»´åº¦è¯æ®èåˆç®—æ³•
        åŸºäºDempster-Shaferç†è®ºå’Œä¿¡æ¯è®ºçš„è¯æ®èåˆ
        """
        fusion_result = {
            'primary_intensity_score': 0.0,
            'confidence_score': 0.0,
            'evidence_consistency': 0.0,
            'fusion_quality': 0.0,
            'component_contributions': {},
            'uncertainty_measure': 0.0
        }
        
        try:
            # === è·å–å½“å‰è‡ªé€‚åº”æƒé‡ ===
            current_weights = self._get_current_evidence_weights()
            
            # === è¯æ®å¼ºåº¦æå– ===
            detection_strength = detection_results.get('combined_score', 0.5)
            statistical_strength = statistical_anomalies.get('combined_anomaly_score', 0.3)
            pattern_strength = pattern_matches.get('average_similarity', 0.2)
            
            # === è¯æ®è´¨é‡è¯„ä¼° ===
            evidence_qualities = {
                'detection': self._assess_detection_evidence_quality(detection_results),
                'statistical': self._assess_statistical_evidence_quality(statistical_anomalies),
                'pattern': self._assess_pattern_evidence_quality(pattern_matches)
            }
            
            # === D-Sç†è®ºè¯æ®èåˆ ===
            mass_functions = self._create_mass_functions(
                detection_strength, statistical_strength, pattern_strength,
                evidence_qualities
            )
            
            fused_mass = self._dempster_shafer_fusion(mass_functions)
            
            # === åŠ æƒå¹³å‡èåˆï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰ ===
            weighted_average = (
                detection_strength * current_weights['detection_results'] +
                statistical_strength * current_weights['statistical_anomalies'] +
                pattern_strength * current_weights['pattern_matches']
            )
            
            # === ä¿¡æ¯è®ºä¸€è‡´æ€§æ£€éªŒ ===
            consistency_score = self._calculate_evidence_consistency(
                [detection_strength, statistical_strength, pattern_strength]
            )
            
            # === ç»¼åˆå¼ºåº¦è®¡ç®— ===
            # æ ¹æ®ä¸€è‡´æ€§é€‰æ‹©èåˆæ–¹æ³•
            if consistency_score > 0.7:
                # é«˜ä¸€è‡´æ€§ï¼šä½¿ç”¨D-Sèåˆ
                primary_intensity = fused_mass.get('manipulation', weighted_average)
                fusion_quality = 0.9
            elif consistency_score > 0.4:
                # ä¸­ç­‰ä¸€è‡´æ€§ï¼šD-Sä¸åŠ æƒå¹³å‡çš„æ··åˆ
                ds_weight = consistency_score
                primary_intensity = (
                    fused_mass.get('manipulation', 0.5) * ds_weight +
                    weighted_average * (1 - ds_weight)
                )
                fusion_quality = 0.7
            else:
                # ä½ä¸€è‡´æ€§ï¼šä¿å®ˆçš„åŠ æƒå¹³å‡
                primary_intensity = weighted_average * 0.8  # é™ä½ç½®ä¿¡åº¦
                fusion_quality = 0.5
            
            # === å¼‚å¸¸å€¼æ£€æµ‹ä¸ä¿®æ­£ ===
            outlier_detection = self._detect_evidence_outliers(
                [detection_strength, statistical_strength, pattern_strength]
            )
            
            if outlier_detection['has_outliers']:
                # é™ä½åŒ…å«å¼‚å¸¸å€¼çš„è¯æ®å½±å“
                primary_intensity *= (1 - outlier_detection['outlier_penalty'])
                fusion_quality *= 0.8
            
            # === ä¸ç¡®å®šæ€§é‡åŒ– ===
            uncertainty = self._calculate_fusion_uncertainty(
                mass_functions, consistency_score, evidence_qualities
            )
            
            # æ›´æ–°èåˆç»“æœ
            fusion_result.update({
                'primary_intensity_score': max(0.0, min(1.0, primary_intensity)),
                'confidence_score': consistency_score,
                'evidence_consistency': consistency_score,
                'fusion_quality': fusion_quality,
                'component_contributions': {
                    'detection_contribution': detection_strength * current_weights['detection_results'],
                    'statistical_contribution': statistical_strength * current_weights['statistical_anomalies'],
                    'pattern_contribution': pattern_strength * current_weights['pattern_matches']
                },
                'uncertainty_measure': uncertainty,
                'outlier_info': outlier_detection,
                'mass_functions': mass_functions,
                'weights_used': current_weights
            })
            
        except Exception as e:
            print(f"âš ï¸ è¯æ®èåˆå¤±è´¥: {e}")
            # å®‰å…¨çš„é»˜è®¤èåˆ
            fusion_result = {
                'primary_intensity_score': 0.5,
                'confidence_score': 0.3,
                'evidence_consistency': 0.3,
                'fusion_quality': 0.3,
                'component_contributions': {},
                'uncertainty_measure': 0.7
            }
        
        return fusion_result

    def _get_current_evidence_weights(self) -> Dict[str, float]:
        """è·å–å½“å‰è‡ªé€‚åº”æƒé‡"""
        current_weights = {}
        total_weight = 0.0
        
        for evidence_type, weight_info in self.evidence_weights.items():
            adaptive_weight = (
                weight_info['base_weight'] * 
                weight_info['adaptive_factor'] * 
                weight_info['reliability_score']
            )
            current_weights[evidence_type] = adaptive_weight
            total_weight += adaptive_weight
        
        # å½’ä¸€åŒ–æƒé‡
        if total_weight > 0:
            for evidence_type in current_weights:
                current_weights[evidence_type] /= total_weight
        
        return current_weights

    def _assess_detection_evidence_quality(self, detection_results: Dict) -> float:
        """è¯„ä¼°æ£€æµ‹è¯æ®è´¨é‡"""
        quality_factors = []
        
        # æ£€æµ‹ä¸€è‡´æ€§
        consensus = detection_results.get('detection_consensus', 0.5)
        quality_factors.append(consensus)
        
        # åˆ†æ•°åˆç†æ€§
        combined_score = detection_results.get('combined_score', 0.5)
        max_score = detection_results.get('max_score', 0.5)
        if max_score > 0:
            score_consistency = 1.0 - abs(combined_score - max_score * 0.7) / max_score
            quality_factors.append(max(0.0, score_consistency))
        
        # è¯¦ç»†ç»“æœä¸°å¯Œåº¦
        detailed_results = detection_results.get('detailed_results', {})
        detail_richness = min(1.0, len(detailed_results) / 8.0)
        quality_factors.append(detail_richness)
        
        return np.mean(quality_factors) if quality_factors else 0.5

    def _assess_statistical_evidence_quality(self, statistical_anomalies: Dict) -> float:
        """è¯„ä¼°ç»Ÿè®¡è¯æ®è´¨é‡"""
        quality_factors = []
        
        # å¼‚å¸¸åˆ†æ•°åˆç†æ€§
        combined_score = statistical_anomalies.get('combined_anomaly_score', 0.3)
        max_score = statistical_anomalies.get('max_anomaly_score', 0.3)
        
        if 0 <= combined_score <= 1 and 0 <= max_score <= 1:
            quality_factors.append(1.0)
        else:
            quality_factors.append(0.5)
        
        # åˆ†æ•°åˆ†å¸ƒåˆç†æ€§
        if max_score > 0 and combined_score <= max_score:
            score_relationship = combined_score / max_score
            if 0.3 <= score_relationship <= 1.0:
                quality_factors.append(1.0)
            else:
                quality_factors.append(0.6)
        
        return np.mean(quality_factors) if quality_factors else 0.5

    def _assess_pattern_evidence_quality(self, pattern_matches: Dict) -> float:
        """è¯„ä¼°æ¨¡å¼è¯æ®è´¨é‡"""
        quality_factors = []
        
        # æ¨¡å¼æ•°é‡åˆç†æ€§
        pattern_count = pattern_matches.get('pattern_count', 0)
        if 0 <= pattern_count <= 10:  # åˆç†çš„æ¨¡å¼æ•°é‡èŒƒå›´
            quality_factors.append(1.0)
        else:
            quality_factors.append(0.6)
        
        # ç›¸ä¼¼åº¦åˆ†æ•°åˆç†æ€§
        avg_similarity = pattern_matches.get('average_similarity', 0.2)
        max_similarity = pattern_matches.get('max_similarity', 0.2)
        
        if 0 <= avg_similarity <= 1 and 0 <= max_similarity <= 1:
            quality_factors.append(1.0)
        else:
            quality_factors.append(0.5)
        
        # å¤šæ ·æ€§è¯„ä¼°
        diversity = pattern_matches.get('pattern_diversity', 0.0)
        if 0 <= diversity <= 1:
            quality_factors.append(0.8 + diversity * 0.2)  # å¤šæ ·æ€§è¶Šé«˜è´¨é‡è¶Šå¥½
        
        return np.mean(quality_factors) if quality_factors else 0.5

    def _create_mass_functions(self, detection_strength: float, statistical_strength: float, 
                              pattern_strength: float, evidence_qualities: Dict) -> List[Dict]:
        """åˆ›å»ºDempster-Shaferè´¨é‡å‡½æ•°"""
        mass_functions = []
        
        # æ£€æµ‹è¯æ®è´¨é‡å‡½æ•°
        detection_quality = evidence_qualities['detection']
        detection_mass = {
            'manipulation': detection_strength * detection_quality,
            'natural': (1 - detection_strength) * detection_quality,
            'unknown': 1 - detection_quality
        }
        mass_functions.append(detection_mass)
        
        # ç»Ÿè®¡è¯æ®è´¨é‡å‡½æ•°
        statistical_quality = evidence_qualities['statistical']
        statistical_mass = {
            'manipulation': statistical_strength * statistical_quality,
            'natural': (1 - statistical_strength) * statistical_quality,
            'unknown': 1 - statistical_quality
        }
        mass_functions.append(statistical_mass)
        
        # æ¨¡å¼è¯æ®è´¨é‡å‡½æ•°
        pattern_quality = evidence_qualities['pattern']
        pattern_mass = {
            'manipulation': pattern_strength * pattern_quality,
            'natural': (1 - pattern_strength) * pattern_quality,
            'unknown': 1 - pattern_quality
        }
        mass_functions.append(pattern_mass)
        
        return mass_functions

    def _dempster_shafer_fusion(self, mass_functions: List[Dict]) -> Dict[str, float]:
        """Dempster-Shaferè¯æ®èåˆ"""
        if not mass_functions:
            return {'manipulation': 0.5, 'natural': 0.5, 'unknown': 0.0}
        
        # ä»ç¬¬ä¸€ä¸ªè´¨é‡å‡½æ•°å¼€å§‹
        fused_mass = mass_functions[0].copy()
        
        # é€ä¸ªèåˆåç»­è´¨é‡å‡½æ•°
        for i in range(1, len(mass_functions)):
            fused_mass = self._combine_two_mass_functions(fused_mass, mass_functions[i])
        
        return fused_mass

    def _combine_two_mass_functions(self, mass1: Dict, mass2: Dict) -> Dict[str, float]:
        """èåˆä¸¤ä¸ªè´¨é‡å‡½æ•°"""
        combined = defaultdict(float)
        normalization_factor = 0.0
        
        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„ç»„åˆ
        for prop1, mass1_val in mass1.items():
            for prop2, mass2_val in mass2.items():
                combined_mass = mass1_val * mass2_val
                
                if prop1 == prop2:
                    # ç›¸åŒå‘½é¢˜ï¼šç›´æ¥ç»„åˆ
                    combined[prop1] += combined_mass
                    normalization_factor += combined_mass
                elif (prop1 == 'unknown' and prop2 != 'unknown'):
                    # æœªçŸ¥ä¸å·²çŸ¥ï¼šå–å·²çŸ¥
                    combined[prop2] += combined_mass
                    normalization_factor += combined_mass
                elif (prop2 == 'unknown' and prop1 != 'unknown'):
                    # å·²çŸ¥ä¸æœªçŸ¥ï¼šå–å·²çŸ¥
                    combined[prop1] += combined_mass
                    normalization_factor += combined_mass
                elif prop1 == 'unknown' and prop2 == 'unknown':
                    # éƒ½æ˜¯æœªçŸ¥ï¼šä¿æŒæœªçŸ¥
                    combined['unknown'] += combined_mass
                    normalization_factor += combined_mass
                # çŸ›ç›¾çš„è¯æ®ï¼ˆmanipulation vs naturalï¼‰è¢«å¿½ç•¥
        
        # å½’ä¸€åŒ–
        if normalization_factor > 0:
            for prop in combined:
                combined[prop] /= normalization_factor
        
        return dict(combined)

    def _calculate_evidence_consistency(self, evidence_strengths: List[float]) -> float:
        """è®¡ç®—è¯æ®ä¸€è‡´æ€§"""
        if len(evidence_strengths) < 2:
            return 1.0
        
        # è®¡ç®—æ ‡å‡†å·®
        std_dev = np.std(evidence_strengths)
        
        # ä¸€è‡´æ€§ = 1 - æ ‡å‡†åŒ–æ ‡å‡†å·®
        max_possible_std = 0.5  # æœ€å¤§å¯èƒ½æ ‡å‡†å·®
        consistency = max(0.0, 1.0 - std_dev / max_possible_std)
        
        return consistency

    def _detect_evidence_outliers(self, evidence_strengths: List[float]) -> Dict[str, Any]:
        """æ£€æµ‹è¯æ®å¼‚å¸¸å€¼"""
        if len(evidence_strengths) < 3:
            return {'has_outliers': False, 'outlier_penalty': 0.0, 'outlier_indices': []}
        
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        q1 = np.percentile(evidence_strengths, 25)
        q3 = np.percentile(evidence_strengths, 75)
        iqr = q3 - q1
        
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        outlier_indices = []
        for i, value in enumerate(evidence_strengths):
            if value < lower_bound or value > upper_bound:
                outlier_indices.append(i)
        
        has_outliers = len(outlier_indices) > 0
        outlier_penalty = len(outlier_indices) / len(evidence_strengths) * 0.3  # æœ€å¤§30%æƒ©ç½š
        
        return {
            'has_outliers': has_outliers,
            'outlier_penalty': outlier_penalty,
            'outlier_indices': outlier_indices,
            'iqr_bounds': {'lower': lower_bound, 'upper': upper_bound}
        }

    def _calculate_fusion_uncertainty(self, mass_functions: List[Dict], consistency: float, qualities: Dict) -> float:
        """è®¡ç®—èåˆä¸ç¡®å®šæ€§"""
        uncertainty_factors = []
        
        # åŸºäºä¸€è‡´æ€§çš„ä¸ç¡®å®šæ€§
        consistency_uncertainty = 1.0 - consistency
        uncertainty_factors.append(consistency_uncertainty)
        
        # åŸºäºè¯æ®è´¨é‡çš„ä¸ç¡®å®šæ€§
        avg_quality = np.mean(list(qualities.values()))
        quality_uncertainty = 1.0 - avg_quality
        uncertainty_factors.append(quality_uncertainty)
        
        # åŸºäºè´¨é‡å‡½æ•°çš„ä¸ç¡®å®šæ€§
        if mass_functions:
            avg_unknown_mass = np.mean([mf.get('unknown', 0.0) for mf in mass_functions])
            uncertainty_factors.append(avg_unknown_mass)
        
        return np.mean(uncertainty_factors)

    def _calculate_bayesian_posterior(self, fusion_result: Dict, validated_evidence: Dict) -> Dict[str, float]:
        """
        è´å¶æ–¯åéªŒæ¦‚ç‡è®¡ç®—
        åŸºäºå†å²æ•°æ®å’Œå½“å‰è¯æ®æ›´æ–°æ¦‚ç‡åˆ†å¸ƒ
        """
        try:
            primary_intensity = fusion_result['primary_intensity_score']
            fusion_quality = fusion_result['fusion_quality']
            
            # è®¡ç®—ä¼¼ç„¶å‡½æ•°
            likelihoods = self._calculate_intensity_likelihoods(primary_intensity, fusion_quality)
            
            # è´å¶æ–¯æ›´æ–°
            posteriors = {}
            total_posterior = 0.0
            
            for intensity_level in ManipulationIntensity:
                prior = self.bayesian_priors[intensity_level]
                likelihood = likelihoods[intensity_level]
                posterior = prior * likelihood
                posteriors[intensity_level] = posterior
                total_posterior += posterior
            
            # å½’ä¸€åŒ–
            if total_posterior > 0:
                for intensity_level in posteriors:
                    posteriors[intensity_level] /= total_posterior
            
            # æ›´æ–°å…ˆéªŒåˆ†å¸ƒï¼ˆåœ¨çº¿å­¦ä¹ ï¼‰
            self._update_bayesian_priors(posteriors)
            
            return posteriors
            
        except Exception as e:
            print(f"âš ï¸ è´å¶æ–¯è®¡ç®—å¤±è´¥: {e}")
            # è¿”å›å‡åŒ€åˆ†å¸ƒ
            uniform_prob = 1.0 / len(ManipulationIntensity)
            return {intensity: uniform_prob for intensity in ManipulationIntensity}

    def _calculate_intensity_likelihoods(self, primary_intensity: float, fusion_quality: float) -> Dict[ManipulationIntensity, float]:
        """è®¡ç®—å„å¼ºåº¦ç­‰çº§çš„ä¼¼ç„¶å‡½æ•°"""
        likelihoods = {}
        
        # å®šä¹‰å„å¼ºåº¦ç­‰çº§çš„ç‰¹å¾åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
        intensity_distributions = {
            ManipulationIntensity.NATURAL: {'mean': 0.1, 'std': 0.15},
            ManipulationIntensity.SUBTLE: {'mean': 0.3, 'std': 0.1},
            ManipulationIntensity.MODERATE: {'mean': 0.5, 'std': 0.1},
            ManipulationIntensity.STRONG: {'mean': 0.7, 'std': 0.1},
            ManipulationIntensity.EXTREME: {'mean': 0.9, 'std': 0.1}
        }
        
        for intensity_level, dist_params in intensity_distributions.items():
            # è®¡ç®—é«˜æ–¯ä¼¼ç„¶
            mean = dist_params['mean']
            std = dist_params['std'] * (2.0 - fusion_quality)  # è´¨é‡ä½æ—¶å¢åŠ ä¸ç¡®å®šæ€§
            
            likelihood = stats.norm.pdf(primary_intensity, mean, std)
            likelihoods[intensity_level] = likelihood
        
        return likelihoods

    def _update_bayesian_priors(self, posteriors: Dict[ManipulationIntensity, float]):
        """æ›´æ–°è´å¶æ–¯å…ˆéªŒåˆ†å¸ƒ"""
        learning_rate = self.config['evidence_weight_learning_rate']
        
        for intensity_level, posterior in posteriors.items():
            current_prior = self.bayesian_priors[intensity_level]
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
            new_prior = current_prior * (1 - learning_rate) + posterior * learning_rate
            self.bayesian_priors[intensity_level] = new_prior

    def _temporal_intensity_analysis(self, fusion_result: Dict, bayesian_posterior: Dict) -> Dict[str, Any]:
        """
        æ—¶é—´åºåˆ—æ“æ§å¼ºåº¦åˆ†æ
        è€ƒè™‘å†å²æ“æ§å¼ºåº¦çš„æ—¶é—´ä¾èµ–æ€§å’Œè¶‹åŠ¿
        """
        temporal_result = {
            'current_intensity': 0.0,
            'trend_component': 0.0,
            'seasonal_component': 0.0,
            'volatility_component': 0.0,
            'temporal_confidence': 0.0
        }
        
        try:
            current_intensity = fusion_result['primary_intensity_score']
            
            if len(self.intensity_history) < 3:
                # å†å²æ•°æ®ä¸è¶³ï¼Œç›´æ¥ä½¿ç”¨å½“å‰å¼ºåº¦
                temporal_result['current_intensity'] = current_intensity
                temporal_result['temporal_confidence'] = 0.5
                return temporal_result
            
            # === è¶‹åŠ¿åˆ†æ ===
            recent_intensities = [record['intensity_score'] for record in list(self.intensity_history)[-10:]]
            trend_component = self._calculate_intensity_trend(recent_intensities, current_intensity)
            
            # === å­£èŠ‚æ€§åˆ†æ ===
            seasonal_component = self._calculate_seasonal_component(current_intensity)
            
            # === æ³¢åŠ¨æ€§åˆ†æ ===
            volatility_component = self._calculate_volatility_component(recent_intensities)
            
            # === æ—¶é—´æƒé‡èåˆ ===
            temporal_weights = self._calculate_temporal_weights(len(recent_intensities))
            
            # èåˆå½“å‰å¼ºåº¦å’Œå†å²è¶‹åŠ¿
            trend_adjusted_intensity = (
                current_intensity * temporal_weights['current'] +
                trend_component * temporal_weights['trend'] +
                seasonal_component * temporal_weights['seasonal']
            )
            
            # æ³¢åŠ¨æ€§è°ƒæ•´
            volatility_adjustment = 1.0 - volatility_component * 0.3
            final_temporal_intensity = trend_adjusted_intensity * volatility_adjustment
            
            # === æ—¶é—´ç½®ä¿¡åº¦è®¡ç®— ===
            temporal_confidence = self._calculate_temporal_confidence(
                recent_intensities, trend_component, seasonal_component, volatility_component
            )
            
            temporal_result.update({
                'current_intensity': max(0.0, min(1.0, final_temporal_intensity)),
                'trend_component': trend_component,
                'seasonal_component': seasonal_component,
                'volatility_component': volatility_component,
                'temporal_confidence': temporal_confidence,
                'temporal_weights': temporal_weights,
                'adjustment_factors': {
                    'volatility_adjustment': volatility_adjustment,
                    'trend_influence': temporal_weights['trend'],
                    'seasonal_influence': temporal_weights['seasonal']
                }
            })
            
        except Exception as e:
            print(f"âš ï¸ æ—¶é—´åºåˆ—åˆ†æå¤±è´¥: {e}")
            temporal_result['current_intensity'] = fusion_result['primary_intensity_score']
            temporal_result['temporal_confidence'] = 0.3
        
        return temporal_result

    def _calculate_intensity_trend(self, recent_intensities: List[float], current_intensity: float) -> float:
        """è®¡ç®—å¼ºåº¦è¶‹åŠ¿"""
        if len(recent_intensities) < 3:
            return current_intensity
        
        # ä½¿ç”¨çº¿æ€§å›å½’è®¡ç®—è¶‹åŠ¿
        x = np.arange(len(recent_intensities))
        y = np.array(recent_intensities)
        
        try:
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
            next_x = len(recent_intensities)
            predicted_intensity = slope * next_x + intercept
            
            # è¶‹åŠ¿å¼ºåº¦åŸºäºRÂ²å€¼
            trend_strength = r_value ** 2 if not np.isnan(r_value) else 0.0
            
            # å¦‚æœè¶‹åŠ¿æ˜¾è‘—ï¼Œè¿”å›é¢„æµ‹å€¼ï¼›å¦åˆ™è¿”å›å½“å‰å€¼
            if trend_strength > 0.3 and abs(slope) > 0.01:
                return max(0.0, min(1.0, predicted_intensity))
            else:
                return current_intensity
                
        except Exception:
            return current_intensity

    def _calculate_seasonal_component(self, current_intensity: float) -> float:
        """è®¡ç®—å­£èŠ‚æ€§ç»„ä»¶"""
        # ç®€åŒ–çš„å­£èŠ‚æ€§åˆ†æï¼ˆåŸºäºå†å²åŒæœŸæ•°æ®ï¼‰
        if len(self.intensity_history) < 7:
            return current_intensity
        
        # è®¡ç®—7æœŸå‘¨æœŸçš„å­£èŠ‚æ€§
        current_position = len(self.intensity_history) % 7
        seasonal_intensities = []
        
        for i, record in enumerate(self.intensity_history):
            if i % 7 == current_position:
                seasonal_intensities.append(record['intensity_score'])
        
        if len(seasonal_intensities) >= 2:
            seasonal_mean = np.mean(seasonal_intensities)
            return seasonal_mean
        else:
            return current_intensity

    def _calculate_volatility_component(self, recent_intensities: List[float]) -> float:
        """è®¡ç®—æ³¢åŠ¨æ€§ç»„ä»¶"""
        if len(recent_intensities) < 3:
            return 0.3  # é»˜è®¤ä¸­ç­‰æ³¢åŠ¨æ€§
        
        # è®¡ç®—æ ‡å‡†å·®ä½œä¸ºæ³¢åŠ¨æ€§åº¦é‡
        volatility = np.std(recent_intensities)
        
        # å½’ä¸€åŒ–åˆ°[0,1]åŒºé—´
        max_possible_volatility = 0.5  # ç†è®ºæœ€å¤§æ ‡å‡†å·®
        normalized_volatility = min(1.0, volatility / max_possible_volatility)
        
        return normalized_volatility

    def _calculate_temporal_weights(self, history_length: int) -> Dict[str, float]:
        """è®¡ç®—æ—¶é—´æƒé‡"""
        # åŸºäºå†å²æ•°æ®é•¿åº¦åŠ¨æ€è°ƒæ•´æƒé‡
        if history_length < 5:
            return {'current': 0.8, 'trend': 0.1, 'seasonal': 0.1}
        elif history_length < 15:
            return {'current': 0.6, 'trend': 0.25, 'seasonal': 0.15}
        else:
            return {'current': 0.5, 'trend': 0.3, 'seasonal': 0.2}

    def _calculate_temporal_confidence(self, recent_intensities: List[float], trend: float, seasonal: float, volatility: float) -> float:
        """è®¡ç®—æ—¶é—´åºåˆ—ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # å†å²æ•°æ®å……è¶³æ€§
        data_sufficiency = min(1.0, len(recent_intensities) / 20.0)
        confidence_factors.append(data_sufficiency)
        
        # ä½æ³¢åŠ¨æ€§æé«˜ç½®ä¿¡åº¦
        volatility_confidence = 1.0 - volatility
        confidence_factors.append(volatility_confidence)
        
        # è¶‹åŠ¿ä¸€è‡´æ€§
        if len(recent_intensities) >= 3:
            trend_consistency = 1.0 - abs(recent_intensities[-1] - trend) / 1.0
            confidence_factors.append(max(0.0, trend_consistency))
        
        return np.mean(confidence_factors)

    def _apply_odds_sensitivity_correction(self, temporal_analysis: Dict, validated_evidence: Dict) -> Dict[str, Any]:
        """
        åº”ç”¨èµ”ç‡æ•æ„Ÿæ€§ä¿®æ­£
        æ ¹æ®ä¸åŒå°¾æ•°çš„èµ”ç‡å·®å¼‚è°ƒæ•´æ“æ§å¼ºåº¦
        """
        odds_corrected = {
            'base_intensity': 0.0,
            'odds_adjustment': 0.0,
            'final_intensity': 0.0,
            'tail_specific_analysis': {},
            'odds_impact_score': 0.0
        }
        
        try:
            base_intensity = temporal_analysis['current_intensity']
            
            # === æå–ç›®æ ‡å°¾æ•°ä¿¡æ¯ ===
            target_tails = self._extract_target_tails(validated_evidence)
            
            if not target_tails:
                # æ²¡æœ‰æ˜ç¡®ç›®æ ‡å°¾æ•°ï¼Œä½¿ç”¨åŸºç¡€å¼ºåº¦
                odds_corrected.update({
                    'base_intensity': base_intensity,
                    'odds_adjustment': 0.0,
                    'final_intensity': base_intensity,
                    'odds_impact_score': 0.0
                })
                return odds_corrected
            
            # === è®¡ç®—èµ”ç‡æ•æ„Ÿæ€§å½±å“ ===
            odds_impacts = []
            tail_analyses = {}
            
            for tail in target_tails:
                odds_multiplier = self.odds_sensitivity_matrix.get(tail, 1.8)
                
                # èµ”ç‡å½±å“è®¡ç®—
                # 0å°¾ï¼ˆ2å€èµ”ç‡ï¼‰æ“æ§å½±å“æ›´å¤§ï¼Œå› ä¸ºåº„å®¶æŸå¤±æ›´å¤š
                if tail == 0:
                    odds_impact = (odds_multiplier - 1.8) / 0.2 * 0.3  # æœ€å¤§30%å¢å¼º
                else:
                    odds_impact = 0.0  # 1-9å°¾èµ”ç‡ç›¸åŒï¼Œæ— å·®å¼‚å½±å“
                
                # æ“æ§åŠ¨æœºåˆ†æ
                manipulation_incentive = self._calculate_manipulation_incentive(tail, odds_multiplier, base_intensity)
                
                tail_analyses[tail] = {
                    'odds_multiplier': odds_multiplier,
                    'odds_impact': odds_impact,
                    'manipulation_incentive': manipulation_incentive
                }
                
                odds_impacts.append(odds_impact)
            
            # === ç»¼åˆèµ”ç‡è°ƒæ•´ ===
            if odds_impacts:
                avg_odds_impact = np.mean(odds_impacts)
                max_odds_impact = max(odds_impacts)
                
                # èµ”ç‡è°ƒæ•´çš„å¼ºåº¦å–å†³äºï¼š
                # 1. å¹³å‡èµ”ç‡å½±å“
                # 2. æœ€å¤§èµ”ç‡å½±å“
                # 3. åŸºç¡€æ“æ§å¼ºåº¦
                odds_adjustment = (
                    avg_odds_impact * 0.6 +
                    max_odds_impact * 0.4
                ) * base_intensity  # åªæœ‰åœ¨æœ‰æ“æ§å€¾å‘æ—¶æ‰åŠ å¼º
                
                final_intensity = base_intensity + odds_adjustment
                final_intensity = max(0.0, min(1.0, final_intensity))
                
                # è®¡ç®—èµ”ç‡å½±å“è¯„åˆ†
                odds_impact_score = avg_odds_impact / 0.3 if avg_odds_impact > 0 else 0.0
                
            else:
                odds_adjustment = 0.0
                final_intensity = base_intensity
                odds_impact_score = 0.0
            
            odds_corrected.update({
                'base_intensity': base_intensity,
                'odds_adjustment': odds_adjustment,
                'final_intensity': final_intensity,
                'tail_specific_analysis': tail_analyses,
                'odds_impact_score': odds_impact_score,
                'target_tails': target_tails,
                'correction_details': {
                    'avg_odds_impact': np.mean(odds_impacts) if odds_impacts else 0.0,
                    'max_odds_impact': max(odds_impacts) if odds_impacts else 0.0,
                    'affected_tail_count': len([t for t in target_tails if t == 0])  # åªæœ‰0å°¾æœ‰ç‰¹æ®Šå½±å“
                }
            })
            
        except Exception as e:
            print(f"âš ï¸ èµ”ç‡æ•æ„Ÿæ€§ä¿®æ­£å¤±è´¥: {e}")
            base_intensity = temporal_analysis.get('current_intensity', 0.5)
            odds_corrected.update({
                'base_intensity': base_intensity,
                'odds_adjustment': 0.0,
                'final_intensity': base_intensity,
                'odds_impact_score': 0.0
            })
        
        return odds_corrected

    def _extract_target_tails(self, validated_evidence: Dict) -> List[int]:
        """ä»éªŒè¯è¯æ®ä¸­æå–ç›®æ ‡å°¾æ•°"""
        target_tails = []
        
        try:
            # ä»æ£€æµ‹ç»“æœä¸­æå–
            detection_results = validated_evidence.get('detection_results', {})
            detailed_results = detection_results.get('detailed_results', {})
            
            for detection_type, result in detailed_results.items():
                if isinstance(result, dict) and 'evidence' in result:
                    evidence = result['evidence']
                    
                    # æå–ç›¸å…³å°¾æ•°ä¿¡æ¯
                    if 'current_tails' in evidence:
                        current_tails = evidence['current_tails']
                        if isinstance(current_tails, list):
                            target_tails.extend(current_tails)
                    
                    if 'target_tails' in evidence:
                        evidence_target_tails = evidence['target_tails']
                        if isinstance(evidence_target_tails, list):
                            target_tails.extend(evidence_target_tails)
            
            # å»é‡å¹¶éªŒè¯
            target_tails = list(set([t for t in target_tails if isinstance(t, int) and 0 <= t <= 9]))
            
        except Exception as e:
            print(f"âš ï¸ ç›®æ ‡å°¾æ•°æå–å¤±è´¥: {e}")
            target_tails = []
        
        return target_tails

    def _calculate_manipulation_incentive(self, tail: int, odds_multiplier: float, base_intensity: float) -> float:
        """è®¡ç®—æ“æ§åŠ¨æœºå¼ºåº¦"""
        # æ“æ§åŠ¨æœºå› ç´ ï¼š
        # 1. èµ”ç‡å·®å¼‚ï¼ˆ0å°¾vså…¶ä»–å°¾æ•°ï¼‰
        # 2. åŸºç¡€æ“æ§å¼ºåº¦
        # 3. å†å²æ“æ§è¯¥å°¾æ•°çš„æˆåŠŸç‡ï¼ˆç®€åŒ–ï¼‰
        
        # èµ”ç‡åŠ¨æœº
        if tail == 0:
            odds_incentive = (odds_multiplier - 1.8) / 0.2  # 0å°¾ç‰¹æ®Šå¤„ç†
        else:
            odds_incentive = 0.0  # 1-9å°¾èµ”ç‡ç›¸åŒ
        
        # åŸºç¡€åŠ¨æœºï¼ˆæ“æ§å¼ºåº¦è¶Šé«˜ï¼ŒåŠ¨æœºè¶Šå¼ºï¼‰
        base_incentive = base_intensity
        
        # ç»¼åˆåŠ¨æœº
        total_incentive = (odds_incentive * 0.4 + base_incentive * 0.6)
        
        return max(0.0, min(1.0, total_incentive))

    def _dynamic_threshold_classification(self, odds_corrected_intensity: Dict) -> Dict[str, Any]:
        """
        åŠ¨æ€é˜ˆå€¼åˆ†ç±»ç®—æ³•
        æ ¹æ®å†å²æ•°æ®å’Œå½“å‰ä¸Šä¸‹æ–‡è‡ªé€‚åº”è°ƒæ•´åˆ†ç±»é˜ˆå€¼
        """
        classification_result = {
            'intensity_level': ManipulationIntensity.NATURAL,
            'classification_score': 0.0,
            'threshold_used': {},
            'classification_confidence': 0.0,
            'boundary_analysis': {}
        }
        
        try:
            final_intensity = odds_corrected_intensity['final_intensity']
            
            # === è‡ªé€‚åº”é˜ˆå€¼è®¡ç®— ===
            adapted_thresholds = self._calculate_adaptive_thresholds()
            
            # === åˆ†ç±»å†³ç­– ===
            if final_intensity >= adapted_thresholds['extreme_threshold']:
                intensity_level = ManipulationIntensity.EXTREME
                classification_score = final_intensity
            elif final_intensity >= adapted_thresholds['strong_threshold']:
                intensity_level = ManipulationIntensity.STRONG
                classification_score = final_intensity
            elif final_intensity >= adapted_thresholds['moderate_threshold']:
                intensity_level = ManipulationIntensity.MODERATE
                classification_score = final_intensity
            elif final_intensity >= adapted_thresholds['subtle_threshold']:
                intensity_level = ManipulationIntensity.SUBTLE
                classification_score = final_intensity
            else:
                intensity_level = ManipulationIntensity.NATURAL
                classification_score = 1.0 - final_intensity  # è‡ªç„¶ç¨‹åº¦
            
            # === è¾¹ç•Œåˆ†æ ===
            boundary_analysis = self._analyze_classification_boundaries(
                final_intensity, adapted_thresholds, intensity_level
            )
            
            # === åˆ†ç±»ç½®ä¿¡åº¦ ===
            classification_confidence = self._calculate_classification_confidence(
                final_intensity, adapted_thresholds, boundary_analysis
            )
            
            classification_result.update({
                'intensity_level': intensity_level,
                'classification_score': classification_score,
                'threshold_used': adapted_thresholds,
                'classification_confidence': classification_confidence,
                'boundary_analysis': boundary_analysis,
                'intensity_value': final_intensity
            })
            
        except Exception as e:
            print(f"âš ï¸ åŠ¨æ€é˜ˆå€¼åˆ†ç±»å¤±è´¥: {e}")
            classification_result['intensity_level'] = ManipulationIntensity.NATURAL
            classification_result['classification_confidence'] = 0.3
        
        return classification_result

    def _calculate_adaptive_thresholds(self) -> Dict[str, float]:
        """è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼"""
        if len(self.intensity_history) < 10:
            # å†å²æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼
            return self.dynamic_thresholds.copy()
        
        # è®¡ç®—å†å²å¼ºåº¦åˆ†å¸ƒ
        historical_intensities = [record['intensity_score'] for record in self.intensity_history]
        
        # ç»Ÿè®¡åˆ†æ
        mean_intensity = np.mean(historical_intensities)
        std_intensity = np.std(historical_intensities)
        
        # åˆ†ä½æ•°åˆ†æ
        percentiles = np.percentile(historical_intensities, [20, 40, 60, 80])
        
        # è‡ªé€‚åº”è°ƒæ•´
        adaptation_rate = self.config['threshold_adaptation_rate']
        
        adapted_thresholds = {}
        
        # å¾®å¦™æ“æ§é˜ˆå€¼ï¼šåŸºäº20åˆ†ä½æ•°
        base_subtle = self.dynamic_thresholds['subtle_threshold']
        adaptive_subtle = percentiles[0] + std_intensity * 0.5
        adapted_thresholds['subtle_threshold'] = (
            base_subtle * (1 - adaptation_rate) + 
            adaptive_subtle * adaptation_rate
        )
        
        # ä¸­ç­‰æ“æ§é˜ˆå€¼ï¼šåŸºäº40åˆ†ä½æ•°
        base_moderate = self.dynamic_thresholds['moderate_threshold']
        adaptive_moderate = percentiles[1] + std_intensity * 0.7
        adapted_thresholds['moderate_threshold'] = (
            base_moderate * (1 - adaptation_rate) + 
            adaptive_moderate * adaptation_rate
        )
        
        # å¼ºçƒˆæ“æ§é˜ˆå€¼ï¼šåŸºäº60åˆ†ä½æ•°
        base_strong = self.dynamic_thresholds['strong_threshold']
        adaptive_strong = percentiles[2] + std_intensity * 0.9
        adapted_thresholds['strong_threshold'] = (
            base_strong * (1 - adaptation_rate) + 
            adaptive_strong * adaptation_rate
        )
        
        # æç«¯æ“æ§é˜ˆå€¼ï¼šåŸºäº80åˆ†ä½æ•°
        base_extreme = self.dynamic_thresholds['extreme_threshold']
        adaptive_extreme = percentiles[3] + std_intensity * 1.1
        adapted_thresholds['extreme_threshold'] = (
            base_extreme * (1 - adaptation_rate) + 
            adaptive_extreme * adaptation_rate
        )
        
        # ç¡®ä¿é˜ˆå€¼å•è°ƒé€’å¢
        thresholds = [
            adapted_thresholds['subtle_threshold'],
            adapted_thresholds['moderate_threshold'],
            adapted_thresholds['strong_threshold'],
            adapted_thresholds['extreme_threshold']
        ]
        
        # ä¿®æ­£éå•è°ƒæƒ…å†µ
        for i in range(1, len(thresholds)):
            if thresholds[i] <= thresholds[i-1]:
                thresholds[i] = thresholds[i-1] + 0.05
        
        adapted_thresholds['subtle_threshold'] = max(0.1, min(0.4, thresholds[0]))
        adapted_thresholds['moderate_threshold'] = max(0.3, min(0.6, thresholds[1]))
        adapted_thresholds['strong_threshold'] = max(0.5, min(0.8, thresholds[2]))
        adapted_thresholds['extreme_threshold'] = max(0.7, min(0.95, thresholds[3]))
        
        # æ›´æ–°åŠ¨æ€é˜ˆå€¼
        for key, value in adapted_thresholds.items():
            self.dynamic_thresholds[key] = value
        
        return adapted_thresholds

    def _analyze_classification_boundaries(self, intensity_value: float, thresholds: Dict, classified_level: ManipulationIntensity) -> Dict[str, Any]:
        """åˆ†æåˆ†ç±»è¾¹ç•Œæƒ…å†µ"""
        boundary_analysis = {
            'distance_to_boundaries': {},
            'is_near_boundary': False,
            'boundary_uncertainty': 0.0,
            'alternative_classifications': []
        }
        
        # è®¡ç®—åˆ°å„é˜ˆå€¼çš„è·ç¦»
        threshold_values = [
            thresholds['subtle_threshold'],
            thresholds['moderate_threshold'],
            thresholds['strong_threshold'],
            thresholds['extreme_threshold']
        ]
        
        distances = [abs(intensity_value - threshold) for threshold in threshold_values]
        min_distance = min(distances)
        
        boundary_analysis['distance_to_boundaries'] = {
            'subtle': abs(intensity_value - thresholds['subtle_threshold']),
            'moderate': abs(intensity_value - thresholds['moderate_threshold']),
            'strong': abs(intensity_value - thresholds['strong_threshold']),
            'extreme': abs(intensity_value - thresholds['extreme_threshold'])
        }
        
        # åˆ¤æ–­æ˜¯å¦æ¥è¿‘è¾¹ç•Œ
        boundary_threshold = 0.05  # 5%å†…è®¤ä¸ºæ¥è¿‘è¾¹ç•Œ
        boundary_analysis['is_near_boundary'] = min_distance < boundary_threshold
        
        if boundary_analysis['is_near_boundary']:
            boundary_analysis['boundary_uncertainty'] = 1.0 - (min_distance / boundary_threshold)
            
            # ç¡®å®šå¯èƒ½çš„æ›¿ä»£åˆ†ç±»
            close_thresholds = [i for i, d in enumerate(distances) if d < boundary_threshold]
            for threshold_idx in close_thresholds:
                if threshold_idx == 0:
                    boundary_analysis['alternative_classifications'].append(ManipulationIntensity.SUBTLE)
                elif threshold_idx == 1:
                    boundary_analysis['alternative_classifications'].append(ManipulationIntensity.MODERATE)
                elif threshold_idx == 2:
                    boundary_analysis['alternative_classifications'].append(ManipulationIntensity.STRONG)
                elif threshold_idx == 3:
                    boundary_analysis['alternative_classifications'].append(ManipulationIntensity.EXTREME)
        
        return boundary_analysis

    def _calculate_classification_confidence(self, intensity_value: float, thresholds: Dict, boundary_analysis: Dict) -> float:
        """è®¡ç®—åˆ†ç±»ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # è·ç¦»è¾¹ç•Œçš„ç½®ä¿¡åº¦
        if boundary_analysis['is_near_boundary']:
            boundary_confidence = 1.0 - boundary_analysis['boundary_uncertainty']
        else:
            boundary_confidence = 1.0
        confidence_factors.append(boundary_confidence)
        
        # å†å²ä¸€è‡´æ€§ç½®ä¿¡åº¦
        if len(self.intensity_history) >= 5:
            recent_classifications = [record.get('classification', ManipulationIntensity.NATURAL) for record in list(self.intensity_history)[-5:]]
            current_classification = self._get_classification_from_intensity(intensity_value, thresholds)
            
            consistency_count = sum(1 for cls in recent_classifications if cls == current_classification)
            consistency_confidence = consistency_count / len(recent_classifications)
            confidence_factors.append(consistency_confidence)
        
        # é˜ˆå€¼ç¨³å®šæ€§ç½®ä¿¡åº¦
        threshold_stability = 0.8  # ç®€åŒ–ï¼šå‡è®¾é˜ˆå€¼ç›¸å¯¹ç¨³å®š
        confidence_factors.append(threshold_stability)
        
        return np.mean(confidence_factors)

    def _get_classification_from_intensity(self, intensity: float, thresholds: Dict) -> ManipulationIntensity:
        """æ ¹æ®å¼ºåº¦å€¼å’Œé˜ˆå€¼ç¡®å®šåˆ†ç±»"""
        if intensity >= thresholds['extreme_threshold']:
            return ManipulationIntensity.EXTREME
        elif intensity >= thresholds['strong_threshold']:
            return ManipulationIntensity.STRONG
        elif intensity >= thresholds['moderate_threshold']:
            return ManipulationIntensity.MODERATE
        elif intensity >= thresholds['subtle_threshold']:
            return ManipulationIntensity.SUBTLE
        else:
            return ManipulationIntensity.NATURAL

    def _calculate_confidence_and_validate(self, intensity_classification: Dict, fusion_result: Dict, bayesian_posterior: Dict) -> Dict[str, Any]:
        """
        ç½®ä¿¡åº¦è®¡ç®—ä¸ç»“æœéªŒè¯
        å¤šç»´åº¦ç½®ä¿¡åº¦è¯„ä¼°å’Œäº¤å‰éªŒè¯
        """
        final_result = {
            'intensity': intensity_classification['intensity_level'],
            'confidence_score': 0.0,
            'validation_passed': True,
            'confidence_breakdown': {},
            'validation_details': {}
        }
        
        try:
            # === å¤šç»´åº¦ç½®ä¿¡åº¦è®¡ç®— ===
            confidence_components = {}
            
            # 1. åˆ†ç±»ç½®ä¿¡åº¦
            classification_confidence = intensity_classification.get('classification_confidence', 0.5)
            confidence_components['classification'] = classification_confidence
            
            # 2. èåˆè´¨é‡ç½®ä¿¡åº¦
            fusion_confidence = fusion_result.get('fusion_quality', 0.5)
            confidence_components['fusion_quality'] = fusion_confidence
            
            # 3. è¯æ®ä¸€è‡´æ€§ç½®ä¿¡åº¦
            evidence_consistency = fusion_result.get('evidence_consistency', 0.5)
            confidence_components['evidence_consistency'] = evidence_consistency
            
            # 4. è´å¶æ–¯ç½®ä¿¡åº¦
            bayesian_confidence = self._calculate_bayesian_confidence(bayesian_posterior, intensity_classification['intensity_level'])
            confidence_components['bayesian'] = bayesian_confidence
            
            # 5. å†å²ä¸€è‡´æ€§ç½®ä¿¡åº¦
            historical_confidence = self._calculate_historical_consistency_confidence(intensity_classification['intensity_level'])
            confidence_components['historical_consistency'] = historical_confidence
            
            # 6. è¾¹ç•Œç¨³å®šæ€§ç½®ä¿¡åº¦
            boundary_confidence = 1.0 - intensity_classification.get('boundary_analysis', {}).get('boundary_uncertainty', 0.0)
            confidence_components['boundary_stability'] = boundary_confidence
            
            # === åŠ æƒç»¼åˆç½®ä¿¡åº¦ ===
            confidence_weights = {
                'classification': 0.25,
                'fusion_quality': 0.20,
                'evidence_consistency': 0.15,
                'bayesian': 0.15,
                'historical_consistency': 0.15,
                'boundary_stability': 0.10
            }
            
            weighted_confidence = sum(
                confidence_components[comp] * confidence_weights[comp]
                for comp in confidence_components
            )
            
            # === ç½®ä¿¡åº¦éªŒè¯ ===
            validation_details = self._validate_confidence_calculation(
                confidence_components, weighted_confidence, intensity_classification
            )
            
            # === æœ€ç»ˆç½®ä¿¡åº¦è°ƒæ•´ ===
            final_confidence = self._adjust_final_confidence(
                weighted_confidence, validation_details, intensity_classification
            )
            
            final_result.update({
                'intensity': intensity_classification['intensity_level'],
                'confidence_score': final_confidence,
                'validation_passed': validation_details['validation_passed'],
                'confidence_breakdown': confidence_components,
                'validation_details': validation_details,
                'intensity_score': intensity_classification.get('intensity_value', 0.0),
                'classification_details': intensity_classification
            })
            
        except Exception as e:
            print(f"âš ï¸ ç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
            final_result.update({
                'intensity': ManipulationIntensity.NATURAL,
                'confidence_score': 0.3,
                'validation_passed': False
            })
        
        return final_result

    def _calculate_bayesian_confidence(self, bayesian_posterior: Dict, classified_intensity: ManipulationIntensity) -> float:
        """è®¡ç®—è´å¶æ–¯ç½®ä¿¡åº¦"""
        if classified_intensity in bayesian_posterior:
            # è¯¥åˆ†ç±»çš„åéªŒæ¦‚ç‡
            posterior_prob = bayesian_posterior[classified_intensity]
            
            # ä¸å…¶ä»–åˆ†ç±»çš„åŒºåˆ†åº¦
            other_probs = [prob for level, prob in bayesian_posterior.items() if level != classified_intensity]
            max_other_prob = max(other_probs) if other_probs else 0.0
            
            # ç½®ä¿¡åº¦ = å½“å‰åˆ†ç±»æ¦‚ç‡ + åŒºåˆ†åº¦
            confidence = posterior_prob + (posterior_prob - max_other_prob) * 0.5
            return max(0.0, min(1.0, confidence))
        else:
            return 0.5

    def _calculate_historical_consistency_confidence(self, current_intensity: ManipulationIntensity) -> float:
        """è®¡ç®—å†å²ä¸€è‡´æ€§ç½®ä¿¡åº¦"""
        if len(self.intensity_history) < 3:
            return 0.5
        
        # æ£€æŸ¥æœ€è¿‘å‡ æ¬¡åˆ†ç±»çš„ä¸€è‡´æ€§
        recent_classifications = [
            record.get('classification', ManipulationIntensity.NATURAL) 
            for record in list(self.intensity_history)[-5:]
        ]
        
        # è®¡ç®—å½“å‰åˆ†ç±»åœ¨æœ€è¿‘å†å²ä¸­çš„å‡ºç°é¢‘ç‡
        consistency_count = recent_classifications.count(current_intensity)
        base_consistency = consistency_count / len(recent_classifications)
        
        # è€ƒè™‘æ¸è¿›å˜åŒ–çš„åˆç†æ€§
        if len(recent_classifications) >= 2:
            last_classification = recent_classifications[-1]
            intensity_values = [m.value for m in ManipulationIntensity]
            
            current_value = current_intensity.value
            last_value = last_classification.value
            
            # å¦‚æœå˜åŒ–å¹…åº¦åˆç†ï¼ˆä¸è¶…è¿‡2ä¸ªç­‰çº§ï¼‰ï¼Œç»™äºˆé¢å¤–ç½®ä¿¡åº¦
            change_magnitude = abs(current_value - last_value)
            if change_magnitude <= 2:
                transition_bonus = (2 - change_magnitude) / 2 * 0.3
                base_consistency += transition_bonus
        
        return max(0.0, min(1.0, base_consistency))

    def _validate_confidence_calculation(self, confidence_components: Dict, weighted_confidence: float, intensity_classification: Dict) -> Dict[str, Any]:
        """éªŒè¯ç½®ä¿¡åº¦è®¡ç®—"""
        validation_details = {
            'validation_passed': True,
            'validation_issues': [],
            'quality_score': 0.0,
            'reliability_assessment': 'high'
        }
        
        # éªŒè¯1ï¼šç½®ä¿¡åº¦åˆ†é‡åˆç†æ€§
        for component, value in confidence_components.items():
            if not (0.0 <= value <= 1.0):
                validation_details['validation_issues'].append(f'Invalid {component} confidence: {value}')
                validation_details['validation_passed'] = False
        
        # éªŒè¯2ï¼šç»¼åˆç½®ä¿¡åº¦åˆç†æ€§
        if not (0.0 <= weighted_confidence <= 1.0):
            validation_details['validation_issues'].append(f'Invalid weighted confidence: {weighted_confidence}')
            validation_details['validation_passed'] = False
        
        # éªŒè¯3ï¼šç½®ä¿¡åº¦ä¸å¼ºåº¦ä¸€è‡´æ€§
        intensity_value = intensity_classification.get('intensity_value', 0.0)
        if intensity_value > 0.8 and weighted_confidence < 0.3:
            validation_details['validation_issues'].append('High intensity with low confidence is inconsistent')
            validation_details['validation_passed'] = False
        
        # éªŒè¯4ï¼šè¾¹ç•Œæƒ…å†µç‰¹æ®Šå¤„ç†
        is_near_boundary = intensity_classification.get('boundary_analysis', {}).get('is_near_boundary', False)
        if is_near_boundary and weighted_confidence > 0.9:
            validation_details['validation_issues'].append('Near boundary should not have very high confidence')
            validation_details['validation_passed'] = False
        
        # è®¡ç®—è´¨é‡è¯„åˆ†
        if validation_details['validation_passed']:
            # åŸºäºç½®ä¿¡åº¦åˆ†é‡çš„æ–¹å·®è®¡ç®—è´¨é‡
            confidence_values = list(confidence_components.values())
            confidence_variance = np.var(confidence_values)
            quality_score = 1.0 - min(1.0, confidence_variance * 4)  # ä½æ–¹å·®=é«˜è´¨é‡
        else:
            quality_score = 0.3
        
        validation_details['quality_score'] = quality_score
        
        # å¯é æ€§è¯„ä¼°
        if quality_score > 0.8 and weighted_confidence > 0.7:
            validation_details['reliability_assessment'] = 'high'
        elif quality_score > 0.6 and weighted_confidence > 0.5:
            validation_details['reliability_assessment'] = 'medium'
        else:
            validation_details['reliability_assessment'] = 'low'
        
        return validation_details

    def _adjust_final_confidence(self, weighted_confidence: float, validation_details: Dict, intensity_classification: Dict) -> float:
        """æœ€ç»ˆç½®ä¿¡åº¦è°ƒæ•´"""
        adjusted_confidence = weighted_confidence
        
        # éªŒè¯å¤±è´¥æƒ©ç½š
        if not validation_details['validation_passed']:
            adjusted_confidence *= 0.6
        
        # è´¨é‡è¯„åˆ†è°ƒæ•´
        quality_score = validation_details['quality_score']
        adjusted_confidence = adjusted_confidence * (0.5 + quality_score * 0.5)
        
        # è¾¹ç•Œä¸ç¡®å®šæ€§è°ƒæ•´
        boundary_uncertainty = intensity_classification.get('boundary_analysis', {}).get('boundary_uncertainty', 0.0)
        adjusted_confidence *= (1.0 - boundary_uncertainty * 0.3)
        
        # å†å²æ•°æ®å……è¶³æ€§è°ƒæ•´
        data_sufficiency = min(1.0, len(self.intensity_history) / 20.0)
        adjusted_confidence = adjusted_confidence * (0.7 + data_sufficiency * 0.3)
        
        return max(0.1, min(0.95, adjusted_confidence))

    def _update_learning_systems(self, final_result: Dict, validated_evidence: Dict):
        """
        æ›´æ–°å­¦ä¹ ç³»ç»Ÿ
        åŸºäºå½“å‰ç»“æœè°ƒæ•´æ¨¡å‹å‚æ•°å’Œæƒé‡
        """
        try:
            # === è®°å½•å½“å‰ç»“æœåˆ°å†å² ===
            intensity_record = {
                'timestamp': datetime.now(),
                'intensity_score': final_result.get('intensity_score', 0.0),
                'classification': final_result['intensity'],
                'confidence': final_result['confidence_score'],
                'evidence_quality': validated_evidence['validation_score'],
                'validation_passed': final_result['validation_passed']
            }
            
            self.intensity_history.append(intensity_record)
            
            # === æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ ===
            self.intensity_statistics['total_calculations'] += 1
            self.intensity_statistics['intensity_distribution'][final_result['intensity']] += 1
            self.intensity_statistics['confidence_history'].append(final_result['confidence_score'])
            
            # === è‡ªé€‚åº”æƒé‡æ›´æ–° ===
            if final_result['validation_passed'] and final_result['confidence_score'] > 0.6:
                self._update_evidence_weights(final_result, validated_evidence)
            
            # === é˜ˆå€¼è‡ªé€‚åº”æ›´æ–° ===
            if len(self.intensity_history) % self.config['bayesian_update_frequency'] == 0:
                self._update_adaptive_thresholds()
            
            print(f"ğŸ“Š å­¦ä¹ ç³»ç»Ÿå·²æ›´æ–°ï¼šæ€»è®¡ç®—æ¬¡æ•°={self.intensity_statistics['total_calculations']}")
            
        except Exception as e:
            print(f"âš ï¸ å­¦ä¹ ç³»ç»Ÿæ›´æ–°å¤±è´¥: {e}")

    def _update_evidence_weights(self, final_result: Dict, validated_evidence: Dict):
        """æ›´æ–°è¯æ®æƒé‡"""
        learning_rate = self.config['evidence_weight_learning_rate']
        confidence = final_result['confidence_score']
        
        # åŸºäºç½®ä¿¡åº¦è°ƒæ•´æƒé‡å¯é æ€§è¯„åˆ†
        for evidence_type in self.evidence_weights:
            current_reliability = self.evidence_weights[evidence_type]['reliability_score']
            
            # é«˜ç½®ä¿¡åº¦ç»“æœæå‡å¯é æ€§ï¼Œä½ç½®ä¿¡åº¦ç»“æœé™ä½å¯é æ€§
            if confidence > 0.7:
                reliability_adjustment = learning_rate * (confidence - 0.7)
                new_reliability = min(1.0, current_reliability + reliability_adjustment)
            elif confidence < 0.4:
                reliability_adjustment = learning_rate * (0.4 - confidence)
                new_reliability = max(0.3, current_reliability - reliability_adjustment)
            else:
                new_reliability = current_reliability
            
            self.evidence_weights[evidence_type]['reliability_score'] = new_reliability

    def _update_adaptive_thresholds(self):
        """æ›´æ–°è‡ªé€‚åº”é˜ˆå€¼"""
        # è¿™ä¸ªæ–¹æ³•åœ¨_calculate_adaptive_thresholdsä¸­å·²ç»å®ç°äº†é˜ˆå€¼æ›´æ–°
        # è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„é˜ˆå€¼ä¼˜åŒ–é€»è¾‘
        pass

    def _record_calculation_statistics(self, final_result: Dict, calculation_duration: float):
        """è®°å½•è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # æ€§èƒ½ç»Ÿè®¡
            if not hasattr(self, 'performance_stats'):
                self.performance_stats = {
                    'total_calculations': 0,
                    'average_duration': 0.0,
                    'duration_history': deque(maxlen=100)
                }
            
            self.performance_stats['total_calculations'] += 1
            self.performance_stats['duration_history'].append(calculation_duration)
            
            if self.performance_stats['duration_history']:
                self.performance_stats['average_duration'] = np.mean(self.performance_stats['duration_history'])
            
            # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            if self.performance_stats['total_calculations'] % 50 == 0:
                print(f"ğŸ“ˆ æ“æ§å¼ºåº¦è®¡ç®—å™¨ç»Ÿè®¡ï¼š")
                print(f"   æ€»è®¡ç®—æ¬¡æ•°: {self.performance_stats['total_calculations']}")
                print(f"   å¹³å‡è€—æ—¶: {self.performance_stats['average_duration']:.3f}ç§’")
                print(f"   å¼ºåº¦åˆ†å¸ƒ: {dict(self.intensity_statistics['intensity_distribution'])}")
                
        except Exception as e:
            print(f"âš ï¸ ç»Ÿè®¡è®°å½•å¤±è´¥: {e}")

    def _handle_insufficient_evidence(self, validated_evidence: Dict) -> ManipulationIntensity:
        """å¤„ç†è¯æ®ä¸è¶³çš„æƒ…å†µ"""
        validation_score = validated_evidence.get('validation_score', 0.0)
        
        if validation_score > 0.05:
            # è¯æ®è´¨é‡å¾ˆä½ä½†ä¸æ˜¯å®Œå…¨æ— æ•ˆï¼Œè¿”å›è‡ªç„¶çŠ¶æ€
            return ManipulationIntensity.NATURAL
        else:
            # è¯æ®å®Œå…¨æ— æ•ˆï¼Œè¿”å›è‡ªç„¶çŠ¶æ€ï¼ˆå®‰å…¨é»˜è®¤å€¼ï¼‰
            print("âš ï¸ è¯æ®è´¨é‡è¿‡ä½ï¼Œè¿”å›è‡ªç„¶çŠ¶æ€")
            return ManipulationIntensity.NATURAL

    def get_calculation_statistics(self) -> Dict[str, Any]:
        """è·å–è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_calculations': self.intensity_statistics['total_calculations'],
            'intensity_distribution': dict(self.intensity_statistics['intensity_distribution']),
            'average_confidence': 0.0,
            'current_thresholds': self.dynamic_thresholds.copy(),
            'evidence_weights': {k: v['reliability_score'] for k, v in self.evidence_weights.items()},
            'bayesian_priors': self.bayesian_priors.copy()
        }
        
        if self.intensity_statistics['confidence_history']:
            stats['average_confidence'] = np.mean(self.intensity_statistics['confidence_history'])
        
        if hasattr(self, 'performance_stats'):
            stats['performance'] = self.performance_stats.copy()
        
        return stats

    def reset_learning_state(self):
        """é‡ç½®å­¦ä¹ çŠ¶æ€"""
        self.intensity_history.clear()
        self.intensity_statistics = {
            'total_calculations': 0,
            'intensity_distribution': defaultdict(int),
            'accuracy_tracking': [],
            'confidence_history': deque(maxlen=100)
        }
        
        # é‡ç½®ä¸ºé»˜è®¤é…ç½®
        self.dynamic_thresholds = {
            'subtle_threshold': 0.25,
            'moderate_threshold': 0.45,
            'strong_threshold': 0.65,
            'extreme_threshold': 0.85
        }
        
        # é‡ç½®è¯æ®æƒé‡
        for evidence_type in self.evidence_weights:
            self.evidence_weights[evidence_type]['adaptive_factor'] = 1.0
            self.evidence_weights[evidence_type]['reliability_score'] = 0.8
        
        print("ğŸ”„ æ“æ§å¼ºåº¦è®¡ç®—å™¨å­¦ä¹ çŠ¶æ€å·²é‡ç½®")

class BankerPsychologyModel:
    """
    ç§‘ç ”çº§åº„å®¶å¿ƒç†æ¨¡å‹
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å¤šç»´åº¦å¿ƒç†çŠ¶æ€å»ºæ¨¡
    2. è¡Œä¸ºç»æµå­¦åå·®åˆ†æ
    3. è®¤çŸ¥è´Ÿè·è¯„ä¼°ç³»ç»Ÿ
    4. é£é™©åå¥½åŠ¨æ€å»ºæ¨¡
    5. æƒ…ç»ªçŠ¶æ€è¯†åˆ«ä¸è¿½è¸ª
    6. å†³ç­–æ¨¡å¼å­¦ä¹ ä¸é¢„æµ‹
    7. å‹åŠ›ååº”æœºåˆ¶åˆ†æ
    8. ç­–ç•¥é˜¶æ®µè¯†åˆ«ç³»ç»Ÿ
    9. å¿ƒç†å‘¨æœŸæ€§åˆ†æ
    10. é€‚åº”æ€§è¡Œä¸ºå»ºæ¨¡
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–åº„å®¶å¿ƒç†æ¨¡å‹"""
        self.config = config or self._get_default_config()
        
        # å¿ƒç†çŠ¶æ€å†å²è®°å½•
        self.psychology_history = deque(maxlen=self.config['max_psychology_history'])
        self.behavior_patterns = deque(maxlen=self.config['max_behavior_patterns'])
        self.decision_timeline = deque(maxlen=self.config['max_decision_timeline'])
        
        # å¿ƒç†åŸºçº¿æ¨¡å‹
        self.psychological_baseline = {
            'stress_tolerance': 0.6,
            'risk_appetite': 0.5,
            'aggression_tendency': 0.4,
            'learning_rate': 0.3,
            'adaptation_speed': 0.5,
            'cognitive_capacity': 0.7,
            'emotional_stability': 0.6,
            'strategic_patience': 0.5
        }
        
        # è®¤çŸ¥åå·®æƒé‡ç³»ç»Ÿ
        self.cognitive_biases = {
            'loss_aversion': {'weight': 2.5, 'current_level': 0.5},
            'overconfidence': {'weight': 1.8, 'current_level': 0.3},
            'anchoring_bias': {'weight': 1.5, 'current_level': 0.4},
            'confirmation_bias': {'weight': 1.7, 'current_level': 0.3},
            'hot_hand_fallacy': {'weight': 1.4, 'current_level': 0.2},
            'gamblers_fallacy': {'weight': 1.6, 'current_level': 0.3},
            'availability_heuristic': {'weight': 1.3, 'current_level': 0.4},
            'representative_heuristic': {'weight': 1.2, 'current_level': 0.3}
        }
        
        # æƒ…ç»ªçŠ¶æ€æ¨¡å‹
        self.emotional_states = {
            'current_mood': 'neutral',
            'arousal_level': 0.5,
            'confidence_level': 0.5,
            'frustration_level': 0.0,
            'excitement_level': 0.0,
            'anxiety_level': 0.0,
            'satisfaction_level': 0.5
        }
        
        # ç­–ç•¥é˜¶æ®µè¯†åˆ«ç³»ç»Ÿ
        self.strategic_phases = {
            'observation': {'duration': 0, 'characteristics': [], 'confidence': 0.0},
            'preparation': {'duration': 0, 'characteristics': [], 'confidence': 0.0},
            'execution': {'duration': 0, 'characteristics': [], 'confidence': 0.0},
            'consolidation': {'duration': 0, 'characteristics': [], 'confidence': 0.0},
            'adaptation': {'duration': 0, 'characteristics': [], 'confidence': 0.0}
        }
        
        # å¿ƒç†å‘¨æœŸè¿½è¸ª
        self.psychological_cycles = {
            'stress_cycle': {'phase': 'low', 'duration': 0, 'amplitude': 0.3},
            'confidence_cycle': {'phase': 'building', 'duration': 0, 'amplitude': 0.4},
            'activity_cycle': {'phase': 'moderate', 'duration': 0, 'amplitude': 0.2}
        }
        
        # å­¦ä¹ å’Œé€‚åº”ç³»ç»Ÿ
        self.learning_metrics = {
            'total_periods_observed': 0,
            'successful_predictions': 0,
            'failed_predictions': 0,
            'adaptation_events': 0,
            'pattern_recognition_accuracy': 0.5
        }
        
        # å‹åŠ›æŒ‡æ ‡ç³»ç»Ÿ
        self.stress_indicators = {
            'decision_inconsistency': 0.0,
            'response_time_variance': 0.0,
            'strategy_switching_frequency': 0.0,
            'error_rate_increase': 0.0,
            'complexity_avoidance': 0.0
        }
        
        print(f"ğŸ§  ç§‘ç ”çº§åº„å®¶å¿ƒç†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   - å¿ƒç†ç»´åº¦: {len(self.psychological_baseline)}ä¸ªåŸºçº¿æŒ‡æ ‡")
        print(f"   - è®¤çŸ¥åå·®: {len(self.cognitive_biases)}ç§åå·®æ¨¡å‹")
        print(f"   - æƒ…ç»ªçŠ¶æ€: {len(self.emotional_states)}ä¸ªæƒ…ç»ªæŒ‡æ ‡")
        print(f"   - ç­–ç•¥é˜¶æ®µ: {len(self.strategic_phases)}ä¸ªé˜¶æ®µè¯†åˆ«")
        print(f"   - å¿ƒç†å‘¨æœŸ: {len(self.psychological_cycles)}ä¸ªå‘¨æœŸè¿½è¸ª")

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'max_psychology_history': 200,
            'max_behavior_patterns': 100,
            'max_decision_timeline': 150,
            'stress_sensitivity': 0.7,
            'emotion_decay_factor': 0.9,
            'learning_adaptation_rate': 0.1,
            'cycle_detection_window': 21,
            'pattern_recognition_threshold': 0.7,
            'psychological_momentum_factor': 0.8,
            'baseline_adaptation_rate': 0.05,
            'bias_evolution_rate': 0.03,
            'confidence_volatility_threshold': 0.3,
            'stress_accumulation_rate': 0.2,
            'recovery_rate': 0.15
        }

    def analyze_state(self, period_data: Dict, historical_context: List[Dict], intensity: Any) -> Dict:
        """
        ç§‘ç ”çº§åº„å®¶å¿ƒç†çŠ¶æ€åˆ†æä¸»æ–¹æ³•
        
        Args:
            period_data: å½“æœŸæ•°æ®
            historical_context: å†å²ä¸Šä¸‹æ–‡
            intensity: æ“æ§å¼ºåº¦ç­‰çº§
            
        Returns:
            Dict: å®Œæ•´çš„å¿ƒç†çŠ¶æ€åˆ†æç»“æœ
        """
        analysis_start_time = datetime.now()
        
        try:
            # === ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å¿ƒç†æŒ‡æ ‡è®¡ç®— ===
            baseline_analysis = self._calculate_baseline_psychology(
                period_data, historical_context, intensity
            )
            
            # === ç¬¬äºŒé˜¶æ®µï¼šè®¤çŸ¥åå·®åˆ†æ ===
            cognitive_analysis = self._analyze_cognitive_biases(
                period_data, historical_context, baseline_analysis
            )
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šæƒ…ç»ªçŠ¶æ€å»ºæ¨¡ ===
            emotional_analysis = self._model_emotional_state(
                period_data, historical_context, intensity, cognitive_analysis
            )
            
            # === ç¬¬å››é˜¶æ®µï¼šå‹åŠ›æ°´å¹³è¯„ä¼° ===
            stress_analysis = self._assess_stress_level(
                period_data, historical_context, emotional_analysis
            )
            
            # === ç¬¬äº”é˜¶æ®µï¼šé£é™©åå¥½åˆ†æ ===
            risk_analysis = self._analyze_risk_preferences(
                period_data, historical_context, stress_analysis
            )
            
            # === ç¬¬å…­é˜¶æ®µï¼šç­–ç•¥é˜¶æ®µè¯†åˆ« ===
            strategic_analysis = self._identify_strategic_phase(
                period_data, historical_context, risk_analysis
            )
            
            # === ç¬¬ä¸ƒé˜¶æ®µï¼šå¿ƒç†å‘¨æœŸåˆ†æ ===
            cycle_analysis = self._analyze_psychological_cycles(
                period_data, historical_context, strategic_analysis
            )
            
            # === ç¬¬å…«é˜¶æ®µï¼šå†³ç­–æ¨¡å¼è¯†åˆ« ===
            decision_analysis = self._analyze_decision_patterns(
                period_data, historical_context, cycle_analysis
            )
            
            # === ç¬¬ä¹é˜¶æ®µï¼šç»¼åˆå¿ƒç†çŠ¶æ€åˆæˆ ===
            integrated_state = self._integrate_psychological_state(
                baseline_analysis, cognitive_analysis, emotional_analysis,
                stress_analysis, risk_analysis, strategic_analysis,
                cycle_analysis, decision_analysis
            )
            
            # === ç¬¬åé˜¶æ®µï¼šå­¦ä¹ ç³»ç»Ÿæ›´æ–° ===
            self._update_psychological_learning(
                integrated_state, period_data, historical_context
            )
            
            # è®°å½•åˆ†ææ—¶é—´
            analysis_duration = (datetime.now() - analysis_start_time).total_seconds()
            integrated_state['analysis_metadata'] = {
                'analysis_duration': analysis_duration,
                'total_periods_analyzed': self.learning_metrics['total_periods_observed'],
                'model_confidence': self._calculate_model_confidence()
            }
            
            return integrated_state
            
        except Exception as e:
            print(f"âŒ åº„å®¶å¿ƒç†åˆ†æå¤±è´¥: {e}")
            return self._get_default_psychological_state()

    def _calculate_baseline_psychology(self, period_data: Dict, historical_context: List[Dict], intensity: Any) -> Dict[str, Any]:
        """
        åŸºç¡€å¿ƒç†æŒ‡æ ‡è®¡ç®—
        åŸºäºå†å²è¡Œä¸ºæ¨¡å¼å’Œå½“å‰æ“æ§å¼ºåº¦è®¡ç®—åŸºçº¿å¿ƒç†çŠ¶æ€
        """
        baseline_result = {
            'stress_level': 0.5,
            'aggressiveness': 0.5,
            'risk_tolerance': 0.5,
            'cognitive_load': 0.5,
            'confidence_level': 0.5,
            'adaptation_pressure': 0.0,
            'baseline_metrics': {}
        }
        
        try:
            # === æ“æ§å¼ºåº¦å¯¹å¿ƒç†çš„å½±å“ ===
            intensity_impact = self._calculate_intensity_psychological_impact(intensity)
            
            # === å†å²è¡Œä¸ºæ¨¡å¼åˆ†æ ===
            behavioral_patterns = self._analyze_historical_behavior_patterns(historical_context)
            
            # === å½“å‰æœŸè¡Œä¸ºå¼‚å¸¸æ£€æµ‹ ===
            current_anomalies = self._detect_current_behavior_anomalies(
                period_data, historical_context
            )
            
            # === åŸºç¡€å‹åŠ›æ°´å¹³è®¡ç®— ===
            base_stress = self._calculate_base_stress_level(
                intensity_impact, behavioral_patterns, current_anomalies
            )
            
            # === æ”»å‡»æ€§å€¾å‘è¯„ä¼° ===
            aggressiveness = self._assess_aggressiveness_level(
                period_data, historical_context, intensity_impact
            )
            
            # === é£é™©å®¹å¿åº¦åˆ†æ ===
            risk_tolerance = self._calculate_risk_tolerance(
                behavioral_patterns, base_stress, aggressiveness
            )
            
            # === è®¤çŸ¥è´Ÿè·è¯„ä¼° ===
            cognitive_load = self._assess_cognitive_load(
                period_data, historical_context, intensity_impact
            )
            
            # === ç½®ä¿¡æ°´å¹³è®¡ç®— ===
            confidence_level = self._calculate_confidence_level(
                behavioral_patterns, current_anomalies, intensity_impact
            )
            
            # === é€‚åº”å‹åŠ›è¯„ä¼° ===
            adaptation_pressure = self._assess_adaptation_pressure(
                historical_context, current_anomalies, intensity_impact
            )
            
            baseline_result.update({
                'stress_level': max(0.0, min(1.0, base_stress)),
                'aggressiveness': max(0.0, min(1.0, aggressiveness)),
                'risk_tolerance': max(0.0, min(1.0, risk_tolerance)),
                'cognitive_load': max(0.0, min(1.0, cognitive_load)),
                'confidence_level': max(0.0, min(1.0, confidence_level)),
                'adaptation_pressure': max(0.0, min(1.0, adaptation_pressure)),
                'baseline_metrics': {
                    'intensity_impact': intensity_impact,
                    'behavioral_consistency': behavioral_patterns.get('consistency_score', 0.5),
                    'anomaly_severity': current_anomalies.get('severity_score', 0.0),
                    'historical_stress_trend': behavioral_patterns.get('stress_trend', 0.0)
                }
            })
            
        except Exception as e:
            print(f"âš ï¸ åŸºç¡€å¿ƒç†æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        return baseline_result

    def _calculate_intensity_psychological_impact(self, intensity: Any) -> Dict[str, float]:
        """è®¡ç®—æ“æ§å¼ºåº¦å¯¹å¿ƒç†çš„å½±å“"""
        if not hasattr(intensity, 'value'):
            intensity_value = 0  # é»˜è®¤ä¸ºNATURAL
        else:
            intensity_value = intensity.value
        
        # å¼ºåº¦æ˜ å°„åˆ°å¿ƒç†å½±å“
        intensity_mapping = {
            0: {'stress': 0.1, 'confidence': 0.8, 'pressure': 0.0},    # NATURAL
            1: {'stress': 0.3, 'confidence': 0.7, 'pressure': 0.2},    # SUBTLE
            2: {'stress': 0.5, 'confidence': 0.6, 'pressure': 0.4},    # MODERATE
            3: {'stress': 0.7, 'confidence': 0.4, 'pressure': 0.7},    # STRONG
            4: {'stress': 0.9, 'confidence': 0.2, 'pressure': 0.9}     # EXTREME
        }
        
        return intensity_mapping.get(intensity_value, intensity_mapping[0])

    def _analyze_historical_behavior_patterns(self, historical_context: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå†å²è¡Œä¸ºæ¨¡å¼"""
        patterns = {
            'consistency_score': 0.5,
            'volatility_score': 0.3,
            'trend_direction': 'stable',
            'pattern_complexity': 0.4,
            'stress_trend': 0.0,
            'learning_evidence': 0.3
        }
        
        if len(historical_context) < 5:
            return patterns
        
        try:
            # === è¡Œä¸ºä¸€è‡´æ€§åˆ†æ ===
            behavior_scores = []
            for i, period in enumerate(historical_context):
                period_tails = period.get('tails', [])
                if i > 0:
                    prev_tails = historical_context[i-1].get('tails', [])
                    # è®¡ç®—æœŸé—´çš„è¡Œä¸ºä¸€è‡´æ€§
                    overlap = len(set(period_tails).intersection(set(prev_tails)))
                    consistency = overlap / max(len(period_tails), len(prev_tails), 1)
                    behavior_scores.append(consistency)
            
            if behavior_scores:
                patterns['consistency_score'] = np.mean(behavior_scores)
                patterns['volatility_score'] = np.std(behavior_scores)
            
            # === å¤æ‚åº¦åˆ†æ ===
            complexity_scores = []
            for period in historical_context:
                period_tails = period.get('tails', [])
                # åŸºäºå°¾æ•°åˆ†å¸ƒè®¡ç®—å¤æ‚åº¦
                if len(period_tails) >= 2:
                    tail_variance = np.var(period_tails) if len(period_tails) > 1 else 0
                    complexity = min(1.0, tail_variance / 10.0)
                    complexity_scores.append(complexity)
            
            if complexity_scores:
                patterns['pattern_complexity'] = np.mean(complexity_scores)
            
            # === è¶‹åŠ¿åˆ†æ ===
            if len(historical_context) >= 10:
                recent_complexity = np.mean(complexity_scores[-5:]) if len(complexity_scores) >= 5 else 0
                earlier_complexity = np.mean(complexity_scores[-10:-5]) if len(complexity_scores) >= 10 else 0
                
                if recent_complexity > earlier_complexity * 1.2:
                    patterns['trend_direction'] = 'increasing_complexity'
                    patterns['stress_trend'] = 0.3
                elif recent_complexity < earlier_complexity * 0.8:
                    patterns['trend_direction'] = 'decreasing_complexity'
                    patterns['stress_trend'] = -0.2
                else:
                    patterns['trend_direction'] = 'stable'
                    patterns['stress_trend'] = 0.0
            
            # === å­¦ä¹ è¯æ®åˆ†æ ===
            if len(behavior_scores) >= 10:
                early_consistency = np.mean(behavior_scores[:5])
                late_consistency = np.mean(behavior_scores[-5:])
                
                if late_consistency > early_consistency:
                    patterns['learning_evidence'] = min(1.0, (late_consistency - early_consistency) * 2)
                else:
                    patterns['learning_evidence'] = max(0.0, 0.3 - (early_consistency - late_consistency))
            
        except Exception as e:
            print(f"âš ï¸ å†å²è¡Œä¸ºæ¨¡å¼åˆ†æå¤±è´¥: {e}")
        
        return patterns

    def _detect_current_behavior_anomalies(self, period_data: Dict, historical_context: List[Dict]) -> Dict[str, Any]:
        """æ£€æµ‹å½“å‰è¡Œä¸ºå¼‚å¸¸"""
        anomalies = {
            'severity_score': 0.0,
            'anomaly_types': [],
            'deviation_magnitude': 0.0,
            'unexpectedness': 0.0
        }
        
        if len(historical_context) < 5:
            return anomalies
        
        try:
            current_tails = set(period_data.get('tails', []))
            
            # === å°¾æ•°æ•°é‡å¼‚å¸¸ ===
            historical_counts = [len(period.get('tails', [])) for period in historical_context]
            mean_count = np.mean(historical_counts)
            std_count = np.std(historical_counts) if len(historical_counts) > 1 else 1.0
            
            current_count = len(current_tails)
            count_deviation = abs(current_count - mean_count) / max(std_count, 0.5)
            
            if count_deviation > 2.0:
                anomalies['anomaly_types'].append('count_anomaly')
                anomalies['severity_score'] += min(0.4, count_deviation / 5.0)
            
            # === å°¾æ•°åˆ†å¸ƒå¼‚å¸¸ ===
            if len(current_tails) >= 2:
                current_variance = np.var(list(current_tails))
                historical_variances = []
                
                for period in historical_context:
                    period_tails = period.get('tails', [])
                    if len(period_tails) >= 2:
                        historical_variances.append(np.var(period_tails))
                
                if historical_variances:
                    mean_variance = np.mean(historical_variances)
                    variance_deviation = abs(current_variance - mean_variance) / max(mean_variance, 1.0)
                    
                    if variance_deviation > 1.5:
                        anomalies['anomaly_types'].append('distribution_anomaly')
                        anomalies['severity_score'] += min(0.3, variance_deviation / 3.0)
            
            # === æ¨¡å¼çªå˜å¼‚å¸¸ ===
            if len(historical_context) >= 3:
                recent_patterns = []
                for i in range(min(3, len(historical_context))):
                    period_tails = set(historical_context[i].get('tails', []))
                    overlap_with_current = len(current_tails.intersection(period_tails))
                    pattern_similarity = overlap_with_current / max(len(current_tails.union(period_tails)), 1)
                    recent_patterns.append(pattern_similarity)
                
                avg_similarity = np.mean(recent_patterns)
                if avg_similarity < 0.2:
                    anomalies['anomaly_types'].append('pattern_break')
                    anomalies['severity_score'] += 0.25
            
            # === ç»¼åˆå¼‚å¸¸è¯„ä¼° ===
            anomalies['deviation_magnitude'] = count_deviation + (variance_deviation if 'variance_deviation' in locals() else 0)
            anomalies['unexpectedness'] = 1.0 - avg_similarity if 'avg_similarity' in locals() else 0.5
            
            # é™åˆ¶è¯„åˆ†èŒƒå›´
            anomalies['severity_score'] = min(1.0, anomalies['severity_score'])
            
        except Exception as e:
            print(f"âš ï¸ è¡Œä¸ºå¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")
        
        return anomalies

    def _calculate_base_stress_level(self, intensity_impact: Dict, behavioral_patterns: Dict, current_anomalies: Dict) -> float:
        """è®¡ç®—åŸºç¡€å‹åŠ›æ°´å¹³"""
        stress_components = []
        
        # æ“æ§å¼ºåº¦å‹åŠ›
        intensity_stress = intensity_impact.get('stress', 0.5)
        stress_components.append(intensity_stress * 0.4)
        
        # è¡Œä¸ºä¸€è‡´æ€§å‹åŠ›ï¼ˆä¸€è‡´æ€§ä½=å‹åŠ›é«˜ï¼‰
        consistency = behavioral_patterns.get('consistency_score', 0.5)
        consistency_stress = (1.0 - consistency) * 0.8
        stress_components.append(consistency_stress * 0.3)
        
        # å¼‚å¸¸è¡Œä¸ºå‹åŠ›
        anomaly_stress = current_anomalies.get('severity_score', 0.0)
        stress_components.append(anomaly_stress * 0.3)
        
        base_stress = sum(stress_components)
        
        # æ·»åŠ åŸºçº¿å‹åŠ›å®¹å¿åº¦è°ƒæ•´
        stress_tolerance = self.psychological_baseline.get('stress_tolerance', 0.6)
        adjusted_stress = base_stress / stress_tolerance
        
        return min(1.0, adjusted_stress)

    def _assess_aggressiveness_level(self, period_data: Dict, historical_context: List[Dict], intensity_impact: Dict) -> float:
        """è¯„ä¼°æ”»å‡»æ€§æ°´å¹³"""
        aggressiveness_factors = []
        
        # åŸºäºæ“æ§å¼ºåº¦çš„æ”»å‡»æ€§
        intensity_aggression = intensity_impact.get('pressure', 0.0) * 0.8
        aggressiveness_factors.append(intensity_aggression)
        
        # åŸºäºè¡Œä¸ºæ¨¡å¼çš„æ”»å‡»æ€§
        current_tails = period_data.get('tails', [])
        if len(current_tails) >= 4:  # å¤šå°¾æ•°é€‰æ‹©å¯èƒ½è¡¨ç¤ºæ”»å‡»æ€§ç­–ç•¥
            selection_aggression = min(1.0, len(current_tails) / 6.0)
            aggressiveness_factors.append(selection_aggression * 0.3)
        
        # åŸºäºå˜åŒ–å¹…åº¦çš„æ”»å‡»æ€§
        if len(historical_context) >= 2:
            prev_tails = set(historical_context[0].get('tails', []))
            current_tails_set = set(current_tails)
            
            change_magnitude = len(current_tails_set.symmetric_difference(prev_tails))
            change_aggression = min(1.0, change_magnitude / 8.0)
            aggressiveness_factors.append(change_aggression * 0.2)
        
        # åŸºçº¿æ”»å‡»å€¾å‘è°ƒæ•´
        baseline_aggression = self.psychological_baseline.get('aggression_tendency', 0.4)
        calculated_aggression = np.mean(aggressiveness_factors) if aggressiveness_factors else 0.5
        
        # æ··åˆåŸºçº¿å’Œè®¡ç®—å€¼
        final_aggression = baseline_aggression * 0.3 + calculated_aggression * 0.7
        
        return min(1.0, final_aggression)

    def _calculate_risk_tolerance(self, behavioral_patterns: Dict, stress_level: float, aggressiveness: float) -> float:
        """è®¡ç®—é£é™©å®¹å¿åº¦"""
        # åŸºçº¿é£é™©åå¥½
        baseline_risk = self.psychological_baseline.get('risk_appetite', 0.5)
        
        # å‹åŠ›å¯¹é£é™©å®¹å¿åº¦çš„å½±å“ï¼ˆé«˜å‹åŠ›é€šå¸¸é™ä½é£é™©å®¹å¿åº¦ï¼‰
        stress_adjustment = -(stress_level - 0.5) * 0.6
        
        # æ”»å‡»æ€§å¯¹é£é™©å®¹å¿åº¦çš„æ­£å‘å½±å“
        aggression_adjustment = (aggressiveness - 0.5) * 0.4
        
        # è¡Œä¸ºä¸€è‡´æ€§å½±å“ï¼ˆé«˜ä¸€è‡´æ€§=é«˜é£é™©å®¹å¿åº¦ï¼‰
        consistency = behavioral_patterns.get('consistency_score', 0.5)
        consistency_adjustment = (consistency - 0.5) * 0.3
        
        # ç»¼åˆé£é™©å®¹å¿åº¦
        risk_tolerance = (
            baseline_risk +
            stress_adjustment +
            aggression_adjustment +
            consistency_adjustment
        )
        
        return max(0.1, min(0.9, risk_tolerance))

    def _assess_cognitive_load(self, period_data: Dict, historical_context: List[Dict], intensity_impact: Dict) -> float:
        """è¯„ä¼°è®¤çŸ¥è´Ÿè·"""
        cognitive_factors = []
        
        # æ“æ§å¤æ‚åº¦å¸¦æ¥çš„è®¤çŸ¥è´Ÿè·
        intensity_cognitive_load = intensity_impact.get('pressure', 0.0) * 0.7
        cognitive_factors.append(intensity_cognitive_load)
        
        # å†³ç­–å¤æ‚åº¦
        current_tails = period_data.get('tails', [])
        decision_complexity = min(1.0, len(current_tails) / 7.0)  # é€‰æ‹©è¶Šå¤šï¼Œè®¤çŸ¥è´Ÿè·è¶Šé«˜
        cognitive_factors.append(decision_complexity * 0.3)
        
        # å†å²æ¨¡å¼å¤æ‚åº¦
        if len(historical_context) >= 5:
            pattern_complexities = []
            for i in range(min(5, len(historical_context))):
                period_tails = historical_context[i].get('tails', [])
                if len(period_tails) >= 2:
                    # åŸºäºå°¾æ•°åˆ†å¸ƒçš„å¤æ‚åº¦
                    tail_spread = max(period_tails) - min(period_tails)
                    complexity = min(1.0, tail_spread / 9.0)
                    pattern_complexities.append(complexity)
            
            if pattern_complexities:
                avg_complexity = np.mean(pattern_complexities)
                cognitive_factors.append(avg_complexity * 0.2)
        
        # åŸºçº¿è®¤çŸ¥èƒ½åŠ›è°ƒæ•´
        cognitive_capacity = self.psychological_baseline.get('cognitive_capacity', 0.7)
        calculated_load = np.mean(cognitive_factors) if cognitive_factors else 0.5
        
        # è®¤çŸ¥è´Ÿè· = éœ€æ±‚ / èƒ½åŠ›
        cognitive_load = calculated_load / cognitive_capacity
        
        return min(1.0, cognitive_load)

    def _calculate_confidence_level(self, behavioral_patterns: Dict, current_anomalies: Dict, intensity_impact: Dict) -> float:
        """è®¡ç®—ç½®ä¿¡æ°´å¹³"""
        confidence_factors = []
        
        # è¡Œä¸ºä¸€è‡´æ€§å¸¦æ¥çš„ç½®ä¿¡åº¦
        consistency = behavioral_patterns.get('consistency_score', 0.5)
        consistency_confidence = consistency * 0.8
        confidence_factors.append(consistency_confidence)
        
        # å­¦ä¹ è¯æ®å¸¦æ¥çš„ç½®ä¿¡åº¦
        learning_evidence = behavioral_patterns.get('learning_evidence', 0.3)
        learning_confidence = learning_evidence * 0.6
        confidence_factors.append(learning_confidence)
        
        # å¼‚å¸¸è¡Œä¸ºé™ä½ç½®ä¿¡åº¦
        anomaly_impact = current_anomalies.get('severity_score', 0.0)
        anomaly_confidence_reduction = -anomaly_impact * 0.7
        confidence_factors.append(anomaly_confidence_reduction)
        
        # æ“æ§æˆåŠŸï¼ˆä½å‹åŠ›ï¼‰æå‡ç½®ä¿¡åº¦
        success_confidence = (1.0 - intensity_impact.get('stress', 0.5)) * 0.4
        confidence_factors.append(success_confidence)
        
        # åŸºçº¿ç½®ä¿¡æ°´å¹³
        baseline_confidence = 0.5
        calculated_confidence = baseline_confidence + sum(confidence_factors)
        
        return max(0.1, min(0.9, calculated_confidence))

    def _assess_adaptation_pressure(self, historical_context: List[Dict], current_anomalies: Dict, intensity_impact: Dict) -> float:
        """è¯„ä¼°é€‚åº”å‹åŠ›"""
        adaptation_factors = []
        
        # å½“å‰å¼‚å¸¸å¸¦æ¥çš„é€‚åº”å‹åŠ›
        anomaly_pressure = current_anomalies.get('severity_score', 0.0)
        adaptation_factors.append(anomaly_pressure * 0.6)
        
        # æ“æ§å¼ºåº¦å˜åŒ–çš„é€‚åº”å‹åŠ›
        intensity_pressure = intensity_impact.get('pressure', 0.0)
        adaptation_factors.append(intensity_pressure * 0.4)
        
        # ç¯å¢ƒå˜åŒ–å‹åŠ›
        if len(historical_context) >= 10:
            # è®¡ç®—æœ€è¿‘å˜åŒ–é¢‘ç‡
            recent_changes = 0
            for i in range(min(5, len(historical_context) - 1)):
                current_period = set(historical_context[i].get('tails', []))
                next_period = set(historical_context[i + 1].get('tails', []))
                
                change_magnitude = len(current_period.symmetric_difference(next_period))
                if change_magnitude >= 3:  # æ˜¾è‘—å˜åŒ–
                    recent_changes += 1
            
            change_pressure = min(1.0, recent_changes / 5.0)
            adaptation_factors.append(change_pressure * 0.3)
        
        return min(1.0, sum(adaptation_factors)) if adaptation_factors else 0.0

    def _analyze_cognitive_biases(self, period_data: Dict, historical_context: List[Dict], baseline_analysis: Dict) -> Dict[str, Any]:
        """
        è®¤çŸ¥åå·®åˆ†æ
        åŸºäºè¡Œä¸ºç»æµå­¦ç†è®ºåˆ†æåº„å®¶çš„è®¤çŸ¥åå·®è¡¨ç°
        """
        bias_analysis = {
            'active_biases': [],
            'bias_strengths': {},
            'bias_interactions': {},
            'overall_bias_score': 0.0,
            'rationality_index': 0.7
        }
        
        try:
            current_tails = set(period_data.get('tails', []))
            
            # === æŸå¤±åŒæ¶åˆ†æ ===
            loss_aversion = self._analyze_loss_aversion(
                current_tails, historical_context, baseline_analysis
            )
            
            # === è¿‡åº¦è‡ªä¿¡åˆ†æ ===
            overconfidence = self._analyze_overconfidence(
                current_tails, historical_context, baseline_analysis
            )
            
            # === é”šå®šåå·®åˆ†æ ===
            anchoring_bias = self._analyze_anchoring_bias(
                current_tails, historical_context, baseline_analysis
            )
            
            # === ç¡®è®¤åè¯¯åˆ†æ ===
            confirmation_bias = self._analyze_confirmation_bias(
                current_tails, historical_context, baseline_analysis
            )
            
            # === èµŒå¾’è°¬è¯¯åˆ†æ ===
            gamblers_fallacy = self._analyze_gamblers_fallacy(
                current_tails, historical_context, baseline_analysis
            )
            
            # === çƒ­æ‰‹é”™è§‰åˆ†æ ===
            hot_hand_fallacy = self._analyze_hot_hand_fallacy(
                current_tails, historical_context, baseline_analysis
            )
            
            # === å¯å¾—æ€§å¯å‘å¼åˆ†æ ===
            availability_heuristic = self._analyze_availability_heuristic(
                current_tails, historical_context, baseline_analysis
            )
            
            # === ä»£è¡¨æ€§å¯å‘å¼åˆ†æ ===
            representative_heuristic = self._analyze_representative_heuristic(
                current_tails, historical_context, baseline_analysis
            )
            
            # === ç»¼åˆåå·®åˆ†æ ===
            all_biases = {
                'loss_aversion': loss_aversion,
                'overconfidence': overconfidence,
                'anchoring_bias': anchoring_bias,
                'confirmation_bias': confirmation_bias,
                'gamblers_fallacy': gamblers_fallacy,
                'hot_hand_fallacy': hot_hand_fallacy,
                'availability_heuristic': availability_heuristic,
                'representative_heuristic': representative_heuristic
            }
            
            # è¯†åˆ«æ´»è·ƒåå·®
            bias_threshold = 0.6
            for bias_name, bias_strength in all_biases.items():
                bias_analysis['bias_strengths'][bias_name] = bias_strength
                if bias_strength > bias_threshold:
                    bias_analysis['active_biases'].append(bias_name)
            
            # è®¡ç®—æ•´ä½“åå·®åˆ†æ•°
            bias_analysis['overall_bias_score'] = np.mean(list(all_biases.values()))
            
            # è®¡ç®—ç†æ€§æŒ‡æ•°ï¼ˆåå·®è¶Šå°‘è¶Šç†æ€§ï¼‰
            bias_analysis['rationality_index'] = max(0.1, 1.0 - bias_analysis['overall_bias_score'])
            
            # åˆ†æåå·®é—´ç›¸äº’ä½œç”¨
            bias_analysis['bias_interactions'] = self._analyze_bias_interactions(all_biases)
            
            # æ›´æ–°è®¤çŸ¥åå·®å†å²
            self._update_cognitive_bias_history(all_biases)
            
        except Exception as e:
            print(f"âš ï¸ è®¤çŸ¥åå·®åˆ†æå¤±è´¥: {e}")
        
        return bias_analysis

    def _analyze_loss_aversion(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†ææŸå¤±åŒæ¶å€¾å‘"""
        if len(historical_context) < 5:
            return 0.3
        
        # æ£€æµ‹ä¿å®ˆè¡Œä¸ºæ¨¡å¼
        conservative_behavior = 0.0
        
        # åˆ†æå°¾æ•°é€‰æ‹©çš„ä¿å®ˆæ€§
        recent_tail_counts = []
        for period in historical_context[:5]:
            recent_tail_counts.append(len(period.get('tails', [])))
        
        current_count = len(current_tails)
        avg_recent_count = np.mean(recent_tail_counts) if recent_tail_counts else 3
        
        # å¦‚æœå½“å‰é€‰æ‹©æ˜æ˜¾å°‘äºå†å²å¹³å‡ï¼Œå¯èƒ½è¡¨ç¤ºæŸå¤±åŒæ¶
        if current_count < avg_recent_count * 0.8:
            conservative_behavior += 0.4
        
        # åˆ†ææ˜¯å¦é¿å…æç«¯å°¾æ•°ï¼ˆ0, 9ï¼‰
        extreme_tails = {0, 9}
        if not current_tails.intersection(extreme_tails) and len(current_tails) >= 2:
            conservative_behavior += 0.3
        
        # åŸºäºå‹åŠ›æ°´å¹³è°ƒæ•´ï¼ˆé«˜å‹åŠ›å¢å¼ºæŸå¤±åŒæ¶ï¼‰
        stress_level = baseline_analysis.get('stress_level', 0.5)
        stress_amplification = stress_level * 0.3
        
        loss_aversion_score = min(1.0, conservative_behavior + stress_amplification)
        return loss_aversion_score

    def _analyze_overconfidence(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†æè¿‡åº¦è‡ªä¿¡å€¾å‘"""
        overconfidence_indicators = []
        
        # åŸºäºé€‰æ‹©æ•°é‡çš„è¿‡åº¦è‡ªä¿¡
        tail_count = len(current_tails)
        if tail_count >= 5:  # é€‰æ‹©å¾ˆå¤šå°¾æ•°å¯èƒ½è¡¨ç¤ºè¿‡åº¦è‡ªä¿¡
            count_confidence = min(1.0, (tail_count - 3) / 4.0)
            overconfidence_indicators.append(count_confidence)
        
        # åŸºäºä¸€è‡´æ€§çš„è¿‡åº¦è‡ªä¿¡
        consistency = baseline_analysis.get('baseline_metrics', {}).get('behavioral_consistency', 0.5)
        if consistency > 0.8:  # è¿‡é«˜ä¸€è‡´æ€§å¯èƒ½è¡¨ç¤ºè¿‡åº¦è‡ªä¿¡
            consistency_confidence = (consistency - 0.8) / 0.2
            overconfidence_indicators.append(consistency_confidence)
        
        # åŸºäºæ”»å‡»æ€§çš„è¿‡åº¦è‡ªä¿¡
        aggressiveness = baseline_analysis.get('aggressiveness', 0.5)
        if aggressiveness > 0.7:
            aggression_confidence = (aggressiveness - 0.7) / 0.3
            overconfidence_indicators.append(aggression_confidence)
        
        # åŸºäºä½å‹åŠ›çš„è¿‡åº¦è‡ªä¿¡
        stress_level = baseline_analysis.get('stress_level', 0.5)
        if stress_level < 0.3:
            low_stress_confidence = (0.3 - stress_level) / 0.3
            overconfidence_indicators.append(low_stress_confidence)
        
        return np.mean(overconfidence_indicators) if overconfidence_indicators else 0.3

    def _analyze_anchoring_bias(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†æé”šå®šåå·®"""
        if len(historical_context) < 3:
            return 0.3
        
        anchoring_evidence = []
        
        # æ£€æµ‹å¯¹æœ€è¿‘æœŸå°¾æ•°çš„é”šå®š
        recent_period_tails = set(historical_context[0].get('tails', []))
        overlap_with_recent = len(current_tails.intersection(recent_period_tails))
        
        if overlap_with_recent >= 2:
            recent_anchoring = min(1.0, overlap_with_recent / len(current_tails))
            anchoring_evidence.append(recent_anchoring)
        
        # æ£€æµ‹å¯¹ç‰¹å®šæ•°å­—èŒƒå›´çš„é”šå®š
        tail_ranges = {
            'low': {0, 1, 2, 3},
            'mid': {4, 5, 6},
            'high': {7, 8, 9}
        }
        
        range_concentrations = []
        for range_name, range_tails in tail_ranges.items():
            concentration = len(current_tails.intersection(range_tails)) / len(current_tails) if current_tails else 0
            range_concentrations.append(concentration)
        
        max_concentration = max(range_concentrations)
        if max_concentration > 0.6:  # è¿‡åº¦é›†ä¸­åœ¨æŸä¸ªèŒƒå›´
            range_anchoring = (max_concentration - 0.6) / 0.4
            anchoring_evidence.append(range_anchoring)
        
        return np.mean(anchoring_evidence) if anchoring_evidence else 0.3

    def _analyze_confirmation_bias(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†æç¡®è®¤åè¯¯"""
        if len(historical_context) < 5:
            return 0.3
        
        confirmation_indicators = []
        
        # æ£€æµ‹æ˜¯å¦æŒç»­é€‰æ‹©ç›¸ä¼¼æ¨¡å¼
        pattern_similarities = []
        for i in range(min(3, len(historical_context))):
            historical_tails = set(historical_context[i].get('tails', []))
            similarity = len(current_tails.intersection(historical_tails)) / len(current_tails.union(historical_tails))
            pattern_similarities.append(similarity)
        
        avg_similarity = np.mean(pattern_similarities)
        if avg_similarity > 0.5:
            pattern_confirmation = (avg_similarity - 0.5) / 0.5
            confirmation_indicators.append(pattern_confirmation)
        
        # æ£€æµ‹æ˜¯å¦å¿½è§†åå‘è¯æ®
        # ç®€åŒ–å®ç°ï¼šå¦‚æœè¡Œä¸ºè¿‡äºä¸€è‡´ï¼Œå¯èƒ½å¿½è§†äº†åå‘ä¿¡æ¯
        consistency = baseline_analysis.get('baseline_metrics', {}).get('behavioral_consistency', 0.5)
        if consistency > 0.8:
            consistency_bias = (consistency - 0.8) / 0.2
            confirmation_indicators.append(consistency_bias)
        
        return np.mean(confirmation_indicators) if confirmation_indicators else 0.3

    def _analyze_gamblers_fallacy(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†æèµŒå¾’è°¬è¯¯"""
        if len(historical_context) < 7:
            return 0.3
        
        fallacy_evidence = []
        
        # æ£€æµ‹å¯¹"çƒ­é—¨"æ•°å­—çš„å›é¿
        tail_frequencies = defaultdict(int)
        for period in historical_context[:7]:
            for tail in period.get('tails', []):
                tail_frequencies[tail] += 1
        
        # è¯†åˆ«çƒ­é—¨å°¾æ•°ï¼ˆå‡ºç°é¢‘ç‡é«˜ï¼‰
        hot_tails = {tail for tail, freq in tail_frequencies.items() if freq >= 4}
        
        if hot_tails:
            hot_avoidance = len(hot_tails - current_tails) / len(hot_tails)
            if hot_avoidance > 0.6:  # æ˜æ˜¾å›é¿çƒ­é—¨æ•°å­—
                fallacy_evidence.append(hot_avoidance)
        
        # æ£€æµ‹å¯¹"å†·é—¨"æ•°å­—çš„åå¥½
        cold_tails = {tail for tail in range(10) if tail_frequencies[tail] <= 1}
        
        if cold_tails:
            cold_preference = len(current_tails.intersection(cold_tails)) / len(current_tails) if current_tails else 0
            if cold_preference > 0.4:  # æ˜æ˜¾åå¥½å†·é—¨æ•°å­—
                fallacy_evidence.append(cold_preference)
        
        return np.mean(fallacy_evidence) if fallacy_evidence else 0.3

    def _analyze_hot_hand_fallacy(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†æçƒ­æ‰‹é”™è§‰"""
        if len(historical_context) < 5:
            return 0.3
        
        hot_hand_evidence = []
        
        # æ£€æµ‹å¯¹è¿ç»­æˆåŠŸçš„è¿‡åº¦ä¿¡å¿ƒ
        # ç®€åŒ–ï¼šå¦‚æœæ”»å‡»æ€§å’Œç½®ä¿¡åº¦éƒ½å¾ˆé«˜ï¼Œå¯èƒ½å­˜åœ¨çƒ­æ‰‹é”™è§‰
        aggressiveness = baseline_analysis.get('aggressiveness', 0.5)
        confidence = baseline_analysis.get('confidence_level', 0.5)
        
        if aggressiveness > 0.7 and confidence > 0.7:
            overconfidence_hot_hand = (aggressiveness + confidence) / 2.0 - 0.7
            hot_hand_evidence.append(overconfidence_hot_hand * 2)
        
        # æ£€æµ‹å¯¹æœ€è¿‘"æˆåŠŸ"æ¨¡å¼çš„å»¶ç»­
        # å¦‚æœæœ€è¿‘å‡ æœŸæ¨¡å¼ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œå¯èƒ½å­˜åœ¨çƒ­æ‰‹é”™è§‰
        if len(historical_context) >= 3:
            recent_similarities = []
            for i in range(min(3, len(historical_context) - 1)):
                tail1 = set(historical_context[i].get('tails', []))
                tail2 = set(historical_context[i + 1].get('tails', []))
                similarity = len(tail1.intersection(tail2)) / len(tail1.union(tail2)) if tail1.union(tail2) else 0
                recent_similarities.append(similarity)
            
            avg_recent_similarity = np.mean(recent_similarities)
            if avg_recent_similarity > 0.6:
                pattern_persistence = (avg_recent_similarity - 0.6) / 0.4
                hot_hand_evidence.append(pattern_persistence)
        
        return np.mean(hot_hand_evidence) if hot_hand_evidence else 0.3

    def _analyze_availability_heuristic(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†æå¯å¾—æ€§å¯å‘å¼"""
        availability_indicators = []
        
        # æ£€æµ‹å¯¹æœ€è¿‘è®°å¿†çš„è¿‡åº¦ä¾èµ–
        if len(historical_context) >= 3:
            # å¯¹æ¯”æœ€è¿‘3æœŸå’Œè¾ƒæ—©æœŸçš„ç›¸ä¼¼åº¦
            recent_overlap = 0
            for period in historical_context[:3]:
                period_tails = set(period.get('tails', []))
                overlap = len(current_tails.intersection(period_tails))
                recent_overlap += overlap
            
            if len(historical_context) >= 6:
                earlier_overlap = 0
                for period in historical_context[3:6]:
                    period_tails = set(period.get('tails', []))
                    overlap = len(current_tails.intersection(period_tails))
                    earlier_overlap += overlap
                
                if recent_overlap > earlier_overlap * 1.5:
                    recent_bias = min(1.0, (recent_overlap - earlier_overlap) / max(earlier_overlap, 1))
                    availability_indicators.append(recent_bias)
        
        # æ£€æµ‹å¯¹"memorable"æ•°å­—çš„åå¥½
        memorable_tails = {0, 5, 8, 9}  # é€šå¸¸æ›´å®¹æ˜“è®°ä½çš„æ•°å­—
        memorable_ratio = len(current_tails.intersection(memorable_tails)) / len(current_tails) if current_tails else 0
        
        if memorable_ratio > 0.5:
            memorable_bias = (memorable_ratio - 0.5) / 0.5
            availability_indicators.append(memorable_bias)
        
        return np.mean(availability_indicators) if availability_indicators else 0.3

    def _analyze_representative_heuristic(self, current_tails: Set[int], historical_context: List[Dict], baseline_analysis: Dict) -> float:
        """åˆ†æä»£è¡¨æ€§å¯å‘å¼"""
        representative_indicators = []
        
        # æ£€æµ‹å¯¹"å…¸å‹"æ¨¡å¼çš„åå¥½
        current_tail_list = sorted(list(current_tails))
        
        if len(current_tail_list) >= 3:
            # æ£€æµ‹æ˜¯å¦è¿‡äº"éšæœº"ï¼ˆå¯èƒ½è¯•å›¾ä»£è¡¨éšæœºæ€§ï¼‰
            gaps = [current_tail_list[i+1] - current_tail_list[i] for i in range(len(current_tail_list)-1)]
            gap_variance = np.var(gaps) if len(gaps) > 1 else 0
            
            # é«˜æ–¹å·®å¯èƒ½è¡¨ç¤ºè¯•å›¾"çœ‹èµ·æ¥éšæœº"
            if gap_variance > 6:
                randomness_mimicking = min(1.0, gap_variance / 12.0)
                representative_indicators.append(randomness_mimicking)
            
            # æ£€æµ‹å‡åŒ€åˆ†å¸ƒåå¥½
            if len(current_tail_list) >= 4:
                expected_gap = 9 / (len(current_tail_list) - 1)
                actual_gaps = gaps
                gap_deviations = [abs(gap - expected_gap) for gap in actual_gaps]
                avg_deviation = np.mean(gap_deviations)
                
                if avg_deviation < 1.0:  # è¿‡äºå‡åŒ€
                    uniformity_bias = (1.0 - avg_deviation) / 1.0
                    representative_indicators.append(uniformity_bias)
        
        return np.mean(representative_indicators) if representative_indicators else 0.3

    def _analyze_bias_interactions(self, all_biases: Dict[str, float]) -> Dict[str, float]:
        """åˆ†æè®¤çŸ¥åå·®é—´çš„ç›¸äº’ä½œç”¨"""
        interactions = {}
        
        # æŸå¤±åŒæ¶ä¸è¿‡åº¦è‡ªä¿¡çš„å¯¹æŠ—
        loss_vs_confidence = abs(all_biases['loss_aversion'] - all_biases['overconfidence'])
        interactions['loss_confidence_conflict'] = loss_vs_confidence
        
        # é”šå®šåå·®ä¸ç¡®è®¤åè¯¯çš„ååŒ
        anchoring_confirmation_synergy = min(all_biases['anchoring_bias'], all_biases['confirmation_bias'])
        interactions['anchoring_confirmation_synergy'] = anchoring_confirmation_synergy
        
        # èµŒå¾’è°¬è¯¯ä¸çƒ­æ‰‹é”™è§‰çš„çŸ›ç›¾
        gambler_hothand_conflict = abs(all_biases['gamblers_fallacy'] - all_biases['hot_hand_fallacy'])
        interactions['gambler_hothand_conflict'] = gambler_hothand_conflict
        
        return interactions

    def _update_cognitive_bias_history(self, current_biases: Dict[str, float]):
        """æ›´æ–°è®¤çŸ¥åå·®å†å²è®°å½•"""
        for bias_name, bias_strength in current_biases.items():
            if bias_name in self.cognitive_biases:
                # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
                decay_factor = self.config['emotion_decay_factor']
                current_level = self.cognitive_biases[bias_name]['current_level']
                new_level = current_level * decay_factor + bias_strength * (1 - decay_factor)
                self.cognitive_biases[bias_name]['current_level'] = new_level

    def _model_emotional_state(self, period_data: Dict, historical_context: List[Dict], 
                              intensity: Any, cognitive_analysis: Dict) -> Dict[str, Any]:
        """
        æƒ…ç»ªçŠ¶æ€å»ºæ¨¡
        åŸºäºæƒ…ç»ªå¿ƒç†å­¦å’Œç¥ç»ç»æµå­¦å»ºæ¨¡åº„å®¶çš„æƒ…ç»ªçŠ¶æ€
        """
        emotional_result = {
            'current_mood': 'neutral',
            'arousal_level': 0.5,
            'emotional_valence': 0.0,
            'emotional_stability': 0.7,
            'mood_trajectory': 'stable',
            'emotion_drivers': {},
            'emotional_conflicts': []
        }
        
        try:
            # === åŸºç¡€æƒ…ç»ªè¯„ä¼° ===
            base_emotions = self._assess_base_emotions(
                period_data, historical_context, intensity
            )
            
            # === è®¤çŸ¥-æƒ…ç»ªäº¤äº’ ===
            cognitive_emotional_interaction = self._analyze_cognitive_emotion_interaction(
                cognitive_analysis, base_emotions
            )
            
            # === æƒ…ç»ªåŠ¨åŠ›å­¦åˆ†æ ===
            emotion_dynamics = self._analyze_emotion_dynamics(
                base_emotions, historical_context
            )
            
            # === æƒ…ç»ªç¨³å®šæ€§è¯„ä¼° ===
            stability_assessment = self._assess_emotional_stability(
                base_emotions, emotion_dynamics
            )
            
            # === æƒ…ç»ªå†²çªæ£€æµ‹ ===
            emotional_conflicts = self._detect_emotional_conflicts(
                base_emotions, cognitive_analysis
            )
            
            # === ç»¼åˆæƒ…ç»ªçŠ¶æ€ ===
            integrated_emotion = self._integrate_emotional_state(
                base_emotions, cognitive_emotional_interaction, 
                emotion_dynamics, stability_assessment, emotional_conflicts
            )
            
            emotional_result.update(integrated_emotion)
            
            # === æ›´æ–°æƒ…ç»ªå†å² ===
            self._update_emotional_history(integrated_emotion)
            
        except Exception as e:
            print(f"âš ï¸ æƒ…ç»ªçŠ¶æ€å»ºæ¨¡å¤±è´¥: {e}")
        
        return emotional_result

    def _assess_base_emotions(self, period_data: Dict, historical_context: List[Dict], intensity: Any) -> Dict[str, float]:
        """è¯„ä¼°åŸºç¡€æƒ…ç»ª"""
        emotions = {
            'anxiety': 0.3,
            'excitement': 0.3,
            'frustration': 0.2,
            'satisfaction': 0.5,
            'confidence_emotion': 0.5,
            'fear': 0.2
        }
        
        # åŸºäºæ“æ§å¼ºåº¦çš„æƒ…ç»ªååº”
        if hasattr(intensity, 'value'):
            intensity_value = intensity.value
            intensity_emotions = {
                0: {'anxiety': 0.1, 'satisfaction': 0.7, 'confidence_emotion': 0.8},  # NATURAL
                1: {'anxiety': 0.3, 'excitement': 0.4, 'confidence_emotion': 0.6},    # SUBTLE
                2: {'anxiety': 0.5, 'excitement': 0.6, 'frustration': 0.3},          # MODERATE
                3: {'anxiety': 0.7, 'frustration': 0.6, 'fear': 0.4},               # STRONG
                4: {'anxiety': 0.9, 'frustration': 0.8, 'fear': 0.7}                # EXTREME
            }
            
            if intensity_value in intensity_emotions:
                for emotion, value in intensity_emotions[intensity_value].items():
                    emotions[emotion] = value
        
        # åŸºäºå†å²è¡¨ç°çš„æƒ…ç»ªè°ƒæ•´
        if len(historical_context) >= 5:
            # ç®€åŒ–çš„æˆåŠŸ/å¤±è´¥è¯„ä¼°
            recent_complexity = []
            for period in historical_context[:5]:
                period_tails = period.get('tails', [])
                complexity = len(period_tails) / 10.0  # ç®€åŒ–çš„å¤æ‚åº¦æŒ‡æ ‡
                recent_complexity.append(complexity)
            
            avg_complexity = np.mean(recent_complexity)
            
            # é«˜å¤æ‚åº¦å¯èƒ½è¡¨ç¤ºæŒ«æŠ˜
            if avg_complexity > 0.6:
                emotions['frustration'] += 0.2
                emotions['anxiety'] += 0.15
            
            # ä½å¤æ‚åº¦å¯èƒ½è¡¨ç¤ºæ»¡è¶³
            if avg_complexity < 0.3:
                emotions['satisfaction'] += 0.2
                emotions['confidence_emotion'] += 0.15
        
        # ç¡®ä¿æƒ…ç»ªå€¼åœ¨åˆç†èŒƒå›´å†…
        for emotion in emotions:
            emotions[emotion] = max(0.0, min(1.0, emotions[emotion]))
        
        return emotions

    def _analyze_cognitive_emotion_interaction(self, cognitive_analysis: Dict, base_emotions: Dict) -> Dict[str, float]:
        """åˆ†æè®¤çŸ¥-æƒ…ç»ªäº¤äº’"""
        interactions = {}
        
        # è®¤çŸ¥åå·®å¯¹æƒ…ç»ªçš„å½±å“
        overall_bias = cognitive_analysis.get('overall_bias_score', 0.5)
        rationality = cognitive_analysis.get('rationality_index', 0.7)
        
        # é«˜åå·®å¯èƒ½å¯¼è‡´æƒ…ç»ªä¸ç¨³å®š
        bias_emotional_impact = overall_bias * 0.3
        interactions['bias_induced_instability'] = bias_emotional_impact
        
        # ç†æ€§ç¨‹åº¦å½±å“æƒ…ç»ªæ§åˆ¶
        rational_control = rationality * 0.4
        interactions['rational_emotional_control'] = rational_control
        
        # ç‰¹å®šåå·®-æƒ…ç»ªç»„åˆ
        active_biases = cognitive_analysis.get('active_biases', [])
        
        if 'overconfidence' in active_biases:
            interactions['overconfidence_excitement'] = base_emotions.get('excitement', 0.3) * 1.2
        
        if 'loss_aversion' in active_biases:
            interactions['loss_aversion_anxiety'] = base_emotions.get('anxiety', 0.3) * 1.3
        
        return interactions

    def _analyze_emotion_dynamics(self, base_emotions: Dict, historical_context: List[Dict]) -> Dict[str, float]:
        """åˆ†ææƒ…ç»ªåŠ¨åŠ›å­¦"""
        dynamics = {
            'emotion_velocity': 0.0,
            'emotion_acceleration': 0.0,
            'volatility': 0.3,
            'trend_direction': 0.0
        }
        
        if len(self.psychology_history) >= 3:
            # è®¡ç®—æƒ…ç»ªå˜åŒ–é€Ÿåº¦
            recent_emotions = []
            for record in list(self.psychology_history)[-3:]:
                if 'emotional_analysis' in record:
                    emotion_sum = sum(record['emotional_analysis'].get('base_emotions', {}).values())
                    recent_emotions.append(emotion_sum)
            
            current_emotion_sum = sum(base_emotions.values())
            recent_emotions.append(current_emotion_sum)
            
            if len(recent_emotions) >= 2:
                # æƒ…ç»ªé€Ÿåº¦ï¼ˆå˜åŒ–ç‡ï¼‰
                emotion_changes = [recent_emotions[i+1] - recent_emotions[i] for i in range(len(recent_emotions)-1)]
                dynamics['emotion_velocity'] = np.mean(emotion_changes)
                
                # æƒ…ç»ªæ³¢åŠ¨æ€§
                dynamics['volatility'] = np.std(recent_emotions) if len(recent_emotions) > 1 else 0.3
                
                # è¶‹åŠ¿æ–¹å‘
                if len(emotion_changes) >= 2:
                    if all(change > 0 for change in emotion_changes[-2:]):
                        dynamics['trend_direction'] = 1.0  # æƒ…ç»ªä¸Šå‡
                    elif all(change < 0 for change in emotion_changes[-2:]):
                        dynamics['trend_direction'] = -1.0  # æƒ…ç»ªä¸‹é™
                    else:
                        dynamics['trend_direction'] = 0.0  # æƒ…ç»ªç¨³å®š
        
        return dynamics

    def _assess_emotional_stability(self, base_emotions: Dict, emotion_dynamics: Dict) -> Dict[str, float]:
        """è¯„ä¼°æƒ…ç»ªç¨³å®šæ€§"""
        stability_metrics = {}
        
        # åŸºäºæƒ…ç»ªæå€¼çš„ç¨³å®šæ€§
        emotion_values = list(base_emotions.values())
        max_emotion = max(emotion_values)
        min_emotion = min(emotion_values)
        emotion_range = max_emotion - min_emotion
        
        stability_metrics['range_stability'] = 1.0 - min(1.0, emotion_range)
        
        # åŸºäºæ³¢åŠ¨æ€§çš„ç¨³å®šæ€§
        volatility = emotion_dynamics.get('volatility', 0.3)
        stability_metrics['volatility_stability'] = 1.0 - min(1.0, volatility)
        
        # åŸºäºå˜åŒ–é€Ÿåº¦çš„ç¨³å®šæ€§
        emotion_velocity = abs(emotion_dynamics.get('emotion_velocity', 0.0))
        stability_metrics['velocity_stability'] = 1.0 - min(1.0, emotion_velocity * 2)
        
        # ç»¼åˆç¨³å®šæ€§
        overall_stability = np.mean(list(stability_metrics.values()))
        stability_metrics['overall_stability'] = overall_stability
        
        return stability_metrics

    def _detect_emotional_conflicts(self, base_emotions: Dict, cognitive_analysis: Dict) -> List[Dict]:
        """æ£€æµ‹æƒ…ç»ªå†²çª"""
        conflicts = []
        
        # çŸ›ç›¾æƒ…ç»ªæ£€æµ‹
        anxiety = base_emotions.get('anxiety', 0.3)
        excitement = base_emotions.get('excitement', 0.3)
        
        if anxiety > 0.6 and excitement > 0.6:
            conflicts.append({
                'type': 'anxiety_excitement_conflict',
                'severity': min(anxiety, excitement),
                'description': 'High anxiety and excitement simultaneously'
            })
        
        # è®¤çŸ¥-æƒ…ç»ªå†²çª
        rationality = cognitive_analysis.get('rationality_index', 0.7)
        frustration = base_emotions.get('frustration', 0.2)
        
        if rationality > 0.8 and frustration > 0.6:
            conflicts.append({
                'type': 'rational_emotional_conflict',
                'severity': frustration - (1.0 - rationality),
                'description': 'High rationality with high frustration'
            })
        
        # è¿‡åº¦è‡ªä¿¡ä¸ç„¦è™‘çš„å†²çª
        active_biases = cognitive_analysis.get('active_biases', [])
        if 'overconfidence' in active_biases and anxiety > 0.7:
            conflicts.append({
                'type': 'overconfidence_anxiety_conflict',
                'severity': anxiety * cognitive_analysis.get('bias_strengths', {}).get('overconfidence', 0.5),
                'description': 'Overconfidence bias with high anxiety'
            })
        
        return conflicts

    def _integrate_emotional_state(self, base_emotions: Dict, cognitive_emotional_interaction: Dict,
                                  emotion_dynamics: Dict, stability_assessment: Dict, 
                                  emotional_conflicts: List[Dict]) -> Dict[str, Any]:
        """æ•´åˆæƒ…ç»ªçŠ¶æ€"""
        integrated = {}
        
        # è®¡ç®—ä¸»å¯¼æƒ…ç»ª
        dominant_emotion = max(base_emotions.items(), key=lambda x: x[1])
        integrated['dominant_emotion'] = dominant_emotion[0]
        integrated['dominant_emotion_strength'] = dominant_emotion[1]
        
        # è®¡ç®—æƒ…ç»ªæ•ˆä»·ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
        positive_emotions = ['excitement', 'satisfaction', 'confidence_emotion']
        negative_emotions = ['anxiety', 'frustration', 'fear']
        
        positive_score = sum(base_emotions.get(emotion, 0) for emotion in positive_emotions)
        negative_score = sum(base_emotions.get(emotion, 0) for emotion in negative_emotions)
        
        integrated['emotional_valence'] = positive_score - negative_score
        
        # è®¡ç®—å”¤èµ·æ°´å¹³
        arousal_emotions = ['excitement', 'anxiety', 'frustration']
        integrated['arousal_level'] = np.mean([base_emotions.get(emotion, 0) for emotion in arousal_emotions])
        
        # åˆ¤æ–­å½“å‰å¿ƒæƒ…
        if integrated['emotional_valence'] > 0.3:
            if integrated['arousal_level'] > 0.6:
                integrated['current_mood'] = 'excited'
            else:
                integrated['current_mood'] = 'content'
        elif integrated['emotional_valence'] < -0.3:
            if integrated['arousal_level'] > 0.6:
                integrated['current_mood'] = 'stressed'
            else:
                integrated['current_mood'] = 'frustrated'
        else:
            integrated['current_mood'] = 'neutral'
        
        # æƒ…ç»ªç¨³å®šæ€§
        integrated['emotional_stability'] = stability_assessment.get('overall_stability', 0.7)
        
        # æƒ…ç»ªè½¨è¿¹
        trend = emotion_dynamics.get('trend_direction', 0.0)
        if trend > 0.3:
            integrated['mood_trajectory'] = 'improving'
        elif trend < -0.3:
            integrated['mood_trajectory'] = 'deteriorating'
        else:
            integrated['mood_trajectory'] = 'stable'
        
        # æƒ…ç»ªé©±åŠ¨å› ç´ 
        integrated['emotion_drivers'] = {
            'cognitive_influence': cognitive_emotional_interaction,
            'base_emotions': base_emotions,
            'dynamics': emotion_dynamics
        }
        
        # æƒ…ç»ªå†²çª
        integrated['emotional_conflicts'] = emotional_conflicts
        
        return integrated

    def _update_emotional_history(self, emotional_state: Dict):
        """æ›´æ–°æƒ…ç»ªå†å²"""
        emotional_record = {
            'timestamp': datetime.now(),
            'mood': emotional_state.get('current_mood', 'neutral'),
            'valence': emotional_state.get('emotional_valence', 0.0),
            'arousal': emotional_state.get('arousal_level', 0.5),
            'stability': emotional_state.get('emotional_stability', 0.7)
        }
        
        # æ›´æ–°æƒ…ç»ªçŠ¶æ€
        for key, value in emotional_state.items():
            if key in self.emotional_states:
                self.emotional_states[key] = value

    def _integrate_psychological_state(self, baseline_analysis: Dict, cognitive_analysis: Dict, 
                                     emotional_analysis: Dict, stress_analysis: Dict,
                                     risk_analysis: Dict, strategic_analysis: Dict,
                                     cycle_analysis: Dict, decision_analysis: Dict) -> Dict[str, Any]:
        """
        ç»¼åˆå¿ƒç†çŠ¶æ€æ•´åˆ
        èåˆæ‰€æœ‰å¿ƒç†åˆ†æç»´åº¦å½¢æˆå®Œæ•´çš„å¿ƒç†ç”»åƒ
        """
        integrated_state = {
            'stress_level': 0.5,
            'aggressiveness': 0.5,
            'risk_tolerance': 0.5,
            'strategic_phase': 'observation',
            'psychological_profile': {},
            'decision_readiness': 0.5,
            'adaptability': 0.5,
            'psychological_momentum': 0.0
        }
        
        try:
            # === æ ¸å¿ƒå¿ƒç†æŒ‡æ ‡æ•´åˆ ===
            # å‹åŠ›æ°´å¹³ï¼ˆå¤šç»´åº¦èåˆï¼‰
            stress_components = [
                baseline_analysis.get('stress_level', 0.5),
                stress_analysis.get('composite_stress', 0.5),
                emotional_analysis.get('arousal_level', 0.5) * 0.7  # é«˜å”¤èµ·é€šå¸¸ä¼´éšå‹åŠ›
            ]
            integrated_state['stress_level'] = np.mean(stress_components)
            
            # æ”»å‡»æ€§ï¼ˆèåˆè®¤çŸ¥å’Œæƒ…ç»ªå› ç´ ï¼‰
            aggression_components = [
                baseline_analysis.get('aggressiveness', 0.5),
                cognitive_analysis.get('bias_strengths', {}).get('overconfidence', 0.3),
                emotional_analysis.get('base_emotions', {}).get('excitement', 0.3) * 0.8
            ]
            integrated_state['aggressiveness'] = np.mean(aggression_components)
            
            # é£é™©å®¹å¿åº¦ï¼ˆç»¼åˆè¯„ä¼°ï¼‰
            risk_components = [
                baseline_analysis.get('risk_tolerance', 0.5),
                risk_analysis.get('overall_risk_appetite', 0.5),
                1.0 - cognitive_analysis.get('bias_strengths', {}).get('loss_aversion', 0.5)
            ]
            integrated_state['risk_tolerance'] = np.mean(risk_components)
            
            # === ç­–ç•¥é˜¶æ®µç¡®å®š ===
            integrated_state['strategic_phase'] = strategic_analysis.get('current_phase', 'observation')
            
            # === å¿ƒç†ç”»åƒæ„å»º ===
            psychological_profile = {
                'cognitive_style': self._determine_cognitive_style(cognitive_analysis),
                'emotional_type': emotional_analysis.get('current_mood', 'neutral'),
                'risk_profile': self._determine_risk_profile(integrated_state['risk_tolerance']),
                'stress_response': self._determine_stress_response(integrated_state['stress_level']),
                'decision_style': decision_analysis.get('decision_style', 'balanced'),
                'adaptability_level': self._calculate_adaptability(cycle_analysis, decision_analysis)
            }
            integrated_state['psychological_profile'] = psychological_profile
            
            # === å†³ç­–å‡†å¤‡åº¦è¯„ä¼° ===
            decision_readiness = self._assess_decision_readiness(
                integrated_state, cognitive_analysis, emotional_analysis
            )
            integrated_state['decision_readiness'] = decision_readiness
            
            # === é€‚åº”æ€§è¯„ä¼° ===
            adaptability = self._assess_adaptability(
                baseline_analysis, cognitive_analysis, cycle_analysis
            )
            integrated_state['adaptability'] = adaptability
            
            # === å¿ƒç†åŠ¨é‡è®¡ç®— ===
            psychological_momentum = self._calculate_psychological_momentum(
                emotional_analysis, strategic_analysis, decision_analysis
            )
            integrated_state['psychological_momentum'] = psychological_momentum
            
            # === é¢å¤–å¿ƒç†æŒ‡æ ‡ ===
            integrated_state.update({
                'cognitive_load': baseline_analysis.get('cognitive_load', 0.5),
                'confidence_level': baseline_analysis.get('confidence_level', 0.5),
                'emotional_stability': emotional_analysis.get('emotional_stability', 0.7),
                'bias_susceptibility': cognitive_analysis.get('overall_bias_score', 0.5),
                'learning_capacity': self._assess_learning_capacity(baseline_analysis, cognitive_analysis),
                'strategic_flexibility': strategic_analysis.get('flexibility_score', 0.5)
            })
            
            # === æ•´åˆè´¨é‡è¯„ä¼° ===
            integration_quality = self._assess_integration_quality(
                baseline_analysis, cognitive_analysis, emotional_analysis,
                stress_analysis, risk_analysis, strategic_analysis,
                cycle_analysis, decision_analysis
            )
            integrated_state['integration_quality'] = integration_quality
            
        except Exception as e:
            print(f"âš ï¸ å¿ƒç†çŠ¶æ€æ•´åˆå¤±è´¥: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤çŠ¶æ€
            integrated_state = self._get_default_psychological_state()
        
        return integrated_state

    def _determine_cognitive_style(self, cognitive_analysis: Dict) -> str:
        """ç¡®å®šè®¤çŸ¥é£æ ¼"""
        rationality = cognitive_analysis.get('rationality_index', 0.7)
        active_biases = cognitive_analysis.get('active_biases', [])
        
        if rationality > 0.8:
            return 'analytical'
        elif len(active_biases) >= 3:
            return 'intuitive'
        elif 'overconfidence' in active_biases:
            return 'aggressive'
        elif 'loss_aversion' in active_biases:
            return 'conservative'
        else:
            return 'balanced'

    def _determine_risk_profile(self, risk_tolerance: float) -> str:
        """ç¡®å®šé£é™©é…ç½®æ–‡ä»¶"""
        if risk_tolerance > 0.7:
            return 'risk_seeking'
        elif risk_tolerance < 0.3:
            return 'risk_averse'
        else:
            return 'risk_neutral'

    def _determine_stress_response(self, stress_level: float) -> str:
        """ç¡®å®šå‹åŠ›å“åº”ç±»å‹"""
        if stress_level > 0.7:
            return 'high_stress_reactive'
        elif stress_level < 0.3:
            return 'stress_resilient'
        else:
            return 'moderate_stress_responsive'

    def _calculate_adaptability(self, cycle_analysis: Dict, decision_analysis: Dict) -> float:
        """è®¡ç®—é€‚åº”æ€§"""
        cycle_flexibility = cycle_analysis.get('adaptation_score', 0.5)
        decision_flexibility = decision_analysis.get('flexibility_score', 0.5)
        
        return (cycle_flexibility + decision_flexibility) / 2.0

    def _assess_decision_readiness(self, integrated_state: Dict, cognitive_analysis: Dict, emotional_analysis: Dict) -> float:
        """è¯„ä¼°å†³ç­–å‡†å¤‡åº¦"""
        readiness_factors = []
        
        # è®¤çŸ¥æ¸…æ™°åº¦
        cognitive_clarity = cognitive_analysis.get('rationality_index', 0.7)
        readiness_factors.append(cognitive_clarity)
        
        # æƒ…ç»ªç¨³å®šæ€§
        emotional_stability = emotional_analysis.get('emotional_stability', 0.7)
        readiness_factors.append(emotional_stability)
        
        # å‹åŠ›æ°´å¹³ï¼ˆé€‚ä¸­çš„å‹åŠ›æœ‰åˆ©äºå†³ç­–ï¼‰
        stress_level = integrated_state.get('stress_level', 0.5)
        optimal_stress_score = 1.0 - abs(stress_level - 0.5) * 2  # 0.5ä¸ºæœ€ä½³å‹åŠ›æ°´å¹³
        readiness_factors.append(optimal_stress_score)
        
        # ç½®ä¿¡æ°´å¹³
        confidence = integrated_state.get('confidence_level', 0.5)
        readiness_factors.append(confidence)
        
        return np.mean(readiness_factors)

    def _assess_adaptability(self, baseline_analysis: Dict, cognitive_analysis: Dict, cycle_analysis: Dict) -> float:
        """è¯„ä¼°é€‚åº”æ€§"""
        adaptability_factors = []
        
        # å­¦ä¹ èƒ½åŠ›
        learning_evidence = baseline_analysis.get('baseline_metrics', {}).get('historical_stress_trend', 0.0)
        learning_adaptability = 0.5 + learning_evidence * 0.5
        adaptability_factors.append(learning_adaptability)
        
        # è®¤çŸ¥çµæ´»æ€§ï¼ˆä½åå·®=é«˜çµæ´»æ€§ï¼‰
        cognitive_flexibility = cognitive_analysis.get('rationality_index', 0.7)
        adaptability_factors.append(cognitive_flexibility)
        
        # å‘¨æœŸé€‚åº”æ€§
        cycle_adaptability = cycle_analysis.get('adaptation_score', 0.5)
        adaptability_factors.append(cycle_adaptability)
        
        return np.mean(adaptability_factors)

    def _calculate_psychological_momentum(self, emotional_analysis: Dict, strategic_analysis: Dict, decision_analysis: Dict) -> float:
        """è®¡ç®—å¿ƒç†åŠ¨é‡"""
        momentum_factors = []
        
        # æƒ…ç»ªè½¨è¿¹åŠ¨é‡
        mood_trajectory = emotional_analysis.get('mood_trajectory', 'stable')
        if mood_trajectory == 'improving':
            momentum_factors.append(0.7)
        elif mood_trajectory == 'deteriorating':
            momentum_factors.append(-0.7)
        else:
            momentum_factors.append(0.0)
        
        # ç­–ç•¥åŠ¨é‡
        strategic_momentum = strategic_analysis.get('momentum_score', 0.0)
        momentum_factors.append(strategic_momentum)
        
        # å†³ç­–æˆåŠŸåŠ¨é‡
        decision_momentum = decision_analysis.get('success_momentum', 0.0)
        momentum_factors.append(decision_momentum)
        
        return np.mean(momentum_factors)

    def _assess_learning_capacity(self, baseline_analysis: Dict, cognitive_analysis: Dict) -> float:
        """è¯„ä¼°å­¦ä¹ èƒ½åŠ›"""
        learning_factors = []
        
        # è®¤çŸ¥è´Ÿè·ï¼ˆè´Ÿè·è¶Šä½ï¼Œå­¦ä¹ èƒ½åŠ›è¶Šå¼ºï¼‰
        cognitive_load = baseline_analysis.get('cognitive_load', 0.5)
        load_capacity = 1.0 - cognitive_load
        learning_factors.append(load_capacity)
        
        # ç†æ€§ç¨‹åº¦ï¼ˆè¶Šç†æ€§ï¼Œè¶Šèƒ½ä»ç»éªŒä¸­å­¦ä¹ ï¼‰
        rationality = cognitive_analysis.get('rationality_index', 0.7)
        learning_factors.append(rationality)
        
        # é€‚åº”å‹åŠ›ï¼ˆé€‚åº¦å‹åŠ›ä¿ƒè¿›å­¦ä¹ ï¼‰
        adaptation_pressure = baseline_analysis.get('adaptation_pressure', 0.0)
        optimal_pressure_score = 1.0 - abs(adaptation_pressure - 0.4) * 2.5  # 0.4ä¸ºæœ€ä½³å­¦ä¹ å‹åŠ›
        learning_factors.append(max(0.0, optimal_pressure_score))
        
        return np.mean(learning_factors)

    def _assess_integration_quality(self, *analyses) -> float:
        """è¯„ä¼°æ•´åˆè´¨é‡"""
        quality_factors = []
        
        # æ•°æ®å®Œæ•´æ€§
        complete_analyses = sum(1 for analysis in analyses if analysis and len(analysis) > 0)
        data_completeness = complete_analyses / len(analyses)
        quality_factors.append(data_completeness)
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        consistency_checks = []
        # æ£€æŸ¥å‹åŠ›æŒ‡æ ‡çš„ä¸€è‡´æ€§
        if analyses[0] and analyses[3]:  # baseline and stress analysis
            baseline_stress = analyses[0].get('stress_level', 0.5)
            detailed_stress = analyses[3].get('composite_stress', 0.5)
            stress_consistency = 1.0 - abs(baseline_stress - detailed_stress)
            consistency_checks.append(stress_consistency)
        
        if consistency_checks:
            avg_consistency = np.mean(consistency_checks)
            quality_factors.append(avg_consistency)
        
        # åˆç†æ€§æ£€æŸ¥
        reasonableness_score = 0.8  # ç®€åŒ–çš„åˆç†æ€§è¯„åˆ†
        quality_factors.append(reasonableness_score)
        
        return np.mean(quality_factors)

    def _assess_stress_level(self, period_data: Dict, historical_context: List[Dict], emotional_analysis: Dict) -> Dict[str, Any]:
        """å‹åŠ›æ°´å¹³è¯„ä¼° - ç®€åŒ–å®ç°"""
        return {
            'composite_stress': emotional_analysis.get('arousal_level', 0.5),
            'stress_sources': ['cognitive_load', 'time_pressure'],
            'stress_indicators': self.stress_indicators.copy()
        }

    def _analyze_risk_preferences(self, period_data: Dict, historical_context: List[Dict], stress_analysis: Dict) -> Dict[str, Any]:
        """é£é™©åå¥½åˆ†æ - ç®€åŒ–å®ç°"""
        return {
            'overall_risk_appetite': 0.5,
            'risk_categories': {
                'financial_risk': 0.5,
                'strategic_risk': 0.5,
                'operational_risk': 0.4
            }
        }

    def _identify_strategic_phase(self, period_data: Dict, historical_context: List[Dict], risk_analysis: Dict) -> Dict[str, Any]:
        """ç­–ç•¥é˜¶æ®µè¯†åˆ« - ç®€åŒ–å®ç°"""
        return {
            'current_phase': 'execution',
            'phase_confidence': 0.7,
            'momentum_score': 0.3,
            'flexibility_score': 0.6
        }

    def _analyze_psychological_cycles(self, period_data: Dict, historical_context: List[Dict], strategic_analysis: Dict) -> Dict[str, Any]:
        """å¿ƒç†å‘¨æœŸåˆ†æ - ç®€åŒ–å®ç°"""
        return {
            'adaptation_score': 0.5,
            'cycle_phase': 'stable',
            'cycle_strength': 0.3
        }

    def _analyze_decision_patterns(self, period_data: Dict, historical_context: List[Dict], cycle_analysis: Dict) -> Dict[str, Any]:
        """å†³ç­–æ¨¡å¼åˆ†æ - ç®€åŒ–å®ç°"""
        return {
            'decision_style': 'analytical',
            'flexibility_score': 0.6,
            'success_momentum': 0.4
        }

    def _update_psychological_learning(self, integrated_state: Dict, period_data: Dict, historical_context: List[Dict]):
        """æ›´æ–°å¿ƒç†å­¦ä¹ ç³»ç»Ÿ"""
        try:
            # è®°å½•å¿ƒç†çŠ¶æ€å†å²
            psychology_record = {
                'timestamp': datetime.now(),
                'stress_level': integrated_state.get('stress_level', 0.5),
                'aggressiveness': integrated_state.get('aggressiveness', 0.5),
                'risk_tolerance': integrated_state.get('risk_tolerance', 0.5),
                'strategic_phase': integrated_state.get('strategic_phase', 'observation'),
                'decision_readiness': integrated_state.get('decision_readiness', 0.5),
                'emotional_analysis': integrated_state.get('psychological_profile', {}),
                'integration_quality': integrated_state.get('integration_quality', 0.7)
            }
            
            self.psychology_history.append(psychology_record)
            
            # æ›´æ–°å­¦ä¹ æŒ‡æ ‡
            self.learning_metrics['total_periods_observed'] += 1
            
            # æ›´æ–°åŸºçº¿å¿ƒç†æ¨¡å‹ï¼ˆç¼“æ…¢é€‚åº”ï¼‰
            adaptation_rate = self.config['baseline_adaptation_rate']
            current_stress = integrated_state.get('stress_level', 0.5)
            current_aggression = integrated_state.get('aggressiveness', 0.5)
            current_risk = integrated_state.get('risk_tolerance', 0.5)
            
            self.psychological_baseline['stress_tolerance'] = (
                self.psychological_baseline['stress_tolerance'] * (1 - adaptation_rate) +
                (1.0 - current_stress) * adaptation_rate
            )
            self.psychological_baseline['aggression_tendency'] = (
                self.psychological_baseline['aggression_tendency'] * (1 - adaptation_rate) +
                current_aggression * adaptation_rate
            )
            self.psychological_baseline['risk_appetite'] = (
                self.psychological_baseline['risk_appetite'] * (1 - adaptation_rate) +
                current_risk * adaptation_rate
            )
            
        except Exception as e:
            print(f"âš ï¸ å¿ƒç†å­¦ä¹ æ›´æ–°å¤±è´¥: {e}")

    def _calculate_model_confidence(self) -> float:
        """è®¡ç®—æ¨¡å‹ç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # æ•°æ®å……è¶³æ€§
        data_sufficiency = min(1.0, len(self.psychology_history) / 50.0)
        confidence_factors.append(data_sufficiency)
        
        # é¢„æµ‹å‡†ç¡®æ€§ï¼ˆç®€åŒ–ï¼‰
        if self.learning_metrics['total_periods_observed'] > 0:
            total_predictions = (
                self.learning_metrics['successful_predictions'] + 
                self.learning_metrics['failed_predictions']
            )
            if total_predictions > 0:
                accuracy = self.learning_metrics['successful_predictions'] / total_predictions
                confidence_factors.append(accuracy)
        
        # æ¨¡å‹ç¨³å®šæ€§
        if len(self.psychology_history) >= 5:
            recent_qualities = [
                record.get('integration_quality', 0.7) 
                for record in list(self.psychology_history)[-5:]
            ]
            avg_quality = np.mean(recent_qualities)
            confidence_factors.append(avg_quality)
        
        return np.mean(confidence_factors) if confidence_factors else 0.5

    def _get_default_psychological_state(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤å¿ƒç†çŠ¶æ€"""
        return {
            'stress_level': 0.5,
            'aggressiveness': 0.4,
            'risk_tolerance': 0.5,
            'strategic_phase': 'observation',
            'psychological_profile': {
                'cognitive_style': 'balanced',
                'emotional_type': 'neutral',
                'risk_profile': 'risk_neutral',
                'stress_response': 'moderate_stress_responsive',
                'decision_style': 'balanced',
                'adaptability_level': 0.5
            },
            'decision_readiness': 0.5,
            'adaptability': 0.5,
            'psychological_momentum': 0.0,
            'cognitive_load': 0.5,
            'confidence_level': 0.5,
            'emotional_stability': 0.7,
            'bias_susceptibility': 0.5,
            'learning_capacity': 0.5,
            'strategic_flexibility': 0.5,
            'integration_quality': 0.5
        }

    def get_psychology_statistics(self) -> Dict[str, Any]:
        """è·å–å¿ƒç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_periods_analyzed': self.learning_metrics['total_periods_observed'],
            'psychological_baseline': self.psychological_baseline.copy(),
            'current_emotional_states': self.emotional_states.copy(),
            'cognitive_bias_levels': {k: v['current_level'] for k, v in self.cognitive_biases.items()},
            'strategic_phases_history': self.strategic_phases.copy(),
            'model_confidence': self._calculate_model_confidence()
        }
        
        if self.psychology_history:
            recent_records = list(self.psychology_history)[-10:]
            stats['recent_trends'] = {
                'stress_trend': [r.get('stress_level', 0.5) for r in recent_records],
                'aggression_trend': [r.get('aggressiveness', 0.5) for r in recent_records],
                'risk_tolerance_trend': [r.get('risk_tolerance', 0.5) for r in recent_records]
            }
        
        return stats

    def reset_psychology_model(self):
        """é‡ç½®å¿ƒç†æ¨¡å‹"""
        self.psychology_history.clear()
        self.behavior_patterns.clear()
        self.decision_timeline.clear()
        
        # é‡ç½®ä¸ºé»˜è®¤åŸºçº¿
        self.psychological_baseline = {
            'stress_tolerance': 0.6,
            'risk_appetite': 0.5,
            'aggression_tendency': 0.4,
            'learning_rate': 0.3,
            'adaptation_speed': 0.5,
            'cognitive_capacity': 0.7,
            'emotional_stability': 0.6,
            'strategic_patience': 0.5
        }
        
        # é‡ç½®è®¤çŸ¥åå·®
        for bias_name in self.cognitive_biases:
            self.cognitive_biases[bias_name]['current_level'] = 0.3
        
        # é‡ç½®æƒ…ç»ªçŠ¶æ€
        self.emotional_states = {
            'current_mood': 'neutral',
            'arousal_level': 0.5,
            'confidence_level': 0.5,
            'frustration_level': 0.0,
            'excitement_level': 0.0,
            'anxiety_level': 0.0,
            'satisfaction_level': 0.5
        }
        
        # é‡ç½®å­¦ä¹ æŒ‡æ ‡
        self.learning_metrics = {
            'total_periods_observed': 0,
            'successful_predictions': 0,
            'failed_predictions': 0,
            'adaptation_events': 0,
            'pattern_recognition_accuracy': 0.5
        }
        
        print("ğŸ”„ åº„å®¶å¿ƒç†æ¨¡å‹å·²é‡ç½®")