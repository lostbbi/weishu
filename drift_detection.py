# drift_detection.py - Ê¶ÇÂøµÊºÇÁßªÊ£ÄÊµãÊ®°Âùó

from typing import List, Dict, Any
from datetime import datetime

class DriftDetectionManager:
    """Ê¶ÇÂøµÊºÇÁßªÊ£ÄÊµãÁÆ°ÁêÜÂô®"""
    
    def __init__(self, ai_config, db_manager):
        self.ai_config = ai_config
        self.db_manager = db_manager
        
    def handle_concept_drift(self, drift_detectors: List[str], ensemble_weights: Dict, 
                           learning_config: Dict, deep_learning_module=None):
        """Â§ÑÁêÜÊ¶ÇÂøµÊºÇÁßª"""
        print(f"üö® Ê£ÄÊµãÂà∞Ê¶ÇÂøµÊºÇÁßªÔºÅÊ£ÄÊµãÂô®: {drift_detectors}")
        
        # ËÆ∞ÂΩïÊºÇÁßª‰∫ã‰ª∂
        for detector in drift_detectors:
            self.db_manager.save_drift_record(detector, 'concept_drift', 1.0, 'adaptive_response')
        
        # Ê¶ÇÂøµÊºÇÁßªÂìçÂ∫îÁ≠ñÁï•
        if len(drift_detectors) >= 2:  # Â§ö‰∏™Ê£ÄÊµãÂô®ÂêåÊó∂Êä•ÂëäÊºÇÁßª
            print("üîÑ ÊâßË°åÂº∫ÂåñÈÄÇÂ∫îÁ≠ñÁï•")
            # ÈáçÁΩÆË°®Áé∞ËæÉÂ∑ÆÁöÑÊ®°ÂûãÊùÉÈáç
            self.reset_poor_performing_models(ensemble_weights)
        else:
            print("‚ö° ÊâßË°åËΩªÈáèÈÄÇÂ∫îÁ≠ñÁï•")
            # ‰ªÖË∞ÉÊï¥Â≠¶‰π†ÁéáÊàñÊùÉÈáç
            self.adjust_learning_parameters(learning_config, deep_learning_module)
        
        # ÈáçÁΩÆÊºÇÁßªÊ£ÄÊµãÂô®
        for detector_name in drift_detectors:
            # ËøôÈáåÈúÄË¶ÅËÆøÈóÆÂÆûÈôÖÁöÑÊºÇÁßªÊ£ÄÊµãÂô®ÂÆû‰æã
            # Áî±‰∫éÊºÇÁßªÊ£ÄÊµãÂô®Âú®‰∏ªÂºïÊìé‰∏≠ÔºåËøô‰∏™ÊñπÊ≥ïÈúÄË¶ÅÂú®‰∏ªÂºïÊìé‰∏≠Ë∞ÉÁî®
            pass
    
    def handle_advanced_concept_drift(self, drift_info: Dict, feature_selector=None, 
                                    feature_weighter=None, ensemble_weights: Dict = None,
                                    deep_learning_module=None):
        """Â§ÑÁêÜÈ´òÁ∫ßÊ¶ÇÂøµÊºÇÁßª"""
        print("üîß ÊâßË°åÈ´òÁ∫ßÊ¶ÇÂøµÊºÇÁßªÂìçÂ∫îÁ≠ñÁï•...")
        
        # ÂàÜÊûêÊºÇÁßªÁ±ªÂûãÂíå‰∏•ÈáçÁ®ãÂ∫¶
        individual_detectors = drift_info.get('individual_detectors', {})
        
        # ÁªüËÆ°Ê£ÄÊµãÂà∞ÊºÇÁßªÁöÑÊ£ÄÊµãÂô®Êï∞Èáè
        drift_count = sum(1 for detector_info in individual_detectors.values() 
                         if detector_info.get('detected', False))
        
        total_detectors = len(individual_detectors)
        drift_severity = drift_count / total_detectors if total_detectors > 0 else 0.0
        
        print(f"   ÊºÇÁßª‰∏•ÈáçÁ®ãÂ∫¶: {drift_severity:.2f} ({drift_count}/{total_detectors}‰∏™Ê£ÄÊµãÂô®)")
        
        # Ê†πÊçÆ‰∏•ÈáçÁ®ãÂ∫¶ÈááÂèñ‰∏çÂêåÁ≠ñÁï•
        if drift_severity >= 0.75:  # ‰∏•ÈáçÊºÇÁßª
            print("   üö® ÊâßË°å‰∏•ÈáçÊºÇÁßªÂìçÂ∫îÁ≠ñÁï•")
            self._severe_drift_response(feature_selector, feature_weighter, ensemble_weights, deep_learning_module)
            
        elif drift_severity >= 0.5:  # ‰∏≠Á≠âÊºÇÁßª
            print("   ‚ö†Ô∏è ÊâßË°å‰∏≠Á≠âÊºÇÁßªÂìçÂ∫îÁ≠ñÁï•")
            self._moderate_drift_response(feature_selector, feature_weighter, ensemble_weights, deep_learning_module)
            
        else:  # ËΩªÂæÆÊºÇÁßª
            print("   ‚ÑπÔ∏è ÊâßË°åËΩªÂæÆÊºÇÁßªÂìçÂ∫îÁ≠ñÁï•")
            self._mild_drift_response(feature_selector, feature_weighter, ensemble_weights)
        
        # ËÆ∞ÂΩïÊºÇÁßª‰∫ã‰ª∂
        self.db_manager.save_drift_record('advanced_ensemble', 'concept_drift', drift_severity, 
                                        f'severity_{drift_severity:.2f}_response')
    
    def _severe_drift_response(self, feature_selector=None, feature_weighter=None, 
                             ensemble_weights: Dict = None, deep_learning_module=None):
        """‰∏•ÈáçÊºÇÁßªÂìçÂ∫î"""
        # 1. ÈáçÁΩÆÁâπÂæÅÂ§ÑÁêÜÁªÑ‰ª∂
        if feature_selector and hasattr(self.ai_config, 'DynamicFeatureSelector'):
            try:
                # ÂàõÂª∫Êñ∞ÁöÑÁâπÂæÅÈÄâÊã©Âô®ÂÆû‰æãÔºàÈúÄË¶ÅÂú®‰∏ªÂºïÊìé‰∏≠Êõ¥Êñ∞ÂºïÁî®Ôºâ
                print("   üîÑ ÈúÄË¶ÅÈáçÁΩÆÁâπÂæÅÂ§ÑÁêÜÁªÑ‰ª∂ÔºàÂú®‰∏ªÂºïÊìé‰∏≠ÊâßË°åÔºâ")
            except Exception as e:
                print(f"   ‚ùå ÈáçÁΩÆÁâπÂæÅÂ§ÑÁêÜÁªÑ‰ª∂Â§±Ë¥•: {e}")
        
        # 2. Â§ßÂπÖÈôç‰ΩéË°®Áé∞Â∑ÆÁöÑÊ®°ÂûãÊùÉÈáç
        if ensemble_weights:
            for model_key, weight_info in ensemble_weights.items():
                if len(weight_info.get('performance_history', [])) >= 10:
                    recent_performance = weight_info['performance_history'][-10:]
                    avg_performance = sum(recent_performance) / len(recent_performance)
                    if avg_performance < 0.3:
                        weight_info['weight'] *= 0.3  # Â§ßÂπÖÈôç‰ΩéÊùÉÈáç
                        print(f"   üìâ Â§ßÂπÖÈôç‰ΩéÊ®°Âûã {model_key} ÊùÉÈáç")
        
        # 3. Â¢ûÂä†Â≠¶‰π†ÁéáÔºàÂ¶ÇÊûúÊîØÊåÅÔºâ
        self._adjust_learning_rates(factor=1.5, deep_learning_module=deep_learning_module, 
                                   feature_weighter=feature_weighter)
    
    def _moderate_drift_response(self, feature_selector=None, feature_weighter=None, 
                               ensemble_weights: Dict = None, deep_learning_module=None):
        """‰∏≠Á≠âÊºÇÁßªÂìçÂ∫î"""
        # 1. Ë∞ÉÊï¥ÁâπÂæÅÈÄâÊã©ÊØî‰æã
        if feature_selector and hasattr(feature_selector, 'selection_ratio'):
            feature_selector.selection_ratio = min(0.95, feature_selector.selection_ratio + 0.05)
            print(f"   üéØ Ë∞ÉÊï¥ÁâπÂæÅÈÄâÊã©ÊØî‰æãÂà∞ {feature_selector.selection_ratio:.2f}")
        
        # 2. ÈÄÇÂ∫¶Ë∞ÉÊï¥Ê®°ÂûãÊùÉÈáç
        if ensemble_weights:
            for model_key, weight_info in ensemble_weights.items():
                if len(weight_info.get('performance_history', [])) >= 5:
                    recent_performance = weight_info['performance_history'][-5:]
                    avg_performance = sum(recent_performance) / len(recent_performance)
                    if avg_performance < 0.4:
                        weight_info['weight'] *= 0.7  # ÈÄÇÂ∫¶Èôç‰ΩéÊùÉÈáç
                        print(f"   üìä ÈÄÇÂ∫¶Ë∞ÉÊï¥Ê®°Âûã {model_key} ÊùÉÈáç")
        
        # 3. ÈÄÇÂ∫¶Ë∞ÉÊï¥Â≠¶‰π†Áéá
        self._adjust_learning_rates(factor=1.2, deep_learning_module=deep_learning_module, 
                                   feature_weighter=feature_weighter)
    
    def _mild_drift_response(self, feature_selector=None, feature_weighter=None, 
                           ensemble_weights: Dict = None):
        """ËΩªÂæÆÊºÇÁßªÂìçÂ∫î"""
        # 1. Â¢ûÂä†ÁâπÂæÅÊõ¥Êñ∞È¢ëÁéá
        if feature_selector and hasattr(feature_selector, 'update_frequency'):
            feature_selector.update_frequency = max(5, feature_selector.update_frequency - 2)
            print(f"   ‚ö° Â¢ûÂä†ÁâπÂæÅÊõ¥Êñ∞È¢ëÁéáÂà∞ {feature_selector.update_frequency}")
        
        # 2. ËΩªÂæÆË∞ÉÊï¥ÊùÉÈáçÂ≠¶‰π†Áéá
        if feature_weighter and hasattr(feature_weighter, 'learning_rate'):
            feature_weighter.learning_rate = min(0.02, feature_weighter.learning_rate * 1.1)
            print(f"   üîß Ë∞ÉÊï¥ÁâπÂæÅÊùÉÈáçÂ≠¶‰π†ÁéáÂà∞ {feature_weighter.learning_rate:.4f}")
        
        # 3. ËΩªÂæÆË∞ÉÊï¥Ê®°ÂûãÊùÉÈáç
        if ensemble_weights:
            for model_key, weight_info in ensemble_weights.items():
                recent_updates = weight_info.get('performance_history', [])[-3:]
                if recent_updates and sum(recent_updates) == 0:  # ÊúÄËøë3Ê¨°ÈÉΩÈ¢ÑÊµãÈîôËØØ
                    weight_info['weight'] *= 0.9
                    print(f"   üìà ËΩªÂæÆË∞ÉÊï¥Ê®°Âûã {model_key} ÊùÉÈáç")
    
    def _adjust_learning_rates(self, factor: float, deep_learning_module=None, feature_weighter=None):
        """Ë∞ÉÊï¥Â≠¶‰π†ÁéáÔºàÂ¶ÇÊûúÊ®°ÂûãÊîØÊåÅÔºâ"""
        adjusted_count = 0
        
        # Ë∞ÉÊï¥PyTorchÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑÂ≠¶‰π†Áéá
        if deep_learning_module and hasattr(deep_learning_module, 'models') and deep_learning_module.models:
            try:
                if hasattr(deep_learning_module, 'optimizers'):
                    for model_name, optimizer in deep_learning_module.optimizers.items():
                        for param_group in optimizer.param_groups:
                            old_lr = param_group['lr']
                            param_group['lr'] *= factor
                            new_lr = param_group['lr']
                            print(f"   üìö Ë∞ÉÊï¥ {model_name} Â≠¶‰π†Áéá: {old_lr:.6f} ‚Üí {new_lr:.6f}")
                            adjusted_count += 1
            except Exception as e:
                print(f"   ‚ö†Ô∏è Ë∞ÉÊï¥PyTorchÂ≠¶‰π†ÁéáÂ§±Ë¥•: {e}")
        
        # Ë∞ÉÊï¥ÁâπÂæÅÂ§ÑÁêÜÁªÑ‰ª∂ÁöÑÂ≠¶‰π†Áéá
        if feature_weighter and hasattr(feature_weighter, 'learning_rate'):
            try:
                old_lr = feature_weighter.learning_rate
                feature_weighter.learning_rate *= factor
                feature_weighter.learning_rate = min(0.05, max(0.001, feature_weighter.learning_rate))
                print(f"   üéØ Ë∞ÉÊï¥ÁâπÂæÅÊùÉÈáçÂ≠¶‰π†Áéá: {old_lr:.6f} ‚Üí {feature_weighter.learning_rate:.6f}")
                adjusted_count += 1
            except Exception as e:
                print(f"   ‚ö†Ô∏è Ë∞ÉÊï¥ÁâπÂæÅÊùÉÈáçÂ≠¶‰π†ÁéáÂ§±Ë¥•: {e}")
        
        if adjusted_count > 0:
            print(f"   ‚úÖ ÊàêÂäüË∞ÉÊï¥‰∫Ü {adjusted_count} ‰∏™ÁªÑ‰ª∂ÁöÑÂ≠¶‰π†Áéá")
        else:
            print("   ‚ÑπÔ∏è Ê≤°ÊúâÂèØË∞ÉÊï¥ÁöÑÂ≠¶‰π†ÁéáÂèÇÊï∞")
    
    def reset_poor_performing_models(self, ensemble_weights: Dict):
        """ÈáçÁΩÆË°®Áé∞ËæÉÂ∑ÆÁöÑÊ®°Âûã"""
        reset_count = 0
        for model_key, weight_info in ensemble_weights.items():
            performance_history = weight_info.get('performance_history', [])
            
            if len(performance_history) >= 10:
                recent_accuracy = sum(performance_history[-10:]) / 10
                
                if recent_accuracy < 0.3:  # Ë°®Áé∞ÂæàÂ∑Æ
                    # ÂáèÂ∞ëÊùÉÈáç
                    old_weight = weight_info['weight']
                    weight_info['weight'] = max(weight_info['weight'] * 0.5, 0.01)
                    reset_count += 1
                    print(f"   üìâ Èôç‰ΩéÊ®°Âûã {model_key} ÊùÉÈáç: {old_weight:.4f} ‚Üí {weight_info['weight']:.4f}")
        
        if reset_count > 0:
            print(f"   ‚úÖ ÈáçÁΩÆ‰∫Ü {reset_count} ‰∏™Ë°®Áé∞ËæÉÂ∑ÆÁöÑÊ®°Âûã")
        else:
            print("   ‚ÑπÔ∏è Ê≤°ÊúâÂèëÁé∞ÈúÄË¶ÅÈáçÁΩÆÁöÑÊ®°Âûã")
    
    def adjust_learning_parameters(self, learning_config: Dict, deep_learning_module=None):
        """Ë∞ÉÊï¥Â≠¶‰π†ÂèÇÊï∞"""
        try:
            # ËΩªÂæÆË∞ÉÊï¥Ê¶ÇÂøµÊºÇÁßªÊïèÊÑüÂ∫¶
            old_sensitivity = learning_config.get('drift_sensitivity', 0.005)
            learning_config['drift_sensitivity'] = min(old_sensitivity * 1.1, 0.01)
            print(f"   üéõÔ∏è Ë∞ÉÊï¥ÊºÇÁßªÊïèÊÑüÂ∫¶: {old_sensitivity:.4f} ‚Üí {learning_config['drift_sensitivity']:.4f}")
            
            # Â¶ÇÊûúÊúâÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂùóÔºåË∞ÉÊï¥ÂÖ∂Â≠¶‰π†ÂèÇÊï∞
            if deep_learning_module and hasattr(deep_learning_module, 'update_learning_rate'):
                try:
                    for model_name in deep_learning_module.models.keys():
                        deep_learning_module.update_learning_rate(model_name, factor=1.1)
                        print(f"   üìö Ë∞ÉÊï¥Ê∑±Â∫¶Â≠¶‰π†Ê®°Âûã {model_name} Â≠¶‰π†Áéá")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Ë∞ÉÊï¥Ê∑±Â∫¶Â≠¶‰π†ÂèÇÊï∞Â§±Ë¥•: {e}")
            
        except Exception as e:
            print(f"   ‚ùå Ë∞ÉÊï¥Â≠¶‰π†ÂèÇÊï∞Â§±Ë¥•: {e}")
    
    def reset_drift_detectors(self, drift_detectors: Dict):
        """ÈáçÁΩÆÊºÇÁßªÊ£ÄÊµãÂô®"""
        reset_count = 0
        for detector_name, detector in drift_detectors.items():
            try:
                if hasattr(detector, 'reset'):
                    detector.reset()
                    reset_count += 1
                    print(f"   üîÑ ÈáçÁΩÆÊºÇÁßªÊ£ÄÊµãÂô®: {detector_name}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è ÈáçÁΩÆÊºÇÁßªÊ£ÄÊµãÂô® {detector_name} Â§±Ë¥•: {e}")
        
        if reset_count > 0:
            print(f"   ‚úÖ ÊàêÂäüÈáçÁΩÆ‰∫Ü {reset_count} ‰∏™ÊºÇÁßªÊ£ÄÊµãÂô®")
        else:
            print("   ‚ÑπÔ∏è Ê≤°ÊúâÂèØÈáçÁΩÆÁöÑÊºÇÁßªÊ£ÄÊµãÂô®")
    
    def get_drift_detection_stats(self) -> Dict:
        """Ëé∑ÂèñÊºÇÁßªÊ£ÄÊµãÁªüËÆ°"""
        try:
            drift_events_count = self.db_manager.get_drift_events_count()
            
            # ÂèØ‰ª•Ê∑ªÂä†Êõ¥Â§öÁªüËÆ°‰ø°ÊÅØ
            stats = {
                'total_drift_events': drift_events_count,
                'detection_enabled': True,
                'response_strategies': ['severe', 'moderate', 'mild'],
                'last_check': datetime.now().isoformat()
            }
            
            return stats
            
        except Exception as e:
            print(f"Ëé∑ÂèñÊºÇÁßªÊ£ÄÊµãÁªüËÆ°Â§±Ë¥•: {e}")
            return {
                'total_drift_events': 0,
                'detection_enabled': False,
                'error': str(e)
            }
    
    def analyze_drift_patterns(self, ensemble_weights: Dict) -> Dict:
        """ÂàÜÊûêÊºÇÁßªÊ®°Âºè"""
        try:
            analysis = {
                'models_with_declining_performance': [],
                'models_with_stable_performance': [],
                'models_with_improving_performance': [],
                'overall_stability': 'unknown'
            }
            
            declining_count = 0
            stable_count = 0
            improving_count = 0
            
            for model_key, weight_info in ensemble_weights.items():
                performance_history = weight_info.get('performance_history', [])
                
                if len(performance_history) >= 10:
                    recent_performance = performance_history[-5:]
                    older_performance = performance_history[-10:-5]
                    
                    if recent_performance and older_performance:
                        recent_avg = sum(recent_performance) / len(recent_performance)
                        older_avg = sum(older_performance) / len(older_performance)
                        
                        performance_change = recent_avg - older_avg
                        
                        if performance_change < -0.1:  # ‰∏ãÈôçË∂ÖËøá10%
                            analysis['models_with_declining_performance'].append({
                                'model': model_key,
                                'change': performance_change,
                                'recent_performance': recent_avg
                            })
                            declining_count += 1
                        elif performance_change > 0.1:  # ‰∏äÂçáË∂ÖËøá10%
                            analysis['models_with_improving_performance'].append({
                                'model': model_key,
                                'change': performance_change,
                                'recent_performance': recent_avg
                            })
                            improving_count += 1
                        else:
                            analysis['models_with_stable_performance'].append({
                                'model': model_key,
                                'change': performance_change,
                                'recent_performance': recent_avg
                            })
                            stable_count += 1
            
            # ÂàÜÊûêÊï¥‰ΩìÁ®≥ÂÆöÊÄß
            total_models = declining_count + stable_count + improving_count
            if total_models > 0:
                if declining_count / total_models > 0.5:
                    analysis['overall_stability'] = 'declining'
                elif improving_count / total_models > 0.3:
                    analysis['overall_stability'] = 'improving'
                else:
                    analysis['overall_stability'] = 'stable'
            
            analysis['summary'] = {
                'total_analyzed': total_models,
                'declining_models': declining_count,
                'stable_models': stable_count,
                'improving_models': improving_count
            }
            
            return analysis
            
        except Exception as e:
            print(f"ÂàÜÊûêÊºÇÁßªÊ®°ÂºèÂ§±Ë¥•: {e}")
            return {'error': str(e)}