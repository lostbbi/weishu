#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import math
from datetime import datetime, timedelta
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict, deque
import json

class MoneyFlowAnalyzer:
    """
    èµ„é‡‘æµå‘åˆ†æå™¨ - ç§‘ç ”çº§å®šåˆ¶ç‰ˆæœ¬
    
    æ ¸å¿ƒç†å¿µï¼šåŸºäºå†å²å¼€å¥–æ¨¡å¼æ¨æ–­è™šæ‹Ÿèµ„é‡‘æµå‘ï¼Œè¯†åˆ«çƒ­é—¨åº¦å’Œèµ”ä»˜å‹åŠ›ï¼Œ
    ä»è€Œé¢„æµ‹åº„å®¶å¯èƒ½çš„"æ€å¤šèµ”å°‘"ç­–ç•¥ç›®æ ‡ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. è™šæ‹Ÿèµ„é‡‘æµå‘åˆ†æ - åŸºäºå†å²æ¨¡å¼æ¨æ–­æŠ•æ³¨çƒ­åº¦
    2. çƒ­é—¨åº¦é‡åŒ–ç®—æ³• - å¤šç»´åº¦è¯„ä¼°å°¾æ•°å—æ¬¢è¿ç¨‹åº¦  
    3. èµ”ä»˜å‹åŠ›è¯„ä¼°æœºåˆ¶ - è®¡ç®—åº„å®¶é¢ä¸´çš„ç†è®ºèµ”ä»˜é£é™©
    4. é£é™©ç‚¹è¯†åˆ«ç³»ç»Ÿ - è¯†åˆ«åº„å®¶æœ€å¯èƒ½"æ€æ‰"çš„çƒ­é—¨å°¾æ•°
    """
    
    def __init__(self):
        """åˆå§‹åŒ–èµ„é‡‘æµå‘åˆ†æå™¨"""
        print("ğŸ’° åˆå§‹åŒ–èµ„é‡‘æµå‘åˆ†æå™¨...")
        
        # === æ ¸å¿ƒåˆ†æå‚æ•° ===
        self.analysis_window = 30           # åˆ†æçª—å£æœŸæ•°
        self.hot_threshold = 0.65          # çƒ­é—¨é˜ˆå€¼
        self.pressure_threshold = 0.70     # èµ”ä»˜å‹åŠ›é˜ˆå€¼
        self.risk_threshold = 0.75         # é£é™©è¯†åˆ«é˜ˆå€¼
        
        # === å†å²æ•°æ®ç¼“å­˜ ===
        self.historical_patterns = {}      # å†å²æ¨¡å¼ç¼“å­˜
        self.flow_cache = deque(maxlen=100) # èµ„é‡‘æµå‘ç¼“å­˜
        self.heat_trends = defaultdict(list) # çƒ­åº¦è¶‹åŠ¿è®°å½•
        
        # === åˆ†ææƒé‡é…ç½® ===
        self.weight_config = {
            'frequency_weight': 0.25,       # é¢‘ç‡æƒé‡
            'continuity_weight': 0.20,      # è¿ç»­æ€§æƒé‡  
            'interval_weight': 0.15,        # é—´éš”æƒé‡
            'pattern_weight': 0.15,         # æ¨¡å¼æƒé‡
            'momentum_weight': 0.10,        # åŠ¨é‡æƒé‡
            'volatility_weight': 0.15       # æ³¢åŠ¨æ€§æƒé‡
        }
        
        # === èµ”ä»˜å‹åŠ›æ¨¡å‹å‚æ•° ===
        self.payout_model = {
            'base_odds': 2.0,               # åŸºç¡€èµ”ç‡
            'popularity_factor': 1.5,       # çƒ­é—¨ç³»æ•°
            'frequency_multiplier': 1.8,    # é¢‘ç‡ä¹˜æ•°
            'consecutive_bonus': 0.3,       # è¿ç»­å¥–åŠ±
            'pattern_bonus': 0.25          # æ¨¡å¼å¥–åŠ±
        }
        
        # === é£é™©è¯†åˆ«ç®—æ³•å‚æ•° ===
        self.risk_model = {
            'kill_probability_base': 0.4,   # åŸºç¡€"æ€"æ¦‚ç‡
            'hot_penalty': 0.6,             # çƒ­é—¨æƒ©ç½š
            'pressure_amplifier': 2.0,      # å‹åŠ›æ”¾å¤§å™¨
            'pattern_danger': 0.8,          # æ¨¡å¼å±é™©ç³»æ•°
            'crowd_following_risk': 0.7     # ä»ä¼—è·Ÿé£é£é™©
        }
        
        # === ç»Ÿè®¡æ•°æ® ===
        self.analysis_stats = {
            'total_analyses': 0,
            'hot_spots_identified': 0,
            'pressure_points_found': 0,
            'successful_predictions': 0,
            'false_positives': 0,
            'accuracy_history': []
        }
        
        print("âœ… èµ„é‡‘æµå‘åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   åˆ†æçª—å£: {self.analysis_window}æœŸ")
        print(f"   çƒ­é—¨é˜ˆå€¼: {self.hot_threshold}")
        print(f"   å‹åŠ›é˜ˆå€¼: {self.pressure_threshold}")
        print(f"   é£é™©é˜ˆå€¼: {self.risk_threshold}")
    
    def analyze_money_flow(self, candidate_tails: List[int], historical_data: List[Dict]) -> Dict:
        """
        åˆ†æèµ„é‡‘æµå‘å¹¶é¢„æµ‹åº„å®¶ç­–ç•¥
        
        Args:
            candidate_tails: ç»è¿‡ä¸‰å¤§å®šå¾‹ç­›é€‰çš„å€™é€‰å°¾æ•°
            historical_data: å†å²å¼€å¥–æ•°æ®ï¼ˆæœ€æ–°åœ¨å‰ï¼‰
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if not candidate_tails or not historical_data:
            return self._create_empty_result("è¾“å…¥æ•°æ®ä¸è¶³")
        
        if len(historical_data) < 10:
            return self._create_empty_result("å†å²æ•°æ®ä¸è¶³10æœŸï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ")
        
        print(f"ğŸ’° å¼€å§‹èµ„é‡‘æµå‘åˆ†æ...")
        print(f"   å€™é€‰å°¾æ•°: {candidate_tails}")
        print(f"   å†å²æ•°æ®: {len(historical_data)}æœŸ")
        
        try:
            # === ç¬¬ä¸€é˜¶æ®µï¼šè™šæ‹Ÿèµ„é‡‘æµå‘æ¨æ–­ ===
            flow_analysis = self._analyze_virtual_money_flow(candidate_tails, historical_data)
            
            # === ç¬¬äºŒé˜¶æ®µï¼šçƒ­é—¨åº¦é‡åŒ–è®¡ç®— ===
            popularity_analysis = self._calculate_popularity_metrics(candidate_tails, historical_data)
            
            # === ç¬¬ä¸‰é˜¶æ®µï¼šèµ”ä»˜å‹åŠ›è¯„ä¼° ===
            pressure_analysis = self._assess_payout_pressure(candidate_tails, historical_data, popularity_analysis)
            
            # === ç¬¬å››é˜¶æ®µï¼šé£é™©ç‚¹è¯†åˆ« ===
            risk_analysis = self._identify_risk_points(candidate_tails, pressure_analysis, flow_analysis)
            
            # === ç¬¬äº”é˜¶æ®µï¼šç­–ç•¥å»ºè®®ç”Ÿæˆ ===
            strategy_recommendations = self._generate_strategy_recommendations(
                candidate_tails, flow_analysis, popularity_analysis, pressure_analysis, risk_analysis
            )
            
            # === ç»¼åˆåˆ†æç»“æœ ===
            analysis_result = {
                'success': True,
                'timestamp': datetime.now().isoformat(),
                'candidate_tails': candidate_tails,
                'analysis_window': min(len(historical_data), self.analysis_window),
                
                # æ ¸å¿ƒåˆ†æç»“æœ
                'flow_analysis': flow_analysis,
                'popularity_analysis': popularity_analysis,
                'pressure_analysis': pressure_analysis,
                'risk_analysis': risk_analysis,
                'strategy_recommendations': strategy_recommendations,
                
                # å†³ç­–æ”¯æŒä¿¡æ¯
                'recommended_tails': strategy_recommendations['recommended_tails'],
                'avoid_tails': strategy_recommendations['avoid_tails'],
                'confidence': strategy_recommendations['overall_confidence'],
                'reasoning': strategy_recommendations['reasoning'],
                
                # è¯¦ç»†æ•°æ®ï¼ˆä¾›è°ƒè¯•å’ŒéªŒè¯ä½¿ç”¨ï¼‰
                'detailed_metrics': self._compile_detailed_metrics(
                    candidate_tails, flow_analysis, popularity_analysis, pressure_analysis, risk_analysis
                ),
                
                # ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
                'analysis_stats': self._update_analysis_stats()
            }
            
            # ç¼“å­˜åˆ†æç»“æœ
            self._cache_analysis_result(analysis_result)
            
            print(f"âœ… èµ„é‡‘æµå‘åˆ†æå®Œæˆ")
            print(f"   æ¨èå°¾æ•°: {strategy_recommendations['recommended_tails']}")
            print(f"   é¿å¼€å°¾æ•°: {strategy_recommendations['avoid_tails']}")
            print(f"   æ•´ä½“ç½®ä¿¡åº¦: {strategy_recommendations['overall_confidence']:.3f}")
            
            return analysis_result
            
        except Exception as e:
            print(f"âŒ èµ„é‡‘æµå‘åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._create_empty_result(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    def _analyze_virtual_money_flow(self, candidate_tails: List[int], historical_data: List[Dict]) -> Dict:
        """
        åˆ†æè™šæ‹Ÿèµ„é‡‘æµå‘
        
        åŸºäºå†å²å¼€å¥–æ¨¡å¼ï¼Œæ¨æ–­å„å°¾æ•°çš„è™šæ‹ŸæŠ•æ³¨çƒ­åº¦å’Œèµ„é‡‘æµå‘è¶‹åŠ¿
        """
        print("   ğŸ“Š æ‰§è¡Œè™šæ‹Ÿèµ„é‡‘æµå‘åˆ†æ...")
        
        flow_data = {}
        analysis_window = min(len(historical_data), self.analysis_window)
        recent_data = historical_data[:analysis_window]
        
        for tail in candidate_tails:
            # === åŸºç¡€æµå‘æŒ‡æ ‡ ===
            appearances = [i for i, period in enumerate(recent_data) if tail in period.get('tails', [])]
            frequency = len(appearances) / analysis_window
            
            # === æµå‘å¼ºåº¦è®¡ç®— ===
            flow_strength = self._calculate_flow_strength(tail, recent_data, appearances)
            
            # === æµå‘è¶‹åŠ¿åˆ†æ ===
            flow_trend = self._analyze_flow_trend(tail, recent_data, appearances)
            
            # === æµå‘ç¨³å®šæ€§è¯„ä¼° ===
            flow_stability = self._assess_flow_stability(tail, recent_data, appearances)
            
            # === æµå‘åŠ¨é‡è®¡ç®— ===
            flow_momentum = self._calculate_flow_momentum(tail, recent_data, appearances)
            
            # === ç»¼åˆæµå‘è¯„åˆ† ===
            comprehensive_flow_score = (
                flow_strength * 0.3 +
                flow_trend * 0.25 +
                flow_stability * 0.2 +
                flow_momentum * 0.25
            )
            
            flow_data[tail] = {
                'frequency': frequency,
                'appearances': appearances,
                'flow_strength': flow_strength,
                'flow_trend': flow_trend,
                'flow_stability': flow_stability,
                'flow_momentum': flow_momentum,
                'comprehensive_score': comprehensive_flow_score,
                'flow_level': self._classify_flow_level(comprehensive_flow_score),
                'trend_direction': 'up' if flow_trend > 0.5 else 'down' if flow_trend < -0.5 else 'stable'
            }
        
        # === ç›¸å¯¹æµå‘åˆ†æ ===
        flow_rankings = self._calculate_relative_flow_rankings(flow_data)
        
        # === æµå‘é›†ä¸­åº¦åˆ†æ ===
        flow_concentration = self._analyze_flow_concentration(flow_data)
        
        print(f"   âœ“ è™šæ‹Ÿèµ„é‡‘æµå‘åˆ†æå®Œæˆï¼Œæ£€æµ‹åˆ°{len([t for t, d in flow_data.items() if d['flow_level'] == 'high'])}ä¸ªé«˜æµå‘å°¾æ•°")
        
        return {
            'individual_flows': flow_data,
            'flow_rankings': flow_rankings,
            'flow_concentration': flow_concentration,
            'analysis_window': analysis_window,
            'high_flow_tails': [tail for tail, data in flow_data.items() if data['flow_level'] == 'high'],
            'summary': self._summarize_flow_analysis(flow_data, flow_rankings, flow_concentration)
        }
    
    def _calculate_popularity_metrics(self, candidate_tails: List[int], historical_data: List[Dict]) -> Dict:
        """
        è®¡ç®—çƒ­é—¨åº¦é‡åŒ–æŒ‡æ ‡
        
        å¤šç»´åº¦è¯„ä¼°å„å°¾æ•°çš„å—æ¬¢è¿ç¨‹åº¦å’Œå¸‚åœºçƒ­åº¦
        """
        print("   ğŸ”¥ æ‰§è¡Œçƒ­é—¨åº¦é‡åŒ–è®¡ç®—...")
        
        popularity_data = {}
        analysis_window = min(len(historical_data), self.analysis_window)
        recent_data = historical_data[:analysis_window]
        
        for tail in candidate_tails:
            # === é¢‘ç‡çƒ­åº¦ ===
            frequency_heat = self._calculate_frequency_heat(tail, recent_data)
            
            # === è¿ç»­æ€§çƒ­åº¦ ===
            continuity_heat = self._calculate_continuity_heat(tail, recent_data)
            
            # === é—´éš”çƒ­åº¦ ===
            interval_heat = self._calculate_interval_heat(tail, recent_data)
            
            # === æ¨¡å¼çƒ­åº¦ ===
            pattern_heat = self._calculate_pattern_heat(tail, recent_data)
            
            # === åŠ¨é‡çƒ­åº¦ ===
            momentum_heat = self._calculate_momentum_heat(tail, recent_data)
            
            # === æ³¢åŠ¨æ€§çƒ­åº¦ ===
            volatility_heat = self._calculate_volatility_heat(tail, recent_data)
            
            # === ç»¼åˆçƒ­é—¨åº¦è¯„åˆ† ===
            comprehensive_popularity = (
                frequency_heat * self.weight_config['frequency_weight'] +
                continuity_heat * self.weight_config['continuity_weight'] +
                interval_heat * self.weight_config['interval_weight'] +
                pattern_heat * self.weight_config['pattern_weight'] +
                momentum_heat * self.weight_config['momentum_weight'] +
                volatility_heat * self.weight_config['volatility_weight']
            )
            
            popularity_data[tail] = {
                'frequency_heat': frequency_heat,
                'continuity_heat': continuity_heat,
                'interval_heat': interval_heat,
                'pattern_heat': pattern_heat,
                'momentum_heat': momentum_heat,
                'volatility_heat': volatility_heat,
                'comprehensive_popularity': comprehensive_popularity,
                'heat_level': self._classify_heat_level(comprehensive_popularity),
                'dominant_factor': self._identify_dominant_heat_factor(
                    frequency_heat, continuity_heat, interval_heat, 
                    pattern_heat, momentum_heat, volatility_heat
                )
            }
        
        # === ç›¸å¯¹çƒ­åº¦æ’å ===
        popularity_rankings = self._calculate_popularity_rankings(popularity_data)
        
        # === çƒ­åº¦åˆ†å¸ƒåˆ†æ ===
        heat_distribution = self._analyze_heat_distribution(popularity_data)
        
        print(f"   âœ“ çƒ­é—¨åº¦é‡åŒ–å®Œæˆï¼Œè¯†åˆ«åˆ°{len([t for t, d in popularity_data.items() if d['heat_level'] == 'hot'])}ä¸ªçƒ­é—¨å°¾æ•°")
        
        return {
            'individual_popularity': popularity_data,
            'popularity_rankings': popularity_rankings,
            'heat_distribution': heat_distribution,
            'hot_tails': [tail for tail, data in popularity_data.items() if data['heat_level'] == 'hot'],
            'summary': self._summarize_popularity_analysis(popularity_data, popularity_rankings, heat_distribution)
        }
    
    def _assess_payout_pressure(self, candidate_tails: List[int], historical_data: List[Dict], 
                               popularity_analysis: Dict) -> Dict:
        """
        è¯„ä¼°èµ”ä»˜å‹åŠ›
        
        åŸºäºçƒ­é—¨åº¦å’Œå†å²æ¨¡å¼ï¼Œè®¡ç®—åº„å®¶é¢ä¸´çš„ç†è®ºèµ”ä»˜å‹åŠ›
        """
        print("   ğŸ’¸ æ‰§è¡Œèµ”ä»˜å‹åŠ›è¯„ä¼°...")
        
        pressure_data = {}
        
        for tail in candidate_tails:
            popularity_info = popularity_analysis['individual_popularity'][tail]
            
            # === åŸºç¡€èµ”ä»˜é£é™© ===
            base_risk = self._calculate_base_payout_risk(tail, historical_data)
            
            # === çƒ­é—¨åº¦å‹åŠ› ===
            popularity_pressure = self._calculate_popularity_pressure(popularity_info)
            
            # === é¢‘ç‡å‹åŠ› ===
            frequency_pressure = self._calculate_frequency_pressure(tail, historical_data)
            
            # === è¿ç»­æ€§å‹åŠ› ===
            continuity_pressure = self._calculate_continuity_pressure(tail, historical_data)
            
            # === æ¨¡å¼å‹åŠ› ===
            pattern_pressure = self._calculate_pattern_pressure(tail, historical_data)
            
            # === ç»¼åˆèµ”ä»˜å‹åŠ› ===
            comprehensive_pressure = (
                base_risk * 0.2 +
                popularity_pressure * 0.25 +
                frequency_pressure * 0.2 +
                continuity_pressure * 0.15 +
                pattern_pressure * 0.2
            )
            
            # === å‹åŠ›ç­‰çº§åˆ†ç±» ===
            pressure_level = self._classify_pressure_level(comprehensive_pressure)
            
            # === æ€å·æ¦‚ç‡é¢„æµ‹ ===
            kill_probability = self._predict_kill_probability(comprehensive_pressure, popularity_info)
            
            pressure_data[tail] = {
                'base_risk': base_risk,
                'popularity_pressure': popularity_pressure,
                'frequency_pressure': frequency_pressure,
                'continuity_pressure': continuity_pressure,
                'pattern_pressure': pattern_pressure,
                'comprehensive_pressure': comprehensive_pressure,
                'pressure_level': pressure_level,
                'kill_probability': kill_probability,
                'risk_factors': self._identify_pressure_risk_factors(
                    base_risk, popularity_pressure, frequency_pressure, 
                    continuity_pressure, pattern_pressure
                )
            }
        
        # === å‹åŠ›æ’å ===
        pressure_rankings = self._calculate_pressure_rankings(pressure_data)
        
        # === æ•´ä½“å‹åŠ›åˆ†æ ===
        overall_pressure = self._analyze_overall_pressure(pressure_data)
        
        print(f"   âœ“ èµ”ä»˜å‹åŠ›è¯„ä¼°å®Œæˆï¼Œå‘ç°{len([t for t, d in pressure_data.items() if d['pressure_level'] == 'high'])}ä¸ªé«˜å‹åŠ›å°¾æ•°")
        
        return {
            'individual_pressure': pressure_data,
            'pressure_rankings': pressure_rankings,
            'overall_pressure': overall_pressure,
            'high_pressure_tails': [tail for tail, data in pressure_data.items() if data['pressure_level'] == 'high'],
            'summary': self._summarize_pressure_analysis(pressure_data, pressure_rankings, overall_pressure)
        }
    
    def _identify_risk_points(self, candidate_tails: List[int], pressure_analysis: Dict, 
                             flow_analysis: Dict) -> Dict:
        """
        è¯†åˆ«é£é™©ç‚¹
        
        ç»¼åˆèµ„é‡‘æµå‘å’Œèµ”ä»˜å‹åŠ›ï¼Œè¯†åˆ«åº„å®¶æœ€å¯èƒ½é‡‡å–è¡ŒåŠ¨çš„é£é™©ç‚¹
        """
        print("   âš ï¸ æ‰§è¡Œé£é™©ç‚¹è¯†åˆ«...")
        
        risk_data = {}
        
        for tail in candidate_tails:
            pressure_info = pressure_analysis['individual_pressure'][tail]
            flow_info = flow_analysis['individual_flows'][tail]
            
            # === å¤åˆé£é™©è¯„åˆ† ===
            compound_risk = self._calculate_compound_risk(pressure_info, flow_info)
            
            # === æ—¶æœºé£é™©è¯„ä¼° ===
            timing_risk = self._assess_timing_risk(tail, pressure_info, flow_info)
            
            # === ç­–ç•¥é£é™©åˆ†æ ===
            strategy_risk = self._analyze_strategy_risk(tail, pressure_info, flow_info)
            
            # === å¸‚åœºé£é™©è¯„ä¼° ===
            market_risk = self._assess_market_risk(tail, pressure_info, flow_info)
            
            # === ç»¼åˆé£é™©è¯„åˆ† ===
            comprehensive_risk = (
                compound_risk * 0.3 +
                timing_risk * 0.25 +
                strategy_risk * 0.25 +
                market_risk * 0.2
            )
            
            # === é£é™©ç­‰çº§åˆ†ç±» ===
            risk_level = self._classify_risk_level(comprehensive_risk)
            
            # === è¡ŒåŠ¨æ¦‚ç‡é¢„æµ‹ ===
            action_probability = self._predict_action_probability(comprehensive_risk)
            
            risk_data[tail] = {
                'compound_risk': compound_risk,
                'timing_risk': timing_risk,
                'strategy_risk': strategy_risk,
                'market_risk': market_risk,
                'comprehensive_risk': comprehensive_risk,
                'risk_level': risk_level,
                'action_probability': action_probability,
                'risk_indicators': self._compile_risk_indicators(
                    compound_risk, timing_risk, strategy_risk, market_risk
                ),
                'warning_signals': self._detect_warning_signals(tail, pressure_info, flow_info)
            }
        
        # === é£é™©æ’å ===
        risk_rankings = self._calculate_risk_rankings(risk_data)
        
        # === ç³»ç»Ÿæ€§é£é™©è¯„ä¼° ===
        systemic_risk = self._assess_systemic_risk(risk_data)
        
        print(f"   âœ“ é£é™©ç‚¹è¯†åˆ«å®Œæˆï¼Œå‘ç°{len([t for t, d in risk_data.items() if d['risk_level'] == 'high'])}ä¸ªé«˜é£é™©å°¾æ•°")
        
        return {
            'individual_risk': risk_data,
            'risk_rankings': risk_rankings,
            'systemic_risk': systemic_risk,
            'high_risk_tails': [tail for tail, data in risk_data.items() if data['risk_level'] == 'high'],
            'critical_risk_tails': [tail for tail, data in risk_data.items() if data['action_probability'] > 0.8],
            'summary': self._summarize_risk_analysis(risk_data, risk_rankings, systemic_risk)
        }
    
    def _generate_strategy_recommendations(self, candidate_tails: List[int], flow_analysis: Dict,
                                         popularity_analysis: Dict, pressure_analysis: Dict, 
                                         risk_analysis: Dict) -> Dict:
        """
        ç”Ÿæˆç­–ç•¥å»ºè®®
        
        åŸºäºæ‰€æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆæœ€ç»ˆçš„æŠ•æ³¨ç­–ç•¥å»ºè®®
        """
        print("   ğŸ¯ ç”Ÿæˆç­–ç•¥å»ºè®®...")
        
        # === æ”¶é›†æ‰€æœ‰åˆ†ææ•°æ® ===
        analysis_data = {}
        for tail in candidate_tails:
            analysis_data[tail] = {
                'flow': flow_analysis['individual_flows'][tail],
                'popularity': popularity_analysis['individual_popularity'][tail],
                'pressure': pressure_analysis['individual_pressure'][tail],
                'risk': risk_analysis['individual_risk'][tail]
            }
        
        # === å®‰å…¨æ€§è¯„åˆ†è®¡ç®— ===
        safety_scores = {}
        for tail in candidate_tails:
            data = analysis_data[tail]
            
            # å®‰å…¨æ€§ = ä½é£é™© + ä½å‹åŠ› + é€‚ä¸­æµå‘ + é€‚ä¸­çƒ­åº¦
            safety_score = (
                (1.0 - data['risk']['comprehensive_risk']) * 0.4 +
                (1.0 - data['pressure']['comprehensive_pressure']) * 0.3 +
                min(data['flow']['comprehensive_score'], 1.0 - data['flow']['comprehensive_score']) * 0.2 +
                min(data['popularity']['comprehensive_popularity'], 1.0 - data['popularity']['comprehensive_popularity']) * 0.1
            )
            
            safety_scores[tail] = safety_score
        
        # === æœºä¼šæ€§è¯„åˆ†è®¡ç®— ===
        opportunity_scores = {}
        for tail in candidate_tails:
            data = analysis_data[tail]
            
            # æœºä¼šæ€§ = ä½æµå‘ + ä½çƒ­åº¦ + ä½é£é™© + å†å²è¡¨ç°
            opportunity_score = (
                (1.0 - data['flow']['comprehensive_score']) * 0.3 +
                (1.0 - data['popularity']['comprehensive_popularity']) * 0.3 +
                (1.0 - data['risk']['comprehensive_risk']) * 0.2 +
                self._calculate_historical_opportunity(tail) * 0.2
            )
            
            opportunity_scores[tail] = opportunity_score
        
        # === ç­–ç•¥åˆ†ç±» ===
        safe_choices = [tail for tail, score in safety_scores.items() if score > 0.6]
        opportunity_choices = [tail for tail, score in opportunity_scores.items() if score > 0.6]
        avoid_choices = [tail for tail in candidate_tails if 
                        risk_analysis['individual_risk'][tail]['risk_level'] == 'high' or
                        pressure_analysis['individual_pressure'][tail]['pressure_level'] == 'high']
        
        # === æœ€ç»ˆæ¨èé€»è¾‘ ===
        if safe_choices and opportunity_choices:
            # æœ‰æ—¢å®‰å…¨åˆæœ‰æœºä¼šçš„é€‰æ‹©
            recommended = list(set(safe_choices) & set(opportunity_choices))
            if not recommended:
                recommended = safe_choices[:2] + opportunity_choices[:2]
                recommended = list(set(recommended))[:3]
        elif safe_choices:
            # åªæœ‰å®‰å…¨é€‰æ‹©
            recommended = safe_choices[:2]
        elif opportunity_choices:
            # åªæœ‰æœºä¼šé€‰æ‹©
            recommended = opportunity_choices[:2]
        else:
            # éƒ½æ²¡æœ‰ï¼Œé€‰æ‹©é£é™©æœ€å°çš„
            recommended = [min(candidate_tails, key=lambda t: risk_analysis['individual_risk'][t]['comprehensive_risk'])]
        
        # === è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦ ===
        overall_confidence = self._calculate_overall_confidence(
            candidate_tails, analysis_data, recommended, avoid_choices
        )
        
        # === ç”Ÿæˆæ¨ç†è¯´æ˜ ===
        reasoning = self._generate_reasoning(
            candidate_tails, analysis_data, recommended, avoid_choices, 
            safe_choices, opportunity_choices, safety_scores, opportunity_scores
        )
        
        return {
            'recommended_tails': recommended,
            'avoid_tails': list(set(avoid_choices)),
            'safe_choices': safe_choices,
            'opportunity_choices': opportunity_choices,
            'safety_scores': safety_scores,
            'opportunity_scores': opportunity_scores,
            'overall_confidence': overall_confidence,
            'reasoning': reasoning,
            'strategy_type': self._determine_strategy_type(recommended, avoid_choices, safe_choices, opportunity_choices),
            'confidence_breakdown': self._breakdown_confidence(overall_confidence, analysis_data)
        }
    
    # === è¾…åŠ©è®¡ç®—æ–¹æ³• ===
    
    def _calculate_flow_strength(self, tail: int, recent_data: List[Dict], appearances: List[int]) -> float:
        """è®¡ç®—æµå‘å¼ºåº¦"""
        if not appearances:
            return 0.0
        
        # åŸºäºå‡ºç°é¢‘ç‡å’Œæ—¶é—´è¡°å‡
        total_strength = 0.0
        for i, appearance_index in enumerate(appearances):
            # æ—¶é—´æƒé‡ï¼šæœ€è¿‘çš„æƒé‡æ›´é«˜
            time_weight = 1.0 - (appearance_index / len(recent_data))
            # é¢‘ç‡æƒé‡ï¼šè¿ç»­å‡ºç°åŠ æƒ
            freq_weight = 1.0 + (0.1 * i)
            total_strength += time_weight * freq_weight
        
        return min(total_strength / len(appearances), 1.0)
    
    def _analyze_flow_trend(self, tail: int, recent_data: List[Dict], appearances: List[int]) -> float:
        """åˆ†ææµå‘è¶‹åŠ¿"""
        if len(appearances) < 2:
            return 0.0
        
        # è®¡ç®—å‡ºç°é—´éš”çš„è¶‹åŠ¿
        intervals = []
        for i in range(1, len(appearances)):
            intervals.append(appearances[i] - appearances[i-1])
        
        if not intervals:
            return 0.0
        
        # çº¿æ€§å›å½’åˆ†æè¶‹åŠ¿
        n = len(intervals)
        x_values = list(range(n))
        y_values = intervals
        
        if n == 1:
            return 0.0
        
        # ç®€åŒ–çš„çº¿æ€§å›å½’
        x_mean = sum(x_values) / n
        y_mean = sum(y_values) / n
        
        numerator = sum((x_values[i] - x_mean) * (y_values[i] - y_mean) for i in range(n))
        denominator = sum((x_values[i] - x_mean) ** 2 for i in range(n))
        
        if denominator == 0:
            return 0.0
        
        slope = numerator / denominator
        
        # è½¬æ¢ä¸º0-1åˆ†æ•°ï¼Œè´Ÿæ–œç‡è¡¨ç¤ºé—´éš”ç¼©çŸ­ï¼ˆè¶‹åŠ¿å‘ä¸Šï¼‰
        return max(0.0, min(1.0, 0.5 - slope * 0.1))
    
    def _assess_flow_stability(self, tail: int, recent_data: List[Dict], appearances: List[int]) -> float:
        """è¯„ä¼°æµå‘ç¨³å®šæ€§"""
        if len(appearances) < 3:
            return 0.5
        
        # è®¡ç®—å‡ºç°é—´éš”çš„å˜å¼‚ç³»æ•°
        intervals = []
        for i in range(1, len(appearances)):
            intervals.append(appearances[i] - appearances[i-1])
        
        if not intervals:
            return 0.5
        
        mean_interval = sum(intervals) / len(intervals)
        if mean_interval == 0:
            return 0.0
        
        variance = sum((interval - mean_interval) ** 2 for interval in intervals) / len(intervals)
        std_dev = math.sqrt(variance)
        cv = std_dev / mean_interval
        
        # å˜å¼‚ç³»æ•°è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜
        stability = 1.0 / (1.0 + cv)
        return min(max(stability, 0.0), 1.0)
    
    def _calculate_flow_momentum(self, tail: int, recent_data: List[Dict], appearances: List[int]) -> float:
        """è®¡ç®—æµå‘åŠ¨é‡"""
        if not appearances:
            return 0.0
        
        # æœ€è¿‘5æœŸçš„åŠ¨é‡
        recent_5 = recent_data[:5]
        recent_appearances = sum(1 for period in recent_5 if tail in period.get('tails', []))
        
        # ä¹‹å‰5æœŸçš„å¯¹æ¯”
        if len(recent_data) >= 10:
            previous_5 = recent_data[5:10]
            previous_appearances = sum(1 for period in previous_5 if tail in period.get('tails', []))
        else:
            previous_appearances = len(appearances) * 0.5
        
        if previous_appearances == 0:
            momentum = 1.0 if recent_appearances > 0 else 0.0
        else:
            momentum = recent_appearances / previous_appearances
        
        return min(momentum / 2.0, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
    
    def _classify_flow_level(self, score: float) -> str:
        """åˆ†ç±»æµå‘ç­‰çº§"""
        if score > 0.7:
            return 'high'
        elif score > 0.4:
            return 'medium'
        else:
            return 'low'
    
    def _calculate_relative_flow_rankings(self, flow_data: Dict) -> List[Tuple[int, float]]:
        """è®¡ç®—ç›¸å¯¹æµå‘æ’å"""
        rankings = [(tail, data['comprehensive_score']) for tail, data in flow_data.items()]
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def _analyze_flow_concentration(self, flow_data: Dict) -> Dict:
        """åˆ†ææµå‘é›†ä¸­åº¦"""
        scores = [data['comprehensive_score'] for data in flow_data.values()]
        if not scores:
            return {'concentration_index': 0.0, 'distribution': 'even'}
        
        mean_score = sum(scores) / len(scores)
        variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
        concentration_index = math.sqrt(variance) / (mean_score + 0.001)
        
        if concentration_index > 0.5:
            distribution = 'concentrated'
        elif concentration_index > 0.2:
            distribution = 'moderate'
        else:
            distribution = 'even'
        
        return {
            'concentration_index': concentration_index,
            'distribution': distribution,
            'mean_score': mean_score,
            'variance': variance
        }
    
    def _summarize_flow_analysis(self, flow_data: Dict, flow_rankings: List, flow_concentration: Dict) -> str:
        """æ€»ç»“æµå‘åˆ†æ"""
        high_flow_count = len([d for d in flow_data.values() if d['flow_level'] == 'high'])
        total_count = len(flow_data)
        concentration = flow_concentration['distribution']
        
        return f"æ£€æµ‹åˆ°{high_flow_count}/{total_count}ä¸ªé«˜æµå‘å°¾æ•°ï¼Œæµå‘åˆ†å¸ƒå‘ˆ{concentration}çŠ¶æ€"
    
    # === çƒ­é—¨åº¦è®¡ç®—æ–¹æ³• ===
    
    def _calculate_frequency_heat(self, tail: int, recent_data: List[Dict]) -> float:
        """è®¡ç®—é¢‘ç‡çƒ­åº¦"""
        appearances = sum(1 for period in recent_data if tail in period.get('tails', []))
        frequency = appearances / len(recent_data)
        
        # Så‹æ›²çº¿è½¬æ¢ï¼Œ0.5é™„è¿‘å˜åŒ–æœ€æ•æ„Ÿ
        if frequency <= 0.5:
            return 2 * frequency * frequency
        else:
            return 1 - 2 * (1 - frequency) * (1 - frequency)
    
    def _calculate_continuity_heat(self, tail: int, recent_data: List[Dict]) -> float:
        """è®¡ç®—è¿ç»­æ€§çƒ­åº¦"""
        max_consecutive = 0
        current_consecutive = 0
        
        for period in recent_data:
            if tail in period.get('tails', []):
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 0
        
        # è¿ç»­3æ¬¡ä»¥ä¸Šå¼€å§‹å¿«é€Ÿå‡æ¸©
        if max_consecutive <= 1:
            return 0.1
        elif max_consecutive == 2:
            return 0.3
        elif max_consecutive == 3:
            return 0.6
        else:
            return min(0.9, 0.6 + (max_consecutive - 3) * 0.1)
    
    def _calculate_interval_heat(self, tail: int, recent_data: List[Dict]) -> float:
        """è®¡ç®—é—´éš”çƒ­åº¦"""
        appearances = [i for i, period in enumerate(recent_data) if tail in period.get('tails', [])]
        
        if len(appearances) < 2:
            return 0.2
        
        intervals = [appearances[i] - appearances[i-1] for i in range(1, len(appearances))]
        avg_interval = sum(intervals) / len(intervals)
        
        # é—´éš”è¶ŠçŸ­ï¼Œçƒ­åº¦è¶Šé«˜
        heat = 1.0 / (1.0 + avg_interval * 0.2)
        return min(max(heat, 0.0), 1.0)
    
    def _calculate_pattern_heat(self, tail: int, recent_data: List[Dict]) -> float:
        """è®¡ç®—æ¨¡å¼çƒ­åº¦"""
        # æ£€æµ‹è§„å¾‹æ€§æ¨¡å¼
        appearances = [1 if tail in period.get('tails', []) else 0 for period in recent_data]
        
        # æ£€æµ‹å‘¨æœŸæ€§
        pattern_strength = 0.0
        
        # æ£€æµ‹2-5æœŸçš„å‘¨æœŸæ¨¡å¼
        for cycle in range(2, 6):
            if len(appearances) >= cycle * 2:
                matches = 0
                total_checks = 0
                
                for i in range(cycle, len(appearances)):
                    if appearances[i] == appearances[i - cycle]:
                        matches += 1
                    total_checks += 1
                
                if total_checks > 0:
                    cycle_strength = matches / total_checks
                    pattern_strength = max(pattern_strength, cycle_strength)
        
        return pattern_strength
    
    def _calculate_momentum_heat(self, tail: int, recent_data: List[Dict]) -> float:
        """è®¡ç®—åŠ¨é‡çƒ­åº¦"""
        if len(recent_data) < 6:
            return 0.5
        
        # æœ€è¿‘3æœŸ vs ä¹‹å‰3æœŸ
        recent_3 = sum(1 for period in recent_data[:3] if tail in period.get('tails', []))
        previous_3 = sum(1 for period in recent_data[3:6] if tail in period.get('tails', []))
        
        if previous_3 == 0:
            return 0.8 if recent_3 > 0 else 0.2
        
        momentum_ratio = recent_3 / previous_3
        
        # å¯¹æ•°å‡½æ•°å¹³æ»‘åŒ–
        heat = 0.5 + 0.3 * math.log(momentum_ratio + 0.1)
        return min(max(heat, 0.0), 1.0)
    
    def _calculate_volatility_heat(self, tail: int, recent_data: List[Dict]) -> float:
        """è®¡ç®—æ³¢åŠ¨æ€§çƒ­åº¦"""
        appearances = [1 if tail in period.get('tails', []) else 0 for period in recent_data]
        
        if len(appearances) < 4:
            return 0.5
        
        # è®¡ç®—æ»‘åŠ¨å¹³å‡çš„å˜åŒ–
        window_size = 3
        moving_averages = []
        
        for i in range(len(appearances) - window_size + 1):
            avg = sum(appearances[i:i+window_size]) / window_size
            moving_averages.append(avg)
        
        if len(moving_averages) < 2:
            return 0.5
        
        # è®¡ç®—å˜åŒ–ç‡çš„å¹³å‡ç»å¯¹å€¼
        changes = [abs(moving_averages[i] - moving_averages[i-1]) for i in range(1, len(moving_averages))]
        avg_change = sum(changes) / len(changes)
        
        # æ³¢åŠ¨æ€§è½¬æ¢ä¸ºçƒ­åº¦ï¼ˆé«˜æ³¢åŠ¨ = é«˜å…³æ³¨åº¦ï¼‰
        volatility_heat = min(avg_change * 3, 1.0)
        return volatility_heat
    
    def _classify_heat_level(self, score: float) -> str:
        """åˆ†ç±»çƒ­åº¦ç­‰çº§"""
        if score > self.hot_threshold:
            return 'hot'
        elif score > 0.4:
            return 'warm'
        else:
            return 'cold'
    
    def _identify_dominant_heat_factor(self, freq: float, cont: float, inter: float, 
                                     patt: float, mom: float, vol: float) -> str:
        """è¯†åˆ«ä¸»å¯¼çƒ­åº¦å› å­"""
        factors = {
            'frequency': freq,
            'continuity': cont,
            'interval': inter,
            'pattern': patt,
            'momentum': mom,
            'volatility': vol
        }
        return max(factors.keys(), key=lambda k: factors[k])
    
    def _calculate_popularity_rankings(self, popularity_data: Dict) -> List[Tuple[int, float]]:
        """è®¡ç®—çƒ­é—¨åº¦æ’å"""
        rankings = [(tail, data['comprehensive_popularity']) for tail, data in popularity_data.items()]
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def _analyze_heat_distribution(self, popularity_data: Dict) -> Dict:
        """åˆ†æçƒ­åº¦åˆ†å¸ƒ"""
        scores = [data['comprehensive_popularity'] for data in popularity_data.values()]
        hot_count = len([s for s in scores if s > 0.65])
        warm_count = len([s for s in scores if 0.4 <= s <= 0.65])
        cold_count = len([s for s in scores if s < 0.4])
        
        return {
            'hot_count': hot_count,
            'warm_count': warm_count,
            'cold_count': cold_count,
            'distribution_type': 'concentrated' if hot_count <= 2 else 'dispersed'
        }
    
    def _summarize_popularity_analysis(self, popularity_data: Dict, popularity_rankings: List, 
                                     heat_distribution: Dict) -> str:
        """æ€»ç»“çƒ­é—¨åº¦åˆ†æ"""
        hot_count = heat_distribution['hot_count']
        total_count = len(popularity_data)
        distribution_type = heat_distribution['distribution_type']
        
        return f"è¯†åˆ«åˆ°{hot_count}/{total_count}ä¸ªçƒ­é—¨å°¾æ•°ï¼Œçƒ­åº¦åˆ†å¸ƒå‘ˆ{distribution_type}æ€åŠ¿"
    
    # === å‹åŠ›è¯„ä¼°æ–¹æ³• ===
    
    def _calculate_base_payout_risk(self, tail: int, historical_data: List[Dict]) -> float:
        """è®¡ç®—åŸºç¡€èµ”ä»˜é£é™©"""
        # åŸºäºå†å²é¢‘ç‡çš„åŸºç¡€é£é™©
        total_periods = min(len(historical_data), 50)  # æœ€å¤šçœ‹50æœŸ
        appearances = sum(1 for period in historical_data[:total_periods] if tail in period.get('tails', []))
        
        frequency = appearances / total_periods
        expected_frequency = 0.4  # å‡è®¾æœŸæœ›é¢‘ç‡ä¸º40%
        
        deviation = abs(frequency - expected_frequency)
        base_risk = min(deviation * 2, 1.0)
        
        return base_risk
    
    def _calculate_popularity_pressure(self, popularity_info: Dict) -> float:
        """è®¡ç®—çƒ­é—¨åº¦å‹åŠ›"""
        comprehensive_popularity = popularity_info['comprehensive_popularity']
        
        # çƒ­é—¨åº¦è¶Šé«˜ï¼Œèµ”ä»˜å‹åŠ›è¶Šå¤§
        if comprehensive_popularity > 0.8:
            return 0.9
        elif comprehensive_popularity > 0.6:
            return 0.7
        elif comprehensive_popularity > 0.4:
            return 0.4
        else:
            return 0.1
    
    def _calculate_frequency_pressure(self, tail: int, historical_data: List[Dict]) -> float:
        """è®¡ç®—é¢‘ç‡å‹åŠ›"""
        recent_10 = historical_data[:10] if len(historical_data) >= 10 else historical_data
        appearances = sum(1 for period in recent_10 if tail in period.get('tails', []))
        
        frequency = appearances / len(recent_10)
        
        # é«˜é¢‘ç‡å¸¦æ¥é«˜å‹åŠ›
        if frequency >= 0.8:
            return 0.95
        elif frequency >= 0.6:
            return 0.75
        elif frequency >= 0.4:
            return 0.5
        else:
            return 0.2
    
    def _calculate_continuity_pressure(self, tail: int, historical_data: List[Dict]) -> float:
        """è®¡ç®—è¿ç»­æ€§å‹åŠ›"""
        max_consecutive = 0
        current_consecutive = 0
        
        for period in historical_data[:20]:  # æ£€æŸ¥æœ€è¿‘20æœŸ
            if tail in period.get('tails', []):
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 0
        
        # è¿ç»­å‡ºç°å¸¦æ¥å·¨å¤§å‹åŠ›
        if max_consecutive >= 4:
            return 0.95
        elif max_consecutive == 3:
            return 0.8
        elif max_consecutive == 2:
            return 0.6
        else:
            return 0.3
    
    def _calculate_pattern_pressure(self, tail: int, historical_data: List[Dict]) -> float:
        """è®¡ç®—æ¨¡å¼å‹åŠ›"""
        # æ£€æµ‹æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„è§„å¾‹æ€§æ¨¡å¼
        appearances = [1 if tail in period.get('tails', []) else 0 for period in historical_data[:20]]
        
        # ç®€å•çš„æ¨¡å¼æ£€æµ‹
        pattern_detected = False
        
        # æ£€æµ‹äº¤æ›¿æ¨¡å¼ (1010...)
        alternating_score = 0
        for i in range(1, len(appearances)):
            if appearances[i] != appearances[i-1]:
                alternating_score += 1
        
        if alternating_score / len(appearances) > 0.8:
            pattern_detected = True
        
        # æ£€æµ‹å‘¨æœŸæ¨¡å¼
        for cycle in [2, 3, 4, 5]:
            if len(appearances) >= cycle * 2:
                matches = sum(1 for i in range(cycle, len(appearances)) 
                            if appearances[i] == appearances[i-cycle])
                if matches / (len(appearances) - cycle) > 0.7:
                    pattern_detected = True
                    break
        
        return 0.8 if pattern_detected else 0.3
    
    def _classify_pressure_level(self, pressure: float) -> str:
        """åˆ†ç±»å‹åŠ›ç­‰çº§"""
        if pressure > self.pressure_threshold:
            return 'high'
        elif pressure > 0.4:
            return 'medium'
        else:
            return 'low'
    
    def _predict_kill_probability(self, pressure: float, popularity_info: Dict) -> float:
        """é¢„æµ‹æ€å·æ¦‚ç‡"""
        base_kill_prob = self.risk_model['kill_probability_base']
        
        # å‹åŠ›è°ƒæ•´
        pressure_adjustment = pressure * self.risk_model['pressure_amplifier']
        
        # çƒ­é—¨åº¦è°ƒæ•´
        popularity_adjustment = popularity_info['comprehensive_popularity'] * self.risk_model['hot_penalty']
        
        # ç»¼åˆæ€å·æ¦‚ç‡
        kill_probability = base_kill_prob + pressure_adjustment + popularity_adjustment
        
        return min(max(kill_probability, 0.0), 1.0)
    
    def _identify_pressure_risk_factors(self, base_risk: float, popularity_pressure: float,
                                      frequency_pressure: float, continuity_pressure: float,
                                      pattern_pressure: float) -> List[str]:
        """è¯†åˆ«å‹åŠ›é£é™©å› å­"""
        risk_factors = []
        
        if base_risk > 0.6:
            risk_factors.append('historical_deviation')
        if popularity_pressure > 0.7:
            risk_factors.append('high_popularity')
        if frequency_pressure > 0.7:
            risk_factors.append('high_frequency')
        if continuity_pressure > 0.7:
            risk_factors.append('consecutive_appearance')
        if pattern_pressure > 0.6:
            risk_factors.append('pattern_detected')
        
        return risk_factors
    
    def _calculate_pressure_rankings(self, pressure_data: Dict) -> List[Tuple[int, float]]:
        """è®¡ç®—å‹åŠ›æ’å"""
        rankings = [(tail, data['comprehensive_pressure']) for tail, data in pressure_data.items()]
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def _analyze_overall_pressure(self, pressure_data: Dict) -> Dict:
        """åˆ†ææ•´ä½“å‹åŠ›"""
        pressures = [data['comprehensive_pressure'] for data in pressure_data.values()]
        avg_pressure = sum(pressures) / len(pressures)
        max_pressure = max(pressures)
        high_pressure_count = len([p for p in pressures if p > 0.7])
        
        return {
            'average_pressure': avg_pressure,
            'maximum_pressure': max_pressure,
            'high_pressure_count': high_pressure_count,
            'pressure_level': 'high' if avg_pressure > 0.6 else 'moderate' if avg_pressure > 0.4 else 'low'
        }
    
    def _summarize_pressure_analysis(self, pressure_data: Dict, pressure_rankings: List, 
                                   overall_pressure: Dict) -> str:
        """æ€»ç»“å‹åŠ›åˆ†æ"""
        high_pressure_count = overall_pressure['high_pressure_count']
        total_count = len(pressure_data)
        pressure_level = overall_pressure['pressure_level']
        
        return f"å‘ç°{high_pressure_count}/{total_count}ä¸ªé«˜å‹åŠ›å°¾æ•°ï¼Œæ•´ä½“å‹åŠ›ç­‰çº§ä¸º{pressure_level}"
    
    # === é£é™©è¯†åˆ«æ–¹æ³• ===
    
    def _calculate_compound_risk(self, pressure_info: Dict, flow_info: Dict) -> float:
        """è®¡ç®—å¤åˆé£é™©"""
        pressure_risk = pressure_info['comprehensive_pressure']
        flow_risk = flow_info['comprehensive_score']
        
        # å‹åŠ›å’Œæµå‘çš„ä¹˜ç§¯æ•ˆåº”
        compound = pressure_risk * flow_risk * 1.5
        
        return min(compound, 1.0)
    
    def _assess_timing_risk(self, tail: int, pressure_info: Dict, flow_info: Dict) -> float:
        """è¯„ä¼°æ—¶æœºé£é™©"""
        # åŸºäºå‹åŠ›ç´¯ç§¯æ—¶é—´å’Œæµå‘å˜åŒ–
        pressure_level = pressure_info['pressure_level']
        flow_trend = flow_info.get('trend_direction', 'stable')
        
        timing_risk = 0.5  # åŸºç¡€æ—¶æœºé£é™©
        
        if pressure_level == 'high' and flow_trend == 'up':
            timing_risk = 0.9  # é«˜å‹åŠ›ä¸”ä¸Šå‡è¶‹åŠ¿
        elif pressure_level == 'high':
            timing_risk = 0.7  # ä»…é«˜å‹åŠ›
        elif flow_trend == 'up':
            timing_risk = 0.6  # ä»…ä¸Šå‡è¶‹åŠ¿
        
        return timing_risk
    
    def _analyze_strategy_risk(self, tail: int, pressure_info: Dict, flow_info: Dict) -> float:
        """åˆ†æç­–ç•¥é£é™©"""
        # åŸºäºåº„å®¶å¯èƒ½çš„ç­–ç•¥é€‰æ‹©
        kill_probability = pressure_info['kill_probability']
        flow_level = flow_info['flow_level']
        
        if kill_probability > 0.8 and flow_level == 'high':
            return 0.95  # æé«˜ç­–ç•¥é£é™©
        elif kill_probability > 0.6:
            return 0.8   # é«˜ç­–ç•¥é£é™©
        elif flow_level == 'high':
            return 0.6   # ä¸­ç­‰ç­–ç•¥é£é™©
        else:
            return 0.3   # ä½ç­–ç•¥é£é™©
    
    def _assess_market_risk(self, tail: int, pressure_info: Dict, flow_info: Dict) -> float:
        """è¯„ä¼°å¸‚åœºé£é™©"""
        # åŸºäºå¸‚åœºæ•´ä½“ç¯å¢ƒ
        risk_factors = pressure_info.get('risk_factors', [])
        flow_stability = flow_info.get('flow_stability', 0.5)
        
        market_risk = 0.4  # åŸºç¡€å¸‚åœºé£é™©
        
        # é£é™©å› å­è¶Šå¤šï¼Œå¸‚åœºé£é™©è¶Šé«˜
        market_risk += len(risk_factors) * 0.1
        
        # æµå‘ä¸ç¨³å®šå¢åŠ å¸‚åœºé£é™©
        market_risk += (1.0 - flow_stability) * 0.3
        
        return min(market_risk, 1.0)
    
    def _classify_risk_level(self, risk: float) -> str:
        """åˆ†ç±»é£é™©ç­‰çº§"""
        if risk > self.risk_threshold:
            return 'high'
        elif risk > 0.5:
            return 'medium'
        else:
            return 'low'
    
    def _predict_action_probability(self, risk: float) -> float:
        """é¢„æµ‹è¡ŒåŠ¨æ¦‚ç‡"""
        # é£é™©è¶Šé«˜ï¼Œåº„å®¶é‡‡å–è¡ŒåŠ¨çš„æ¦‚ç‡è¶Šé«˜
        action_prob = risk * 0.8 + 0.1  # åŸºç¡€æ¦‚ç‡10%
        return min(action_prob, 0.95)
    
    def _compile_risk_indicators(self, compound_risk: float, timing_risk: float,
                                strategy_risk: float, market_risk: float) -> List[str]:
        """ç¼–è¯‘é£é™©æŒ‡æ ‡"""
        indicators = []
        
        if compound_risk > 0.7:
            indicators.append('high_compound_risk')
        if timing_risk > 0.7:
            indicators.append('critical_timing')
        if strategy_risk > 0.8:
            indicators.append('strategy_target')
        if market_risk > 0.6:
            indicators.append('market_volatility')
        
        return indicators
    
    def _detect_warning_signals(self, tail: int, pressure_info: Dict, flow_info: Dict) -> List[str]:
        """æ£€æµ‹è­¦å‘Šä¿¡å·"""
        warnings = []
        
        if pressure_info['kill_probability'] > 0.8:
            warnings.append('high_kill_probability')
        if flow_info['flow_level'] == 'high' and flow_info['trend_direction'] == 'up':
            warnings.append('accelerating_flow')
        if len(pressure_info.get('risk_factors', [])) >= 3:
            warnings.append('multiple_risk_factors')
        
        return warnings
    
    def _calculate_risk_rankings(self, risk_data: Dict) -> List[Tuple[int, float]]:
        """è®¡ç®—é£é™©æ’å"""
        rankings = [(tail, data['comprehensive_risk']) for tail, data in risk_data.items()]
        return sorted(rankings, key=lambda x: x[1], reverse=True)
    
    def _assess_systemic_risk(self, risk_data: Dict) -> Dict:
        """è¯„ä¼°ç³»ç»Ÿæ€§é£é™©"""
        risks = [data['comprehensive_risk'] for data in risk_data.values()]
        avg_risk = sum(risks) / len(risks)
        max_risk = max(risks)
        high_risk_count = len([r for r in risks if r > 0.7])
        
        return {
            'average_risk': avg_risk,
            'maximum_risk': max_risk,
            'high_risk_count': high_risk_count,
            'systemic_level': 'high' if avg_risk > 0.6 else 'moderate' if avg_risk > 0.4 else 'low'
        }
    
    def _summarize_risk_analysis(self, risk_data: Dict, risk_rankings: List, 
                                systemic_risk: Dict) -> str:
        """æ€»ç»“é£é™©åˆ†æ"""
        high_risk_count = systemic_risk['high_risk_count']
        total_count = len(risk_data)
        systemic_level = systemic_risk['systemic_level']
        
        return f"å‘ç°{high_risk_count}/{total_count}ä¸ªé«˜é£é™©å°¾æ•°ï¼Œç³»ç»Ÿé£é™©ç­‰çº§ä¸º{systemic_level}"
    
    # === ç­–ç•¥å»ºè®®æ–¹æ³• ===
    
    def _calculate_historical_opportunity(self, tail: int) -> float:
        """è®¡ç®—å†å²æœºä¼šæ€§"""
        # è¿™é‡Œå¯ä»¥åŸºäºæ›´é•¿æœŸçš„å†å²æ•°æ®è®¡ç®—
        # ç®€åŒ–å®ç°ï¼šåŸºäºå°¾æ•°ç‰¹æ€§
        if tail in [0, 5]:  # 0å’Œ5ç»“å°¾çš„æ•°å­—é€šå¸¸è¾ƒå°‘
            return 0.7
        elif tail in [1, 2, 3, 4, 6, 7, 8, 9]:
            return 0.5
        else:
            return 0.3
    
    def _calculate_overall_confidence(self, candidate_tails: List[int], analysis_data: Dict,
                                    recommended: List[int], avoid_choices: List[int]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        if not recommended:
            return 0.2
        
        # åŸºäºæ¨èå°¾æ•°çš„ç»¼åˆè¯„åˆ†
        total_confidence = 0.0
        for tail in recommended:
            data = analysis_data[tail]
            
            # ç»¼åˆå„é¡¹æŒ‡æ ‡
            safety = 1.0 - data['risk']['comprehensive_risk']
            opportunity = 1.0 - data['pressure']['comprehensive_pressure']
            stability = data['flow']['flow_stability']
            
            tail_confidence = (safety * 0.4 + opportunity * 0.3 + stability * 0.3)
            total_confidence += tail_confidence
        
        avg_confidence = total_confidence / len(recommended)
        
        # æ ¹æ®é¿å¼€é€‰æ‹©çš„æ•°é‡è°ƒæ•´ç½®ä¿¡åº¦
        avoidance_factor = 1.0 - (len(avoid_choices) / len(candidate_tails)) * 0.2
        
        return min(avg_confidence * avoidance_factor, 0.95)
    
    def _generate_reasoning(self, candidate_tails: List[int], analysis_data: Dict,
                          recommended: List[int], avoid_choices: List[int],
                          safe_choices: List[int], opportunity_choices: List[int],
                          safety_scores: Dict, opportunity_scores: Dict) -> str:
        """ç”Ÿæˆæ¨ç†è¯´æ˜"""
        reasoning_parts = []
        
        # æ¨èç†ç”±
        if recommended:
            if set(recommended) & set(safe_choices) & set(opportunity_choices):
                reasoning_parts.append(f"æ¨èå°¾æ•°{recommended}åŒæ—¶å…·å¤‡å®‰å…¨æ€§å’Œæœºä¼šæ€§")
            elif set(recommended) & set(safe_choices):
                reasoning_parts.append(f"æ¨èå°¾æ•°{recommended}å…·æœ‰è¾ƒé«˜çš„å®‰å…¨æ€§è¯„åˆ†")
            elif set(recommended) & set(opportunity_choices):
                reasoning_parts.append(f"æ¨èå°¾æ•°{recommended}å…·æœ‰è‰¯å¥½çš„æœºä¼šæ€§è¯„åˆ†")
            else:
                reasoning_parts.append(f"æ¨èå°¾æ•°{recommended}åœ¨ç»¼åˆåˆ†æä¸­é£é™©ç›¸å¯¹è¾ƒä½")
        
        # é¿å¼€ç†ç”±
        if avoid_choices:
            high_risk_reasons = []
            for tail in avoid_choices:
                data = analysis_data[tail]
                if data['risk']['risk_level'] == 'high':
                    high_risk_reasons.append(f"å°¾æ•°{tail}(é£é™©{data['risk']['comprehensive_risk']:.2f})")
                elif data['pressure']['pressure_level'] == 'high':
                    high_risk_reasons.append(f"å°¾æ•°{tail}(å‹åŠ›{data['pressure']['comprehensive_pressure']:.2f})")
            
            if high_risk_reasons:
                reasoning_parts.append(f"é¿å¼€{', '.join(high_risk_reasons[:3])}ç­‰é«˜é£é™©å°¾æ•°")
        
        # åˆ†ææ€»ç»“
        total_analyzed = len(candidate_tails)
        safe_count = len(safe_choices)
        opportunity_count = len(opportunity_choices)
        
        reasoning_parts.append(f"å…±åˆ†æ{total_analyzed}ä¸ªå€™é€‰å°¾æ•°ï¼Œè¯†åˆ«å‡º{safe_count}ä¸ªå®‰å…¨é€‰æ‹©å’Œ{opportunity_count}ä¸ªæœºä¼šé€‰æ‹©")
        
        return "ï¼›".join(reasoning_parts)
    
    def _determine_strategy_type(self, recommended: List[int], avoid_choices: List[int],
                               safe_choices: List[int], opportunity_choices: List[int]) -> str:
        """ç¡®å®šç­–ç•¥ç±»å‹"""
        if set(recommended) & set(safe_choices) & set(opportunity_choices):
            return 'balanced'  # å¹³è¡¡ç­–ç•¥
        elif set(recommended) & set(safe_choices):
            return 'conservative'  # ä¿å®ˆç­–ç•¥
        elif set(recommended) & set(opportunity_choices):
            return 'aggressive'  # æ¿€è¿›ç­–ç•¥
        else:
            return 'defensive'  # é˜²å®ˆç­–ç•¥
    
    def _breakdown_confidence(self, overall_confidence: float, analysis_data: Dict) -> Dict:
        """åˆ†è§£ç½®ä¿¡åº¦"""
        return {
            'overall': overall_confidence,
            'data_quality': min(len(analysis_data) / 10.0, 1.0),  # æ•°æ®è´¨é‡è¯„åˆ†
            'analysis_depth': 0.9,  # åˆ†ææ·±åº¦è¯„åˆ†
            'model_reliability': 0.8  # æ¨¡å‹å¯é æ€§è¯„åˆ†
        }
    
    # === è¯¦ç»†æŒ‡æ ‡ç¼–è¯‘æ–¹æ³• ===
    
    def _compile_detailed_metrics(self, candidate_tails: List[int], flow_analysis: Dict,
                                popularity_analysis: Dict, pressure_analysis: Dict, 
                                risk_analysis: Dict) -> Dict:
        """ç¼–è¯‘è¯¦ç»†æŒ‡æ ‡"""
        detailed_metrics = {}
        
        for tail in candidate_tails:
            detailed_metrics[tail] = {
                'flow_metrics': {
                    'strength': flow_analysis['individual_flows'][tail]['flow_strength'],
                    'trend': flow_analysis['individual_flows'][tail]['flow_trend'],
                    'stability': flow_analysis['individual_flows'][tail]['flow_stability'],
                    'momentum': flow_analysis['individual_flows'][tail]['flow_momentum'],
                    'level': flow_analysis['individual_flows'][tail]['flow_level']
                },
                'popularity_metrics': {
                    'frequency_heat': popularity_analysis['individual_popularity'][tail]['frequency_heat'],
                    'continuity_heat': popularity_analysis['individual_popularity'][tail]['continuity_heat'],
                    'pattern_heat': popularity_analysis['individual_popularity'][tail]['pattern_heat'],
                    'overall_heat': popularity_analysis['individual_popularity'][tail]['comprehensive_popularity'],
                    'heat_level': popularity_analysis['individual_popularity'][tail]['heat_level']
                },
                'pressure_metrics': {
                    'payout_pressure': pressure_analysis['individual_pressure'][tail]['comprehensive_pressure'],
                    'kill_probability': pressure_analysis['individual_pressure'][tail]['kill_probability'],
                    'pressure_level': pressure_analysis['individual_pressure'][tail]['pressure_level'],
                    'risk_factors': pressure_analysis['individual_pressure'][tail]['risk_factors']
                },
                'risk_metrics': {
                    'comprehensive_risk': risk_analysis['individual_risk'][tail]['comprehensive_risk'],
                    'action_probability': risk_analysis['individual_risk'][tail]['action_probability'],
                    'risk_level': risk_analysis['individual_risk'][tail]['risk_level'],
                    'warning_signals': risk_analysis['individual_risk'][tail]['warning_signals']
                }
            }
        
        return detailed_metrics
    
    # === ç»Ÿè®¡å’Œç¼“å­˜æ–¹æ³• ===
    
    def _update_analysis_stats(self) -> Dict:
        """æ›´æ–°åˆ†æç»Ÿè®¡"""
        self.analysis_stats['total_analyses'] += 1
        return self.analysis_stats.copy()
    
    def _cache_analysis_result(self, result: Dict):
        """ç¼“å­˜åˆ†æç»“æœ"""
        cache_entry = {
            'timestamp': result['timestamp'],
            'candidate_tails': result['candidate_tails'],
            'recommended_tails': result['recommended_tails'],
            'confidence': result['confidence']
        }
        
        self.flow_cache.append(cache_entry)
    
    def _create_empty_result(self, message: str) -> Dict:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'success': False,
            'message': message,
            'timestamp': datetime.now().isoformat(),
            'recommended_tails': [],
            'avoid_tails': [],
            'confidence': 0.0,
            'reasoning': f"åˆ†æå¤±è´¥ï¼š{message}"
        }
    
    def learn_from_outcome(self, prediction_result: Dict, actual_tails: List[int]) -> Dict:
        """ä»ç»“æœä¸­å­¦ä¹ """
        try:
            if not prediction_result or not actual_tails:
                return {'learning_success': False, 'message': 'è¾“å…¥æ•°æ®ä¸è¶³'}
            
            recommended_tails = prediction_result.get('recommended_tails', [])
            avoid_tails = prediction_result.get('avoid_tails', [])
            
            # è®¡ç®—æ¨èå‡†ç¡®æ€§
            recommend_correct = any(tail in actual_tails for tail in recommended_tails) if recommended_tails else False
            
            # è®¡ç®—é¿å¼€å‡†ç¡®æ€§  
            avoid_correct = not any(tail in actual_tails for tail in avoid_tails) if avoid_tails else True
            
            # æ›´æ–°ç»Ÿè®¡
            if recommend_correct:
                self.analysis_stats['successful_predictions'] += 1
            else:
                self.analysis_stats['false_positives'] += 1
            
            # è®¡ç®—å‡†ç¡®ç‡
            total_predictions = self.analysis_stats['successful_predictions'] + self.analysis_stats['false_positives']
            current_accuracy = self.analysis_stats['successful_predictions'] / total_predictions if total_predictions > 0 else 0.0
            
            self.analysis_stats['accuracy_history'].append(current_accuracy)
            
            # ä¿æŒå†å²è®°å½•åœ¨åˆç†é•¿åº¦
            if len(self.analysis_stats['accuracy_history']) > 100:
                self.analysis_stats['accuracy_history'].pop(0)
            
            print(f"ğŸ’° èµ„é‡‘æµå‘åˆ†æå™¨å­¦ä¹ å®Œæˆ:")
            print(f"   æ¨èå‡†ç¡®: {recommend_correct}")
            print(f"   é¿å¼€å‡†ç¡®: {avoid_correct}")
            print(f"   å½“å‰å‡†ç¡®ç‡: {current_accuracy:.3f}")
            
            return {
                'learning_success': True,
                'recommend_accuracy': recommend_correct,
                'avoid_accuracy': avoid_correct,
                'overall_accuracy': current_accuracy,
                'total_predictions': total_predictions
            }
            
        except Exception as e:
            print(f"âŒ èµ„é‡‘æµå‘åˆ†æå™¨å­¦ä¹ å¤±è´¥: {e}")
            return {'learning_success': False, 'message': f'å­¦ä¹ è¿‡ç¨‹å‡ºé”™: {str(e)}'}
    
    def get_analysis_statistics(self) -> Dict:
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_analyses': self.analysis_stats['total_analyses'],
            'successful_predictions': self.analysis_stats['successful_predictions'],
            'false_positives': self.analysis_stats['false_positives'],
            'current_accuracy': self.analysis_stats['accuracy_history'][-1] if self.analysis_stats['accuracy_history'] else 0.0,
            'accuracy_trend': self.analysis_stats['accuracy_history'][-10:] if len(self.analysis_stats['accuracy_history']) >= 10 else self.analysis_stats['accuracy_history'],
            'model_status': 'active',
            'last_analysis': self.flow_cache[-1] if self.flow_cache else None
        }